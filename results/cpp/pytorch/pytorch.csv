File, Function, Length, Total Width, Leading Space(s), Leading Tab(s)
repos/cpp/pytorch/tools/cwrap/plugins/templates/nn_tail.cpp,"torch::nn::short_name( PyObject * c_module)",15, 67, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_nn_functions.cpp,"torch::autograd::THPVariable__parse_to( PyObject * module , PyObject * args , PyObject * kwargs)",25, 112, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_nn_functions.cpp,"torch::autograd::initNNFunctions( PyObject * module)",22, 65, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_functions.cpp,"torch::autograd::generated::addClass( PyTypeObject & type , const char * name , PyGetSetDef * function_properties = NULL , PyMethodDef * function_methods = NULL)",7, 80, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_functions.cpp,"torch::autograd::generated::initialize_autogenerated_functions()",3, 44, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_apply_( PyObject * self , PyObject * arg)",12, 68, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_size( PyObject * self , PyObject * args , PyObject * kwargs)",24, 85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_stride( PyObject * self , PyObject * args , PyObject * kwargs)",22, 87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_get_device( PyObject * self_ , PyObject * args)",7, 74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage_offset( PyObject * self_ , PyObject * args)",7, 78, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_dim( PyObject * self , PyObject * args)",7, 66, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_contiguous( const Tensor & self)",5, 57, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_contiguous( PyObject * self , PyObject * args)",24, 89, 6, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_copy_( Tensor & self , const Tensor & other , bool non_blocking)",5, 87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_copy_( PyObject * self , PyObject * args , PyObject * kwargs)",13, 86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CDouble( const Tensor & self)",8, 85, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CComplexDouble( const Tensor & self)",8, 85, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CLong( const Tensor & self)",8, 85, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_float_scalar( PyObject * self , PyObject * args)",7, 97, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_integral_scalar( PyObject * self , PyObject * args)",13, 99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_index_scalar( PyObject * self , PyObject * args)",12, 97, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_invert( const Tensor & self)",5, 53, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_invert( PyObject * self , PyObject * args)",9, 80, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , Device device , bool non_blocking , bool copy)",9, 109, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , ScalarType dtype , bool non_blocking , bool copy)",4, 97, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , Device device , ScalarType dtype , bool non_blocking , bool copy)",4, 112, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_cpu( PyObject * self , PyObject * args)",7, 95, 3, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_cuda( PyObject * self , PyObject * args , PyObject * kwargs)",16, 85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_to_type( PyObject * self , ScalarType scalarType)",6, 79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_byte( PyObject * self , PyObject * args)",3, 69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_char( PyObject * self , PyObject * args)",3, 69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_double( PyObject * self , PyObject * args)",3, 71, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_float( PyObject * self , PyObject * args)",3, 70, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_half( PyObject * self , PyObject * args)",3, 69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_int( PyObject * self , PyObject * args)",3, 68, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_long( PyObject * self , PyObject * args)",3, 69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_short( PyObject * self , PyObject * args)",3, 70, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_element_size( PyObject * self , PyObject * args)",8, 75, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_numpy( PyObject * self , PyObject * arg)",13, 96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_record_stream( PyObject * self , PyObject * arg)",16, 112, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_requires_grad_( PyObject * self , PyObject * args , PyObject * kwargs)",22, 95, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_is_contiguous( Tensor & self)",3, 52, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_is_contiguous( PyObject * self_ , PyObject * args)",7, 77, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_item( PyObject * self , PyObject * args)",14, 98, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_map_( PyObject * self , PyObject * args , PyObject * kwargs)",16, 85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_map2_( PyObject * self , PyObject * args , PyObject * kwargs)",17, 86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new( PyObject * self , PyObject * args , PyObject * kwargs)",8, 88, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_empty( PyObject * self , PyObject * args , PyObject * kwargs)",9, 90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_full( PyObject * self , PyObject * args , PyObject * kwargs)",9, 89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_ones( PyObject * self , PyObject * args , PyObject * kwargs)",9, 89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",9, 91, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_zeros( PyObject * self , PyObject * args , PyObject * kwargs)",9, 90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage( PyObject * self , PyObject * arg)",7, 69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage_type( PyObject * self , PyObject * arg)",10, 74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_to( PyObject * self , PyObject * args , PyObject * kwargs)",25, 91, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_tolist( PyObject * self , PyObject * args)",8, 96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_type( PyObject * self , PyObject * args , PyObject * kwargs)",47, 116, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_bool( PyObject * self , PyObject * args)",4, 99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::check_out_type_matches( Tensor result , ScalarType scalarType , bool scalarType_is_none , const THPLayout & layout , bool layout_is_none , const Device & device , bool device_is_none)",17, 95, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar end , Tensor result)",4, 59, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar end , const TensorOptions & options)",5, 74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar start , Scalar end , Scalar step , Tensor result)",4, 86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",5, 101, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::allIntegral( std :: initializer_list<std::reference_wrapper<Scalar>> l)",8, 90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_arange( PyObject * self , PyObject * args , PyObject * kwargs)",49, 173, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_range( Scalar start , Scalar end , Scalar step , Tensor result)",5, 85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_range( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",6, 100, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_range( PyObject * self , PyObject * args , PyObject * kwargs)",30, 172, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntArrayRef size , Generator * generator , Tensor result)",4, 103, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntArrayRef size , Generator * generator , const TensorOptions & options)",5, 119, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntArrayRef size , Tensor result)",4, 80, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntArrayRef size , const TensorOptions & options)",5, 96, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntArrayRef size , Generator * generator , Tensor result)",4, 116, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntArrayRef size , Generator * generator , const TensorOptions & options)",5, 132, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntArrayRef size , Tensor result)",4, 93, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntArrayRef size , const TensorOptions & options)",5, 109, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_randint( PyObject * self_ , PyObject * args , PyObject * kwargs)",96, 199, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_as_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",7, 90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_from_numpy( PyObject * module , PyObject * arg)",8, 84, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable__promote_types( PyObject * self , PyObject * args , PyObject * kwargs)",15, 95, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_sparse_coo_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",7, 98, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",7, 87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_get_device( PyObject * self_ , PyObject * args , PyObject * kwargs)",15, 92, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::initTorchFunctions( PyObject * module)",9, 96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::IndexRangeGenerator::range( size_t range_size)",4, 40, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::IndexRangeGenerator::size()",1, 30, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::copy_range( variable_list & out , IndexRange range , const Tensor & t)",5, 87, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::copy_range( variable_list & out , IndexRange range , at :: ArrayRef<Tensor> t)",5, 98, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::not_implemented( const char * name)",4, 76, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::maybe_multiply( const Tensor & t , const Scalar & s)",14, 60, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_safe_size( IntArrayRef sizes , IntArrayRef dim)",11, 57, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::norm_backward( const Tensor & grad , const Tensor & self , const optional<Scalar> & p_ , const Tensor & norm)",25, 115, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::norm_backward( Tensor grad , const Tensor & self , const optional<Scalar> & p_ , Tensor norm , IntArrayRef dim , bool keepdim)",18, 130, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward( Tensor grad , const Tensor & self , const Scalar & exponent_)",8, 82, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_self( Tensor grad , const Tensor & self , const Tensor & exponent)",3, 110, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_exponent( Tensor grad , const Tensor & self , const Tensor & exponent)",3, 90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_exponent( Tensor grad , const Scalar & base , const Tensor & exponent)",3, 90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mvlgamma_backward( Tensor grad , const Tensor & self , int64_t p)",5, 72, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::permute_backwards( const Tensor & grad , IntArrayRef fwd_dims)",9, 70, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_multiple( const Tensor & t , IntArrayRef dim , size_t n_dims)",10, 78, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_backward( const Tensor & grad , IntArrayRef sizes , IntArrayRef dims , bool keepdim)",12, 94, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::reverse_list( const IntArrayRef list)",8, 65, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::reverse_dim( const Tensor & t , int64_t dim)",4, 84, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_safe_zeros_backward( const Tensor & grad , const Tensor & inp , int64_t dim)",17, 94, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_backward( const Tensor & grad , const Tensor & input , const Tensor & result)",13, 90, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_backward( Tensor grad , const Tensor & input , Tensor result , int64_t dim , bool keepdim)",19, 99, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_scan_exclusive( const Tensor & x , int64_t dim)",9, 58, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumprod_backward( const Tensor & grad , const Tensor & input , int64_t dim)",120, 93, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumprod_backward( const Tensor & grad , const Tensor & input , int64_t dim , ScalarType dtype)",3, 98, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::gesv_backward_self( const Tensor & grad , const Tensor & self , const Tensor & A)",3, 88, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::gesv_backward_A( const Tensor & grad , const Tensor & self , const Tensor & A , const Tensor & solution)",7, 110, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumsum_backward( const Tensor & x , int64_t dim)",10, 64, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumsum_backward( const Tensor & x , int64_t dim , ScalarType input_dtype)",3, 79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::logsumexp_backward( Tensor grad , const Tensor & self , Tensor result , IntArrayRef dim , bool keepdim)",7, 108, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unbind_backward( const variable_list & grads , int64_t dim)",16, 80, 8, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_to( const Tensor & self , IntArrayRef sizes)",11, 62, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_to( const Tensor & self , int64_t dim , IntArrayRef sizes)",9, 84, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cat_tensors_backward( const Tensor & grad , const std :: vector<std::vector<int64_t>> & sizes , int64_t dim)",17, 125, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::clamp_backward( const Tensor & grad , const Tensor & self , const optional<Scalar> & min , const optional<Scalar> & max)",12, 125, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mm_mat1_backward( const Tensor & grad , const Tensor & mat2 , const Tensor & mat1 , const Scalar & alpha)",13, 111, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mm_mat2_backward( const Tensor & grad , const Tensor & mat1 , IntArrayRef sizes , IntArrayRef strides , const Scalar & alpha)",8, 130, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_sparse_addmm_sparse_backward( const Tensor & grad , const Tensor & sparse_ , const Tensor & dense , const Scalar & alpha)",6, 124, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::renorm_backward( const Tensor & grad , const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",25, 106, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_tensorlist( TensorList tl)",10, 64, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::repeat_backward( Tensor grad , int64_t input_dims , IntArrayRef repeats)",15, 79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_fused_dropout_backward( Tensor grad , Tensor mask , double p1m)",8, 71, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::select_equals_backward( Tensor grad , const Tensor & input , const Tensor & value)",5, 89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::index_select_backward( Tensor grad , int64_t dim , Tensor indices , IntArrayRef sizes , bool keepdim)",7, 106, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::slice_backward( Tensor grad , IntArrayRef input_sizes , int64_t dim , int64_t start , int64_t end , int64_t step)",5, 117, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::select_backward( Tensor grad , IntArrayRef input_sizes , int64_t dim , int64_t index)",5, 91, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::trace_backward( const Tensor & grad , IntArrayRef sizes)",10, 99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unfold_backward( const Tensor & grad , IntArrayRef input_sizes , int64_t dim , int64_t size , int64_t step)",13, 112, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::var_backward( const Tensor & grad , const Tensor & self , bool unbiased)",3, 79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::var_backward( Tensor grad , const Tensor & self , IntArrayRef dim , bool unbiased , bool keepdim)",9, 102, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::masked_scatter_backward( const Tensor & grad , const Tensor & mask , IntArrayRef sizes)",15, 94, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cholesky_backward( Tensor grad , bool upper , Tensor L)",28, 68, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::split_with_sizes_backward( const std :: vector<torch::autograd::Variable> & grads , IntArrayRef split_sizes , int64_t dim , IntArrayRef sizes , const Type & type)",21, 110, 33, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::split_backward( const std :: vector<torch::autograd::Variable> & grads , int64_t split_size , int64_t dim , IntArrayRef sizes , const Type & type)",9, 94, 22, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::max_pool_double_backward( const Tensor & grad , const Tensor & indices , int dim)",7, 88, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::glu_double_backward( const Tensor & grad , const Tensor & grad_output , const Tensor & input , int64_t dim)",18, 121, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::glu_double_backward_grad_output( const Tensor & grad , const Tensor & input , int64_t dim)",7, 97, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::kl_div_double_backward_grad_output( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",9, 129, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::kl_div_target_backward( Tensor grad_output , Tensor self , Tensor target , int64_t reduction)",9, 112, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::binary_cross_entropy_with_logits_target_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",18, 194, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log_sigmoid_double_backward( const Tensor & grad , const Tensor & input)",4, 80, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::softmax_double_backward( const Tensor & grad , const Tensor & grad_output , int dim , const Tensor & output)",16, 114, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log_softmax_double_backward( const Tensor & grad , const Tensor & grad_output , int dim , const Tensor & output)",4, 118, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::l1_loss_double_backward_grad_output( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",9, 130, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::smooth_l1_loss_double_backward( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",8, 125, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::smooth_l1_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",7, 165, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::diag_backward( const Tensor & grad , IntArrayRef input_sizes , int64_t diagonal)",14, 87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::diagonal_backward( const Tensor & grad , IntArrayRef input_sizes , int64_t offset , int64_t dim1 , int64_t dim2)",6, 117, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mse_loss_double_backward( const Tensor & grad , const Tensor & input , int64_t reduction)",7, 96, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mse_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",7, 159, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::soft_margin_loss_double_backward( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",9, 127, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::soft_margin_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",7, 167, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::softplus_double_backward( const Tensor & grad , const Tensor & input , Scalar beta , Scalar threshold)",4, 108, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_maybe_overlapping_memory( IntArrayRef sizes , IntArrayRef strides)",18, 87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_min_storage_size( IntArrayRef sizes , IntArrayRef strides , int64_t storage_offset)",12, 106, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::as_strided_backward( Tensor grad , TensorGeometry input_geometry , IntArrayRef sizes , IntArrayRef strides , optional<int64_t> storage_offset_)",101, 148, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::atan2_backward( const Tensor & grad , const Tensor & self , const Tensor & other , std :: array<bool,2> output_mask)",6, 138, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prelu_double_backward( const Tensor & grad_grad_input , const Tensor & grad_grad_weight , const Tensor & grad_out , const Tensor & input_ , const Tensor & weight_)",80, 109, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::svd_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , bool some , bool compute_uv , const Tensor & raw_u , const Tensor & sigma , const Tensor & raw_v)",81, 105, 11, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::symeig_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , bool eigenvectors , bool upper , const Tensor & lambda , const Tensor & v)",34, 109, 13, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::det_backward( const Tensor & grad , const Tensor & self , const Tensor & det)",11, 82, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::logdet_backward( const Tensor & grad , const Tensor & self , const Tensor & logdet)",12, 88, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::slogdet_backward( const Tensor & grad_logabsdet , const Tensor & self , const Tensor & signdet , const Tensor & logabsdet)",16, 78, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::trtrs_backward( const Tensor & grad_x , const Tensor & grad_m , const Tensor & b , const Tensor & a , const Tensor & x , const bool upper , const bool transpose , const bool unitriangular , std :: array<bool,2> output_mask)",28, 77, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::fft_backward( const Tensor & self , const Tensor & grad , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntArrayRef checked_signal_sizes , bool normalized , bool onesided , IntArrayRef output_sizes)",100, 105, 8, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_exclude_dim1( const Tensor & to_sum , bool keepdim = true)",8, 72, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_dim1( const Tensor & src , const Tensor & target)",10, 68, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::expand_as_dim1( const Tensor & src , const Tensor & target)",7, 68, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::batchnorm_double_backward( const Tensor & input , const Tensor & gamma , const Tensor & ggI , const Tensor & ggG , const Tensor & ggB , const Tensor & gO , const Tensor & running_mean , const Tensor & running_var , bool training , double eps , const Tensor & save_mean , const Tensor & save_invstd , std :: array<bool,3> output_mask)",128, 96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_trilinear_backward( const Tensor & grad_out , const Tensor & i1 , const Tensor & i2 , const Tensor & i3 , IntArrayRef expand1 , IntArrayRef expand2 , IntArrayRef expand3 , IntArrayRef sumdim , int64_t unroll_dim , std :: array<bool,3> grad_mask)",12, 133, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log1p_backward( const Tensor & grad , const Tensor & self)",10, 88, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sparse_constructor_values_backward( const Tensor & sparse_grad_out , const Tensor & indices , IntArrayRef values_shape)",10, 124, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::to_dense_backward( const Tensor & grad , const Tensor & input_)",5, 69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::constant_pad_nd_backward( const Tensor & grad , IntArrayRef pad)",5, 105, 2, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::toOptionalTensor( const IValue & v)",6, 47, 0, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::toListOfOptionalTensor( const IValue & v)",10, 63, 2, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::as_bool_array( const std :: vector<bool> & vec)",6, 66, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension2.cpp,"exp_add( Tensor x , Tensor y)",3, 37, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"sigmoid_add( torch :: Tensor x , torch :: Tensor y)",3, 62, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::MatrixMultiplier( int A , int B)",4, 80, 8, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::forward( torch :: Tensor weights)",3, 49, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::get() const",3, 30, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"function_taking_optional( c10 :: optional<torch::Tensor> tensor)",3, 69, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",11, 65, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"cudnn_relu_check( const torch :: Tensor & inputs , const torch :: Tensor & outputs)",19, 81, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"cudnn_relu( const torch :: Tensor & inputs , const torch :: Tensor & outputs)",34, 81, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",4, 75, 2, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"get_dtype_tensor( caffe2 :: TypeMeta dtype)",8, 89, 10, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"zeros_override( IntArrayRef size , const TensorOptions & options)",4, 73, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"add_override( const Tensor & a , const Tensor & b , Scalar c)",4, 69, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"sum_override( const Tensor & self)",4, 43, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"expand_override( const Tensor & self , IntArrayRef size , bool implicit)",3, 79, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"kl_div_override( const Tensor & self , const Tensor & target , int64_t reduction)",5, 69, 4, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"kl_div_backward_override( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",8, 41, 2, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"numel_override( const Tensor & self)",3, 46, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"ones_like_override( const Tensor & self , const TensorOptions & options)",3, 80, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"init_msnpu_extension()",30, 100, 4, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"get_test_int()",3, 21, 0, 0
repos/cpp/pytorch/test/cpp_extensions/msnpu_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",4, 56, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::CPUComplexFloatType()",5, 39, 12, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::empty( IntArrayRef size , const TensorOptions & options) const",5, 81, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::ComplexHooks::ComplexHooks( ComplexHooksArgs)",1, 36, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::ComplexHooks::registerComplexTypes( Context * context) const",4, 76, 8, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::scalarType() const",3, 53, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::typeMeta() const",3, 57, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::backend() const",3, 47, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::toString() const",3, 52, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::ID() const",3, 41, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::elementSizeInBytes() const",3, 57, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",1, 45, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cuda_extension.cpp,"sigmoid_add( torch :: Tensor x , torch :: Tensor y)",8, 79, 6, 0
repos/cpp/pytorch/test/cpp_extensions/cuda_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",3, 65, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::Net( int64_t in , int64_t out)",3, 54, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::reset()",4, 62, 4, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::forward( torch :: Tensor x)",3, 43, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::set_bias( torch :: Tensor bias)",4, 38, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::get_bias() const",3, 35, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_parameter( const std :: string & name , torch :: Tensor tensor)",3, 74, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_buffer( const std :: string & name , torch :: Tensor tensor)",3, 71, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_submodule( const std :: string & name)",3, 59, 4, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",9, 58, 6, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension.cpp,"tanh_add( Tensor x , Tensor y)",3, 38, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",8, 53, 2, 0
repos/cpp/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp,"dummy( int)",1, 20, 0, 0
repos/cpp/pytorch/test/custom_operator/op.cpp,"custom_op( torch :: Tensor tensor , double scalar , int64_t repeat)",11, 41, 2, 0
repos/cpp/pytorch/test/custom_operator/op.cpp,"custom_op2( std :: string s1 , std :: string s2)",3, 53, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"helpers::check_all_parameters( const torch :: jit :: script :: Module & module , Predicate predicate)",10, 58, 2, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"get_operator_from_registry_and_execute()",22, 57, 6, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"load_serialized_module_with_custom_op_and_execute( const std :: string & path_to_exported_script_module)",12, 57, 4, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_argument_checking_for_serialized_modules( const std :: string & path_to_exported_script_module)",35, 76, 12, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_move_to_device( const std :: string & path_to_exported_script_module)",21, 78, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_move_to_dtype( const std :: string & path_to_exported_script_module)",17, 77, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"main( int argc , const char * argv [ ])",19, 81, 2, 0
repos/cpp/pytorch/test/cpp/common/main.cpp,"add_negative_flag( const std :: string & flag)",10, 57, 0, 0
repos/cpp/pytorch/test/cpp/common/main.cpp,"main( int argc , char * argv [ ])",14, 77, 4, 0
repos/cpp/pytorch/test/cpp/jit/no-gtest.cpp,"torch::jit::runJITCPPTests()",39, 42, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TestValue::TestValue( const int & x)",1, 51, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TestValue::TestValue( int && x)",1, 46, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , ForwardRvaluesCorrectly)",6, 49, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , ForwardLvaluesCorrectly)",7, 48, 0, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , CanConstructUniquePtrOfArray)",7, 55, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"xor_model()",7, 32, 6, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"save_and_load( torch :: Tensor input)",7, 51, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Basic)",10, 47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , BasicToFile)",15, 47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Resized)",11, 47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Sliced)",11, 47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , NonContiguous)",11, 47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , XOR)",41, 80, 6, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Optim)",68, 81, 8, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , XOR_CUDA)",59, 80, 6, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , CanSerializeModulesWithIntermediateModulesWithoutParametersOrBuffers)",30, 76, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"CPUDevice()",3, 31, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"CUDADevice( DeviceIndex index)",3, 43, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCUDATypes_CUDA)",20, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCUDATensors_MultiCUDA)",24, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DifferentiableScatter_MultiCUDA)",21, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DifferentiableGather_MultiCUDA)",27, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , Replicate_MultiCUDA)",34, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApply_MultiCUDA)",26, 66, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApplyWithDifferentOutputDevice_MultiCUDA)",25, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApplyRethrowsException_MultiCUDA)",12, 69, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DataParallelPlacesTheOutputOnTheRequestedDevice_MultiCUDA)",34, 80, 8, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DataParallelUsesAllAvailableCUDADevices_CUDA)",18, 69, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , DefaultsToTheRightValues)",4, 52, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ReturnsTheCorrectType)",5, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , UtilityFunctionsReturnTheRightTensorOptions)",16, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCPUTypes)",19, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCPUTensors)",7, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromVariables)",9, 60, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( DeviceTest , ParsesCorrectlyFromString)",37, 65, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"DefaultDtypeTest::DefaultDtypeTest()",3, 56, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"DefaultDtypeTest::~DefaultDtypeTest()",3, 56, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , CanSetAndGetDefaultDtype)",5, 53, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , NewTensorOptionsHasCorrectDefault)",6, 62, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , NewTensorsHaveCorrectDefaultDtype)",16, 62, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( NoGradTest , SetsGradModeCorrectly)",11, 58, 2, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"AutogradTest::AutogradTest()",5, 54, 4, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanTakeDerivatives)",4, 43, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanTakeDerivativesOfZeroDimTensors)",4, 59, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanPassCustomGradientInputs)",4, 52, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"test::AGIUnit2::AGIUnit2()",1, 43, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanEnableAndDisableTrainingMode)",10, 54, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ZeroGrad)",17, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ZeroGradWithUndefined)",23, 75, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterModuleThrowsForEmptyOrDottedName)",11, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterModuleThrowsForDuplicateModuleName)",10, 65, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterParameterThrowsForEmptyOrDottedName)",11, 71, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterParameterThrowsForDuplicateModuleName)",10, 68, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterBufferThrowsForEmptyOrDottedName)",11, 68, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterBufferThrowsForDuplicateModuleName)",9, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanGetName)",10, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , AsCastsModulesCorrectly)",27, 53, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , Conversion_MultiCUDA)",47, 71, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CallingCloneOnModuleThatDoesNotOverrideCloneThrows)",5, 74, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CallingCloneOnModuleThatDoesOverrideCloneDoesNotThrow)",11, 76, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneCreatesDistinctParameters)",47, 73, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ClonePreservesExternalReferences)",29, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneCopiesTheValuesOfVariablesOfSubmodules)",39, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneToDevicePreservesTheDeviceOfParameters_CUDA)",31, 71, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloningToAParticularDevicePlacesAllParametersThere_MultiCUDA)",31, 68, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"ParameterTestModule::ParameterTestModule()",5, 58, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , HasCorrectNumberOfParameters)",5, 51, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ContainsParametersWithTheCorrectName)",7, 59, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"BufferTestModule::BufferTestModule()",5, 55, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , HasCorrectNumberOfBuffers)",5, 48, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ContainsBuffersWithTheCorrectName)",7, 56, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"AImpl::AImpl()",1, 23, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"AImpl::AImpl( int x)",1, 26, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , DefaultConstructorOfModuleHolderCallsDefaultConstructorOfImpl)",8, 69, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ValueConstructorOfModuleHolderCallsCorrectConstructorInImpl)",8, 67, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NullptrConstructorLeavesTheModuleHolderInEmptyState)",6, 74, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestModule::TestModule( int64_t size)",6, 57, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestModule::forward( torch :: Tensor input)",3, 47, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesReturnsExpectedSubmodulesForFlatModel)",11, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesExcludesSelfWhenIncludeSelfSetToFalse)",12, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesReturnsExpectedNamedSubmodulesForFlatModel)",13, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesExcludesSelfWhenIncludeSelfSetToFalse)",14, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ChildrenReturnsExpectedSubmodulesForFlatModel)",14, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedChildrenReturnsExpectedNamedSubmodulesForFlatModel)",13, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ParametersReturnsExpectedTensorsForFlatModel)",7, 67, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedParametersReturnsExpectedTensorsForFlatModel)",10, 72, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , BuffersReturnsExpectedTensorsForFlatModel)",7, 64, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedBuffersReturnsExpectedTensorsForFlatModel)",10, 69, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestContainer::TestContainer( int64_t number , std :: vector<TestContainer> modules = { })",8, 73, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"get_test_container_item( std :: shared_ptr<torch::nn::Module> module)",4, 77, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"make_deeply_nested_test_container()",10, 73, 12, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"make_key_value_pairs_for_deeply_nested_container()",12, 53, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesReturnsExpectedSubmodulesForDeepModel)",9, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesReturnsExpectedNamedSubmodulesForDeepModel)",13, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ChildrensReturnsExpectedSubmodulesForDeepModel)",9, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedChildrensReturnsExpectedNamedSubmodulesForDeepModel)",16, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModuleApplyIteratesCorreclty)",8, 76, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ConstModuleApplyIteratesCorreclty)",9, 76, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModuleApplyIteratesCorreclty)",14, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ConstNamedModuleApplyIteratesCorreclty)",16, 70, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulePointerApplyIteratesCorreclty)",8, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulePointerApplyIteratesCorreclty)",14, 78, 8, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ThrowsWhenAttemptingtoGetTopLevelModuleAsSharedPtr)",17, 73, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , PrettyPrint)",17, 71, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"ModuleWithNonTensorForwardImpl::forward( torch :: Tensor x)",3, 37, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanCallForwardOnNonTensorForwardThroughPimpl)",4, 67, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , IsEmptyAfterDefaultConstruction)",6, 57, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , InsertAddsElementsWhenTheyAreYetNotPresent)",6, 68, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , GetReturnsValuesWhenTheyArePresent)",7, 60, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , GetThrowsWhenPassedKeysThatAreNotPresent)",7, 66, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanInitializeFromList)",6, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , InsertThrowsWhenPassedElementsThatArePresent)",5, 70, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FrontReturnsTheFirstItem)",5, 50, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FrontThrowsWhenEmpty)",4, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , BackReturnsTheLastItem)",5, 48, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , BackThrowsWhenEmpty)",4, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FindReturnsPointersToValuesWhenPresent)",7, 64, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FindReturnsNullPointersWhenPasesdKeysThatAreNotPresent)",5, 80, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorThrowsWhenPassedKeysThatAreNotPresent)",5, 80, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorReturnsItemsPositionallyWhenPassedIntegers)",9, 67, 4, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorsThrowswhenPassedKeysThatAreNotPresent)",5, 81, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , UpdateInsertsAllItemsFromAnotherOrderedDict)",9, 69, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , UpdateAlsoChecksForDuplicates)",5, 69, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanIterateItems)",13, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ClearMakesTheDictEmpty)",6, 48, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanCopyConstruct)",7, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanCopyAssign)",10, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanMoveConstruct)",7, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanMoveAssign)",10, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanInsertWithBraces)",7, 45, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ErrorMessagesIncludeTheKeyDescription)",8, 74, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , KeysReturnsAllKeys)",4, 64, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ValuesReturnsAllValues)",4, 54, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ItemsReturnsAllItems)",9, 60, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , SimpleReturnType)",9, 42, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , SimpleReturnTypeAndSingleArgument)",9, 59, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , StringLiteralReturnTypeAndArgument)",9, 70, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , StringReturnTypeWithConstArgument)",10, 66, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , TensorReturnTypeAndStringArgumentsWithFunkyQualifications)",18, 75, 6, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , WrongArgumentType)",12, 51, 6, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , WrongNumberOfArguments)",17, 67, 6, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"M::M( int value_)",1, 68, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"M::forward( float x)",3, 25, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , GetWithCorrectTypeSucceeds)",4, 52, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , GetWithIncorrectTypeThrows)",9, 64, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithBaseClassSucceeds)",6, 50, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithGoodDowncastSuccceeds)",6, 54, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithBadDowncastThrows)",9, 64, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , DefaultStateIsEmpty)",14, 46, 4, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , AllMethodsThrowForEmptyAnyModule)",16, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , CanMoveAssignDifferentModules)",20, 55, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , ConstructsFromModuleHolder)",22, 74, 4, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , ConvertsVariableToTensorCorrectly)",18, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::TestValue::TestValue( T && value)",1, 68, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::TestValue::operator ( )()",3, 34, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::make_value( T && value)",3, 46, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesIntWhenCorrectType)",7, 60, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesConstIntWhenCorrectType)",6, 65, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesStringLiteralWhenCorrectType)",5, 70, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesStringWhenCorrectType)",5, 63, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesPointersWhenCorrectType)",7, 65, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesReferencesWhenCorrectType)",7, 67, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TryGetReturnsNullptrForTheWrongType)",7, 60, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , GetThrowsForTheWrongType)",12, 49, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , MoveConstructionIsAllowed)",6, 50, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , MoveAssignmentIsAllowed)",7, 48, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForInt)",4, 69, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForStringLiteral)",4, 77, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForString)",4, 77, 2, 0
repos/cpp/pytorch/test/cpp/api/jit.cpp,"TEST( TorchScriptTest , CanCompileMultipleFunctions)",28, 83, 6, 0
repos/cpp/pytorch/test/cpp/api/jit.cpp,"TEST( TorchScriptTest , TestNestedIValueModuleArgMatching)",51, 81, 18, 0
repos/cpp/pytorch/test/cpp/api/jit.cpp,"TEST( TorchScriptTest , TestDictArgMatching)",10, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/jit.cpp,"TEST( TorchScriptTest , TestTupleArgMatching)",13, 69, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromSharedPointer)",12, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromConcreteType)",12, 53, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromModuleHolder)",17, 56, 4, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , PushBackAddsAnElement)",18, 49, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , AccessWithAt)",28, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , AccessWithPtr)",29, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardOnEmptySequentialIsDisallowed)",5, 78, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardChainsCorrectly)",14, 70, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardWithTheWrongReturnTypeThrows)",13, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , TheReturnTypeOfForwardDefaultsToTensor)",11, 65, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ForwardReturnsTheLastValue)",10, 70, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , SanityCheckForHoldingStandardModules)",9, 63, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ExtendPushesModulesFromOtherSequential)",45, 65, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , HasReferenceSemantics)",14, 62, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , IsCloneable)",30, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , RegistersElementsAsSubmodules)",8, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CloneToDevice_CUDA)",12, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , PrettyPrintSequential)",19, 107, 6, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::getState()",3, 29, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::getReward()",3, 23, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::isDone()",3, 20, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::reset()",5, 53, 4, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::CartPole()",3, 15, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::step( int action)",38, 79, 4, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"test_mnist( size_t batch_size , size_t number_of_epochs , bool with_cuda , M && model , F && forward_op , O && optimizer)",45, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , CartPole)",97, 80, 10, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , MNIST_CUDA)",35, 74, 6, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , MNISTBatchNorm_CUDA)",35, 74, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TestModel::TestModel()",4, 53, 8, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"NestedModel::NestedModel()",4, 71, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv1d)",15, 60, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv2dEven)",15, 63, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv2dUneven)",15, 63, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv3d)",15, 66, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear)",14, 58, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , SimpleContainer)",17, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , EmbeddingBasic)",22, 73, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , EmbeddingList)",12, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Dropout)",15, 62, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Parameters)",19, 56, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalCallsSuppliedFunction)",16, 69, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalWithTorchFunction)",6, 64, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalArgumentBinding)",5, 75, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormStateful)",25, 43, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormStateless)",14, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormPureForward)",13, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear_CUDA)",16, 78, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear2_CUDA)",16, 58, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintLinear)",4, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintConv)",16, 100, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintDropout)",5, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintFunctional)",3, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintBatchNorm)",7, 95, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintEmbedding)",5, 54, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , PrettyPrintNestedModel)",35, 81, 10, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"test_RNN_xor( Func && model_maker , bool cuda = false)",52, 73, 8, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"check_lstm_sizes( RNNOutput output)",18, 77, 2, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , CheckOutputSizes)",19, 64, 2, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , CheckOutputValuesMatchPyTorch)",60, 69, 2, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndLSTM)",4, 65, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndGRU)",4, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNRelu)",4, 70, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNTanh)",4, 70, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , Sizes_CUDA)",21, 80, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndLSTM_CUDA)",4, 71, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndGRU_CUDA)",4, 69, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNRelu_CUDA)",4, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNTanh_CUDA)",4, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , PrettyPrintRNNs)",11, 97, 6, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromInitializerList)",7, 62, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromVector)",7, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromArray)",7, 71, 2, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromSingleValue)",7, 58, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , ThrowsWhenConstructedWithIncorrectNumberOfArgumentsInInitializerList)",7, 76, 4, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , ThrowsWhenConstructedWithIncorrectNumberOfArgumentsInVector)",7, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"test_optimizer_xor( Options options)",41, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"assign_parameter( const Parameters & parameters , const char * name , torch :: Tensor new_tensor)",9, 41, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"check_exact_values( Options options , std :: vector<std::vector<torch::Tensor>> expected_parameters)",57, 81, 10, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , BasicInterface)",26, 71, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_SGD)",4, 73, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_Adagrad)",4, 63, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_RMSprop)",3, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_RMSpropWithMomentum)",4, 62, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_Adam)",3, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_AdamWithAmsgrad)",4, 59, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_Adam)",3, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdamWithWeightDecay)",5, 61, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdamWithWeightDecayAndAMSGrad)",5, 71, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_Adagrad)",4, 60, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdagradWithWeightDecay)",5, 64, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdagradWithWeightDecayAndLRDecay)",5, 74, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSprop)",4, 60, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecay)",5, 64, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecayAndCentered)",5, 75, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecayAndCenteredAndMomentum)",8, 75, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGD)",3, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecay)",5, 60, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecayAndMomentum)",5, 71, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecayAndNesterovMomentum)",5, 79, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ZeroGrad)",26, 56, 4, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ExternalVectorOfParameters)",21, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , AddParameter_LBFGS)",18, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , AllocatesTensorOnTheCorrectDevice_MultiCUDA)",5, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDevice_MultiCUDA)",40, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToTensorAndTensorAttributes_MultiCUDA)",22, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDoesNotCopyWhenOptionsAreAllTheSame_CUDA)",13, 93, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDeviceAndDtype_MultiCUDA)",16, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"check_exact_values( const std :: vector<torch::Tensor> & parameters , const std :: vector<std::vector<torch::Tensor>> & expected_parameters)",30, 81, 8, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"check_initializer_against_baseline( std :: function<void(torch::Tensor)> initializer , std :: vector<std::vector<torch::Tensor>> expected)",25, 56, 4, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , ProducesPyTorchValues_XavierUniform)",7, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , ProducesPyTorchValues_XavierNormal)",7, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , ProducesPyTorchValues_KaimingNormal)",7, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , ProducesPyTorchValues_KaimingUniform)",7, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , CanInitializeTensorThatRequiresGrad)",8, 71, 2, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , CalculateGainWithTanh)",5, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , CalculateGainWithRelu)",5, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/init.cpp,"TEST( InitTest , CalculateGainWithLeakyRelu)",5, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"exactly_equal( at :: Tensor left , T right)",3, 47, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"almost_equal( at :: Tensor left , T right , T tolerance = 1e - 4)",3, 66, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToDtype)",22, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToTensorAndTensorAttributes)",22, 67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToOptionsWithRequiresGrad)",30, 74, 8, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToDoesNotCopyWhenOptionsAreAllTheSame)",27, 71, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValueForSingleValue)",16, 55, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesForManyValues)",15, 55, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesForManyValuesVariable)",17, 63, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesWhenConstructedFromVector)",17, 79, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , UsesOptionsThatAreSupplied)",14, 62, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlob)",12, 78, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlobUsesDeleter)",12, 62, 8, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlobWithStrides)",25, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , Item)",12, 49, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , Item_CUDA)",12, 62, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::DummyDataset( size_t size = 100)",1, 60, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::get( size_t index)",3, 35, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DatasetCallsGetCorrectly)",6, 57, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TransformCallsGetApplyCorrectly)",12, 65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyChunkDataReader::read_chunk( size_t chunk_index)",12, 70, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyChunkDataReader::chunk_count()",3, 34, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyChunkDataReader::reset()",1, 27, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , ChunkDataSetWithInvalidInitParameter)",38, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"InfiniteStreamDataset::get_batch( size_t batch_size)",7, 59, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"InfiniteStreamDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , InfiniteStreamDataset)",22, 66, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , NoSequencerIsIdentity)",6, 66, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , OrderedSequencerIsSetUpWell)",10, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , OrderedSequencerReOrdersValues)",30, 77, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , BatchLambdaAppliesFunctionToBatch)",10, 79, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , LambdaAppliesFunctionToExample)",6, 68, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CollateReducesBatch)",7, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CollationReducesBatch)",9, 61, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerReturnsIndicesInOrder)",7, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerReturnsLessValuesForLastBatch)",6, 70, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerResetsWell)",8, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerResetsWithNewSizeWell)",12, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanSaveAndLoadSequentialSampler)",24, 50, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerReturnsIndicesInCorrectRange)",23, 60, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerReturnsLessValuesForLastBatch)",6, 61, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerResetsWell)",8, 48, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerResetsWithNewSizeWell)",11, 53, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SavingAndLoadingRandomSamplerYieldsSameSequence)",29, 66, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerReturnsTheBatchSizeAndThenRemainder)",8, 67, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerResetsWell)",8, 53, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerResetsWithNewSizeWell)",11, 53, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorDatasetConstructsFromSingleTensor)",5, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorDatasetConstructsFromInitializerListOfTensors)",6, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StackTransformWorksForExample)",23, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StackTransformWorksForTensorExample)",10, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"T::operator ( )( torch :: Tensor input)",3, 59, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TensorStringDataset::get( size_t index)",3, 79, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TensorStringDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorTransformWorksForAnyTargetType)",11, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorLambdaWorksforAnyTargetType)",12, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyTensorDataset::get( size_t index)",6, 78, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyTensorDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , NormalizeTransform)",59, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnCopyableDataset::get( size_t index)",4, 57, 12, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnCopyableDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , MapDoesNotCopy)",13, 76, 25, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePushAndPopFromSameThread)",7, 48, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePopWithTimeoutThrowsUponTimeout)",7, 66, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePushAndPopFromDifferentThreads)",23, 74, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueueClearEmptiesTheQueue)",8, 62, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleCanPushAndPopJob)",7, 54, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleCanPushAndPopResult)",14, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttlePopResultReturnsNulloptWhenNoJobsInFlight)",10, 71, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleDrainMeansPopResultReturnsNullopt)",7, 63, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttlePopResultTimesOut)",5, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::UncopyableDataset( const std :: string &)",1, 56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::get( size_t index)",3, 35, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SharedBatchDatasetReallyIsShared)",17, 78, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SharedBatchDatasetDoesNotIncurCopyWhenPassedDatasetObject)",7, 76, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndex::TestIndex( size_t offset , std :: vector<size_t> index)",2, 63, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndex::size() const",3, 33, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::TestIndexDataset( size_t size)",3, 56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::get_batch( TestIndex index)",7, 57, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::TestIndexSampler( size_t size)",1, 58, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::reset( torch :: optional<size_t> new_size = torch :: nullopt)",1, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::next( size_t batch_size)",9, 64, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::save( torch :: serialize :: OutputArchive & archive) const",1, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::load( torch :: serialize :: InputArchive & archive)",1, 65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanUseCustomTypeAsIndexType)",13, 63, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DistributedRandomSamplerSingleReplicaProduceCorrectSamples)",17, 77, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DistributedRandomSamplerMultiReplicaProduceCorrectSamples)",41, 79, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanSaveAndLoadDistributedRandomSampler)",34, 57, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DistributedSequentialSamplerSingleReplicaProduceCorrectSamples)",18, 81, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DistributedSequentialSamplerMultiReplicaProduceCorrectSamples)",42, 80, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanSaveAndLoadDistributedSequentialSampler)",24, 61, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DataLoaderOptionsDefaultAsExpected)",10, 59, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DataLoaderOptionsCoalesceOptionalValues)",6, 64, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , MakeDataLoaderDefaultsAsExpected)",5, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnsizedDataset::get( size_t i)",3, 45, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnsizedDataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , MakeDataLoaderThrowsWhenConstructingSamplerWithUnsizedDataset)",7, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsCompareEqualToThemselves)",7, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ValidIteratorsCompareUnequalToEachOther)",8, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , SentinelIteratorsCompareEqualToEachOther)",6, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsCompareEqualToSentinelWhenExhausted)",16, 74, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsShareState)",16, 74, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanDereferenceIteratorMultipleTimes)",19, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanUseIteratorAlgorithms)",21, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CallingBeginWhileOtherIteratorIsInFlightThrows)",10, 79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IncrementingExhaustedValidIteratorThrows)",8, 75, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DereferencingExhaustedValidIteratorThrows)",9, 70, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IncrementingSentinelIteratorThrows)",9, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DereferencingSentinelIteratorThrows)",9, 78, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , YieldsCorrectBatchSize)",10, 65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ReturnsLastBatchWhenSmallerThanBatchSizeWhenDropLastIsFalse)",13, 67, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DoesNotReturnLastBatchWhenSmallerThanBatchSizeWhenDropLastIsTrue)",12, 72, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , RespectsTimeout)",33, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"Barrier::Barrier( size_t target)",1, 56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"Barrier::wait()",8, 62, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::Dataset( const Dataset & other)",4, 43, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::get_batch( torch :: ArrayRef<size_t> indices)",17, 79, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::size() const",3, 50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , EnforcesOrderingAmongThreadsWhenConfigured)",16, 81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , Reset)",21, 74, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , TestExceptionsArePropagatedFromWorkers)",25, 78, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , StatefulDatasetWithNoWorkers)",33, 77, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , StatefulDatasetWithManyWorkers)",38, 77, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , StatefulDatasetWithMap)",40, 81, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , StatefulDatasetWithCollate)",41, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ChunkDataSetGetBatch)",66, 81, 12, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ChunkDataSetWithBatchSizeMismatch)",32, 76, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ChunkDataSetWithEmptyBatch)",43, 77, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ChunkDataSetGetBatchWithUnevenBatchSize)",51, 79, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanAccessChunkSamplerWithChunkDataSet)",40, 77, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ChunkDatasetDoesNotHang)",35, 80, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"f( T && m)",3, 16, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"f( T && m)",3, 54, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , AllOf)",8, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , AnyOf)",6, 63, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , EnableIfModule)",10, 81, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"A::forward()",3, 18, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"B::forward( torch :: Tensor tensor)",3, 46, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"C::forward( torch :: Tensor & tensor)",3, 41, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"D::forward( torch :: Tensor && tensor)",3, 41, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"assert_has_expected_type()",7, 77, 6, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , ReturnTypeOfForward)",7, 61, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , Apply)",8, 64, 2, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::Saturation( float * clip , const int length , const int crop_height , const int crop_width , const float alpha_rand , std :: mt19937 * randgen)",27, 80, 6, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::Brightness( float * clip , const int length , const int crop_height , const int crop_width , const float alpha_rand , std :: mt19937 * randgen)",21, 80, 6, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::Contrast( float * clip , const int length , const int crop_height , const int crop_width , const float alpha_rand , std :: mt19937 * randgen)",36, 80, 6, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::ColorJitter( float * clip , const int length , const int crop_height , const int crop_width , const float saturation , const float brightness , const float contrast , std :: mt19937 * randgen)",28, 79, 2, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::ColorLighting( float * clip , const int length , const int crop_height , const int crop_width , const float alpha_std , const std :: vector<std::vector<float>> & eigvecs , const std :: vector<float> & eigvals , std :: mt19937 * randgen)",33, 62, 6, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::ColorNormalization( float * clip , const int length , const int crop_height , const int crop_width , const int channels , const std :: vector<float> & mean , const std :: vector<float> & inv_std)",20, 54, 10, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::ClipTransformRGB( const unsigned char * buffer_rgb , const int multi_crop_count , const int crop_height , const int crop_width , const int length_rgb , const int channels_rgb , const int sampling_rate_rgb , const int height , const int width , const int h_off , const int w_off , const int * multi_crop_h_off , const int * multi_crop_w_off , const bool mirror_me , const bool color_jitter , const float saturation , const float brightness , const float contrast , const bool color_lighting , const float color_lighting_std , const std :: vector<std::vector<float>> & color_lighting_eigvecs , const std :: vector<float> & color_lighting_eigvals , const std :: vector<float> & mean_rgb , const std :: vector<float> & inv_std_rgb , std :: mt19937 * randgen , float * transformed_clip)",143, 81, 6, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::ClipTransformOpticalFlow( const unsigned char * buffer_rgb , const int crop_height , const int crop_width , const int length_of , const int channels_of , const int sampling_rate_of , const int height , const int width , const cv :: Rect & rect , const int channels_rgb , const bool mirror_me , const int flow_alg_type , const int flow_data_type , const int frame_gap_of , const bool do_flow_aggregation , const std :: vector<float> & mean_of , const std :: vector<float> & inv_std_of , float * transformed_clip)",157, 79, 8, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::FreeDecodedData( std :: vector<std::unique_ptr<DecodedFrame>> & sampledFrames)",9, 65, 4, 0
repos/cpp/pytorch/caffe2/video/video_io.cc,"caffe2::DecodeMultipleClipsFromVideo( const char * video_buffer , const std :: string & video_filename , const int encoded_size , const Params & params , const int start_frm , const int clip_per_video , const bool use_local_file , int & height , int & width , std :: vector<unsigned char*> & buffer_rgb)",62, 76, 4, 0
repos/cpp/pytorch/caffe2/video/video_decoder.cc,"caffe2::VideoDecoder::VideoDecoder()",11, 45, 2, 0
repos/cpp/pytorch/caffe2/video/video_decoder.cc,"caffe2::VideoDecoder::ResizeAndKeepAspectRatio( const int origHeight , const int origWidth , const int heightMin , const int widthMin , int & outHeight , int & outWidth)",17, 61, 2, 0
repos/cpp/pytorch/caffe2/video/video_decoder.cc,"caffe2::VideoDecoder::decodeLoop( const string & videoName , VideoIOContext & ioctx , const Params & params , const int start_frm , std :: vector<std::unique_ptr<DecodedFrame>> & sampledFrames)",460, 83, 20, 0
repos/cpp/pytorch/caffe2/video/video_decoder.cc,"caffe2::VideoDecoder::decodeMemory( const char * buffer , const int size , const Params & params , const int start_frm , std :: vector<std::unique_ptr<DecodedFrame>> & sampledFrames)",9, 80, 2, 0
repos/cpp/pytorch/caffe2/video/video_decoder.cc,"caffe2::VideoDecoder::decodeFile( const string & file , const Params & params , const int start_frm , std :: vector<std::unique_ptr<DecodedFrame>> & sampledFrames)",8, 65, 4, 0
repos/cpp/pytorch/caffe2/video/video_decoder.cc,"caffe2::VideoDecoder::ffmpegErrorStr( int result)",5, 50, 0, 0
repos/cpp/pytorch/caffe2/video/optical_flow.cc,"caffe2::OpticalFlowExtractor( const cv :: Mat & prev_gray , const cv :: Mat & curr_gray , const int flow_alg_type , cv :: Mat & flow)",38, 77, 2, 0
repos/cpp/pytorch/caffe2/video/optical_flow.cc,"caffe2::MergeOpticalFlow( cv :: Mat & prev_flow , const cv :: Mat & curr_flow)",18, 70, 0, 0
repos/cpp/pytorch/caffe2/video/optical_flow.cc,"caffe2::MultiFrameOpticalFlowExtractor( const std :: vector<cv::Mat> & grays , const int optical_flow_alg_type , cv :: Mat & flow)",21, 78, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/local_response_normalization_op.cc,"caffe2::IDEEPLRNOp::IDEEPLRNOp( const OperatorDef & operator_def , Workspace * ws)",12, 68, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/local_response_normalization_op.cc,"caffe2::IDEEPLRNOp::~IDEEPLRNOp()",1, 28, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/local_response_normalization_op.cc,"caffe2::IDEEPLRNOp::RunOnDevice()",8, 69, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/local_response_normalization_op.cc,"caffe2::IDEEPLRNGradientOp::IDEEPLRNGradientOp( const OperatorDef & operator_def , Workspace * ws)",12, 69, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/local_response_normalization_op.cc,"caffe2::IDEEPLRNGradientOp::~IDEEPLRNGradientOp()",1, 36, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/local_response_normalization_op.cc,"caffe2::IDEEPLRNGradientOp::RunOnDevice()",10, 78, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/expand_squeeze_dims_op.cc,"caffe2::IDEEPExpandDimsOp::IDEEPExpandDimsOp( const OperatorDef & operator_def , Workspace * ws)",13, 79, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/expand_squeeze_dims_op.cc,"caffe2::IDEEPExpandDimsOp::~IDEEPExpandDimsOp()",1, 35, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/expand_squeeze_dims_op.cc,"caffe2::IDEEPExpandDimsOp::RunOnDevice()",30, 70, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/expand_squeeze_dims_op.cc,"caffe2::IDEEPSqueezeOp::IDEEPSqueezeOp( const OperatorDef & operator_def , Workspace * ws)",14, 79, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/expand_squeeze_dims_op.cc,"caffe2::IDEEPSqueezeOp::~IDEEPSqueezeOp()",1, 32, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/expand_squeeze_dims_op.cc,"caffe2::IDEEPSqueezeOp::RunOnDevice()",26, 77, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"need_type_zero_pad( const mkldnn_memory_desc_t * pd)",11, 58, 0, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"caffe2::IDEEPConvTransposeOp::IDEEPConvTransposeOp( const OperatorDef & operator_def , Workspace * ws)",9, 71, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"caffe2::IDEEPConvTransposeOp::~IDEEPConvTransposeOp()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"caffe2::IDEEPConvTransposeOp::RunOnDeviceWithOrderNCHW()",76, 95, 10, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"caffe2::IDEEPConvTransposeGradientOp::IDEEPConvTransposeGradientOp( const OperatorDef & operator_def , Workspace * ws)",15, 79, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"caffe2::IDEEPConvTransposeGradientOp::~IDEEPConvTransposeGradientOp()",1, 46, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_transpose_op.cc,"caffe2::IDEEPConvTransposeGradientOp::RunOnDeviceWithOrderNCHW()",77, 97, 10, 0
repos/cpp/pytorch/caffe2/ideep/operators/adam_op.cc,"caffe2::adam_ideep_update( int N , const float * g , const float * m , const float * v , float * ng , float * nm , float * nv , float beta1 , float beta2 , float eps_hat , float correction , const float * lr)",23, 65, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/adam_op.cc,"caffe2::adam_ideep_compute( int N , const float * w , const float * g , const float * m , const float * v , float * nw , float * nm , float * nv , float beta1 , float beta2 , float eps_hat , float correction , const float * lr)",24, 72, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/adam_op.cc,"caffe2::adam_ideep_compute_output_grad( int N , const float * w , const float * g , const float * m , const float * v , float * nw , float * nm , float * nv , float * ng , float beta1 , float beta2 , float eps_hat , float correction , const float * lr)",27, 69, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/adam_op.cc,"caffe2::IDEEPAdamOp::IDEEPAdamOp( const OperatorDef & operator_def , Workspace * ws)",5, 78, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/adam_op.cc,"caffe2::IDEEPAdamOp::RunOnDevice()",75, 80, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_fusion_op.cc,"caffe2::IDEEPConvFusionOp::IDEEPConvFusionOp( const OperatorDef & operator_def , Workspace * ws)",29, 91, 12, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_fusion_op.cc,"caffe2::IDEEPConvFusionOp::~IDEEPConvFusionOp()",1, 35, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_fusion_op.cc,"caffe2::IDEEPConvFusionOp::RunOnDeviceWithOrderNCHW()",88, 79, 17, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_fusion_op.cc,"caffe2::ConvFusionDocGenerator( const char * dim)",45, 81, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/reshape_op.cc,"caffe2::IDEEPReshapeOp::IDEEPReshapeOp( const OperatorDef & operator_def , Workspace * ws)",3, 71, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/reshape_op.cc,"caffe2::IDEEPReshapeOp::RunOnDevice()",108, 79, 10, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_op.cc,"caffe2::IDEEPConvOp::IDEEPConvOp( const OperatorDef & operator_def , Workspace * ws)",10, 91, 12, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_op.cc,"caffe2::IDEEPConvOp::~IDEEPConvOp()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_op.cc,"caffe2::IDEEPConvOp::RunOnDeviceWithOrderNCHW()",82, 81, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_op.cc,"caffe2::IDEEPConvGradientOp::IDEEPConvGradientOp( const OperatorDef & operator_def , Workspace * ws)",14, 71, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_op.cc,"caffe2::IDEEPConvGradientOp::~IDEEPConvGradientOp()",1, 37, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/conv_op.cc,"caffe2::IDEEPConvGradientOp::RunOnDeviceWithOrderNCHW()",48, 69, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/momentum_sgd_op.cc,"caffe2::momentum_sgd_update( const int N , const float * g , const float * m , float * ng , float * nm , const float * lr , const float momentum , const bool nesterov , float * param)",31, 67, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/momentum_sgd_op.cc,"caffe2::IDEEPMomentumSGDOp::IDEEPMomentumSGDOp( const OperatorDef & operator_def , Workspace * ws)",4, 76, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/momentum_sgd_op.cc,"caffe2::IDEEPMomentumSGDOp::RunOnDevice()",25, 77, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/momentum_sgd_op.cc,"caffe2::IDEEPMomentumSGDUpdateOp::IDEEPMomentumSGDUpdateOp( const OperatorDef & operator_def , Workspace * ws)",4, 76, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/momentum_sgd_op.cc,"caffe2::IDEEPMomentumSGDUpdateOp::RunOnDevice()",25, 77, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/concat_split_op.cc,"caffe2::IDEEPConcatOp::IDEEPConcatOp( const OperatorDef & operator_def , Workspace * ws)",16, 82, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/concat_split_op.cc,"caffe2::IDEEPConcatOp::~IDEEPConcatOp()",1, 31, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/concat_split_op.cc,"caffe2::IDEEPConcatOp::RunOnDevice()",37, 75, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/concat_split_op.cc,"caffe2::IDEEPSplitOp::IDEEPSplitOp( const OperatorDef & operator_def , Workspace * ws)",17, 82, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/concat_split_op.cc,"caffe2::IDEEPSplitOp::~IDEEPSplitOp()",1, 30, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/concat_split_op.cc,"caffe2::IDEEPSplitOp::RunOnDevice()",56, 77, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/relu_op.cc,"caffe2::IDEEPReluOp::IDEEPReluOp( const OperatorDef & operator_def , Workspace * ws)",14, 72, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/relu_op.cc,"caffe2::IDEEPReluOp::~IDEEPReluOp()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/relu_op.cc,"caffe2::IDEEPReluOp::RunOnDevice()",9, 70, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/relu_op.cc,"caffe2::IDEEPReluGradientOp::IDEEPReluGradientOp( const OperatorDef & operator_def , Workspace * ws)",14, 75, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/relu_op.cc,"caffe2::IDEEPReluGradientOp::~IDEEPReluGradientOp()",1, 37, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/relu_op.cc,"caffe2::IDEEPReluGradientOp::RunOnDevice()",9, 79, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/dropout_op.cc,"caffe2::IDEEPDropoutOp::IDEEPDropoutOp( const OperatorDef & operator_def , Workspace * ws)",8, 77, 12, 0
repos/cpp/pytorch/caffe2/ideep/operators/dropout_op.cc,"caffe2::IDEEPDropoutOp::~IDEEPDropoutOp()",1, 32, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/dropout_op.cc,"caffe2::IDEEPDropoutOp::RunOnDevice()",16, 59, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/dropout_op.cc,"caffe2::IDEEPDropoutGradientOp::IDEEPDropoutGradientOp( const OperatorDef & operator_def , Workspace * ws)",8, 77, 12, 0
repos/cpp/pytorch/caffe2/ideep/operators/dropout_op.cc,"caffe2::IDEEPDropoutGradientOp::~IDEEPDropoutGradientOp()",1, 40, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/dropout_op.cc,"caffe2::IDEEPDropoutGradientOp::RunOnDevice()",16, 53, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/queue_ops.cc,"caffe2::IDEEPCreateBlobsQueueOp::IDEEPCreateBlobsQueueOp( const OperatorDef & operator_def , Workspace * ws)",4, 74, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/queue_ops.cc,"caffe2::IDEEPCreateBlobsQueueOp::RunOnDevice()",16, 80, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/queue_ops.cc,"caffe2::IDEEPSafeEnqueueBlobsOp::IDEEPSafeEnqueueBlobsOp( const OperatorDef & operator_def , Workspace * ws)",2, 74, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/queue_ops.cc,"caffe2::IDEEPSafeEnqueueBlobsOp::RunOnDevice()",17, 80, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/sigmoid_op.cc,"caffe2::IDEEPSigmoidOp::IDEEPSigmoidOp( const OperatorDef & operator_def , Workspace * ws)",3, 65, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/sigmoid_op.cc,"caffe2::IDEEPSigmoidOp::~IDEEPSigmoidOp()",1, 32, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/sigmoid_op.cc,"caffe2::IDEEPSigmoidOp::RunOnDevice()",9, 66, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/sigmoid_op.cc,"caffe2::IDEEPSigmoidGradientOp::IDEEPSigmoidGradientOp( const OperatorDef & operator_def , Workspace * ws)",3, 73, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/sigmoid_op.cc,"caffe2::IDEEPSigmoidGradientOp::~IDEEPSigmoidGradientOp()",1, 40, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/sigmoid_op.cc,"caffe2::IDEEPSigmoidGradientOp::RunOnDevice()",9, 75, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/utility_ops.cc,"caffe2::CopyCPUToIDEEPOp::RunOnDevice()",14, 75, 10, 0
repos/cpp/pytorch/caffe2/ideep/operators/utility_ops.cc,"caffe2::IDEEPCopyOp::RunOnDevice()",10, 53, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/utility_ops.cc,"caffe2::CopyIDEEPToCPUOp::RunOnDevice()",22, 81, 12, 0
repos/cpp/pytorch/caffe2/ideep/operators/utility_ops.cc,"caffe2::IDEEPWeightedSumOp::IDEEPWeightedSumOp( const OperatorDef & operator_def , Workspace * ws)",2, 69, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/utility_ops.cc,"caffe2::IDEEPWeightedSumOp::RunOnDevice()",26, 73, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/elementwise_sum_op.cc,"caffe2::IDEEPSumOp::IDEEPSumOp( const OperatorDef & operator_def , Workspace * ws)",4, 61, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/elementwise_sum_op.cc,"caffe2::IDEEPSumOp::~IDEEPSumOp()",1, 28, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/elementwise_sum_op.cc,"caffe2::IDEEPSumOp::RunOnDevice()",45, 70, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/pool_op.cc,"caffe2::IDEEPPoolOp::IDEEPPoolOp( const OperatorDef & operator_def , Workspace * ws)",22, 75, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/pool_op.cc,"caffe2::IDEEPPoolOp::~IDEEPPoolOp()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/pool_op.cc,"caffe2::IDEEPPoolOp::RunOnDeviceWithOrderNCHW()",12, 82, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/pool_op.cc,"caffe2::IDEEPPoolGradientOp::IDEEPPoolGradientOp( const OperatorDef & operator_def , Workspace * ws)",20, 77, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/pool_op.cc,"caffe2::IDEEPPoolGradientOp::~IDEEPPoolGradientOp()",1, 37, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/pool_op.cc,"caffe2::IDEEPPoolGradientOp::RunOnDeviceWithOrderNCHW()",11, 54, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleOp::ChannelShuffleOp( const OperatorDef & operator_def , Workspace * ws)",2, 67, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleOp::RunOnDeviceWithOrderNCHW()",8, 60, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleGradientOp::ChannelShuffleGradientOp( const OperatorDef & operator_def , Workspace * ws)",2, 75, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleGradientOp::RunOnDeviceWithOrderNCHW()",8, 63, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/shape_op.cc,"caffe2::IDEEPShapeOp::IDEEPShapeOp( const OperatorDef & operator_def , Workspace * ws)",3, 66, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/shape_op.cc,"caffe2::IDEEPShapeOp::RunOnDevice()",40, 82, 4, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::CanonicalDims( itensor :: dims adims , int32_t axis)",19, 80, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::IDEEPFullyConnectedOp::IDEEPFullyConnectedOp( const OperatorDef & operator_def , Workspace * ws)",4, 74, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::IDEEPFullyConnectedOp::~IDEEPFullyConnectedOp()",1, 39, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::IDEEPFullyConnectedOp::RunOnDevice()",25, 79, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::IDEEPFullyConnectedGradientOp::IDEEPFullyConnectedGradientOp( const OperatorDef & operator_def , Workspace * ws)",4, 80, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::IDEEPFullyConnectedGradientOp::~IDEEPFullyConnectedGradientOp()",1, 47, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/fully_connected_op.cc,"caffe2::IDEEPFullyConnectedGradientOp::RunOnDevice()",36, 87, 5, 0
repos/cpp/pytorch/caffe2/ideep/operators/spatial_batch_norm_op.cc,"caffe2::IDEEPSpatialBNOp::IDEEPSpatialBNOp( const OperatorDef & operator_def , Workspace * ws)",12, 81, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/spatial_batch_norm_op.cc,"caffe2::IDEEPSpatialBNOp::~IDEEPSpatialBNOp()",1, 34, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/spatial_batch_norm_op.cc,"caffe2::IDEEPSpatialBNOp::RunOnDevice()",28, 61, 6, 0
repos/cpp/pytorch/caffe2/ideep/operators/spatial_batch_norm_op.cc,"caffe2::IDEEPSpatialBNGradientOp::IDEEPSpatialBNGradientOp( const OperatorDef & operator_def , Workspace * ws)",6, 76, 8, 0
repos/cpp/pytorch/caffe2/ideep/operators/spatial_batch_norm_op.cc,"caffe2::IDEEPSpatialBNGradientOp::~IDEEPSpatialBNGradientOp()",1, 42, 2, 0
repos/cpp/pytorch/caffe2/ideep/operators/spatial_batch_norm_op.cc,"caffe2::IDEEPSpatialBNGradientOp::RunOnDevice()",16, 50, 4, 0
repos/cpp/pytorch/caffe2/ideep/utils/ideep_register.cc,"at::CopyBytesWrapper( size_t nbytes , const void * src , Device src_device , void * dst , Device dst_device)",13, 28, 2, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::CudaEventWrapper::CudaEventWrapper( const DeviceOption & option)",9, 67, 8, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::CudaEventWrapper::~CudaEventWrapper()",4, 47, 4, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventCreateCUDA( const DeviceOption & option , Event * event)",3, 65, 0, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventRecordCUDA( Event * event , const void * context , const char * err_msg)",45, 83, 10, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventFinishCUDA( const Event * event)",24, 77, 4, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventWaitCUDACUDA( const Event * event , void * context)",21, 80, 6, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventWaitCPUCUDA( const Event * event , void * context)",3, 59, 0, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventWaitCUDACPU( const Event * event , void * context)",3, 59, 0, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventQueryCUDA( const Event * event)",16, 71, 2, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventErrorMessageCUDA( const Event * event)",9, 71, 2, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventSetFinishedCUDA( const Event * event , const char * err_msg)",19, 71, 2, 0
repos/cpp/pytorch/caffe2/core/event_gpu.cc,"caffe2::EventResetCUDA( Event * event)",7, 71, 2, 0
repos/cpp/pytorch/caffe2/core/init.cc,"caffe2::internal::Caffe2InitializeRegistry::Registry()",4, 65, 0, 0
repos/cpp/pytorch/caffe2/core/init.cc,"caffe2::internal::GlobalInitState()",4, 45, 2, 0
repos/cpp/pytorch/caffe2/core/init.cc,"caffe2::GlobalInitAlreadyRun()",3, 70, 2, 0
repos/cpp/pytorch/caffe2/core/init.cc,"caffe2::GlobalInit( int * pargc , char ** * pargv)",48, 81, 4, 0
repos/cpp/pytorch/caffe2/core/init.cc,"caffe2::GlobalInit()",10, 73, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::AsyncTaskFuture()",1, 74, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::AsyncTaskFuture( const std :: vector<AsyncTaskFuture*> & futures)",38, 79, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::IsCompleted() const",3, 44, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::IsFailed() const",3, 41, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::ErrorMessage() const",3, 52, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::Wait() const",6, 45, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::SetCallback( std :: function<void(const AsyncTaskFuture*)> callback)",9, 60, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::SetCompleted( const char * err_msg)",17, 76, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::ResetState()",9, 45, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_task_future.cc,"caffe2::AsyncTaskFuture::~AsyncTaskFuture()",1, 39, 0, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::DummyObserver::DummyObserver<T>( T * subject_)",1, 72, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::DummyObserver::~DummyObserver()",1, 31, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::DummyObserver<NetBase>::Start()",7, 78, 4, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::DummyObserver<OperatorBase>::Start()",3, 44, 0, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::DummyObserver<NetBase>::Stop()",3, 38, 0, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::DummyObserver<OperatorBase>::Stop()",3, 43, 0, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::ObsTestDummyOp::Run( int)",5, 40, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::CreateNetTestHelper( Workspace * ws , bool isDAG = false)",22, 77, 0, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::TEST( ObserverTest , TestNotify)",14, 77, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::TEST( ObserverTest , TestUniqueMap)",16, 77, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::TEST( ObserverTest , TestNotifyAfterDetach)",14, 59, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::TEST( ObserverTest , TestDAGNetBase)",13, 59, 2, 0
repos/cpp/pytorch/caffe2/core/observer_test.cc,"caffe2::TEST( ObserverTest , TestMultipleNetBase)",25, 75, 2, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::gModuleChangeMutex()",4, 42, 0, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::MutableCurrentModules()",4, 72, 0, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::CurrentModuleHandles()",4, 56, 0, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::CurrentModules()",3, 64, 0, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::ModuleSchema::ModuleSchema( const char * name , const char * description)",5, 70, 0, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::HasModule( const string & name)",4, 47, 1, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::LoadModule( const string & name , const string & filename)",5, 76, 4, 0
repos/cpp/pytorch/caffe2/core/module.cc,"caffe2::LoadModule( const string & name , const string & filename)",66, 77, 8, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::PrintBlobSizes()",49, 76, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::LocalBlobs() const",8, 47, 0, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::Blobs() const",19, 73, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::CreateBlob( const string & name)",13, 81, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::CreateLocalBlob( const string & name)",9, 64, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::RenameBlob( const string & old_name , const string & new_name)",22, 78, 0, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::RemoveBlob( const string & name)",12, 68, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::GetBlob( const string & name) const",18, 76, 2, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::AddBlobMapping( const Workspace * parent , const std :: unordered_map<string,string> & forwarded_blobs , bool skip_defined_blobs)",30, 81, 10, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::GetBlob( const string & name)",3, 80, 2, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::CreateNet( const NetDef & net_def , bool overwrite)",4, 71, 0, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::CreateNet( const std :: shared_ptr<const NetDef> & net_def , bool overwrite)",31, 80, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::GetNet( const string & name)",7, 49, 0, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::DeleteNet( const string & name)",5, 48, 0, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::RunNet( const string & name)",7, 64, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::RunOperatorOnce( const OperatorDef & op_def)",12, 70, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::RunNetOnce( const NetDef & net_def)",13, 67, 4, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::RunPlan( const PlanDef & plan , ShouldContinue shouldContinue)",3, 78, 0, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::GetThreadPool()",7, 66, 2, 0
repos/cpp/pytorch/caffe2/core/workspace.cc,"caffe2::Workspace::bookkeeper()",4, 66, 2, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::MyCaffeClass::MyCaffeClass( const std :: string & name)",1, 67, 2, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::MyCaffeClass::tryRun( int)",1, 22, 2, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::MyCaffeClass::run( int numRuns)",10, 69, 6, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::filterMap( const ExportedStatMap & map , const ExportedStatMap & keys)",11, 36, 4, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::TEST( StatsTest , StatsTestClass)",18, 45, 6, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::TEST( StatsTest , StatsTestDuration)",20, 63, 4, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::TEST( StatsTest , StatsTestSimple)",28, 81, 6, 0
repos/cpp/pytorch/caffe2/core/stats_test.cc,"caffe2::TEST( StatsTest , StatsTestStatic)",26, 81, 6, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::Reporter::ReporterInstance::ReporterInstance( int intervalMillis , bool * done , std :: function<void()> f)",11, 80, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::Reporter::start( int64_t intervalMillis , std :: function<void()> f)",3, 77, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::Reporter::~Reporter()",10, 49, 6, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::getContinuationTest( Workspace * , const ExecutionStep & step)",26, 80, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::getShouldStop( const Blob * b)",9, 95, 2, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::WorkspaceIdInjector::InjectWorkspaceId( Workspace * workspace)",17, 81, 6, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::ExecutionStepWrapper( const ExecutionStep * step , Workspace * externalWorkspace , ShouldContinue externalShouldContinue , NetDefMap * netDefs , WorkspaceIdInjector * ws_id_injector)",18, 72, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::CompiledGuard::reset( std :: unique_ptr<CompiledExecutionStep> && compiled)",4, 68, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::CompiledGuard::reset( CompiledExecutionStep * compiledRef)",4, 53, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::CompiledGuard::operator ->()",3, 42, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::CompiledGuard::CompiledGuard()",1, 23, 4, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::step()",3, 32, 2, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::compiled()",9, 40, 6, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::CompiledExecutionStep::CompiledExecutionStep( const ExecutionStep * mainStep , Workspace * externalWorkspace , ShouldContinue externalShouldContinue , NetDefMap * netDefs , WorkspaceIdInjector * ws_id_injector)",83, 81, 6, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecutionStepWrapper::doCompile()",8, 75, 0, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::ExecuteStepRecursive( ExecutionStepWrapper & stepWrapper)",118, 80, 16, 0
repos/cpp/pytorch/caffe2/core/plan_executor.cc,"caffe2::RunPlanOnWorkspace( Workspace * ws , const PlanDef & plan , ShouldContinue shouldContinue)",44, 80, 2, 0
repos/cpp/pytorch/caffe2/core/int8_serialization.cc,"caffe2::int8::Int8TensorCPUSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",44, 73, 4, 0
repos/cpp/pytorch/caffe2/core/int8_serialization.cc,"caffe2::int8::Int8TensorCPUDeserializer::Deserialize( const BlobProto & blob_proto , Blob * blob)",29, 72, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::AsyncSchedulingNet( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",4, 52, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::reset()",4, 35, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::Wait()",6, 53, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::isInlineTask( int parent_id , int child_id) const",10, 75, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::schedule( int task_id , bool run_inline)",154, 80, 14, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::parentCallback( int parent_id)",13, 64, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::pollAndSchedule( int task_id)",18, 79, 8, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::finishRun()",12, 76, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::RunAsync()",43, 79, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_scheduling.cc,"caffe2::AsyncSchedulingNet::~AsyncSchedulingNet()",3, 44, 0, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::GraphDummyOp::Run( int)",4, 40, 2, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::compare_netdefs( const NetDef & net_a , const NetDef & net_b)",14, 69, 4, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::TEST( GraphTest , TestGenerateGraphChain)",23, 55, 2, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::TEST( GraphTest , TestGenerateGraphChainInPlace)",23, 53, 2, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::TEST( GraphTest , TestGenerateGraphBranch)",25, 62, 2, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::TEST( GraphTest , TestReusedInputs)",34, 61, 2, 0
repos/cpp/pytorch/caffe2/core/graph_test.cc,"caffe2::TEST( GraphTest , TestGetPerimeter)",26, 67, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::StringSerializer::StringSerializer()",1, 24, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::StringSerializer::~StringSerializer()",1, 34, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::StringSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",13, 73, 4, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::StringDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",3, 66, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::SerializeBlob( const void * pointer , TypeMeta typeMeta , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor , int chunk_size)",12, 74, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::SerializeBlob( const void * pointer , TypeMeta typeMeta , const string & name)",10, 76, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::SerializeBlob( const Blob & blob , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor , int chunk_size)",7, 73, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::SerializeBlob( const Blob & blob , const string & name)",3, 66, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor)",8, 61, 6, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorSerializer::SerializeWithChunkSize( const void * pointer , TypeMeta typeMeta , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor , int chunk_size)",72, 78, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorSerializer::Serialize( const Tensor & input , const string & name , TensorProto * proto_ptr , size_t chunkBegin , int32_t chunkSize)",159, 81, 12, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorSerializer::StoreDeviceDetail( const Tensor & input , TensorProto * proto)",5, 74, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::DeserializeBlob( const string & content , Blob * result)",7, 60, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::DeserializeBlob( const BlobProto & blob_proto , Blob * result)",20, 77, 4, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::DimsFromTensorProto( const TensorProto & proto)",8, 76, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::NumelFromTensorProto( const TensorProto & tensor_proto)",7, 71, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::GetDataType( const TensorProto & tensor_proto)",11, 68, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorOptionsFromProto( const TensorProto & tensor_proto)",5, 61, 6, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::ContextFromProto( const TensorProto & tensor_proto)",5, 62, 2, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::EmptyTensorFromProto( const TensorProto & tensor_proto)",16, 80, 4, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorDeserializer::Deserialize( const BlobProto & blob_proto , Blob * blob)",22, 80, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorDeserializer::DeserializeToTensor( const TensorProto & tensor_proto , Tensor * tensor)",160, 81, 6, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::TensorDeserializer::Deserialize( const TensorProto & tensor_proto)",5, 74, 0, 0
repos/cpp/pytorch/caffe2/core/blob_serialization.cc,"caffe2::SerializeAsString_EnforceCheck( const google :: protobuf :: MessageLite & msg , const char * error_location)",13, 68, 8, 0
repos/cpp/pytorch/caffe2/core/common.cc,"caffe2::HasCudaRuntime()",3, 42, 2, 0
repos/cpp/pytorch/caffe2/core/common.cc,"caffe2::HasHipRuntime()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/core/common.cc,"caffe2::internal::SetCudaRuntimeFlag()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/core/common.cc,"caffe2::internal::SetHipRuntimeFlag()",3, 39, 2, 0
repos/cpp/pytorch/caffe2/core/common.cc,"caffe2::GetBuildOptions()",7, 69, 2, 0
repos/cpp/pytorch/caffe2/core/context.cc,"caffe2::RandomNumberSeed()",16, 79, 2, 0
repos/cpp/pytorch/caffe2/core/context.cc,"caffe2::CopyBytesImpl( size_t nbytes , const void * src , void * dst)",8, 71, 0, 0
repos/cpp/pytorch/caffe2/core/context.cc,"caffe2::CopyBytesWrapper( size_t nbytes , const void * src , Device src_device , void * dst , Device dst_device)",8, 35, 2, 0
repos/cpp/pytorch/caffe2/core/context.cc,"caffe2::CPUContext::CopyBytesSameDevice( size_t nbytes , const void * src , void * dst)",6, 38, 0, 0
repos/cpp/pytorch/caffe2/core/blob_stats.cc,"caffe2::BlobStatRegistry::get( TypeIdentifier id)",7, 65, 0, 0
repos/cpp/pytorch/caffe2/core/blob_stats.cc,"caffe2::BlobStatRegistry::instance()",4, 49, 0, 0
repos/cpp/pytorch/caffe2/core/blob_stats.cc,"caffe2::BlobStatRegistry::doRegister( TypeIdentifier id , std :: unique_ptr<BlobStatGetter> && v)",9, 77, 2, 0
repos/cpp/pytorch/caffe2/core/blob_stats.cc,"caffe2::BlobStat::sizeBytes( const Blob & blob)",4, 64, 2, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::TensorPrinter::TensorPrinter( const std :: string & tensor_name , const std :: string & file_name , int limit)",20, 68, 4, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::TensorPrinter::~TensorPrinter()",5, 34, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::TensorPrinter::PrintMeta( const Tensor & tensor)",7, 54, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::TensorPrinter::MetaStr( const Tensor & tensor)",10, 59, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::GetTensorType( const void * c)",4, 52, 2, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::GetTypeCallFunction( TypeIdentifier id)",7, 50, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::RegisterTypeCallFunction( TypeIdentifier id , TypeCall c)",3, 63, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::GetTensorInfo( const void * c , size_t * capacity , DeviceOption * device)",13, 70, 2, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::GetTensorInfoFunction( TypeIdentifier id)",7, 58, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::RegisterTensorInfoFunction( TypeIdentifier id , TensorInfoCall c)",3, 71, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::TensorVectorResize( std :: vector<Tensor> & tensors , int size , DeviceType type)",9, 36, 2, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::empty( at :: IntArrayRef dims , at :: TensorOptions options)",6, 64, 0, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::ReinitializeTensor( Tensor * tensor , at :: IntArrayRef dims , at :: TensorOptions options)",34, 87, 4, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::ReinitializeAndCopyFrom( Tensor * t , at :: TensorOptions options , const Tensor & src , bool async)",19, 85, 6, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::Tensor::enforce_invariants()",15, 74, 4, 0
repos/cpp/pytorch/caffe2/core/tensor.cc,"caffe2::TensorStatGetter::sizeBytes( const Blob & blob) const",11, 54, 2, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDATest , HasCudaRuntime)",3, 33, 0, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , TestAllocDealloc)",7, 52, 2, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , TestSetGetDeviceWithoutCaffeMode)",11, 58, 0, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , MemoryPoolAllocateDealloc)",28, 80, 4, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::getStreamForHandle( cublasHandle_t handle)",6, 57, 0, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , TestSameThreadSameObject)",11, 79, 6, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , TestSameThreadTempObject)",32, 81, 4, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , TestSameThreadDifferntObjectIfDifferentDevices)",11, 81, 8, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST_GetStreamAddress( cudaStream_t * ptr)",7, 63, 2, 0
repos/cpp/pytorch/caffe2/core/context_gpu_test.cc,"caffe2::TEST( CUDAContextTest , TestDifferntThreadDifferentobject)",18, 59, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::getStreamCounters()",4, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::AsyncNetBase( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",47, 78, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::handleRunError()",30, 81, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::RunAsync()",5, 32, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::poolGetter( PoolsMap & pools , int device_type , int device_id , int pool_size)",17, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::pool()",6, 46, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::pool( const DeviceOption & device_option)",27, 81, 6, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::stream( int task_id)",17, 78, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::isStreamFree( int task_id , int stream_id) const",5, 68, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::canSchedule( int task_id , const std :: vector<EventStatus> * status , bool * parent_failed)",33, 70, 6, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::canSchedule( int parent_id , int child_id)",10, 62, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::tasksNum() const",3, 37, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::event( int task_id) const",5, 48, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::query( int task_id) const",3, 53, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::children( int task_id) const",4, 68, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::parents( int task_id) const",4, 67, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::getParentCount( int child_id)",5, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::updateParentCount( int child_id)",7, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::testAndSetScheduled( int task_id)",5, 58, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::numOps( int task_id) const",3, 46, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::firstTaskOpId( int task_id) const",3, 53, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::lastTaskOpId( int task_id) const",3, 52, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::firstTaskOp( int task_id) const",3, 67, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::lastTaskOp( int task_id) const",3, 66, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::firstTaskOp( int task_id)",3, 66, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::lastTaskOp( int task_id)",3, 65, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::asyncWait( int task_id , int stream_id , const std :: vector<int> & wait_task_ids) const",13, 51, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::reset()",13, 66, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::handleChainError( int task_id , OperatorBase * op , const char * err_str , bool save_exception)",19, 75, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::run( int task_id , int stream_id)",54, 73, 8, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::finishTasks( const std :: unordered_set<int> & task_ids)",5, 74, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::finalizeEvents()",13, 64, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::GetOperatorStats() const",3, 55, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::GetPerOperatorCost() const",3, 57, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::GetProfReport() const",3, 52, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::AsyncNetBase::~AsyncNetBase()",5, 40, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_base.cc,"caffe2::ExecutionOptions::ExecutionOptions( const std :: shared_ptr<const NetDef> & net_def)",58, 73, 2, 0
repos/cpp/pytorch/caffe2/core/init_denormals.cc,"caffe2::Caffe2SetDenormals( int * , char ** *)",11, 56, 4, 0
repos/cpp/pytorch/caffe2/core/event_gpu_test.cc,"caffe2::TEST( EventCUDATest , EventBasics)",41, 43, 2, 0
repos/cpp/pytorch/caffe2/core/context_test.cc,"caffe2::TEST( CPUContextTest , TestAllocAlignment)",7, 71, 4, 0
repos/cpp/pytorch/caffe2/core/context_test.cc,"caffe2::TEST( CPUContextTest , TestAllocDealloc)",18, 61, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::getCounterForNetName( const std :: string & net_name)",11, 77, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::Tracer( const NetBase * net , const std :: string & net_name , TracingConfig config)",14, 67, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::recordEvent( const TracerEvent & event)",4, 53, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::opTraceName( const OperatorBase * op)",9, 68, 6, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::opBlobsInfo( const OperatorBase & op)",15, 58, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::serializeEvent( const TracerEvent & event)",73, 81, 8, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::linearizeEvents()",33, 79, 8, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::renameThreads()",33, 79, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::setEnabled( bool enabled)",3, 40, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::isEnabled() const",3, 33, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::bumpIter()",3, 25, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::bumpDumpingIter()",3, 32, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::dumpTracingResultAndClearEvents( const std :: string & file_suffix)",21, 79, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::Tracer::~Tracer()",3, 50, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::TracerGuard::init( Tracer * tracer)",4, 41, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::TracerGuard::addArgument()",1, 35, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::TracerGuard::addArgument( TracingField field , const char * value)",15, 71, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::TracerGuard::addArgument( TracingField field , int value)",23, 63, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::TracerGuard::recordEventStart()",10, 77, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::TracerGuard::~TracerGuard()",7, 77, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::extractShardId( const std :: string & name)",17, 78, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::getUniqueShardId( const OperatorDef & op_def)",15, 64, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::isTraceableNetName( const std :: string & net_name)",6, 81, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::hasEnableTracingFlag( const NetBase * net)",6, 69, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::getTracingConfigFromNet( const NetBase * net)",24, 79, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::create( const NetBase * net , const std :: string & net_name)",11, 78, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing.cc,"caffe2::tracing::startIter( const std :: shared_ptr<Tracer> & tracer)",27, 76, 8, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TYPED_TEST( TensorGPUTest , TensorInitializedEmpty)",17, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TYPED_TEST( TensorGPUTest , TensorInitializedNonEmpty)",26, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TYPED_TEST( TensorGPUTest , TensorAlias)",13, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TYPED_TEST( TensorGPUTest , TensorAliasCanUseDifferentShapes)",18, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TYPED_TEST( TensorGPUTest , NoLongerAliasAfterNumelChanges)",17, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TYPED_TEST( TensorGPUDeathTest , CannotAccessDataWhenEmpty)",8, 60, 0, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TEST( TensorConstruction , ReinitializeTensorTest)",10, 69, 2, 0
repos/cpp/pytorch/caffe2/core/blob_gpu_test.cc,"caffe2::TEST( TensorTest , TensorSerializationMultiDevices)",40, 76, 4, 0
repos/cpp/pytorch/caffe2/core/module_test.cc,"caffe2::Caffe2ModuleTestStaticDummyOp::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/module_test.cc,"caffe2::Caffe2ModuleTestStaticDummyOp::type()",3, 26, 2, 0
repos/cpp/pytorch/caffe2/core/module_test.cc,"caffe2::TEST( ModuleTest , StaticModule)",20, 80, 2, 0
repos/cpp/pytorch/caffe2/core/module_test.cc,"caffe2::TEST( ModuleTest , DynamicModule)",23, 72, 2, 0
repos/cpp/pytorch/caffe2/core/common_cudnn.cc,"caffe2::CuDNNWrapper::cudnn_states()",7, 71, 2, 0
repos/cpp/pytorch/caffe2/core/common_cudnn.cc,"caffe2::PrintCuDNNInfo( int * , char ** *)",4, 69, 2, 0
repos/cpp/pytorch/caffe2/core/stats.cc,"caffe2::toMap( const ExportedStatList & stats)",8, 55, 0, 0
repos/cpp/pytorch/caffe2/core/stats.cc,"caffe2::StatRegistry::add( const std :: string & name)",11, 56, 0, 0
repos/cpp/pytorch/caffe2/core/stats.cc,"caffe2::StatRegistry::publish( ExportedStatList & exported , bool reset)",11, 69, 0, 0
repos/cpp/pytorch/caffe2/core/stats.cc,"caffe2::StatRegistry::update( const ExportedStatList & data)",5, 58, 0, 0
repos/cpp/pytorch/caffe2/core/stats.cc,"caffe2::StatRegistry::~StatRegistry()",1, 33, 0, 0
repos/cpp/pytorch/caffe2/core/stats.cc,"caffe2::StatRegistry::get()",4, 36, 0, 0
repos/cpp/pytorch/caffe2/core/init_omp.cc,"caffe2::Caffe2SetOpenMPThreads( int * , char ** *)",15, 80, 2, 0
repos/cpp/pytorch/caffe2/core/init_omp.cc,"caffe2::Caffe2SetMKLThreads( int * , char ** *)",21, 80, 2, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::Transform::PatternMatch( const Graph & graph)",32, 76, 0, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::Transform::TryNeighbors( const Graph & graph , const std :: map<int,std::vector<string>> & neighbors , const std :: vector<bool> & matched , std :: vector<int> * subgraph_ptr , std :: vector<int> * best_subgraph_ptr)",18, 77, 8, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::Transform::PatternMatchHelper( const Graph & graph , const std :: vector<bool> & matched , std :: vector<int> * subgraph_ptr , std :: vector<int> * best_subgraph_ptr)",86, 83, 10, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::Transform::ReplacePattern( const std :: vector<vector<int>> & matches , Graph * graph)",18, 69, 4, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::Transform::ApplyTo( const NetDef & orig_net)",6, 52, 0, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::CreateTransform( string key)",5, 73, 2, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::ApplyTransform( const string & key , const NetDef & netdef)",4, 65, 0, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::average_net_run_duration( const NetDef & netdef , const NetDef & init_netdef , const int warmup_runs , const int main_runs)",39, 78, 4, 0
repos/cpp/pytorch/caffe2/core/transform.cc,"caffe2::ApplyTransformIfFaster( const string & key , const NetDef & netdef , const NetDef & init_netdef , const int warmup_runs , const int main_runs , const double improvement_threshold)",17, 77, 6, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing_test.cc,"caffe2::tracing::testExtractShardId( const string & name , int expectedId)",3, 62, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing_test.cc,"caffe2::tracing::TEST( NetAsyncTracingTest , ExtractShardId)",11, 63, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing_test.cc,"caffe2::tracing::TEST( NetAsyncTracingTest , EveryKIteration)",37, 62, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_tracing_test.cc,"caffe2::tracing::TEST( NetAsyncTracingTest , GlobalTimeSlice)",33, 62, 2, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::transform::Graph::Graph( const NetDef & net)",46, 67, 2, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::transform::Graph::GetSubgraphInput( const std :: vector<int> & match)",4, 67, 0, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::transform::Graph::GetSubgraphOutput( const std :: vector<int> & match)",4, 68, 0, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::transform::Graph::GetSubgraphPerimeterHelper( bool from_children , const std :: vector<int> & match)",26, 79, 8, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::transform::Graph::GetNetDef()",75, 81, 2, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::transform::Graph::DeactivateSubgraph( std :: vector<int> subgraph)",15, 60, 0, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::AddOp( NetDef * netdef_ptr , string op_type , std :: vector<string> inputs , std :: vector<string> outputs)",18, 39, 2, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::MatchStrings( string p , string s)",13, 75, 2, 0
repos/cpp/pytorch/caffe2/core/graph.cc,"caffe2::MatchArguments( const OperatorDef & p_op , const OperatorDef & g_op)",56, 72, 0, 0
repos/cpp/pytorch/caffe2/core/net_simple_refcount.cc,"caffe2::SimpleRefCountNet::SimpleRefCountNet( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",41, 74, 2, 0
repos/cpp/pytorch/caffe2/core/net_simple_refcount.cc,"caffe2::SimpleRefCountNet::Run()",19, 78, 6, 0
repos/cpp/pytorch/caffe2/core/workspace_test.cc,"caffe2::TEST( WorkspaceTest , BlobAccess)",43, 78, 2, 0
repos/cpp/pytorch/caffe2/core/workspace_test.cc,"caffe2::TEST( WorkspaceTest , RunEmptyPlan)",5, 37, 2, 0
repos/cpp/pytorch/caffe2/core/workspace_test.cc,"caffe2::TEST( WorkspaceTest , Sharing)",31, 79, 4, 0
repos/cpp/pytorch/caffe2/core/workspace_test.cc,"caffe2::TEST( WorkspaceTest , BlobMapping)",22, 56, 4, 0
repos/cpp/pytorch/caffe2/core/workspace_test.cc,"caffe2::forEachCheck( std :: initializer_list<Workspace*> workspaces)",9, 73, 0, 0
repos/cpp/pytorch/caffe2/core/workspace_test.cc,"caffe2::TEST( WorkspaceTest , ForEach)",17, 34, 6, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DummySyncOp::DummySyncOp( const OperatorDef & operator_def , Workspace * ws)",2, 62, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DummySyncOp::RunOnDevice()",3, 32, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DummyAsyncOp::DummyAsyncOp( const OperatorDef & operator_def , Workspace * ws)",2, 63, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DummyAsyncOp::RunOnDevice()",3, 32, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DummyAsyncOp::HasAsyncPart() const",3, 39, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DagUtilTestContext::DagUtilTestContext( const std :: string & spec , Workspace * ws)",5, 70, 4, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::DagUtilTestContext::computeChains()",3, 54, 4, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::PrintChains( const dag_utils :: ExecutionChains & chains)",10, 61, 0, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::TEST( DagUtilTest , Empty)",10, 35, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::TEST( DagUtilTest , AllSync)",34, 58, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::TEST( DagUtilTest , AllAsync)",30, 69, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::TEST( DagUtilTest , Mixed0)",34, 72, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::TEST( DagUtilTest , Mixed1)",36, 72, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils_test.cc,"caffe2::TEST( DagUtilTest , Mixed2)",65, 76, 6, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::NetBase::NetBase( const std :: shared_ptr<const NetDef> & def , Workspace *)",63, 79, 10, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::NetBase::RunAsync()",6, 36, 2, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::GetNetObserverCreators()",4, 60, 0, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::defaultOverrides()",13, 77, 2, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::ApplyPotentialExecutorOverride( std :: string * net_type)",17, 75, 6, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::AddGlobalNetObserverCreator( NetObserverCreator creator)",4, 63, 0, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::ClearGlobalNetObservers()",4, 42, 2, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::CreateNet( const NetDef & net_def , Workspace * ws)",4, 70, 0, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::CreateNet( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",23, 80, 4, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::ExecutorHelper::GetPool( const DeviceOption &) const",4, 46, 4, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::ExecutorHelper::GetOperators() const",3, 66, 0, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::ExecutorHelper::GetNumWorkers() const",3, 44, 0, 0
repos/cpp/pytorch/caffe2/core/net.cc,"caffe2::NetBase::TEST_Benchmark( const int warmup_runs , const int main_runs , const bool run_individual)",34, 70, 12, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::Verify( const OperatorDef & def) const",85, 83, 8, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumInputs( int min , int max)",5, 50, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumInputs( int n)",3, 39, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumInputs( std :: function<bool(int)> func)",4, 63, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumInputs( set<int> allowed_input_nums)",6, 61, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumOutputs( int min , int max)",5, 51, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumOutputs( int n)",3, 40, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumOutputs( std :: function<bool(int)> func)",4, 64, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumOutputs( set<int> allowed_output_nums)",6, 63, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NumInputsOutputs( std :: function<bool(int,int)> func)",4, 75, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::OutputCalculator( std :: function<int(int)> calc)",4, 69, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::SameNumberOfOutput()",3, 58, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::AllowInplace( std :: function<bool(int,int)> inplace)",4, 74, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::AllowInplace( set<std::pair<int,int>> inplace)",6, 69, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::AllowOneToOneInplace()",3, 66, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::EnforceInplace( std :: function<bool(int,int)> inplace)",4, 76, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::EnforceInplace( set<std::pair<int,int>> inplace)",6, 71, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::EnforceOneToOneInplace()",3, 68, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::Private()",4, 32, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::InputsCanCrossDevices()",4, 46, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::TensorInferenceFunction( TensorInferenceFunctionType function)",5, 45, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::NeedsAllInputShapes( TensorInferenceFunctionType f)",15, 70, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::InheritOnnxSchema( const std :: string & onnx_schema_name)",4, 77, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::IdenticalTypeAndShape()",6, 71, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::IdenticalTypeAndShapeOfInput( int idx)",8, 74, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::IdenticalTypeAndShapeOfMultipleInputs( const vector<int> & indices)",11, 78, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::IdenticalTypeAndShapeOfInputDim( int idx , int dim)",9, 79, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::ScalarType( :: caffe2 :: TensorProto_DataType dt)",9, 81, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::CostInferenceFunction( CostInferenceFunctionType function)",5, 80, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::DeviceInferenceFunction( DeviceInferenceFunctionType function)",5, 45, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::SetDoc( const string & doc)",4, 48, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::Arg( const char * name , const char * description , bool required)",4, 74, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::Input( const int n , const char * name , const char * description)",7, 84, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::Output( const int n , const char * name , const char * description)",7, 85, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::FillUsing( std :: function<void(OpSchema&)> populator)",6, 74, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::CalculateOutput( int num_input) const",9, 53, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::SparseLengthsFillerHelper( const std :: vector<std::vector<int64_t>> & shapes , size_t value_index , size_t length_index , std :: vector<TensorFiller> * fillers)",9, 76, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::SparseWeightsFillerHelper( const std :: vector<std::vector<int64_t>> & shapes , size_t weight_index , std :: vector<TensorFiller> * fillers)",9, 53, 4, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::SparseSegmentsFillerHelper( const std :: vector<std::vector<int64_t>> & shapes , size_t value_index , size_t segment_index , std :: vector<TensorFiller> * fillers)",13, 81, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::ValueKeyLengthInputFillers( size_t value_index , size_t key_index , size_t length_index)",15, 76, 25, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::WeightedValueKeyLengthInputFillers( size_t value_index , size_t key_index , size_t length_index , size_t weight_index)",18, 81, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::ValueLengthInputFillers( size_t value_index , size_t length_index)",12, 76, 25, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::DisallowInputFillers()",8, 77, 8, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::InputFillers( const std :: vector<std::vector<int64_t>> & shapes) const",4, 61, 4, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchema::SupplyDenseFillers( const std :: vector<std::vector<int64_t>> & shapes)",8, 56, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::operator < <( std :: ostream & out , const OpSchema & schema)",43, 81, 0, 0
repos/cpp/pytorch/caffe2/core/operator_schema.cc,"caffe2::OpSchemaRegistry::map()",4, 54, 0, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::NumCudaDevices()",62, 81, 8, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::SetDefaultGPUID( const int deviceid)",11, 70, 6, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::GetDefaultGPUID()",1, 48, 0, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::CaffeCudaGetDevice()",5, 40, 2, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::CaffeCudaSetDevice( const int id)",3, 40, 0, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::GetGPUIDForPointer( const void * ptr)",22, 69, 4, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::CudaDevicePropWrapper::CudaDevicePropWrapper()",5, 59, 6, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::GetDeviceProperty( const int deviceid)",16, 110, 2, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::DeviceQuery( const int device)",40, 81, 2, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::GetCudaPeerAccessPattern( vector<vector<bool>> * pattern)",19, 67, 2, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::TensorCoreAvailable()",11, 42, 2, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::cublasGetErrorString( cublasStatus_t error)",36, 57, 0, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::curandGetErrorString( curandStatus_t error)",36, 57, 0, 0
repos/cpp/pytorch/caffe2/core/common_gpu.cc,"caffe2::CudaRuntimeFlagFlipper::CudaRuntimeFlagFlipper()",3, 36, 4, 0
repos/cpp/pytorch/caffe2/core/timer_test.cc,"caffe2::TEST( TimerTest , Test)",23, 63, 2, 0
repos/cpp/pytorch/caffe2/core/timer_test.cc,"caffe2::TEST( TimerTest , TestLatency)",22, 82, 2, 0
repos/cpp/pytorch/caffe2/core/event_test.cc,"caffe2::TEST( EventCPUTest , EventBasics)",17, 44, 2, 0
repos/cpp/pytorch/caffe2/core/common_test.cc,"caffe2::TEST( CommonTest , TestStoi)",7, 69, 2, 0
repos/cpp/pytorch/caffe2/core/common_test.cc,"caffe2::TEST( CommonTest , TestStod)",16, 47, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NetTestDummyOp::NetTestDummyOp( const OperatorDef & operator_def , Workspace * ws)",3, 71, 8, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NetTestDummyOp::Run( int)",7, 54, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NetTestDummyOp::HasAsyncPart() const",3, 68, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NetTestDummyOp::SupportsAsyncScheduling() const",3, 68, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::CreateNetTestHelper( Workspace * ws , const vector<string> & input , const vector<string> & output)",26, 41, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , ConstructionNoDeclaredInputOutput)",7, 69, 6, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , ConstructionDeclaredInput)",7, 73, 6, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , ConstructionDeclaredOutput)",7, 74, 6, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DeclaredInputInsufficient)",7, 81, 6, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetDeathTest , DeclaredOutputNotMet)",8, 68, 10, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::testExecution( std :: unique_ptr<NetBase> & net , int num_ops)",8, 65, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::checkChainingAndRun( const char * spec , const dag_utils :: ExecutionChains & expected)",18, 64, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::checkNumChainsAndRun( const char * spec , const int expected_num_chains)",22, 77, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_ChainingForLinearModel)",18, 49, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_ChainingForFork)",23, 61, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_ChainingForForkJoin)",29, 64, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_ChainingForwardBackward)",210, 50, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_ChainingForHogwildModel)",38, 50, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_FailingOperator)",45, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::ExecutorHelperDummyOp::ExecutorHelperDummyOp( const OperatorDef & operator_def , Workspace * ws)",2, 72, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::ExecutorHelperDummyOp::Run( int)",9, 54, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , OperatorWithExecutorHelper)",18, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_OperatorWithDisabledEvent)",36, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , ExecutorOverride)",22, 73, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , AsyncEmptyNet)",22, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , DISABLED_RunAsyncFailure)",34, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , NoTypeNet)",15, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NotFinishingOp::NotFinishingOp( const OperatorDef & operator_def , Workspace * ws)",2, 65, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NotFinishingOp::RunOnDevice()",4, 32, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::NotFinishingOp::HasAsyncPart() const",3, 39, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , PendingOpsAndNetFailure)",26, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::AsyncErrorOp::AsyncErrorOp( const OperatorDef & operator_def , Workspace * ws)",9, 78, 8, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::AsyncErrorOp::RunOnDevice()",26, 76, 10, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::AsyncErrorOp::HasAsyncPart() const",3, 39, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::AsyncErrorOp::~AsyncErrorOp()",5, 29, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::AsyncErrorNet( Workspace * ws , const std :: string & net_name , bool throw_ , bool fail_in_sync)",30, 64, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , AsyncErrorOpTest)",19, 81, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , AsyncErrorTimingsTest)",57, 62, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::SyncErrorOp::SyncErrorOp( const OperatorDef & operator_def , Workspace * ws)",4, 73, 8, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::SyncErrorOp::RunOnDevice()",11, 41, 8, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::SyncErrorOp::~SyncErrorOp()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::ChainErrorNet( Workspace * ws , const std :: string & net_name , bool throw_)",32, 73, 0, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , ChainErrorTest)",9, 58, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::testProfDAGNetErrorCase( bool test_error)",52, 78, 2, 0
repos/cpp/pytorch/caffe2/core/net_test.cc,"caffe2::TEST( NetTest , ProfDAGNetErrorTest)",4, 49, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::AsyncTask( const std :: vector<OperatorBase*> & ops)",8, 74, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::handleChainError( OperatorBase * op , const char * err_str , bool save_exception)",22, 75, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::Run( const ExecutionOptions & options)",48, 74, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::Reset()",6, 26, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::GetDeviceOption() const",3, 50, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::GetFuture()",3, 42, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task.cc,"caffe2::AsyncTask::GetFuture() const",3, 54, 0, 0
repos/cpp/pytorch/caffe2/core/types.cc,"caffe2::TypeMetaToDataType( const TypeMeta & meta)",24, 73, 2, 0
repos/cpp/pytorch/caffe2/core/types.cc,"caffe2::DataTypeToTypeMeta( const TensorProto :: DataType & dt)",21, 70, 0, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::TestInitFunction( int * , char ** *)",4, 39, 0, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::TestFailInitFunction( int * , char ** *)",4, 43, 0, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::TEST( InitTest , TestInitFunctionHasRun)",5, 49, 2, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::TEST( InitTest , CanRerunGlobalInit)",4, 61, 2, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::LateRegisterInitFunction()",4, 64, 6, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::LateRegisterEarlyInitFunction()",4, 66, 6, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::LateRegisterFailInitFunction()",4, 72, 6, 0
repos/cpp/pytorch/caffe2/core/init_test.cc,"caffe2::TEST( InitTest , FailLateRegisterInitFunction)",8, 63, 2, 0
repos/cpp/pytorch/caffe2/core/init_intrinsics_check.cc,"caffe2::QuitIfFeatureUnsupported( const bool cpu_has_feature , const string & feature)",17, 76, 8, 0
repos/cpp/pytorch/caffe2/core/init_intrinsics_check.cc,"caffe2::WarnIfFeatureUnused( const bool cpu_has_feature , const string & feature)",17, 77, 4, 0
repos/cpp/pytorch/caffe2/core/init_intrinsics_check.cc,"caffe2::Caffe2CheckIntrinsicsFeatures( int * , char ** *)",22, 55, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , BasicSchema)",21, 71, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , SpecifiedInputOutput)",21, 69, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , InputOutputRelation)",21, 76, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , SameInputOutput)",20, 77, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , CalculateOutput)",20, 77, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , Inplace)",24, 69, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , TensorInferenceIdentical)",21, 74, 6, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , TensorInferenceArbitrary)",18, 77, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , TestCastSchema)",26, 78, 2, 0
repos/cpp/pytorch/caffe2/core/operator_schema_test.cc,"caffe2::TEST( OperatorSchemaTest , TestCostInference)",20, 81, 6, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTest::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTest::type()",3, 26, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestAndNeverConstructs::JustTestAndNeverConstructs( const OperatorDef & def , Workspace * ws)",4, 68, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestAndNeverConstructs::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestAndNeverConstructs::type()",3, 27, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestAndDoesConstruct::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestAndDoesConstruct::type()",3, 27, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithSomeOutput::Run( int)",4, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithSomeOutput::type()",3, 34, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , DeviceTypeRegistryWorks)",3, 51, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , RegistryWorks)",15, 80, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , RegistryWrongDevice)",12, 63, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , ExceptionWorks)",22, 62, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , FallbackIfEngineDoesNotBuild)",9, 63, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , MultipleEngineChoices)",9, 62, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , CannotUseUninitializedBlob)",9, 60, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , TestParameterAccess)",25, 75, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , CannotAccessParameterWithWrongType)",13, 69, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorDeathTest , DISABLED_CannotAccessRepeatedParameterWithWrongType)",16, 79, 0, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , TestDefaultValue)",6, 79, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , TestSetUp)",12, 60, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , TestSetUpInputOutputCount)",23, 51, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorTest , TestOutputValues)",15, 52, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::GetNetDefForTest()",15, 38, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( NetTest , TestScaffoldingSimpleNet)",12, 52, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( NetTest , TestScaffoldingDAGNet)",13, 52, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::GetFooGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( OperatorGradientRegistryTest , GradientSimple)",30, 72, 6, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( EnginePrefTest , PerOpEnginePref)",18, 76, 6, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( EnginePrefTest , GlobalEnginePref)",27, 74, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( EnginePrefTest , GlobalEnginePrefAndPerOpEnginePref)",17, 64, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( EnginePrefTest , GlobalEnginePrefAndPerOpEnginePrefAndOpDef)",18, 67, 0, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( EnginePrefTest , SetOpEnginePref)",17, 64, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( EnginePrefTest , SetDefaultEngine)",17, 65, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithRequiredArg::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithRequiredArg::type()",3, 38, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( RequiredArg , Basic)",22, 78, 8, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithStandardIsTestArg::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithStandardIsTestArg::type()",3, 44, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( IsTestArg , standard)",23, 71, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithNonStandardIsTestArg::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::JustTestWithNonStandardIsTestArg::type()",3, 47, 4, 0
repos/cpp/pytorch/caffe2/core/operator_test.cc,"caffe2::TEST( IsTestArg , non_standard)",11, 55, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::OperatorBase::OperatorBase( const OperatorDef & operator_def , Workspace * ws)",28, 75, 0, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::compute_input_size_( const std :: vector<c10::IValue> & inputs)",26, 80, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::OperatorBase::OperatorBase( const c10 :: FunctionSchema & fn_schema , std :: vector<c10::IValue> inputs , std :: vector<c10::IValue*> outputs)",11, 74, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::OperatorBase::InputTensorShapes() const",7, 62, 0, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::g_per_op_engine_pref()",4, 66, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::g_global_engine_pref()",5, 70, 6, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::TryCreateC2Operator( const string & key , const OperatorDef & operator_def , Workspace * ws)",23, 76, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::TryCreateC10Operator( const string & key , const OperatorDef & operator_def , Workspace * ws)",6, 63, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::TryCreateOperator( const string & key , const OperatorDef & operator_def , Workspace * ws)",10, 63, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::_CreateOperator( const OperatorDef & operator_def , Workspace * ws)",94, 81, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::OpRegistryKey( const std :: string & op_type , const std :: string & engine)",9, 45, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::SetPerOpEnginePref( const PerOpEnginePrefType & per_op_engine_pref)",23, 73, 0, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::SetGlobalEnginePref( const GlobalEnginePrefType & global_engine_pref)",11, 75, 0, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::SetEnginePref( const PerOpEnginePrefType & per_op_engine_pref , const GlobalEnginePrefType & global_engine_pref)",6, 54, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::SetOpEnginePref( const std :: string & op_type , const CaffeMap<DeviceType,EnginePrefType> & op_pref)",22, 76, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::CreateOperator( const OperatorDef & operator_def , Workspace * ws , int net_position)",19, 75, 6, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::gDeviceTypeRegistry()",4, 73, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::GetGradientForOp( const OperatorDef & def , const vector<GradientWrapper> & g_output)",53, 79, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::InferBlobShapesAndTypes( CaffeMap<string,TensorShape> & blob_desc , const vector<NetDef*> & nets)",148, 80, 8, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::GetTensorShapeOfBlob( const Blob * b)",20, 74, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::InferBlobShapesAndTypesFromWorkspace( Workspace * ws , const vector<NetDef*> & nets)",13, 53, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::InferBlobShapesAndTypesFromMap( const CaffeMap<std::string,std::vector<int64_t>> & blob_dimensions , const vector<NetDef*> & nets)",15, 72, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::InferBlobShapesAndTypesFromMap( const CaffeMap<std::string,std::vector<int64_t>> & blob_dimensions , const CaffeMap<std::string,TensorProto_DataType> & blob_types , const vector<NetDef*> & nets)",24, 72, 4, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::ValidateTensorDevices( OperatorBase & op , const OperatorDef & op_def)",43, 79, 0, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::GetRegisteredOperators()",24, 60, 2, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::SetOperatorLogger( std :: function<void(const OperatorDef&)> tracer)",3, 73, 0, 0
repos/cpp/pytorch/caffe2/core/operator.cc,"caffe2::GetOperatorLogger()",3, 62, 0, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::prune( int node_idx , std :: vector<OpGraphNode> & nodes)",56, 72, 4, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::pruneOpNodeGraph( const std :: vector<OperatorNode> & nodes)",27, 70, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::updateOperatorNodes( std :: vector<OperatorNode> & nodes , const ExecutionChains & chains)",14, 48, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::computeChains( std :: vector<OperatorNode> & orig_nodes)",157, 80, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::computeGroups( std :: vector<OperatorNode> & orig_nodes)",80, 81, 2, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::singleChains( std :: vector<OperatorNode> & nodes)",8, 65, 0, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::prepareOperatorNodes( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",91, 80, 6, 0
repos/cpp/pytorch/caffe2/core/net_dag_utils.cc,"caffe2::dag_utils::prepareChainGraphNodes( const std :: vector<dag_utils::OperatorNode> & operator_nodes , const std :: vector<std::vector<int>> & execution_chains)",46, 83, 2, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"assertTensorEqualsWithType( const caffe2 :: TensorCPU & tensor1 , const caffe2 :: TensorCPU & tensor2)",8, 70, 4, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::assertNear( float value1 , float value2 , float epsilon)",5, 61, 0, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::assertTensorEquals( const TensorCPU & tensor1 , const TensorCPU & tensor2)",14, 78, 0, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::assertTensorListEquals( const std :: vector<std::string> & tensorNames , const Workspace & workspace1 , const Workspace & workspace2)",12, 55, 4, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::getTensor( const caffe2 :: Workspace & workspace , const std :: string & name)",6, 57, 2, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::createTensor( const std :: string & name , caffe2 :: Workspace * workspace)",5, 73, 2, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::createOperator( const std :: string & type , const std :: vector<std::string> & inputs , const std :: vector<std::string> & outputs , caffe2 :: NetDef * net)",15, 45, 4, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::NetMutator::newOp( const std :: string & type , const std :: vector<std::string> & inputs , const std :: vector<std::string> & outputs)",7, 64, 2, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::NetMutator::externalInputs( const std :: vector<std::string> & externalInputs)",7, 54, 4, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::NetMutator::externalOutputs( const std :: vector<std::string> & externalOutputs)",7, 55, 4, 0
repos/cpp/pytorch/caffe2/core/test_utils.cc,"caffe2::testing::NetMutator::setDeviceOptionName( const std :: string & name)",5, 71, 0, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::MiniDBCursor( FILE * f , std :: mutex * mutex)",5, 52, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::~MiniDBCursor()",1, 30, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::Seek( const string &)",3, 72, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::SeekToFirst()",7, 53, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::Next()",26, 78, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::key()",4, 61, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::value()",4, 61, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBCursor::Valid()",1, 43, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBTransaction::MiniDBTransaction( FILE * f , std :: mutex * mutex)",2, 57, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBTransaction::~MiniDBTransaction()",3, 34, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBTransaction::Put( const string & key , const string & value)",10, 75, 8, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDBTransaction::Commit()",6, 42, 6, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDB::MiniDB( const string & source , Mode mode)",16, 79, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDB::~MiniDB()",3, 23, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDB::Close()",6, 26, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDB::NewCursor()",4, 66, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::MiniDB::NewTransaction()",4, 71, 4, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::DBReaderSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor)",20, 71, 2, 0
repos/cpp/pytorch/caffe2/core/db.cc,"caffe2::db::DBReaderDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",7, 77, 0, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::JustTest::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::JustTest::type()",3, 31, 2, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::JustTestCUDA::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::JustTestCUDA::type()",3, 32, 2, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::JustTestCUDNN::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::JustTestCUDNN::type()",3, 32, 2, 0
repos/cpp/pytorch/caffe2/core/operator_gpu_test.cc,"caffe2::TEST( EnginePrefTest , GPUDeviceDefaultPreferredEngines)",16, 77, 4, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventCreateCPU( const DeviceOption & option , Event * event)",3, 64, 0, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventRecordCPU( Event * event , const void * , const char * err_msg)",28, 72, 2, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventFinishCPU( const Event * event)",8, 70, 2, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventWaitCPUCPU( const Event * event , void *)",3, 64, 0, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventQueryCPU( const Event * event)",4, 70, 2, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventErrorMessageCPU( const Event * event)",10, 70, 2, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventSetFinishedCPU( const Event * event , const char * err_msg)",22, 70, 2, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventSetCallbackCPU( Event * event , EventCallbackFunction callback)",10, 73, 0, 0
repos/cpp/pytorch/caffe2/core/event.cc,"caffe2::EventResetCPU( Event * event)",7, 70, 2, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::ParallelNet( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",79, 81, 2, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::RunAsync()",13, 36, 2, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::Wait()",4, 30, 2, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::reset()",3, 28, 0, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::handleRunError()",9, 60, 2, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::poolGetter( PoolsMap & pools , int device_type , int device_id , int pool_size)",17, 57, 2, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::Pool( const DeviceOption & device_option)",27, 81, 6, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::SupportsAsync()",3, 36, 0, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::finishRun()",1, 33, 0, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::ParallelNet::GetOperators() const",3, 63, 0, 0
repos/cpp/pytorch/caffe2/core/net_parallel.cc,"caffe2::GetAsyncTaskGraph( ExecutorHelper * helper , const ExecutionOptions & options)",5, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::BlobTestNonDefaultConstructible::BlobTestNonDefaultConstructible( int x)",1, 53, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::BlobTestFooSerializer::BlobTestFooSerializer()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::BlobTestFooSerializer::~BlobTestFooSerializer()",1, 39, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::BlobTestFooSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",17, 77, 4, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::BlobTestFooDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",4, 70, 8, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , Blob)",18, 74, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobUninitialized)",4, 48, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobWrongType)",9, 74, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobReset)",7, 55, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobMove)",12, 57, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobNonConstructible)",15, 79, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobShareExternalPointer)",8, 68, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , BlobShareExternalObject)",8, 58, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , StringSerialization)",13, 51, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorNonTypedTest , TensorChangeType)",35, 81, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorNonTypedTest , NonDefaultConstructible)",14, 73, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorInitializedEmpty)",17, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorInitializedNonEmpty)",25, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorInitializedZeroDim)",13, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorResizeZeroDim)",26, 75, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorInitializedScalar)",8, 60, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorAlias)",17, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorShareDataRawPointer)",16, 65, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorShareDataRawPointerWithMeta)",17, 69, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , TensorAliasCanUseDifferentShapes)",22, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , NoLongerAliassAfterNumelChanges)",16, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , NoLongerAliasAfterFreeMemory)",15, 71, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , KeepOnShrink)",30, 79, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUTest , MaxKeepOnShrink)",25, 79, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUDeathTest , CannotAccessRawDataWhenEmpty)",6, 63, 0, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TensorCPUDeathTest , CannotAccessDataWhenEmpty)",6, 60, 0, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , TensorNonFundamentalType)",8, 62, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , TensorNonFundamentalTypeClone)",23, 63, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , Tensor64BitDimension)",32, 81, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , UndefinedTensor)",4, 36, 0, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorDeathTest , CannotCastDownLargeDims)",8, 65, 6, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , TensorSerialization_CustomType)",25, 59, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , Half)",39, 81, 6, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorTest , TensorFactory)",8, 63, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( QTensorTest , QTensorSerialization)",42, 80, 6, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::VectorCursor( StringMap * data)",3, 57, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::~VectorCursor()",1, 30, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::Seek( const string &)",1, 52, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::SeekToFirst()",1, 33, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::Next()",3, 25, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::key()",3, 33, 4, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::value()",3, 34, 4, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorCursor::Valid()",3, 33, 4, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::VectorDB( const string & source , db :: Mode mode)",2, 48, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::~VectorDB()",3, 25, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::Close()",1, 27, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::NewCursor()",3, 53, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::NewTransaction()",3, 63, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::registerData( const string & name , StringMap && data)",4, 67, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::VectorDB::getData()",5, 60, 4, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TYPED_TEST( TypedTensorTest , BigTensorSerialization)",66, 78, 8, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyType::DummyType( int n_chunks_init = 0)",1, 79, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyType::serialize( const std :: string & name , const int32_t chunk_id) const",10, 81, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyType::deserialize( const BlobProto &)",3, 52, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyTypeSerializer::DummyTypeSerializer()",1, 27, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyTypeSerializer::~DummyTypeSerializer()",1, 37, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyTypeSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",12, 72, 6, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::DummyTypeDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",4, 66, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( ContentChunks , Serialization)",49, 78, 8, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( CustomChunkSize , BigTensorSerialization)",29, 67, 6, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( QTensor , QTensorSizingTest)",10, 50, 2, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( BlobTest , CastingMessage)",14, 66, 4, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorConstruction , UninitializedCopyTest)",9, 50, 0, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorConstruction , CopyConstructorTest)",15, 48, 0, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorConstruction , MoveAssignmentOpTest)",9, 49, 0, 0
repos/cpp/pytorch/caffe2/core/blob_test.cc,"caffe2::TEST( TensorSerialization , MistakenlySerializingDtypeUninitializedTensor)",28, 79, 2, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::SleepOp::SleepOp( const OperatorDef & operator_def , Workspace * ws)",6, 64, 8, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::SleepOp::RunOnDevice()",12, 75, 6, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::RunNetAndGetDuration( const string & net_def_str , const string & type)",17, 79, 2, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( DAGNetTest , TestDAGNetTiming)",4, 68, 2, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( SimpleNetTest , TestSimpleNetTiming)",4, 71, 2, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( DAGNetTest , TestDAGNetTimingReadAfterRead)",4, 81, 2, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( SimpleNetTest , TestSimpleNetTimingReadAfterRead)",4, 84, 2, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( DAGNetTest , TestDAGNetTimingWriteAfterWrite)",5, 57, 6, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( SimpleNetTest , TestSimpleNetTimingWriteAfterWrite)",5, 60, 6, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( DAGNetTest , TestDAGNetTimingWriteAfterRead)",5, 56, 6, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( SimpleNetTest , TestSimpleNetTimingWriteAfterRead)",5, 59, 6, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( DAGNetTest , TestDAGNetTimingControlDependency)",5, 59, 6, 0
repos/cpp/pytorch/caffe2/core/parallel_net_test.cc,"caffe2::TEST( SimpleNetTest , TestSimpleNetTimingControlDependency)",5, 62, 6, 0
repos/cpp/pytorch/caffe2/core/net_simple.cc,"caffe2::SimpleNet::SimpleNet( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",27, 81, 6, 0
repos/cpp/pytorch/caffe2/core/net_simple.cc,"caffe2::SimpleNet::Run()",25, 78, 6, 0
repos/cpp/pytorch/caffe2/core/net_simple.cc,"caffe2::SimpleNet::RunAsync()",3, 29, 0, 0
repos/cpp/pytorch/caffe2/core/net_simple.cc,"caffe2::PairLargerThan( const std :: pair<A,B> & x , const std :: pair<A,B> & y)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/core/net_simple.cc,"caffe2::SimpleNet::TEST_Benchmark( const int warmup_runs , const int main_runs , const bool run_individual)",191, 81, 8, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TransformDummyOp::Run( int)",4, 40, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::DummyTransform::PatternRule( const Graph & g , const std :: vector<int> & subgraph , int idx)",23, 78, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::DummyTransform::ValidatorRule( const Graph & g , const std :: vector<int> & subgraph)",10, 71, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::DummyTransform::ReplaceRule( const std :: vector<int> & match , Graph * g_ptr)",39, 81, 4, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestPatternMatch)",20, 59, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestReplacePattern)",35, 64, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestTransformApply)",17, 73, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::SortedDummyTransform::SortedDummyTransform()",3, 53, 4, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::SortedDummyTransform::PatternRule( const Graph & g , const std :: vector<int> & subgraph , int idx)",7, 78, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::SortedDummyTransform::ValidatorRule( const Graph & g , const std :: vector<int> & subgraph)",10, 71, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::SortedDummyTransform::ReplaceRule( const std :: vector<int> & match , Graph * g_ptr)",7, 75, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestPatternMatchTypeSortedOrder)",17, 64, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::GeneralDummyTransform::GeneralDummyTransform()",3, 34, 4, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::GeneralDummyTransform::PatternRule( const Graph & g , const std :: vector<int> & subgraph , int idx)",10, 80, 4, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::GeneralDummyTransform::ValidatorRule( const Graph & g , const std :: vector<int> & subgraph)",10, 71, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::GeneralDummyTransform::ReplaceRule( const std :: vector<int> & match , Graph * g_ptr)",7, 75, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestPatternMatchTypeGeneral)",17, 64, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TransformSleepFastOp::Run( int)",4, 64, 4, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TransformSleepSlowOp::Run( int)",4, 65, 4, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TypeSwapTransform::TypeSwapTransform( string old_type , string new_type)",2, 63, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TypeSwapTransform::PatternRule( const Graph & g , const std :: vector<int> & subgraph , int idx)",7, 78, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TypeSwapTransform::ValidatorRule( const Graph & g , const std :: vector<int> & subgraph)",9, 71, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TypeSwapTransform::ReplaceRule( const std :: vector<int> & match , Graph * g_ptr)",6, 75, 2, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::FastToSlowTransform::FastToSlowTransform()",2, 77, 6, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::SlowToFastTransform::SlowToFastTransform()",2, 77, 6, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestApplyTransformIfFasterIsFaster)",18, 78, 6, 0
repos/cpp/pytorch/caffe2/core/transform_test.cc,"caffe2::TEST( TransformTest , TestApplyTransformIfFasterButSlower)",18, 78, 6, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::NetTestDummyOp::NetTestDummyOp( const OperatorDef & operator_def , Workspace * ws)",3, 71, 8, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::NetTestDummyOp::Run( int)",7, 54, 2, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::NetTestDummyOp::HasAsyncPart() const",3, 68, 4, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::NetTestDummyOp::SupportsAsyncScheduling() const",3, 68, 4, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::testExecution( std :: unique_ptr<NetBase> & net , int num_ops)",8, 65, 0, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::checkChainingAndRun( const char * spec , const dag_utils :: ExecutionChains & expected)",17, 64, 4, 0
repos/cpp/pytorch/caffe2/core/net_gpu_test.cc,"caffe2::TEST( NetTest , DISABLED_ChainingForDifferentDevices)",40, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_simple_refcount_test.cc,"caffe2::NetSimpleRefCountTestOp::NetSimpleRefCountTestOp( const OperatorDef & operator_def , Workspace * ws)",2, 74, 2, 0
repos/cpp/pytorch/caffe2/core/net_simple_refcount_test.cc,"caffe2::NetSimpleRefCountTestOp::RunOnDevice()",6, 60, 4, 0
repos/cpp/pytorch/caffe2/core/net_simple_refcount_test.cc,"caffe2::TEST( NetSimpleRefCountTest , TestCorrectness)",33, 74, 2, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::ProfDAGCounters( const std :: shared_ptr<const NetDef> & net_def)",22, 81, 0, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::ReportRunStart()",11, 49, 2, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::AddPerOpStartTime( size_t op_id)",8, 67, 2, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::AddPerOpEndTime( size_t op_id)",8, 65, 2, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::AddPerOpAsyncEndTime( size_t op_id)",8, 71, 2, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::ReportRunEnd()",48, 79, 4, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGCounters::GetReport() const",3, 51, 0, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGReport::hasStats() const",3, 39, 0, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGReport::statsProto( const std :: string & name , const ProfDAGStats & stats , const std :: vector<std::string> & op_extra_info) const",14, 59, 4, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGReport::GetOperatorStats() const",11, 81, 6, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGReport::GetPerOperatorCost() const",15, 78, 10, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGReport::PrintStats()",28, 77, 12, 0
repos/cpp/pytorch/caffe2/core/prof_dag_counters.cc,"caffe2::ProfDAGReport::operator +=( const ProfDAGReport & rhs)",43, 76, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::AsyncTaskGraph( ExecutorHelper * helper , const ExecutionOptions & options)",4, 60, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::CreateNode( int node_id , const std :: vector<OperatorBase*> & ops)",11, 59, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::AddDependency( int child_node_id , const std :: vector<int> & parent_node_ids)",52, 80, 10, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::FreezeGraph()",30, 69, 2, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::ExecuteGraph()",12, 81, 4, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::GetFuture()",4, 47, 0, 0
repos/cpp/pytorch/caffe2/core/net_async_task_graph.cc,"caffe2::AsyncTaskGraph::Reset()",12, 39, 2, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::optimize_inference_net( const NetDef & net , const std :: set<string> & static_blobs)",114, 81, 4, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::ComputeBlobRecyclingForDag( const int size)",5, 54, 2, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::OptimizeNet( const NetDef & net , const std :: vector<string> & heads , const std :: vector<int> & op_indices , const std :: unordered_set<string> & shareable_blob_names , const string & namescope , const std :: unordered_set<string> & dont_share_blob_names , const std :: unordered_map<string,vector<int>> & blob_shapes)",112, 80, 4, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::apply_assignments( const NetDef & net)",24, 71, 8, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::apply_recurrent_blob_assignments( OperatorDef * op)",43, 75, 4, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::has_key( const std :: unordered_map<K,V> & in_map , const K & key)",3, 78, 2, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::has_key( const std :: unordered_set<K> & in_set , const K & key)",3, 75, 2, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::process_op( const NetDef & net , const std :: unordered_set<string> & shareable_blob_names , const string & namescope , const std :: unordered_set<string> & dont_share_blob_names , const std :: unordered_map<string,vector<int>> & blob_shapes , int op_index , std :: vector<std::pair<int,string>> * free_blobs , std :: unordered_set<int> * tokens)",124, 81, 4, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::infer_blob_size( const string & blob_name , const std :: unordered_map<string,vector<int>> & blob_shapes)",13, 68, 6, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::get_blob_or_mapped_blob( const string & blob_name)",8, 67, 2, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::can_use_blob( const string & blob_name , std :: unordered_set<int> * tokens , const DeviceOption & device_option)",16, 68, 4, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::ComputeBlobRecyclingForDag::get_free_blob( const string & blob_name , const std :: unordered_map<string,vector<int>> & blob_shapes , std :: unordered_set<int> * tokens , std :: vector<std::pair<int,string>> * free_blobs , const DeviceOption & device)",57, 73, 10, 0
repos/cpp/pytorch/caffe2/core/memonger.cc,"caffe2::memonger::compute_blob_recycling_for_dag( const NetDef & net , const std :: vector<string> & heads , const std :: vector<int> & op_indices , const std :: unordered_set<string> & shareable_blob_names , const string & namescope , const std :: unordered_set<string> & dont_share_blob_names , const std :: unordered_map<string,vector<int>> & blob_shapes)",18, 66, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NeuralNetOperator::~NeuralNetOperator()",1, 43, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NeuralNetOperator::getName() const",13, 57, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NeuralNetData::~NeuralNetData()",1, 35, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NeuralNetData::getName() const",9, 51, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NNModule::createUniqueDataNode( const std :: string & s)",19, 79, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NNModule::replaceSubgraph( const NNSubgraph & subgraph , const NNGraph :: NodeRef & node , const std :: vector<NNGraph::NodeRef> & node_inputs , const std :: vector<NNGraph::NodeRef> & node_outputs)",35, 72, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::NNModule::deleteSubgraph( const NNSubgraph & subgraph)",3, 60, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::hasProducer( NNGraph :: NodeRef n)",3, 39, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getProducer( NNGraph :: NodeRef n)",11, 68, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::hasConsumer( NNGraph :: NodeRef n)",3, 39, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getConsumers( NNGraph :: NodeRef n)",10, 65, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::hasInputs( NNGraph :: NodeRef n)",3, 38, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getInputs( NNGraph :: NodeRef n)",10, 62, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getOutputs( NNGraph :: NodeRef n)",10, 63, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getName( NNGraph :: NodeRef n)",8, 53, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getInputs( const NNSubgraph & subgraph)",14, 67, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getOutputs( const NNSubgraph & subgraph)",16, 68, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::replaceProducer( NNGraph :: NodeRef tensorNode , NNGraph :: NodeRef newProducer)",15, 80, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::replaceAllUsesWith( NNGraph :: NodeRef oldTensorNode , NNGraph :: NodeRef newTensorNode)",10, 51, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::replaceAsConsumer( NNGraph :: NodeRef oldConsumer , NNGraph :: NodeRef newConsumer)",10, 48, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::createOutput( NNModule * nn , NNGraph :: NodeRef producer , std :: string name)",6, 75, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::getTrackedNodes( repr :: NNCFGraph & cf)",11, 67, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::coalesceInsertedDataDependenciesHelper( repr :: NNModule * m)",24, 74, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::coalesceInsertedDataDependencies( repr :: NNModule * m)",48, 80, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::hasSingleOutputAndConsumer( NNGraph :: NodeRef nodeRef)",6, 62, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::hasUniqueConsumer( NNGraph :: NodeRef nodeRef)",13, 66, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/Representations/NeuralNet.cc,"nom::repr::nn::matchExternalTensorNode()",3, 79, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc,"TEST( BinaryMatch , NoMatch)",6, 65, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc,"TEST( BinaryMatch , AllMatch)",7, 78, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc,"TEST( BinaryMatch , EmptyGraph)",6, 64, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc,"TEST( BinaryMatch , Basic)",23, 75, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc,"TEST( BinaryMatch , RemovedMiddleNode)",31, 75, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/AlgorithmsTest.cc,"TEST( DominatorTree , Test1)",53, 70, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/AlgorithmsTest.cc,"TEST( DominatorTree , Test2)",37, 74, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/AlgorithmsTest.cc,"TEST( Subgraph , InduceEdges)",13, 49, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/AlgorithmsTest.cc,"TEST( Subgraph , InduceEdgesCycle)",13, 49, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , CreateNodeAndEdge)",8, 33, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , DeleteNode)",9, 31, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , DeleteEdge)",7, 33, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , ReplaceEdges)",53, 35, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , HasNode)",36, 68, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , Moves)",21, 45, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , MoveSubgraph)",27, 45, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/GraphTest.cc,"TEST( Basic , DotGenerator)",40, 81, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TopoSortTest.cc,"TEST( TopoSort , Simple)",11, 48, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TopoSortTest.cc,"TEST( TopoSort , DAG)",26, 63, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TopoSortTest.cc,"TEST( TopoSort , Cycle1)",10, 51, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TopoSortTest.cc,"TEST( TopoSort , Cycle2)",14, 51, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TarjansImplTest.cc,"TEST( Tarjans , Simple)",11, 67, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TarjansImplTest.cc,"TEST( Tarjans , WithEdgeStorage)",11, 78, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TarjansImplTest.cc,"TEST( Tarjans , DAG)",5, 47, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TarjansImplTest.cc,"TEST( Tarjans , Cycle)",5, 47, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/TarjansImplTest.cc,"TEST( Tarjans , Random)",16, 53, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/MatchTest.cc,"TEST( Match , Basic)",31, 55, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"to_string( T value)",5, 33, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"createGraph()",22, 55, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"createGraphWithCycle()",23, 55, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"BBPrinter( typename nom :: repr :: NNCFGraph :: NodeRef node)",29, 92, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"cfgEdgePrinter( typename nom :: repr :: NNCFGraph :: EdgeRef edge)",9, 97, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"NNPrinter( typename nom :: repr :: NNGraph :: NodeRef node)",15, 108, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"createTestNode( nom :: Graph<TestClass> & g)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/test_util.cc,"TestNodePrinter( nom :: Graph<TestClass> :: NodeRef)",6, 52, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/NeuralNetTest.cc,"TEST( NeuralNetGraph , ReplaceGraph)",82, 79, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::reset()",3, 28, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::testMatchPredicate( const Criteria & criteria)",5, 66, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::any()",3, 24, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::Tree( const Criteria & root , const std :: vector<TestMatchGraph::NodeRef> & children = { } , int count = 1)",11, 74, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::NonTerminal( const Criteria & root , int count = 1)",4, 75, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::TestGraphNodePrinter( TestGraph :: NodeRef node)",6, 57, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::DataFlowTestGraph::DataFlowTestGraph()",39, 77, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::DataFlowTestGraphCriteria::DataFlowTestGraphCriteria()",31, 73, 8, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::getInNode( TestGraph :: NodeRef node , int index)",3, 67, 0, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"nom::matcher::isSubgraphMatch( TestGraph :: NodeRef nodeRef , const TestMatchGraph :: NodeRef & criteria , bool invertGraphTraversal = true)",7, 72, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , IsNodeMatch)",11, 67, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , IsSubtreeMatch)",52, 67, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , IsSubtreeMatchRepeated)",86, 78, 6, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , DagMatching)",72, 67, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , DagMatchingMultiEdges)",34, 56, 4, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , DagMatchingRandomLargeGraph)",62, 74, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , IsSubtreeMatchRealistic)",12, 57, 2, 0
repos/cpp/pytorch/caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,"TEST( SubgraphMatcher , ReplaceGraphRealistic)",74, 81, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_test.cc,"caffe2::TEST( MPITest , TestMPIBroadcast)",27, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_test.cc,"caffe2::TEST( MPITest , TestMPIReduce)",34, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_test.cc,"caffe2::TEST( MPITest , TestMPIAllgather)",31, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_test.cc,"caffe2::TEST( MPITest , TestMPIAllreduce)",30, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_test.cc,"caffe2::TEST( MPITest , TestInPlaceMPIAllreduce)",24, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_test.cc,"main( int argc , char ** argv)",9, 64, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_gpu_test.cc,"caffe2::TEST( MPITest , TestMPIBroadcast)",27, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_gpu_test.cc,"caffe2::TEST( MPITest , TestMPIReduce)",35, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_gpu_test.cc,"caffe2::TEST( MPITest , TestMPIAllgather)",33, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_gpu_test.cc,"caffe2::TEST( MPITest , TestMPIAllreduce)",32, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_gpu_test.cc,"caffe2::TEST( MPITest , TestInPlaceMPIAllreduce)",25, 69, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_gpu_test.cc,"main( int argc , char ** argv)",9, 64, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::MPIMutex()",3, 26, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::GlobalMPIComm()",3, 27, 0, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::SetGlobalMPIComm( MPI_Comm new_comm)",6, 43, 0, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::MPICommSize( MPI_Comm comm)",5, 46, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::MPICommRank( MPI_Comm comm)",5, 46, 2, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::AssimilateComm( MPI_Comm intra , MPI_Comm inter)",47, 78, 8, 0
repos/cpp/pytorch/caffe2/mpi/mpi_common.cc,"caffe2::MPISetupPeers( const int replicas , const string & role , const string & job_path)",83, 81, 4, 0
repos/cpp/pytorch/caffe2/test/caffe2_gtest_main.cc,"main( int argc , char ** argv)",6, 57, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_unmask_ops_test.cc,"caffe2::AddScalarInput( const DataT & value , const string & name , Workspace * ws , bool isEmpty = false)",16, 55, 4, 0
repos/cpp/pytorch/caffe2/operators/boolean_unmask_ops_test.cc,"caffe2::TEST( BooleanUnmaskTest , Test)",32, 62, 2, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::StringToPadMode( const string & mode)",11, 50, 4, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::PadImageOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",136, 79, 12, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::PadImageOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",84, 71, 12, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::PadImageGradientOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",80, 73, 0, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::PadImageGradientOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",87, 73, 0, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::PadImageOp<float,CPUContext>::PadTensorInference( const OperatorDef & def , const vector<TensorShape> & in)",5, 76, 0, 0
repos/cpp/pytorch/caffe2/operators/pad_op.cc,"caffe2::GetPadImageGradient::GetGradientDefs()",4, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils_test.cc,"caffe2::TEST( TextFileReaderUtilsTest , TokenizeTest)",104, 78, 55, 0
repos/cpp/pytorch/caffe2/operators/pack_segments.cc,"caffe2::PackSegmentsOp<CPUContext>::DoRunWithType()",5, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/pack_segments.cc,"caffe2::PackSegmentsOp<CPUContext>::DoRunWithType2()",90, 78, 8, 0
repos/cpp/pytorch/caffe2/operators/pack_segments.cc,"caffe2::UnpackSegmentsOp<CPUContext>::DoRunWithType()",5, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/pack_segments.cc,"caffe2::UnpackSegmentsOp<CPUContext>::DoRunWithType2()",42, 86, 8, 0
repos/cpp/pytorch/caffe2/operators/pack_segments.cc,"caffe2::GetPackSegmentsGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/pack_segments.cc,"caffe2::GetUnpackSegmentsGradient::GetGradientDefs()",4, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeAveragePoolGradient1D<float,StorageOrder::NCHW>( const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & dY_arr , EigenArrayMap<float> * dX_arr)",9, 62, 0, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeAveragePoolGradient1D<float,StorageOrder::NHWC>( const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & dY_arr , EigenArrayMap<float> * dX_arr)",11, 62, 0, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeAveragePoolGradient2D<float,StorageOrder::NCHW>( const int , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & dY_arr , EigenArrayMap<float> * dX_arr)",12, 62, 0, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeAveragePoolGradient2D<float,StorageOrder::NHWC>( const int W , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & dY_arr , EigenArrayMap<float> * dX_arr)",16, 62, 0, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeAveragePoolGradient3D<float,StorageOrder::NCHW>( const int H , const int , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & dY_arr , EigenArrayMap<float> * dX_arr)",17, 68, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeAveragePoolGradient3D<float,StorageOrder::NHWC>( const int H , const int W , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & dY_arr , EigenArrayMap<float> * dX_arr)",21, 69, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::RunAveragePoolGradient1D( const int N , const int C , const int X_size , const int Y_size , const int kernel , const int stride , const int pad , const bool count_include_pad , const T * dY , T * dX)",34, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::RunAveragePoolGradient2D( const int N , const int C , const int X_H , const int X_W , const int Y_H , const int Y_W , const int kernel_h , const int kernel_w , const int stride_h , const int stride_w , const int pad_t , const int pad_l , const bool count_include_pad , const T * dY , T * dX)",49, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::RunAveragePoolGradient3D( const int N , const int C , const int X_D , const int X_H , const int X_W , const int Y_D , const int Y_H , const int Y_W , const int kernel_d , const int kernel_h , const int kernel_w , const int stride_d , const int stride_h , const int stride_w , const int pad_p , const int pad_t , const int pad_l , const bool count_include_pad , const T * dY , T * dX)",58, 80, 14, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeMaxPoolGradient1D<float,StorageOrder::NCHW>( const int l , const int r , const int y , const ConstEigenArrayMap<float> & dY_arr , const ConstEigenArrayMap<float> & X_arr , const ConstEigenArrayMap<float> & Y_arr , EigenArrayMap<float> * dX_arr)",11, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeMaxPoolGradient1D<float,StorageOrder::NHWC>( const int l , const int r , const int y , const ConstEigenArrayMap<float> & dY_arr , const ConstEigenArrayMap<float> & X_arr , const ConstEigenArrayMap<float> & Y_arr , EigenArrayMap<float> * dX_arr)",13, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeMaxPoolGradient2D<float,StorageOrder::NCHW>( const int , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & dY_arr , const ConstEigenArrayMap<float> & X_arr , const ConstEigenArrayMap<float> & Y_arr , EigenArrayMap<float> * dX_arr)",14, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeMaxPoolGradient2D<float,StorageOrder::NHWC>( const int W , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & dY_arr , const ConstEigenArrayMap<float> & X_arr , const ConstEigenArrayMap<float> & Y_arr , EigenArrayMap<float> * dX_arr)",19, 72, 10, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeMaxPoolGradient3D<float,StorageOrder::NCHW>( const int H , const int , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & dY_arr , const ConstEigenArrayMap<float> & X_arr , const ConstEigenArrayMap<float> & Y_arr , EigenArrayMap<float> * dX_arr)",20, 78, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::ComputeMaxPoolGradient3D<float,StorageOrder::NHWC>( const int H , const int W , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & dY_arr , const ConstEigenArrayMap<float> & X_arr , const ConstEigenArrayMap<float> & Y_arr , EigenArrayMap<float> * dX_arr)",24, 74, 12, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::RunMaxPoolGradient1D( const int N , const int C , const int X_size , const int Y_size , const int kernel , const int stride , const int pad , const T * dY , const T * X , const T * Y , T * dX)",45, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::RunMaxPoolGradient2D( const int N , const int C , const int X_H , const int X_W , const int Y_H , const int Y_W , const int kernel_h , const int kernel_w , const int stride_h , const int stride_w , const int pad_t , const int pad_l , const T * dY , const T * X , const T * Y , T * dX)",57, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::RunMaxPoolGradient3D( const int N , const int C , const int X_D , const int X_H , const int X_W , const int Y_D , const int Y_H , const int Y_W , const int kernel_d , const int kernel_h , const int kernel_w , const int stride_d , const int stride_h , const int stride_w , const int pad_p , const int pad_t , const int pad_l , const T * dY , const T * X , const T * Y , T * dX)",66, 77, 14, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::AveragePoolFunctor<CPUContext>::GlobalPoolingBackward<float,StorageOrder::NCHW>( const int N , const int C , const int HxW , const float * dY , const float * , const float * , float * dX , CPUContext *) const",18, 54, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::AveragePoolFunctor<CPUContext>::GlobalPoolingBackward<float,StorageOrder::NHWC>( const int N , const int C , const int HxW , const float * dY , const float * , const float * , float * dX , CPUContext *) const",18, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::AveragePoolFunctor<CPUContext>::Backward( const int N , const int C , const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<int> & kernel , const std :: vector<int> & , const std :: vector<int> & stride , const std :: vector<int> & pads , const T * dY , const T * , const T * , T * dX , CPUContext *) const",79, 54, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::MaxPoolFunctor<CPUContext>::GlobalPoolingBackward<float,StorageOrder::NCHW>( const int N , const int C , const int HxW , const float * dY , const float * X , const float * Y , float * dX , CPUContext *) const",18, 75, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::MaxPoolFunctor<CPUContext>::GlobalPoolingBackward<float,StorageOrder::NHWC>( const int N , const int C , const int HxW , const float * dY , const float * X , const float * Y , float * dX , CPUContext *) const",22, 81, 10, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::MaxPoolFunctor<CPUContext>::Backward( const int N , const int C , const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<int> & kernel , const std :: vector<int> & , const std :: vector<int> & stride , const std :: vector<int> & pads , const T * dY , const T * X , const T * Y , T * dX , CPUContext *) const",82, 54, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_gradient_op.cc,"caffe2::GetPoolGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_linear_op.cc,"caffe2::ElementwiseLinearOp<float,CPUContext>::RunOnDevice()",30, 61, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_linear_op.cc,"caffe2::ElementwiseLinearGradientOp<float,CPUContext>::RunOnDevice()",37, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_linear_op.cc,"caffe2::GetElementwiseLinearGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op.cc,"caffe2::LRNOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",61, 80, 10, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op.cc,"caffe2::LRNOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",51, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op.cc,"caffe2::LRNGradientOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",101, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op.cc,"caffe2::LRNGradientOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",74, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op.cc,"caffe2::GetLRNGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::arg_sort( const TDATA * data , TIDX * idx , const size_t N , bool reverse)",11, 76, 0, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::LambdaRankNdcgOp<float,CPUContext>::ResizeInvLogITensor( int size)",19, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::LambdaRankNdcgOp<float,CPUContext>::ComputeDiscounts( int * idx , int N)",10, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::LambdaRankNdcgOp<float,CPUContext>::LambdaRankNdcgSession( int start_index , int end_index , const Tensor & y , const Tensor & r , Tensor ** dy)",100, 80, 17, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::LambdaRankNdcgOp<float,CPUContext>::RunOnDevice()",22, 65, 8, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::LambdaRankNdcgGradientOp<float,CPUContext>::RunOnDevice()",28, 73, 8, 0
repos/cpp/pytorch/caffe2/operators/listwise_l2r_op.cc,"caffe2::GetLambdaRankNdcgGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/reduction_ops.cc,"caffe2::GetSumElementsGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/reduction_ops.cc,"caffe2::GetRowwiseMaxGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/reduction_ops.cc,"caffe2::GetColwiseMaxGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/reduction_ops.cc,"caffe2::__attribute__((__no_sanitize__('float-divide-by-zero')))",17, 78, 10, 0
repos/cpp/pytorch/caffe2/operators/reduction_ops.cc,"caffe2::MaxReductionGradientOp<T,Context,ROWWISE>::RunOnDevice()",55, 66, 0, 0
repos/cpp/pytorch/caffe2/operators/roi_align_rotated_gradient_op.cc,"caffe2::GetRoIAlignRotatedGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/upsample_op.cc,"caffe2::UpsampleBilinearOp<float,CPUContext>::RunOnDevice()",59, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/upsample_op.cc,"caffe2::UpsampleBilinearGradientOp<float,CPUContext>::RunOnDevice()",65, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/upsample_op.cc,"caffe2::GetUpsampleBilinearGradient::GetGradientDefs()",16, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Workspace * ws)",14, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_test.cc,"caffe2::TEST( UtilityOpTest , testReshapeWithScalar)",18, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/mod_op.cc,"caffe2::ModOp<CPUContext>::DoRunWithType()",17, 65, 2, 0
repos/cpp/pytorch/caffe2/operators/instance_norm_op.cc,"caffe2::InstanceNormOp<T,Context>::RunOnDeviceWithOrderNHWC()",50, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/instance_norm_op.cc,"caffe2::InstanceNormOp<T,Context>::RunOnDeviceWithOrderNCHW()",42, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op.cc,"caffe2::ComputeStartIndex( const TensorCPU & tensor , const std :: vector<int> & index)",12, 51, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op.cc,"caffe2::GetSubTensorView( const TensorCPU & tensor , int dim0_start_index)",20, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op.cc,"caffe2::utils::ComputeAllAnchors( const TensorCPU & anchors , int height , int width , float feat_stride)",55, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op.cc,"caffe2::utils::ComputeSortedAnchors( const Eigen :: Map<const ERArrXXf> & anchors , int height , int width , float feat_stride , const vector<int> & order)",38, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op.cc,"caffe2::GenerateProposalsOp<CPUContext>::ProposalsForOneImage( const Eigen :: Array3f & im_info , const Eigen :: Map<const ERArrXXf> & anchors , const utils :: ConstTensorView<float> & bbox_deltas_tensor , const utils :: ConstTensorView<float> & scores_tensor , ERArrXXf * out_boxes , EArrXf * out_probs) const",109, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op.cc,"caffe2::GenerateProposalsOp<CPUContext>::RunOnDevice()",86, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::RowwiseMaxAndArg( const float * mat , int32_t N , int32_t D , float * rowMax , int32_t * argMax)",12, 57, 2, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::ColwiseMaxAndArg( const float * mat , int32_t N , int32_t D , float * colMax , int32_t * argMax)",12, 57, 2, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::ViterbiPathOp::ViterbiPathOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::ViterbiPathOp::GatherRow( const TensorCPU & data , int32_t rowIndex , int32_t block_size , int32_t block_bytesize , TensorCPU * outRow)",14, 75, 4, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::ViterbiPathOp::AddColToMat( const TensorCPU & mat , const TensorCPU & col , TensorCPU * result)",17, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::ViterbiPathOp::RunOnDevice()",68, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::SwapBestPathOp::SwapBestPathOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/crf_viterbi_op.cc,"caffe2::SwapBestPathOp::RunOnDevice()",41, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/tan_op.cc,"caffe2::TanGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/tan_op.cc,"caffe2::GetTanGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op.cc,"caffe2::TensorInferenceForBatchMatMul( const OperatorDef & def , const vector<TensorShape> & in)",81, 82, 4, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op.cc,"caffe2::CostInferenceForBatchMatMul( const OperatorDef & def , const vector<TensorShape> & in)",29, 69, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op.cc,"caffe2::GetBatchMatMulGradient::GetGradientDefs()",94, 77, 43, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op.cc,"caffe2::GetBatchMatMulGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/loss_op.cc,"caffe2::GetAveragedLossGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/expand_squeeze_dims_op.cc,"caffe2::GetSqueezeGradient::GetGradientDefs()",4, 73, 8, 0
repos/cpp/pytorch/caffe2/operators/expand_squeeze_dims_op.cc,"caffe2::GetExpandDimsGradient::GetGradientDefs()",4, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops.cc,"caffe2::GetNHWC2NCHWGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops.cc,"caffe2::GetNCHW2NHWCGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNOrderSwithOpBase::CuDNNOrderSwithOpBase( Args && ... args)",6, 60, 6, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNOrderSwithOpBase::~CuDNNOrderSwithOpBase()",4, 58, 4, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNOrderSwithOpBase::SetTensorDescriptor( const cudnnDataType_t data_type , const StorageOrder order , const std :: vector<int> & data_dims , cudnnTensorDescriptor_t data_desc) const",34, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNNHWC2NCHWOp::CuDNNNHWC2NCHWOp( Args && ... args)",2, 62, 6, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNNHWC2NCHWOp::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNNHWC2NCHWOp::DoRunWithType()",31, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNNCHW2NHWCOp::CuDNNNCHW2NHWCOp( Args && ... args)",2, 62, 6, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNNCHW2NHWCOp::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/order_switch_ops_cudnn.cc,"caffe2::CuDNNNCHW2NHWCOp::DoRunWithType()",31, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_test.cc,"caffe2::TEST( UtilsNMSTest , TestNMS)",40, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_test.cc,"caffe2::TEST( UtilsNMSTest , TestNMS1)",50, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_test.cc,"caffe2::TEST( UtilsNMSTest , TestSoftNMS)",114, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_test.cc,"caffe2::TEST( UtilsNMSTest , TestNMSRotatedAngle0)",49, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_test.cc,"caffe2::TEST( UtilsNMSTest , TestSoftNMSRotatedAngle0)",123, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_test.cc,"caffe2::TEST( UtilsNMSTest , RotatedBBoxOverlaps)",49, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/normalize_op.cc,"caffe2::NormalizeGradientOp<T,Context>::DoNormalize( const T * xData , const T * gOutData , T * gInData , const int m , const int n , const int sf)",26, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/normalize_op.cc,"caffe2::GetNormalizeGradient::GetGradientDefs()",8, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/moments_op.cc,"caffe2::MomentsGradientOp<T,Context>::Compute( const std :: vector<int> & dY_dims , const std :: vector<int> & dX_dims , const T * dmean_data , const T * dvariance_data , const T * X_data , const T * mean_data , T * dX_data)",27, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/moments_op.cc,"caffe2::GetMomentsGradient::GetGradientDefs()",7, 60, 8, 0
repos/cpp/pytorch/caffe2/operators/transpose_op.cc,"caffe2::GetTransposeGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/transpose_op.cc,"caffe2::GetTransposeGradient::GetGradientDefs()",15, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/lengths_top_k_op.cc,"caffe2::LengthsTopKOp<T,Context>::RunOnDevice()",59, 83, 2, 0
repos/cpp/pytorch/caffe2/operators/lengths_top_k_op.cc,"caffe2::LengthsTopKGradientOp<T,Context>::RunOnDevice()",36, 80, 10, 0
repos/cpp/pytorch/caffe2/operators/lengths_top_k_op.cc,"caffe2::GetLengthsTopKGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/create_scope_op.cc,"caffe2::CreateScopeOp<CPUContext>::RunOnDevice()",5, 68, 2, 0
repos/cpp/pytorch/caffe2/operators/create_scope_op.cc,"caffe2::HasScopeOp<CPUContext>::RunOnDevice()",8, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/mean_op.cc,"caffe2::GetMeanGradient::GetGradientDefs()",8, 66, 8, 0
repos/cpp/pytorch/caffe2/operators/softmax_op.cc,"caffe2::SoftmaxOp<float,CPUContext>::RunOnDevice()",41, 92, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op.cc,"caffe2::SoftmaxGradientOp<float,CPUContext>::RunOnDevice()",41, 92, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op.cc,"caffe2::GetSoftmaxGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/logit_op.cc,"caffe2::LogitFunctor<CPUContext>::operator ( )( const int size , const T * X , T * Y , CPUContext *) const",9, 80, 0, 0
repos/cpp/pytorch/caffe2/operators/logit_op.cc,"caffe2::LogitGradientOp<float,CPUContext>::RunOnDevice()",16, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/logit_op.cc,"caffe2::GetLogitGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/exp_op.cc,"caffe2::GetExpGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/operator_fallback_gpu_test.cc,"caffe2::IncrementByOneOp::IncrementByOneOp( Args && ... args)",2, 61, 6, 0
repos/cpp/pytorch/caffe2/operators/operator_fallback_gpu_test.cc,"caffe2::IncrementByOneOp::RunOnDevice()",11, 59, 4, 0
repos/cpp/pytorch/caffe2/operators/operator_fallback_gpu_test.cc,"caffe2::TEST( OperatorFallbackTest , IncrementByOneOp)",21, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/operator_fallback_gpu_test.cc,"caffe2::TEST( OperatorFallbackTest , GPUIncrementByOneOp)",24, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/perplexity_op.cc,"caffe2::PerplexityOp<float,CPUContext>::RunOnDevice()",16, 62, 2, 0
repos/cpp/pytorch/caffe2/operators/glu_op.cc,"caffe2::sigmoid( const float x)",8, 32, 4, 0
repos/cpp/pytorch/caffe2/operators/glu_op.cc,"caffe2::GluOp<float,CPUContext>::ComputeGlu( const int M , const int split_dim , const int N , const float * Xdata , float * Ydata)",24, 50, 6, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::CastHelper::call( SrcType data)",3, 39, 4, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::CastHelper<std::string,SrcType>::call( SrcType data)",3, 42, 2, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::CastOp<CPUContext>::DoRunWithType()",12, 65, 2, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::CastOp<CPUContext>::SetBody( TensorProto_DataType to)",47, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::CastOp<CPUContext>::DoRunWithDstType()",14, 46, 0, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::GetCastGradient::GetGradientDefs()",26, 108, 4, 0
repos/cpp/pytorch/caffe2/operators/cast_op.cc,"caffe2::GetCastGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeIterator::TreeIterator( const std :: vector<std::string> & fields)",64, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeIterator::advance( const std :: vector<const TLength*> & lengths , std :: vector<TOffset> & offsets , std :: vector<TOffset> & sizes , std :: vector<TOffset> & limits , TOffset num)",39, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeWalker::TreeWalker( const vector<const Blob*> & inputs , TreeCursor & cursor)",18, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeWalker::advance()",4, 69, 2, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeWalker::fieldDim( int fieldId) const",5, 63, 0, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeWalker::fieldPtr( int fieldId) const",5, 69, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeWalker::gatherLengthData()",12, 52, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeWalker::gatherSizeLimits()",8, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CreateTreeCursorOp::CreateTreeCursorOp( Args && ... args)",3, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CreateTreeCursorOp::RunOnDevice()",5, 76, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::GetCursorOffsetOp::GetCursorOffsetOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::GetCursorOffsetOp::RunOnDevice()",9, 72, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ResetCursorOp::ResetCursorOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ResetCursorOp::RunOnDevice()",6, 72, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CheckDatasetConsistencyOp::CheckDatasetConsistencyOp( Args && ... args)",3, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CheckDatasetConsistencyOp::RunOnDevice()",49, 71, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::PackRecordsOp::PackRecordsOp( Args && ... args)",3, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::PackRecordsOp::RunOnDevice()",33, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::UnPackRecordsOp::UnPackRecordsOp( Args && ... args)",3, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::UnPackRecordsOp::RunOnDevice()",65, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::UnPackRecordsOp::getShapeAndMetaFromInput( std :: vector<std::vector<int64_t>> & outputDims , std :: vector<const TypeMeta*> & metas)",19, 74, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::UnPackRecordsOp::getShapeAndMetaFromPrototypeBlobs( std :: vector<std::vector<int64_t>> & outputDims , std :: vector<const TypeMeta*> & metas)",13, 53, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ReadNextBatchOp::ReadNextBatchOp( Args && ... args)",6, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ReadNextBatchOp::RunOnDevice()",63, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ComputeOffsetOp::ComputeOffsetOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ComputeOffsetOp::RunOnDevice()",42, 78, 10, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::SortAndShuffleOp::SortAndShuffleOp( Args && ... args)",7, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::SortAndShuffleOp::RunOnDevice()",69, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ReadRandomBatchOp::ReadRandomBatchOp( Args && ... args)",6, 81, 12, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ReadRandomBatchOp::RunOnDevice()",79, 79, 12, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::AppendOp::AppendOp( Args && ... args)",2, 58, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::AppendOp::RunOnDevice()",22, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::AtomicAppendOp::AtomicAppendOp( Args && ... args)",2, 58, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::AtomicAppendOp::RunOnDevice()",42, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CreateTensorVectorOp::RunOnDevice()",5, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TensorVectorSizeOp::RunOnDevice()",8, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::ConcatTensorVectorOp::RunOnDevice()",30, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CollectTensorOp::CollectTensorOp( Args && ... args)",7, 73, 12, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::CollectTensorOp::RunOnDevice()",50, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TrimDatasetOp::TrimDatasetOp( Args && ... args)",6, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TrimDatasetOp::RunOnDevice()",20, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeCursorSerializer::TreeCursorSerializer()",1, 28, 2, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeCursorSerializer::~TreeCursorSerializer()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeCursorSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",35, 77, 10, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::TreeCursorDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",31, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::SharedTensorVectorPtrSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor)",18, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/dataset_ops.cc,"caffe2::dataset_ops::SharedTensorVectorPtrDeserializer::Deserialize( const BlobProto & , Blob * blob)",7, 63, 2, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::BatchOneHotOp<CPUContext>::DoRunWithType()",39, 72, 6, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::TensorInferenceForBatchOneHot( const OperatorDef & , const vector<TensorShape> & in)",9, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::TensorInferenceForBucketBatchOneHot( const OperatorDef & , const vector<TensorShape> & in)",9, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::CostInferenceForBatchOneHot( const OperatorDef & def , const vector<TensorShape> & in)",20, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::OneHotOp<CPUContext>::DoOneHotOp( int64_t batch_size , int64_t index_size , const Tensor & indices , Tensor * one_hots)",15, 66, 2, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::BatchBucketOneHotOp<CPUContext>::RunOnDevice()",60, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::SegmentOneHotOp::SegmentOneHotOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/one_hot_ops.cc,"caffe2::SegmentOneHotOp::RunOnDevice()",32, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/lp_pool_op.cc,"caffe2::LpPoolFunctor::LpPoolFunctor( const OperatorBase &)",1, 58, 2, 0
repos/cpp/pytorch/caffe2/operators/lp_pool_op.cc,"caffe2::PoolOp<float,CPUContext,LpPoolFunctor>::RunOnDeviceWithOrderNCHW()",44, 78, 14, 0
repos/cpp/pytorch/caffe2/operators/lp_pool_op.cc,"caffe2::PoolOp<float,CPUContext,LpPoolFunctor>::RunOnDeviceWithOrderNHWC()",47, 76, 0, 0
repos/cpp/pytorch/caffe2/operators/lp_pool_op.cc,"caffe2::PoolGradientOp<float,CPUContext,LpPoolFunctor>::RunOnDeviceWithOrderNCHW()",57, 72, 14, 0
repos/cpp/pytorch/caffe2/operators/lp_pool_op.cc,"caffe2::PoolGradientOp<float,CPUContext,LpPoolFunctor>::RunOnDeviceWithOrderNHWC()",59, 80, 22, 0
repos/cpp/pytorch/caffe2/operators/lp_pool_op.cc,"caffe2::GetPoolGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops.cc,"caffe2::FloatToHalfOp<CPUContext>::RunOnDevice()",14, 66, 2, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops.cc,"caffe2::HalfToFloatOp<CPUContext>::RunOnDevice()",13, 63, 2, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops.cc,"caffe2::Float16ConstantFillOp::RunOnDevice()",12, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops.cc,"caffe2::Float16UniformFillOp::RunOnDevice()",19, 66, 8, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops.cc,"caffe2::GetFloatToHalfGradient::GetGradientDefs()",4, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops.cc,"caffe2::GetHalfToFloatGradient::GetGradientDefs()",4, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNOp::CuDNNLRNOp( Args && ... args)",13, 73, 8, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNOp::~CuDNNLRNOp()",4, 61, 4, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNGradientOp::CuDNNLRNGradientOp( Args && ... args)",13, 73, 8, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNGradientOp::~CuDNNLRNGradientOp()",4, 61, 4, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNOp::DoRunWithType()",37, 50, 8, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNOp::RunOnDevice()",15, 45, 2, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNGradientOp::DoRunWithType()",41, 50, 8, 0
repos/cpp/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc,"caffe2::CuDNNLRNGradientOp::RunOnDevice()",19, 45, 2, 0
repos/cpp/pytorch/caffe2/operators/unique_ops.cc,"caffe2::UniqueOp<CPUContext>::DoRunWithType()",39, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/filler_op.cc,"caffe2::RangeFillOp<float,CPUContext>::Fill( Tensor * output)",7, 60, 0, 0
repos/cpp/pytorch/caffe2/operators/filler_op.cc,"caffe2::DiagonalFillOp<CPUContext>::FillWithType( Tensor * output)",14, 68, 2, 0
repos/cpp/pytorch/caffe2/operators/rmac_regions_op.cc,"caffe2::RMACRegionsOp<CPUContext>::RunOnDevice()",95, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_shared.cc,"caffe2::createSharedBuffer<CPUContext>( Workspace * ws)",6, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_shared.cc,"caffe2::runWithSharedBuffer<CPUContext>( Workspace * ws , std :: function<void(Tensor*buffer)> f)",12, 76, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_gpu_test.cc,"caffe2::BatchMatMulOpGPUTest::SetUp()",13, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_gpu_test.cc,"caffe2::BatchMatMulOpGPUTest::AddConstInput( const std :: vector<int64_t> & dims , const float value , const string & name)",13, 53, 4, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_gpu_test.cc,"caffe2::BatchMatMulOpGPUTest::VerifyOutput( const std :: vector<int64_t> & dims , const float value) const",14, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_gpu_test.cc,"caffe2::TEST_F( BatchMatMulOpGPUTest , BatchMatMulOpGPUNormalTest)",11, 64, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_gpu_test.cc,"caffe2::TEST_F( BatchMatMulOpGPUTest , BatchMatMulOpGPUBroadcastTest)",14, 64, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"CopyVector<caffe2::CPUContext,bool>( const int N , const bool * x , bool * y)",3, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"CopyVector<caffe2::CPUContext,int32_t>( const int N , const int32_t * x , int32_t * y)",6, 46, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"TEST( ElementwiseCPUTest , And)",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"TEST( ElementwiseTest , Or)",3, 39, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"TEST( ElementwiseTest , Xor)",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"TEST( ElementwiseTest , Not)",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_test.cc,"TEST( ElementwiseTest , EQ)",3, 39, 2, 0
repos/cpp/pytorch/caffe2/operators/cube_op.cc,"caffe2::CubeGradientFunctor<CPUContext>::Forward( const std :: vector<int> & dY_dims , const std :: vector<int> & , const T * dY , const T * X , T * dX , CPUContext *) const",13, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/cube_op.cc,"caffe2::GetCubeGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/softmax_with_loss_op.cc,"caffe2::SoftmaxWithLossOp<float,CPUContext>::RunOnDevice()",116, 92, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_with_loss_op.cc,"caffe2::SoftmaxWithLossGradientOp<float,CPUContext>::RunOnDevice()",94, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/softmax_with_loss_op.cc,"caffe2::GetSoftmaxWithLossGradient::GetGradientDefs()",12, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/tanh_op.cc,"caffe2::TanhFunctor<CPUContext>::operator ( ) < float >( const int N , const float * X , float * Y , CPUContext *) const",8, 49, 0, 0
repos/cpp/pytorch/caffe2/operators/batch_sparse_to_dense_op.cc,"caffe2::BatchSparseToDenseOp<T,Context>::RunOnDevice()",54, 82, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_sparse_to_dense_op.cc,"caffe2::BatchDenseToSparseOp<T,Context>::RunOnDevice()",39, 82, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_sparse_to_dense_op.cc,"caffe2::GetBatchSparseToDenseGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_sparse_to_dense_op.cc,"caffe2::GetBatchDenseToSparseGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/softplus_op.cc,"caffe2::SoftplusOp<float,CPUContext>::RunOnDevice()",11, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/softplus_op.cc,"caffe2::SoftplusGradientOp<float,CPUContext>::RunOnDevice()",16, 61, 2, 0
repos/cpp/pytorch/caffe2/operators/softplus_op.cc,"caffe2::GetSoftplusGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/elu_op.cc,"caffe2::EluFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",7, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/elu_op.cc,"caffe2::EluGradientFunctor<CPUContext>::Forward( const std :: vector<int> & Y_dims , const std :: vector<int> & , const T * Y , const T * dY , T * dX , CPUContext *) const",15, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/elu_op.cc,"caffe2::GetEluGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/reshape_op.cc,"caffe2::GetReshapeGradient::GetGradientDefs()",7, 55, 8, 0
repos/cpp/pytorch/caffe2/operators/reshape_op.cc,"caffe2::GetReshapeGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::StatRegistryCreateOp::StatRegistryCreateOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::StatRegistryCreateOp::RunOnDevice()",5, 62, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::StatRegistryExportOp::StatRegistryExportOp( Args && ... args)",3, 58, 8, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::StatRegistryExportOp::RunOnDevice()",24, 72, 10, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::StatRegistryUpdateOp::StatRegistryUpdateOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::StatRegistryUpdateOp::RunOnDevice()",19, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerInstance::TimerInstance( const std :: string & name)",2, 50, 2, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerInstance::begin()",5, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerInstance::end()",8, 68, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerInstance::get_ns()",7, 68, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerBeginOp::TimerBeginOp( const OperatorDef & operator_def , Workspace * ws)",6, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerBeginOp::RunOnDevice()",5, 56, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerEndOp::TimerEndOp( Args && ... args)",1, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerEndOp::RunOnDevice()",4, 51, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerGetAndEndOp::TimerGetAndEndOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerGetAndEndOp::RunOnDevice()",8, 70, 4, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerGetOp::TimerGetOp( Args && ... args)",1, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/stats_ops.cc,"caffe2::TimerGetOp::RunOnDevice()",7, 70, 4, 0
repos/cpp/pytorch/caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,"caffe2::utils::BoxesArea( const ERArrXXf & boxes)",12, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,"caffe2::utils::MapRoIsToFpnLevels( Eigen :: Ref<const ERArrXXf> rois , const float k_min , const float k_max , const float s0 , const float lvl0)",16, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,"caffe2::utils::SortAndLimitRoIsByScores( Eigen :: Ref<const EArrXf> scores , int n , ERArrXXf & rois)",29, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,"caffe2::utils::ArgSort( EArrXi & arr)",12, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,"caffe2::utils::RowsWhereRoILevelEquals( Eigen :: Ref<const ERArrXXf> rois , const ERArrXXf & lvls , const int lvl , ERArrXXf * out_filtered , EArrXi * out_indices)",20, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,"caffe2::CollectAndDistributeFpnRpnProposalsOp<CPUContext>::RunOnDevice()",117, 83, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_op.cc,"caffe2::ConvDocGenerator( const char * dim)",72, 272, 8, 0
repos/cpp/pytorch/caffe2/operators/scale_op_gpu.cc,"caffe2::ScaleOp<CUDAContext>::RunOnDevice()",3, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/expand_op.cc,"caffe2::GetExpandGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/group_norm_op.cc,"caffe2::ComputeInternalGradients( const std :: array<int,4> & dims , const T * dY , const T * X , const T * gamma , T * ds , T * db)",19, 68, 4, 0
repos/cpp/pytorch/caffe2/operators/group_norm_op.cc,"caffe2::GroupNormBackward( const std :: array<int,4> & dims , const T * dY , const T * X , const T * mu , const T * rsig , const T * gamma , const T * ds , const T * db , T * dX , T * dgamma , T * dbeta)",30, 71, 4, 0
repos/cpp/pytorch/caffe2/operators/group_norm_op.cc,"caffe2::GroupNormGradientOp<T,Context>::RunOnDeviceImpl( const int N , const int G , const int D , const int HxW , const T * dY_data , const T * X_data , const T * mu_data , const T * rsig_data , const T * gamma_data , T * dX_data , T * dgamma_data , T * dbeta_data)",69, 70, 6, 0
repos/cpp/pytorch/caffe2/operators/group_norm_op.cc,"caffe2::GetGroupNormGradient::GetGradientDefs()",7, 61, 8, 0
repos/cpp/pytorch/caffe2/operators/channel_stats_op.cc,"caffe2::ChannelStatsOp<CPUContext>::RunOnDevice()",32, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::CuDNNWeightedSumOp( Args && ... args)",9, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::~CuDNNWeightedSumOp()",4, 62, 4, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::DoRunWithType()",89, 79, 13, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::SetTensorDescriptor( const cudnnDataType_t data_type , const int input_size)",17, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::CopyWeightToHost( const float * src , T * dst)",5, 70, 0, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_cudnn.cc,"caffe2::CuDNNWeightedSumOp::CopyWeightToHost<float>( const float * src , float * dst)",3, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/rank_loss_op.cc,"caffe2::logLogit( T x)",11, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/rank_loss_op.cc,"caffe2::PairWiseLossOp<T,Context>::RunOnDevice()",62, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/rank_loss_op.cc,"caffe2::PairWiseLossGradientOp<T,Context>::RunOnDevice()",68, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/rank_loss_op.cc,"caffe2::GetPairWiseLossGradient::GetGradientDefs()",10, 72, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::SetTensorDescriptor( const cudnnDataType_t data_type , const StorageOrder order , const std :: vector<std::int64_t> & dims , cudnnTensorDescriptor_t * desc)",34, 73, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolOp::CuDNNPoolOp( Args && ... args)",35, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolOp::~CuDNNPoolOp()",5, 65, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolOp::RunOnDevice()",3, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolOp::DoRunWithType()",77, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolGradientOp::CuDNNPoolGradientOp( Args && ... args)",35, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolGradientOp::~CuDNNPoolGradientOp()",5, 65, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolGradientOp::RunOnDevice()",3, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNPoolGradientOp::DoRunWithType()",89, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNAveragePoolFunctor::CuDNNAveragePoolFunctor( const OperatorBase & op)",2, 59, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNAveragePoolFunctor::GetPoolingMode() const",5, 55, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNAveragePoolFunctor::GlobalPoolingForward( const int N , const int C , const int HxW , const T * X , T * Y , CUDAContext * context) const",10, 63, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNAveragePoolFunctor::Forward( const int N , const int C , const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<int> & kernel , const std :: vector<int> & dilation , const std :: vector<int> & stride , const std :: vector<int> & pads , const T * X , T * Y , CUDAContext * context) const",15, 80, 10, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNAveragePoolFunctor::GlobalPoolingBackward( const int N , const int C , const int HxW , const T * dY , const T * X , const T * Y , T * dX , CUDAContext * context) const",12, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNAveragePoolFunctor::Backward( const int N , const int C , const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<int> & kernel , const std :: vector<int> & dilation , const std :: vector<int> & stride , const std :: vector<int> & pads , const T * dY , const T * X , const T * Y , T * dX , CUDAContext * context) const",29, 51, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNMaxPoolFunctor::CuDNNMaxPoolFunctor( const OperatorBase & op)",3, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNMaxPoolFunctor::GetPoolingMode() const",7, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNMaxPoolFunctor::GlobalPoolingForward( const int N , const int C , const int HxW , const T * X , T * Y , CUDAContext * context) const",10, 63, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNMaxPoolFunctor::Forward( const int N , const int C , const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<int> & kernel , const std :: vector<int> & dilation , const std :: vector<int> & stride , const std :: vector<int> & pads , const T * X , T * Y , CUDAContext * context) const",15, 80, 10, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNMaxPoolFunctor::GlobalPoolingBackward( const int N , const int C , const int HxW , const T * dY , const T * X , const T * Y , T * dX , CUDAContext * context) const",12, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_cudnn.cc,"caffe2::CuDNNMaxPoolFunctor::Backward( const int N , const int C , const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<int> & kernel , const std :: vector<int> & dilation , const std :: vector<int> & stride , const std :: vector<int> & pads , const T * dY , const T * X , const T * Y , T * dX , CUDAContext * context) const",29, 51, 6, 0
repos/cpp/pytorch/caffe2/operators/spatial_batch_norm_gradient_op.cc,"caffe2::SpatialBNGradientOp<CPUContext>::ComputeMultiBatchScaleBiasGradientsAndFusedParams( const int N , const int C , const int HxW , const T * scale , const T * mean , const T * rstd , const T * dscale_sum , const T * dbias_sum , T * dscale , T * dbias , T * alpha , T * beta , T * gamma)",34, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/spatial_batch_norm_gradient_op.cc,"caffe2::SpatialBNGradientOp<CPUContext>::ComputeScaleBiasGradientsAndFusedParams( const int N , const int C , const int HxW , const T * dY , const T * X , const T * scale , const T * mean , const T * rstd , T * dscale , T * dbias , T * alpha , T * beta , T * gamma , T *)",51, 80, 12, 0
repos/cpp/pytorch/caffe2/operators/spatial_batch_norm_gradient_op.cc,"caffe2::SpatialBNGradientOp<CPUContext>::ComputeXGradient( const int N , const int C , const int HxW , const T * dY , const T * X , const T * alpha , const T * beta , const T * gamma , T * dX)",38, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/spatial_batch_norm_gradient_op.cc,"caffe2::GetSpatialBNGradient::GetGradientDefs()",29, 81, 10, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::SquaredL2DistanceOp<float,CPUContext>::RunOnDevice()",26, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::L1DistanceOp<float,CPUContext>::RunOnDevice()",24, 65, 8, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::L1DistanceGradientOp<float,CPUContext>::RunOnDevice()",44, 73, 10, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::CosineSimilarityOp<float,CPUContext>::RunOnDevice()",28, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::CosineSimilarityGradientOp<float,CPUContext>::RunOnDevice()",62, 78, 8, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::DotProductOp<float,CPUContext>::RunOnDevice()",27, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::TensorInferenceForDotProduct( const OperatorDef & , const vector<TensorShape> & in)",9, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::CostInferenceForDotProduct( const OperatorDef & def , const vector<TensorShape> & in)",12, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::DotProductGradientOp<float,CPUContext>::RunOnDevice()",36, 72, 8, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::DotProductWithPaddingOp<float,CPUContext>::RunOnDevice()",68, 78, 10, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::GetSquaredL2DistanceGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::GetL1DistanceGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::GetDotProductGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::GetCosineSimilarityGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/distance_op.cc,"caffe2::GetDotProductWithPaddingGradient::GetGradientDefs()",21, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/elu_op_cudnn.cc,"caffe2::CuDNNActivationOp<CUDNN_ACTIVATION_ELU>::CuDNNActivationOp( Args && ... args)",9, 60, 6, 0
repos/cpp/pytorch/caffe2/operators/elu_op_cudnn.cc,"caffe2::CuDNNActivationOp<CUDNN_ACTIVATION_ELU>::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/elu_op_cudnn.cc,"caffe2::CuDNNActivationOp<CUDNN_ACTIVATION_ELU>::DoRunWithType()",20, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/elu_op_cudnn.cc,"caffe2::CuDNNActivationGradientOp<CUDNN_ACTIVATION_ELU>::CuDNNActivationGradientOp( Args && ... args)",9, 60, 6, 0
repos/cpp/pytorch/caffe2/operators/elu_op_cudnn.cc,"caffe2::CuDNNActivationGradientOp<CUDNN_ACTIVATION_ELU>::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/elu_op_cudnn.cc,"caffe2::CuDNNActivationGradientOp<CUDNN_ACTIVATION_ELU>::DoRunWithType()",25, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/margin_ranking_criterion_op.cc,"caffe2::MarginRankingCriterionOp<CPUContext>::RunOnDevice()",22, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/margin_ranking_criterion_op.cc,"caffe2::MarginRankingCriterionGradientOp<CPUContext>::RunOnDevice()",27, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/margin_ranking_criterion_op.cc,"caffe2::GetMarginRankingCriterionGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/feature_maps_ops.cc,"caffe2::GetMergeSingleScalarFeatureTensorsGradient::GetGradientDefs()",16, 75, 4, 0
repos/cpp/pytorch/caffe2/operators/feature_maps_ops.cc,"caffe2::GetMergeSingleListFeatureTensorsGradient::GetGradientDefs()",17, 75, 4, 0
repos/cpp/pytorch/caffe2/operators/feature_maps_ops.cc,"caffe2::GetMergeSingleMapFeatureTensorsGradient::GetGradientDefs()",17, 75, 4, 0
repos/cpp/pytorch/caffe2/operators/feature_maps_ops.cc,"caffe2::GetMergeMultiScalarFeatureTensorsGradient::GetGradientDefs()",17, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/feature_maps_ops.cc,"caffe2::GetMergeMultiListFeatureTensorsGradient::GetGradientDefs()",18, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/feature_maps_ops.cc,"caffe2::GetMergeMultiMapFeatureTensorsGradient::GetGradientDefs()",18, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::sigmoid_xent_forward( float lgt , float tgt)",3, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::sigmoid_xent_backward( float lgt , float tgt)",3, 59, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::sigmoid_partition( float lgt)",4, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::sigmoid_xent_forward_with_log_d_trick( float lgt , float tgt)",3, 75, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::sigmoid_xent_backward_with_log_d_trick( float lgt , float tgt)",3, 76, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::unjoined_sigmoid_xent_forward( float lgt , float tgt)",4, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::unjoined_sigmoid_xent_backward( float lgt , float tgt)",3, 68, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::LabelCrossEntropyOp<float,CPUContext>::RunOnDevice()",31, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::SigmoidCrossEntropyWithLogitsOp<float,CPUContext>::RunOnDevice()",38, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::SigmoidCrossEntropyWithLogitsGradientOp<float,CPUContext>::RunOnDevice()",36, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::WeightedSigmoidCrossEntropyWithLogitsOp<float,CPUContext>::RunOnDevice()",34, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::WeightedSigmoidCrossEntropyWithLogitsGradientOp<float,CPUContext>::RunOnDevice()",32, 75, 10, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::LabelCrossEntropyGradientOp<float,CPUContext>::RunOnDevice()",31, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::MakeTwoClassOp<float,CPUContext>::RunOnDevice()",17, 56, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::MakeTwoClassGradientOp<float,CPUContext>::RunOnDevice()",17, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::CrossEntropyOp<float,CPUContext>::RunOnDevice()",35, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::CrossEntropyGradientOp<float,CPUContext>::RunOnDevice()",32, 74, 7, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::GetLabelCrossEntropyGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::GetMakeTwoClassGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::GetSigmoidCrossEntropyWithLogitsGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::GetWeightedSigmoidCrossEntropyWithLogitsGradient::GetGradientDefs()",7, 57, 8, 0
repos/cpp/pytorch/caffe2/operators/cross_entropy_op.cc,"caffe2::GetCrossEntropyGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::TEST( UtilsNMSTest , TestNMSGPU)",85, 78, 30, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::generateRandomBoxes( float * h_boxes , float * h_scores , const int nboxes)",26, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::generateRandomRotatedBoxes( float * h_boxes , float * h_scores , const int nboxes)",35, 76, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::TEST( UtilsNMSTest , TestPerfNMS)",110, 104, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::TEST( UtilsNMSTest , GPUEqualsCPUCorrectnessTest)",100, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::TEST( UtilsNMSTest , TestNMSGPURotatedAngle0)",91, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::TEST( UtilsNMSTest , TestPerfRotatedNMS)",110, 111, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu_test.cc,"caffe2::TEST( UtilsNMSTest , GPUEqualsCPURotatedCorrectnessTest)",100, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op.cc,"caffe2::pre_calc_for_bilinear_interpolate( const int height , const int width , const int pooled_height , const int pooled_width , const int iy_upper , const int ix_upper , T roi_start_h , T roi_start_w , T bin_size_h , T bin_size_w , int roi_bin_grid_h , int roi_bin_grid_w , std :: vector<PreCalc<T>> & pre_calc)",94, 73, 10, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op.cc,"caffe2::ROIAlignForward( const int nthreads , const T * bottom_data , const T & spatial_scale , const int channels , const int height , const int width , const int pooled_height , const int pooled_width , const int sampling_ratio , const T * bottom_rois , int roi_cols , T * top_data , StorageOrder order)",147, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op.cc,"caffe2::RoIAlignOp<float,CPUContext>::RunOnDevice()",67, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxOp::CuDNNSoftmaxOp( Args && ... args)",6, 65, 8, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxOp::~CuDNNSoftmaxOp()",3, 56, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxOp::DoRunWithType()",35, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxOp::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxGradientOp::CuDNNSoftmaxGradientOp( Args && ... args)",6, 65, 8, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxGradientOp::~CuDNNSoftmaxGradientOp()",3, 56, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxGradientOp::DoRunWithType()",39, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_op_cudnn.cc,"caffe2::CuDNNSoftmaxGradientOp::RunOnDevice()",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/fused_rowwise_random_quantization_ops.cc,"caffe2::FloatToFusedRandRowwiseQuantizedOp<Context>::RunOnDevice()",71, 87, 6, 0
repos/cpp/pytorch/caffe2/operators/fused_rowwise_random_quantization_ops.cc,"caffe2::FusedRandRowwiseQuantizedToFloatOp<Context>::RunOnDevice()",33, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader.cc,"caffe2::TextFileReaderInstance::TextFileReaderInstance( const std :: vector<char> & delims , char escape , const std :: string & filename , int numPasses , const std :: vector<int> & types)",15, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader.cc,"caffe2::CreateTextFileReaderOp::CreateTextFileReaderOp( Args && ... args)",7, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader.cc,"caffe2::CreateTextFileReaderOp::RunOnDevice()",6, 76, 8, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader.cc,"caffe2::convert( TensorProto_DataType dst_type , const char * src_start , const char * src_end , void * dst)",24, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader.cc,"caffe2::TextFileReaderReadOp::TextFileReaderReadOp( Args && ... args)",3, 63, 8, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader.cc,"caffe2::TextFileReaderReadOp::RunOnDevice()",64, 84, 4, 0
repos/cpp/pytorch/caffe2/operators/negative_op.cc,"caffe2::GetNegativeGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/im2col_op.cc,"caffe2::GetIm2ColGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/im2col_op.cc,"caffe2::GetCol2ImGradient::GetGradientDefs()",4, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/roi_align_rotated_op.cc,"caffe2::pre_calc_for_bilinear_interpolate( const int height , const int width , const int pooled_height , const int pooled_width , const int iy_upper , const int ix_upper , T roi_start_h , T roi_start_w , T bin_size_h , T bin_size_w , int roi_bin_grid_h , int roi_bin_grid_w , T roi_center_h , T roi_center_w , T theta , std :: vector<PreCalc<T>> & pre_calc)",101, 73, 10, 0
repos/cpp/pytorch/caffe2/operators/roi_align_rotated_op.cc,"caffe2::ROIAlignRotatedForward( const int nthreads , const T * bottom_data , const T & spatial_scale , const int channels , const int height , const int width , const int pooled_height , const int pooled_width , const int sampling_ratio , const T * bottom_rois , int roi_cols , T * top_data , StorageOrder order)",150, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/roi_align_rotated_op.cc,"caffe2::RoIAlignRotatedOp<float,CPUContext>::RunOnDevice()",69, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/square_root_divide_op.cc,"caffe2::GetSquareRootDivideGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops.cc,"caffe2::SRLHelper::sum2one( const T * x , T * y , size_t n)",3, 54, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops.cc,"caffe2::SRLHelper::RunWithBroadcastFront( const T * x , T * y , size_t pre , size_t n , CPUContext *)",8, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops.cc,"caffe2::SRLHelper::RunWithBroadcastBack( const T * x , T * y , size_t post , size_t n , CPUContext *)",8, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops.cc,"caffe2::SRLHelper::RunWithBroadcast2( const T * a , T * y , size_t pre , size_t n , size_t post , CPUContext *)",16, 43, 8, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops.cc,"caffe2::SumReduceLikeOp<CPUContext>::DoRunWithType()",25, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/lengths_reducer_ops.cc,"caffe2::FormatDoc()",8, 55, 2, 0
repos/cpp/pytorch/caffe2/operators/sparse_normalize_op.cc,"caffe2::SparseNormalizeOp<float,CPUContext>::RunOnDevice()",8, 62, 2, 0
repos/cpp/pytorch/caffe2/operators/sparse_normalize_op.cc,"caffe2::SparseNormalizeOp<float,CPUContext>::DoRunWithType()",33, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/summarize_op.cc,"caffe2::SummarizeOp<float,CPUContext>::RunOnDevice()",37, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::TileArrayIntoVector( const T * a , int D , int K , vector<T> * b)",6, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::TileIndicesInPlace( vector<int> * v , int D , int K)",9, 56, 0, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::BatchBoxCoxOp<CPUContext>::DoRunWithType()",143, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::BatchBoxCoxOp<CPUContext>::BoxCoxNaive( int64_t N , int64_t D , const T * data_ptr , const T * lambda1_ptr , const T * lambda2_ptr , T k_eps , T * output_ptr)",21, 66, 8, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::BatchBoxCoxOp<CPUContext>::BoxCoxNonzeroLambda( int64_t D , const T * data_ptr , const T * lambda1 , const T * lambda2 , T k_eps , T * out)",17, 59, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::BatchBoxCoxOp<CPUContext>::BoxCoxZeroLambda( int64_t D , const T * data_ptr , const T * lambda2 , T k_eps , T * output_ptr)",12, 66, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_box_cox_op.cc,"caffe2::BatchBoxCoxOp<CPUContext>::BoxCoxMixedLambda( const T * data_ptr , const vector<int> & nonzeros , const vector<int> & zeros , const T * lambda1 , const T * lambda2 , const T * lambda2_z , T k_eps , T * buffer , T * output_ptr)",18, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/fused_rowwise_8bit_conversion_ops.cc,"caffe2::convertfp32fp32( float * dst , const float * src , size_t N)",3, 63, 0, 0
repos/cpp/pytorch/caffe2/operators/fused_rowwise_8bit_conversion_ops.cc,"caffe2::convertfp16fp32( float * dst , const at :: Half * src , size_t N)",5, 66, 0, 0
repos/cpp/pytorch/caffe2/operators/fused_rowwise_8bit_conversion_ops.cc,"caffe2::convertfp32fp16( at :: Half * dst , const float * src , size_t N)",5, 66, 0, 0
repos/cpp/pytorch/caffe2/operators/batch_moments_op.cc,"caffe2::BatchMomentsOp<float,CPUContext>::ComputeBatchMomentsNCHW( const int N , const int C , const int HxW , const float * X , float * mu , float * var)",24, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_moments_op.cc,"caffe2::BatchMomentsOp<float,CPUContext>::ComputeBatchMomentsNHWC( const int N , const int C , const int HxW , const float * X , float * mu , float * var)",12, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_moments_op.cc,"caffe2::BatchMomentsGradientOp<float,CPUContext>::ComputeBatchMomentsGradientNCHW( const int N , const int C , const int HxW , const float * dmu , const float * dvar , const float * X , float * dX)",25, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/batch_moments_op.cc,"caffe2::BatchMomentsGradientOp<float,CPUContext>::ComputeBatchMomentsGradientNHWC( const int N , const int C , const int HxW , const float * dmu , const float * dvar , const float * X , float * dX)",16, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/batch_moments_op.cc,"caffe2::GetBatchMomentsGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops_test.cc,"caffe2::TEST( Float16 , SimpleTest)",51, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/half_float_ops_test.cc,"caffe2::TEST( Float16 , UniformDistributionTest)",40, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::AddLinSpacedInput( const vector<int64_t> & shape , const float min_val , const float max_val , const string & name , Workspace * ws)",17, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Context * context , Workspace * ws)",13, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::AddInput<CPUContext>( const vector<int64_t> & shape , const vector<float> & values , const string & name , Workspace * ws)",12, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::AddInput<CUDAContext>( const vector<int64_t> & shape , const vector<float> & values , const string & name , Workspace * ws)",13, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::TEST( GenerateProposalsTest , TestRealDownSampledGPU)",154, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::TEST( GenerateProposalsTest , TestRealDownSampledRotatedAngle0GPU)",197, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_gpu_test.cc,"caffe2::TEST( GenerateProposalsTest , TestRealDownSampledRotatedGPU)",204, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/sigmoid_gradient_op.cc,"caffe2::SigmoidGradientFunctor<CPUContext>::Forward( const std :: vector<int> & Y_dims , const std :: vector<int> & , const T * Y , const T * dY , T * dX , CPUContext *) const",14, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/sigmoid_gradient_op.cc,"caffe2::GetSigmoidGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/cos_op.cc,"caffe2::CosGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/cos_op.cc,"caffe2::GetCosGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_boxes_test.cc,"caffe2::TEST( UtilsBoxesTest , TestBboxTransformRandom)",29, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_boxes_test.cc,"caffe2::TEST( UtilsBoxesTest , TestBboxTransformRotated)",32, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_boxes_test.cc,"caffe2::TEST( UtilsBoxesTest , TestBboxTransformRotatedNormalized)",33, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_util_boxes_test.cc,"caffe2::TEST( UtilsBoxesTest , ClipRotatedBoxes)",28, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/rsqrt_op.cc,"caffe2::RsqrtGradientFunctor<CPUContext>::Forward( const std :: vector<int> & dY_dims , const std :: vector<int> & , const T * dY , const T * Y , T * dX , CPUContext *) const",13, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/rsqrt_op.cc,"caffe2::GetRsqrtGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/log_op.cc,"caffe2::GetLogGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/lstm_unit_op.cc,"caffe2::GetLSTMUnitGradient::GetGradientDefs()",16, 76, 10, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_sum_ops.cc,"caffe2::SumReduceDimsOp<CPUContext,true,false>::Compute( int rows , int cols , const T * in_data , const int32_t * lengths_data , T * out_data)",15, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_sum_ops.cc,"caffe2::SumReduceDimsOp<CPUContext,false,false>::Compute( int rows , int cols , const T * in_data , const int32_t * lengths_data , T * out_data)",16, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_sum_ops.cc,"caffe2::SumReduceDimsGradientOp<CPUContext,true,false>::Compute( int rows , int cols , const T * dYdata , const int * lengths_data , T * dXdata)",16, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_sum_ops.cc,"caffe2::SumReduceDimsGradientOp<CPUContext,false,false>::Compute( int rows , int cols , const T * dYdata , const int * lengths_data , T * dXdata)",16, 65, 0, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_sum_ops.cc,"caffe2::GetReduceFrontSumGradient::GetGradientDefs()",8, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_sum_ops.cc,"caffe2::GetReduceBackSumGradient::GetGradientDefs()",8, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/tanh_gradient_op.cc,"caffe2::TanhGradientFunctor<CPUContext>::Forward<float>( const std :: vector<int> & Y_dims , const std :: vector<int> & , const float * Y , const float * dY , float * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/tanh_gradient_op.cc,"caffe2::GetTanhGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/matmul_op.cc,"caffe2::GetMatMulGradient::GetGradientDefs()",99, 73, 33, 0
repos/cpp/pytorch/caffe2/operators/matmul_op.cc,"caffe2::GetMatMulGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/asin_op.cc,"caffe2::AsinGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/asin_op.cc,"caffe2::GetAsinGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/reservoir_sampling.cc,"caffe2::ReservoirSamplingOp::ReservoirSamplingOp( const OperatorDef operator_def , Workspace * ws)",6, 74, 12, 0
repos/cpp/pytorch/caffe2/operators/reservoir_sampling.cc,"caffe2::ReservoirSamplingOp::RunOnDevice()",167, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/reservoir_sampling.cc,"caffe2::ReservoirSamplingOp::countNewEntries( const std :: set<int64_t> & unique_object_ids)",14, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxOp<float,CPUContext>::RunForwardSingle( const float * X , const float * W , const float * b , int target , float * int_output , const float * bias_multiplier , int dim_out , int dim_in , int & int_output_offset)",63, 83, 4, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxOp<float,CPUContext>::RunOnDevice()",55, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxGradientOp<float,CPUContext>::RunBackwardSingle( const float * X , const float * dY , const float * W , int target , const float * int_output , float * dX , float * dW , float * db , float * dint_output , int dim_in , int dim_out , int & int_output_offset)",68, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxGradientOp<float,CPUContext>::RunOnDevice()",52, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxSearchOp<float,CPUContext>::pruning( const float * X , int sample , int K , const float * W , const float * b , const NodeProto & src_node , NodeProto & dst_node , float parent_score , float beam)",69, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxSearchOp<float,CPUContext>::extractNodes( const NodeProto & node , std :: vector<std::pair<string,float>> & info)",17, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HSoftmaxSearchOp<float,CPUContext>::RunOnDevice()",77, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::HuffmanTreeHierarchyOp<T,Context>::RunOnDevice()",118, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/h_softmax_op.cc,"caffe2::GetHSoftmaxGradient::GetGradientDefs()",8, 61, 8, 0
repos/cpp/pytorch/caffe2/operators/heatmap_max_keypoint_op.cc,"caffe2::HeatmapMaxKeypointOp<float,CPUContext>::RunOnDevice()",134, 81, 10, 0
repos/cpp/pytorch/caffe2/operators/quant_decode_op.cc,"caffe2::GetQuantDecodeGradient::GetGradientDefs()",12, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::StringJoinOpTest::runOp( const Tensor & input)",13, 41, 4, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::StringJoinOpTest::checkAndGetOutput( int outputSize)",10, 57, 2, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::TEST_F( StringJoinOpTest , testString1DJoin)",18, 61, 2, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::TEST_F( StringJoinOpTest , testString2DJoin)",20, 70, 49, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::TEST_F( StringJoinOpTest , testFloat1DJoin)",18, 60, 2, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::TEST_F( StringJoinOpTest , testFloat2DJoin)",20, 67, 43, 0
repos/cpp/pytorch/caffe2/operators/string_ops_test.cc,"caffe2::TEST_F( StringJoinOpTest , testLong2DJoin)",19, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_gather_ops.cc,"caffe2::GetBatchGatherGradient::GetGradientDefs()",8, 60, 8, 0
repos/cpp/pytorch/caffe2/operators/lpnorm_op.cc,"caffe2::LpNormOp<float,CPUContext>::RunOnDevice()",20, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/lpnorm_op.cc,"caffe2::LpNormGradientOp<float,CPUContext>::RunOnDevice()",32, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/lpnorm_op.cc,"caffe2::GetLpNormGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/concat_split_op.cc,"caffe2::splitOpDevInfer( const OperatorDef & def)",14, 81, 0, 0
repos/cpp/pytorch/caffe2/operators/concat_split_op.cc,"caffe2::CostInferenceForConcat( const OperatorDef & def , const vector<TensorShape> & in)",35, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/concat_split_op.cc,"caffe2::concatOpDevInfer( const OperatorDef & def)",11, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/concat_split_op.cc,"caffe2::TensorInferenceForConcat( const OperatorDef & def , const vector<TensorShape> & in)",73, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/concat_split_op.cc,"caffe2::GetSplitGradient::GetGradientDefs()",16, 55, 8, 0
repos/cpp/pytorch/caffe2/operators/concat_split_op.cc,"caffe2::GetConcatGradient::GetGradientDefs()",10, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/sparse_to_dense_op.cc,"caffe2::GetSparseToDenseGradient::GetGradientDefs()",4, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/sinh_op.cc,"caffe2::SinhGradientFunctor<CPUContext>::Forward( const std :: vector<int> & , const std :: vector<int> & X_dims , const T * dY , const T * X , T * dX , CPUContext *) const",14, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/sinh_op.cc,"caffe2::GetSinhGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/fc_inference.cc,"caffe2::FCShapeInference( const OperatorDef & def , const vector<TensorShape> & in , bool pretransposed_weight)",30, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/fc_inference.cc,"caffe2::CostInferenceForFC( const OperatorDef & def , const vector<TensorShape> & in , bool pretransposed_weight)",26, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/softsign_op.cc,"caffe2::SoftsignFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",6, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/softsign_op.cc,"caffe2::SoftsignGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",15, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/softsign_op.cc,"caffe2::GetSoftsignGradient::GetGradientDefs()",12, 57, 8, 0
repos/cpp/pytorch/caffe2/operators/boolean_unmask_ops.cc,"caffe2::BooleanUnmaskOp<CPUContext>::RunOnDevice()",47, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/conv_gradient_op.cc,"caffe2::TensorInferenceForConvGradient( const OperatorDef & def , const std :: vector<TensorShape> & in)",29, 68, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_gradient_op.cc,"caffe2::CostInferenceForConvGradient( const OperatorDef & def , const vector<TensorShape> & inputs)",46, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_gradient_op.cc,"caffe2::GetConvGradient::GetGradientDefs()",39, 86, 4, 0
repos/cpp/pytorch/caffe2/operators/layer_norm_op.cc,"caffe2::LayerNormOp<CPUContext>::ComputeStdDevAndFusedParams( const int N , const T * mean , const T * var , T * stddev , T * scale , T * bias , float epsilon , CPUContext *)",17, 64, 2, 0
repos/cpp/pytorch/caffe2/operators/layer_norm_op.cc,"caffe2::LayerNormOp<CPUContext>::LayerNormForward( const int M , const int N , const T * X , const T * scale , const T * bias , T * Y , CPUContext * context)",14, 58, 7, 0
repos/cpp/pytorch/caffe2/operators/layer_norm_op.cc,"caffe2::LayerNormGradientOp<CPUContext>::ComputeInternalGradients( const int M , const int N , const T * dY , const T * X , T * ds , T * db)",14, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/layer_norm_op.cc,"caffe2::LayerNormGradientOp<CPUContext>::ComputeFusedParams( const int M , const int N , const T * mean , const T * sig , const T * ds , const T * db , T * dY_scale , T * X_scale , T * bias)",21, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/layer_norm_op.cc,"caffe2::LayerNormGradientOp<CPUContext>::LayerNormBackward( const int M , const int N , const T * dY_scale , const T * dY , const T * X_scale , const T * X , const T * bias , T * dX)",17, 66, 11, 0
repos/cpp/pytorch/caffe2/operators/layer_norm_op.cc,"caffe2::GetLayerNormGradient::GetGradientDefs()",7, 65, 8, 0
repos/cpp/pytorch/caffe2/operators/roi_pool_op.cc,"caffe2::RoIPoolOp<float,CPUContext>::RunOnDevice()",104, 81, 12, 0
repos/cpp/pytorch/caffe2/operators/roi_pool_op.cc,"caffe2::GetRoIPoolGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/apmeter_op.cc,"caffe2::APMeterOp<float,CPUContext>::BufferPredictions( const float * XData , const int * labelData , int N , int D)",38, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/apmeter_op.cc,"caffe2::APMeterOp<float,CPUContext>::RunOnDevice()",47, 65, 8, 0
repos/cpp/pytorch/caffe2/operators/norm_planar_yuv_op.cc,"caffe2::NormalizePlanarYUVOp::RunOnDevice()",32, 71, 4, 0
repos/cpp/pytorch/caffe2/operators/tile_op.cc,"caffe2::GetTileGradient::GetGradientDefs()",13, 62, 8, 0
repos/cpp/pytorch/caffe2/operators/relu_op.cc,"caffe2::ReluFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",5, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/relu_op.cc,"caffe2::ReluFunctor<CPUContext>::operator ( ) < float >( const int N , const float * X , float * Y , CPUContext *) const",9, 49, 0, 0
repos/cpp/pytorch/caffe2/operators/relu_op.cc,"caffe2::ReluGradientFunctor<CPUContext>::Forward( const std :: vector<int> & Y_dims , const std :: vector<int> & , const T * Y , const T * dY , T * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/relu_op.cc,"caffe2::CostInferenceForRelu( const OperatorDef & def , const vector<TensorShape> & in)",7, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/relu_op.cc,"caffe2::GetReluGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/reciprocal_gradient_op.cc,"caffe2::ReciprocalGradientFunctor<CPUContext>::Forward( const std :: vector<int> & Y_dims , const std :: vector<int> & , const T * Y , const T * dY , T * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/reciprocal_gradient_op.cc,"caffe2::GetReciprocalGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/selu_op.cc,"caffe2::SeluOp<float,CPUContext>::RunOnDevice()",11, 76, 2, 0
repos/cpp/pytorch/caffe2/operators/selu_op.cc,"caffe2::SeluGradientOp<float,CPUContext>::RunOnDevice()",16, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/selu_op.cc,"caffe2::GetSeluGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/segment_reduction_op.cc,"caffe2::CostInferenceForSparseLengths( const OperatorDef & def , const vector<TensorShape> & inputs , bool use_weight)",39, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/segment_reduction_op.cc,"caffe2::FormatDoc()",17, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/segment_reduction_op.cc,"caffe2::equal( char const * lhs , char const * rhs1 , char const * rhs2 , char const * rhs3 = '')",12, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/negate_gradient_op.cc,"caffe2::GetNegateGradientGradient::GetGradientDefs()",5, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutOp::CuDNNDropoutOp( const OperatorDef & operator_def , Workspace * ws)",21, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutOp::~CuDNNDropoutOp()",4, 65, 4, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutOp::scratch_blob_name( string mask_blob_name)",3, 59, 2, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutGradientOp::CuDNNDropoutGradientOp( const OperatorDef & operator_def , Workspace * ws)",21, 82, 2, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutGradientOp::~CuDNNDropoutGradientOp()",4, 65, 4, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutOp::DoRunWithType()",71, 78, 8, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutOp::RunOnDevice()",13, 45, 2, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutGradientOp::DoRunWithType()",57, 60, 6, 0
repos/cpp/pytorch/caffe2/operators/dropout_op_cudnn.cc,"caffe2::CuDNNDropoutGradientOp::RunOnDevice()",14, 45, 0, 0
repos/cpp/pytorch/caffe2/operators/cosine_embedding_criterion_op.cc,"caffe2::CosineEmbeddingCriterionOp<CPUContext>::RunOnDevice()",18, 78, 8, 0
repos/cpp/pytorch/caffe2/operators/cosine_embedding_criterion_op.cc,"caffe2::CosineEmbeddingCriterionGradientOp<CPUContext>::RunOnDevice()",17, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/cosine_embedding_criterion_op.cc,"caffe2::GetCosineEmbeddingCriterionGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/flexible_top_k.cc,"caffe2::ValueCmp::operator ( )( const std :: pair<T,int64_t> & lhs , const std :: pair<T,int64_t> & rhs)",7, 62, 8, 0
repos/cpp/pytorch/caffe2/operators/flexible_top_k.cc,"caffe2::FlexibleTopKOp<T,Context>::RunOnDevice()",70, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/flexible_top_k.cc,"caffe2::FlexibleTopKGradientOp<T,Context>::RunOnDevice()",32, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/flexible_top_k.cc,"caffe2::GetFlexibleTopKGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_sub_gradient_op.cc,"caffe2::GetSubGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_gpu_test.cc,"CopyVector<caffe2::CUDAContext>( const int N , const bool * x , bool * y)",3, 76, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_gpu_test.cc,"CreateOperatorDef<caffe2::CUDAContext>()",5, 68, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_gpu_test.cc,"TEST( ElementwiseGPUTest , And)",5, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_gpu_test.cc,"TEST( ElementwiseGPUTest , Or)",5, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_gpu_test.cc,"TEST( ElementwiseGPUTest , Xor)",5, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_op_gpu_test.cc,"TEST( ElementwiseGPUTest , Not)",5, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_bucketize_op.cc,"caffe2::BatchBucketizeOp<CPUContext>::RunOnDevice()",49, 76, 2, 0
repos/cpp/pytorch/caffe2/operators/dropout_op.cc,"caffe2::DropoutOp<float,CPUContext>::RunOnDevice()",28, 74, 10, 0
repos/cpp/pytorch/caffe2/operators/dropout_op.cc,"caffe2::DropoutGradientOp<float,CPUContext>::RunOnDevice()",23, 77, 10, 0
repos/cpp/pytorch/caffe2/operators/dropout_op.cc,"caffe2::GetDropoutGradient::GetGradientDefs()",14, 76, 10, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexCreateOp::IndexCreateOp( Args && ... args)",5, 59, 8, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexCreateOp::RunOnDevice()",5, 64, 8, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexGetOp::IndexGetOp( Args && ... args)",1, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexGetOp::RunOnDevice()",3, 64, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexGetOp::DoRunWithType()",13, 71, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexLoadOp::IndexLoadOp( Args && ... args)",4, 76, 12, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexLoadOp::RunOnDevice()",3, 64, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexLoadOp::DoRunWithType()",14, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexStoreOp::IndexStoreOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexStoreOp::RunOnDevice()",4, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexStoreOp::DoRunWithType()",6, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexFreezeOp::IndexFreezeOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexFreezeOp::RunOnDevice()",5, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexSizeOp::IndexSizeOp( Args && ... args)",2, 49, 6, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexSizeOp::RunOnDevice()",7, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexSerializer::IndexSerializer()",1, 23, 2, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexSerializer::~IndexSerializer()",1, 33, 2, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",36, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexSerializer::doStore( const std :: unique_ptr<IndexBase> & base , Tensor * tensor_out)",5, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",27, 74, 4, 0
repos/cpp/pytorch/caffe2/operators/index_ops.cc,"caffe2::IndexDeserializer::doLoad( std :: unique_ptr<IndexBase> * base , int64_t maxElements , const Tensor & tensor_in)",8, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/channel_backprop_stats_op.cc,"caffe2::ChannelBackpropStatsOp<CPUContext>::RunOnDevice()",39, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/gru_unit_op.cc,"caffe2::GetGRUUnitGradient::GetGradientDefs()",15, 63, 10, 0
repos/cpp/pytorch/caffe2/operators/leaky_relu_op.cc,"caffe2::LeakyReluOp<float,CPUContext>::RunOnDevice()",9, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/leaky_relu_op.cc,"caffe2::LeakyReluGradientOp<float,CPUContext>::RunOnDevice()",14, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/leaky_relu_op.cc,"caffe2::GetLeakyReluGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/sigmoid_op.cc,"caffe2::SigmoidFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",6, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/atan_op.cc,"caffe2::AtanGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/atan_op.cc,"caffe2::GetAtanGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/reduce_ops.cc,"caffe2::ComputeReduceMinMaxGradient( const std :: vector<int> & dY_dims , const std :: vector<int> & dX_dims , const T * dY_data , const T * X_data , const T * Y_data , T * dX_data)",19, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_ops.cc,"caffe2::MinReducer<CPUContext>::Backward( const std :: vector<int> & dY_dims , const std :: vector<int> & dX_dims , const T * dY_data , const T * X_data , const T * Y_data , T * dX_data , CPUContext *) const",12, 59, 6, 0
repos/cpp/pytorch/caffe2/operators/reduce_ops.cc,"caffe2::MaxReducer<CPUContext>::Backward( const std :: vector<int> & dY_dims , const std :: vector<int> & dX_dims , const T * dY_data , const T * X_data , const T * Y_data , T * dX_data , CPUContext *) const",12, 59, 6, 0
repos/cpp/pytorch/caffe2/operators/reduce_ops.cc,"caffe2::L1Reducer<CPUContext>::Backward( const std :: vector<int> & dY_dims , const std :: vector<int> & dX_dims , const T * dY_data , const T * X_data , const T * , T * dX_data , CPUContext *) const",28, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_ops.cc,"caffe2::L2Reducer<CPUContext>::Backward( const std :: vector<int> & dY_dims , const std :: vector<int> & dX_dims , const T * dY_data , const T * X_data , const T * Y_data , T * dX_data , CPUContext *) const",26, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_ops.cc,"caffe2::GetReduceGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/sqr_op.cc,"caffe2::GetSqrGradient::GetGradientDefs()",16, 79, 40, 0
repos/cpp/pytorch/caffe2/operators/elementwise_add_gradient_op.cc,"caffe2::GetAddGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/load_save_op.cc,"caffe2::LoadOp<CPUContext>::SetCurrentDevice( BlobProto * proto)",7, 71, 4, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::CreateMutexOp::CreateMutexOp( Args && ... args)",2, 61, 6, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::CreateMutexOp::RunOnDevice()",5, 60, 4, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::AtomicFetchAddOp::AtomicFetchAddOp( Args && ... args)",2, 61, 6, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::AtomicFetchAddOp::RunOnDevice()",17, 71, 4, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::CreateAtomicBoolOp::RunOnDevice()",5, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::ConditionalSetAtomicBoolOp::RunOnDevice()",8, 78, 8, 0
repos/cpp/pytorch/caffe2/operators/atomic_ops.cc,"caffe2::fb::CheckAtomicBoolOp::RunOnDevice()",6, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/spatial_softmax_with_loss_op.cc,"caffe2::SpatialSoftmaxWithLossOp<float,CPUContext>::RunOnDevice()",96, 81, 14, 0
repos/cpp/pytorch/caffe2/operators/spatial_softmax_with_loss_op.cc,"caffe2::SpatialSoftmaxWithLossGradientOp<float,CPUContext>::RunOnDevice()",77, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/spatial_softmax_with_loss_op.cc,"caffe2::GetSoftmaxWithLossGradient::GetGradientDefs()",15, 56, 6, 0
repos/cpp/pytorch/caffe2/operators/abs_op.cc,"caffe2::AbsGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",15, 76, 6, 0
repos/cpp/pytorch/caffe2/operators/abs_op.cc,"caffe2::GetAbsGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/roi_align_gradient_op.cc,"caffe2::bilinear_interpolate_gradient( const int height , const int width , T y , T x , T & w1 , T & w2 , T & w3 , T & w4 , int & x_low , int & x_high , int & y_low , int & y_high , const int)",61, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/roi_align_gradient_op.cc,"caffe2::add( const T & val , T * address)",3, 44, 0, 0
repos/cpp/pytorch/caffe2/operators/roi_align_gradient_op.cc,"caffe2::ROIAlignBackwardFeature( const int nthreads , const T * top_diff , const int , const T & spatial_scale , const int channels , const int height , const int width , const int pooled_height , const int pooled_width , const int sampling_ratio , T * bottom_diff , const T * bottom_rois , int rois_cols)",106, 81, 10, 0
repos/cpp/pytorch/caffe2/operators/roi_align_gradient_op.cc,"caffe2::RoIAlignGradientOp<float,CPUContext>::RunOnDevice()",39, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/roi_align_gradient_op.cc,"caffe2::GetRoIAlignGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Workspace * ws)",14, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::AddLinSpacedInput( const vector<int64_t> & shape , const float min_val , const float max_val , const string & name , Workspace * ws)",17, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::AddInput( const vector<int64_t> & shape , const vector<float> & values , const string & name , Workspace * ws)",16, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestComputeAllAnchors)",31, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestComputeSortedAnchors)",49, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestComputeAllAnchorsRotated)",48, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestComputeSortedAnchorsRotated)",58, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestEmpty)",36, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestRealDownSampled)",127, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestRealDownSampledRotatedAngle0)",170, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/generate_proposals_op_test.cc,"caffe2::TEST( GenerateProposalsTest , TestRealDownSampledRotated)",136, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/ctc_greedy_decoder_op.cc,"caffe2::getTensorDataPtr( const Tensor & tensor , int t , int n)",7, 68, 0, 0
repos/cpp/pytorch/caffe2/operators/ctc_greedy_decoder_op.cc,"caffe2::CTCGreedyDecoderOp<CPUContext>::RunOnDevice()",49, 76, 10, 0
repos/cpp/pytorch/caffe2/operators/top_k.cc,"caffe2::ValueComp::operator ( )( const std :: pair<T,int64_t> & lhs , const std :: pair<T,int64_t> & rhs) const",6, 61, 8, 0
repos/cpp/pytorch/caffe2/operators/top_k.cc,"caffe2::GetTopK( const T * input , const int64_t n , const int64_t k , const int64_t src_offset , const int64_t dst_offset , const int64_t stride , T * values , int64_t * indices , int64_t * flatten_indices)",41, 68, 6, 0
repos/cpp/pytorch/caffe2/operators/top_k.cc,"caffe2::SetTopKGradient( const T * values , const int64_t * indices , const int k , const int64_t src_offset , const int64_t dst_offset , const int64_t stride , T * gradient)",17, 72, 4, 0
repos/cpp/pytorch/caffe2/operators/top_k.cc,"caffe2::TopKOp<T,Context>::RunOnDevice()",67, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/top_k.cc,"caffe2::TopKGradientOp<T,Context>::RunOnDevice()",47, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/top_k.cc,"caffe2::GetTopKGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/reverse_packed_segs_op.cc,"caffe2::GetReversePackedSegsGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/scale_op.cc,"caffe2::GetScaleGradient::GetGradientDefs()",5, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/reshape_op_gpu_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Workspace * ws)",15, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/reshape_op_gpu_test.cc,"caffe2::TEST( ReshapeOpGPUTest , testReshapeWithScalar)",21, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/onnxifi_op.cc,"caffe2::SetInputTensorDescriptorTypeAndBuffer( const Tensor & cpu_tensor , onnxTensorDescriptorV1 * desc)",29, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/onnxifi_op.cc,"caffe2::OnnixfiTypeToDataType( uint64_t onnxifi_type)",14, 93, 2, 0
repos/cpp/pytorch/caffe2/operators/onnxifi_op.cc,"caffe2::SetOutputTensorDescriptorTypeAndBuffer( uint64_t onnxifi_type , Tensor * cpu_tensor , onnxTensorDescriptorV1 * desc)",7, 115, 2, 0
repos/cpp/pytorch/caffe2/operators/onnxifi_op.cc,"caffe2::BlobToTensorDescriptor( const std :: string & name , Workspace * ws , onnxTensorDescriptorV1 * desc , std :: vector<std::vector<uint64_t>> * shapes)",28, 60, 2, 0
repos/cpp/pytorch/caffe2/operators/onnxifi_op.cc,"caffe2::OnnxifiOp<float,CPUContext>::buildInitializationList( Workspace * ws , std :: unordered_set<std::string> * initialization_list , std :: vector<std::string> * weight_names , std :: vector<std::vector<uint64_t>> * weight_shapes)",26, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/onnxifi_op.cc,"caffe2::OnnxifiOp<float,CPUContext>::RunOnDevice()",99, 108, 4, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_gpu_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Workspace * ws)",15, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/utility_ops_gpu_test.cc,"caffe2::TEST( UtilityOpGPUTest , testReshapeWithScalar)",21, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/locally_connected_op.cc,"caffe2::LCDocGenerator( const char * dim)",30, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/locally_connected_op.cc,"caffe2::GetLocallyConnectedGradient::GetGradientDefs()",38, 72, 8, 0
repos/cpp/pytorch/caffe2/operators/normalize_l1_op.cc,"caffe2::NormalizeL1Op<T,Context>::DoNormalize( const T * xData , T * yData , const int m , const int n , const int sf)",22, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::WeightedSumOp<CPUContext>::RunOnDevice()",3, 48, 0, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::WeightedSumGradientOp<CPUContext>::RunOnDevice()",3, 56, 0, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::WeightedSumShapeInference( const OperatorDef & , const vector<TensorShape> & in)",7, 52, 0, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::CostInferenceForWeightedSum( const OperatorDef & , const vector<TensorShape> & in)",16, 74, 6, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::GetEnsureDenseGradient::GetGradientDefs()",18, 55, 8, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::GetAliasGradient::GetGradientDefs()",6, 64, 4, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::GetSumGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::GetWeightedSumGradient::GetGradientDefs()",20, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::GetFlattenToVecGradient::GetGradientDefs()",4, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::NanCheckOp<CPUContext>::RunOnDevice()",34, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/utility_ops.cc,"caffe2::RangeOp<CPUContext>::DoRunOnDevice( const T & start , const T & step , Tensor * output)",10, 58, 2, 0
repos/cpp/pytorch/caffe2/operators/copy_op.cc,"caffe2::GetCopyGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/copy_op.cc,"caffe2::GetGPUToCPUGradient::GetGradientDefs()",17, 77, 10, 0
repos/cpp/pytorch/caffe2/operators/copy_op.cc,"caffe2::GetCPUToGPUGradient::GetGradientDefs()",17, 77, 10, 0
repos/cpp/pytorch/caffe2/operators/rowmul_op.cc,"caffe2::GetRowMulGradient::GetGradientDefs()",15, 79, 12, 0
repos/cpp/pytorch/caffe2/operators/ctc_beam_search_decoder_op.cc,"caffe2::getTensorDataPtr( const Tensor & tensor , int t , int n)",7, 68, 0, 0
repos/cpp/pytorch/caffe2/operators/ctc_beam_search_decoder_op.cc,"caffe2::CTCBeamSearchDecoderOp<CPUContext>::RunOnDevice()",115, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/zero_gradient_op.cc,"caffe2::GetZeroGradientOpGradient::GetGradientDefs()",8, 62, 8, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_gradient_op.cc,"caffe2::GetConvTransposeGradient::GetGradientDefs()",33, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/ensure_clipped_op.cc,"caffe2::EnsureClippedOp<float,CPUContext>::DoRunWithType()",23, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/enforce_finite_op.cc,"caffe2::EnforceFiniteOp<CPUContext>::DoRunWithType()",4, 52, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_utils.cc,"caffe2::elementwise_ops_utils::ComputeLegacyBroadcastSizes( const Tensor & A , const Tensor & B , int axis)",37, 74, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_utils.cc,"caffe2::elementwise_ops_utils::ComputeBinaryBroadcastForwardDims( const std :: vector<int> & A_dims , const std :: vector<int> & B_dims)",28, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_utils.cc,"caffe2::elementwise_ops_utils::ComputeBinaryBroadcastBackwardAxes( const std :: vector<int> & A_dims , const std :: vector<int> & B_dims , std :: vector<int> * A_axes , std :: vector<int> * B_axes)",36, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_utils.cc,"caffe2::elementwise_ops_utils::ComputeBinaryBroadcastBackwardDims( const std :: vector<int> & A_dims , const std :: vector<int> & B_dims , std :: vector<int> * A_back_dims , std :: vector<int> * B_back_dims)",11, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeOpBase::CudnnConvTransposeOpBase( Args && ... args)",44, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeOpBase::~CudnnConvTransposeOpBase()",9, 66, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeOp::CudnnConvTransposeOp( Args && ... args)",2, 65, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeOp::~CudnnConvTransposeOp()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeGradientOp::CudnnConvTransposeGradientOp( Args && ... args)",7, 76, 8, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeGradientOp::~CudnnConvTransposeGradientOp()",1, 46, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeOp<T>::RunOnDevice()",228, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc,"caffe2::CudnnConvTransposeGradientOp<T>::RunOnDevice()",303, 81, 10, 0
repos/cpp/pytorch/caffe2/operators/sqrt_op.cc,"caffe2::GetSqrtGradient::GetGradientDefs()",16, 79, 40, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Context * context , Workspace * ws)",13, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::AddInput<CPUContext>( const vector<int64_t> & shape , const vector<float> & values , const string & name , Workspace * ws)",12, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::AddInput<CUDAContext>( const vector<int64_t> & shape , const vector<float> & values , const string & name , Workspace * ws)",13, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::GetDeviceType()",3, 29, 0, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::GetDeviceType<CUDAContext>()",3, 42, 0, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::randInt( int a , int b)",5, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::CreateAndRun( TensorCPU * outResult , const string & order , const TestParams & test_params , bool random_test)",106, 82, 23, 0
repos/cpp/pytorch/caffe2/operators/roi_align_op_gpu_test.cc,"caffe2::TEST( RoiAlignTest , CheckCPUGPUEqual)",71, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_schema.cc,"caffe2::MathDocGenerator( const char * name , const char * extra)",28, 106, 4, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_schema.cc,"caffe2::ElementwiseOpShapeInference( const OperatorDef & def , const std :: vector<TensorShape> & in)",21, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_schema.cc,"caffe2::ComparisonDocGenerator( const char * name , const char * desc , const char * extra)",33, 105, 8, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_schema.cc,"caffe2::LogicalDocGenerator( const char * name , const char * extra)",27, 116, 4, 0
repos/cpp/pytorch/caffe2/operators/elementwise_ops_schema.cc,"caffe2::BitwiseDocGenerator( const char * name)",22, 105, 8, 0
repos/cpp/pytorch/caffe2/operators/minmax_gradient_ops.cc,"caffe2::SelectGradientOpBase<T,Context>::RunOnDevice()",15, 72, 4, 0
repos/cpp/pytorch/caffe2/operators/minmax_gradient_ops.cc,"caffe2::GetMaxGradient::GetGradientDefs()",9, 70, 4, 0
repos/cpp/pytorch/caffe2/operators/minmax_gradient_ops.cc,"caffe2::GetMinGradient::GetGradientDefs()",9, 70, 4, 0
repos/cpp/pytorch/caffe2/operators/softmax_shared.cc,"caffe2::SoftmaxCPU( CPUContext & context , const int N , const int D , const float * Xdata , float * Ydata , float * scale , const float * sum_multiplier , bool logarithmic , float * rowmax)",47, 73, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::CudnnConvOpBase( const OperatorDef & operator_def , Workspace * ws)",74, 81, 12, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::~CudnnConvOpBase()",8, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::SetTensorNdDescriptorWithGroup( int size , cudnnTensorDescriptor_t tensorDesc , int N , int C , int H , int W , int D)",66, 77, 10, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::DuplicateConvDesc( cudnnConvolutionDescriptor_t input , size_t kernelDims , size_t dilationDims , cudnnConvolutionDescriptor_t copy)",85, 53, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::DetermineComputeTypeFromInput( const T & X)",22, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::SetConvDescFromArguments()",47, 54, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOpBase::SetConvDescComputeType( cudnnConvolutionDescriptor_t conv_desc , cudnnDataType_t math)",83, 53, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOp::CudnnConvOp( const OperatorDef & operator_def , Workspace * ws)",2, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOp::~CudnnConvOp()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvGradientOp::CudnnConvGradientOp( const OperatorDef & operator_def , Workspace * ws)",10, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvGradientOp::~CudnnConvGradientOp()",4, 77, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOp::DoRunWithType()",339, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvOp::RunOnDevice()",20, 74, 15, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvGradientOp::DoRunWithType()",545, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cudnn.cc,"caffe2::CudnnConvGradientOp::RunOnDevice()",24, 45, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cache_cudnn_test.cc,"caffe2::TEST( AlgorithmsCacheTest , CachesCorrectly)",11, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cache_cudnn_test.cc,"caffe2::TEST( AlgorithmsCacheTest , KeysDifferIfOneVectorIsEmpty)",13, 83, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_op_cache_cudnn_test.cc,"caffe2::TEST( AlgorithmsCacheTest , KeysDifferIfFlagsAreDifferent)",22, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/percentile_op.cc,"caffe2::PercentileOp<CPUContext>::RunOnDevice()",83, 81, 42, 0
repos/cpp/pytorch/caffe2/operators/conv_op_eigen.cc,"caffe2::EigenConvOp::EigenConvOp( const OperatorDef & operator_def , Workspace * ws)",4, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/conv_op_eigen.cc,"caffe2::EigenConvOp::~EigenConvOp()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_eigen.cc,"caffe2::EigenConvOp<T>::RunOnDeviceWithOrderNCHW()",92, 78, 17, 0
repos/cpp/pytorch/caffe2/operators/conv_op_eigen.cc,"caffe2::EigenConvOp<T>::RunOnDeviceWithOrderNHWC()",85, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::to_v4_f32( uint16x4_t v)",3, 45, 0, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::to_f32_v4_x4( uint8x16_t v)",15, 50, 0, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::clamp( float32x4_t & v)",4, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::addMeanAndClamp( float32x4_t & v , float mean)",4, 58, 0, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::convertNarrowAndPack( float32x4_t v0 , float32x4_t v1)",6, 72, 0, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp( const OperatorDef & operator_def , Workspace * ws)",2, 105, 2, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::RunOnDevice()",45, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::initNoiseCPU( Tensor * noise , int size)",10, 67, 8, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::initNoiseCPUNeon( Tensor * noise , int size)",14, 70, 4, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::runBatch( int N , int , int H , int W , int noiseCycle , const uint8_t * input , const float * meanChannel , const float * noise , float * output)",23, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::runCPU( int H , int W , int noiseCycle , const uint8_t * input , const float * meanChannel , const float * noise , float * output)",22, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::PackedInt8BGRANHWCToNCHWCStylizerPreprocessOp::runCPUNeon( int H , int W , int noiseCycle , const uint8_t * input , const float * meanChannel , const float * noise , float * output)",178, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::clamped_cast( float f)",9, 44, 2, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::BRGNCHWCToPackedInt8BGRAStylizerDeprocessOp::RunOnDevice()",23, 78, 4, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::BRGNCHWCToPackedInt8BGRAStylizerDeprocessOp::runBatch( int N , int , int H , int W , const float * input , const float * meanChannel , uint8_t * output)",21, 65, 6, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::BRGNCHWCToPackedInt8BGRAStylizerDeprocessOp::runCPU( int H , int W , const float * input , const float * meanChannel , uint8_t * output)",20, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/stylizer_ops.cc,"caffe2::BRGNCHWCToPackedInt8BGRAStylizerDeprocessOp::runCPUNeon( int H , int W , const float * input , const float * meanChannel , uint8_t * output)",88, 78, 10, 0
repos/cpp/pytorch/caffe2/operators/arg_ops.cc,"caffe2::ComputeArgImpl( const int prev_size , const int next_size , const int n , const Compare & comp , const T * X , int64_t * Y , Context * context)",22, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/arg_ops.cc,"caffe2::ArgMaxReducer<CPUContext>::operator ( )( const int prev_size , const int next_size , const int n , const T * X , int64_t * Y , CPUContext * context) const",10, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/arg_ops.cc,"caffe2::ArgMinReducer<CPUContext>::operator ( )( const int prev_size , const int next_size , const int n , const T * X , int64_t * Y , CPUContext * context) const",10, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/arg_ops.cc,"caffe2::InferTensor( const OperatorDef & def , const std :: vector<TensorShape> & in)",24, 69, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeAveragePool1D<float,StorageOrder::NCHW>( const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",9, 62, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeAveragePool1D<float,StorageOrder::NHWC>( const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",13, 54, 0, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeAveragePool2D<float,StorageOrder::NCHW>( const int , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",12, 63, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeAveragePool2D<float,StorageOrder::NHWC>( const int W , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",18, 54, 0, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeAveragePool3D<float,StorageOrder::NCHW>( const int H , const int , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",19, 66, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeAveragePool3D<float,StorageOrder::NHWC>( const int H , const int W , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const float scale , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",23, 59, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::RunAveragePool1D( const int N , const int C , const int X_size , const int Y_size , const int kernel , const int stride , const int pad , const bool count_include_pad , const T * X , T * Y)",33, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::RunAveragePool2D( const int N , const int C , const int X_H , const int X_W , const int Y_H , const int Y_W , const int kernel_h , const int kernel_w , const int stride_h , const int stride_w , const int pad_t , const int pad_l , const bool count_include_pad , const T * X , T * Y)",48, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::RunAveragePool3D( const int N , const int C , const int X_D , const int X_H , const int X_W , const int Y_D , const int Y_H , const int Y_W , const int kernel_d , const int kernel_h , const int kernel_w , const int stride_d , const int stride_h , const int stride_w , const int pad_p , const int pad_t , const int pad_l , const bool count_include_pad , const T * X , T * Y)",57, 80, 14, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeMaxPool1D<float,StorageOrder::NCHW>( const int l , const int r , const int y , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",8, 59, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeMaxPool1D<float,StorageOrder::NHWC>( const int l , const int r , const int y , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",11, 53, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeMaxPool2D<float,StorageOrder::NCHW>( const int , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",11, 60, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeMaxPool2D<float,StorageOrder::NHWC>( const int W , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",16, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeMaxPool3D<float,StorageOrder::NCHW>( const int H , const int , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",18, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::ComputeMaxPool3D<float,StorageOrder::NHWC>( const int H , const int W , const int p , const int a , const int t , const int b , const int l , const int r , const int y , const ConstEigenArrayMap<float> & X_arr , EigenArrayMap<float> * Y_arr)",21, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::RunMaxPool1D( const int N , const int C , const int X_size , const int Y_size , const int kernel , const int stride , const int pad , const T * X , T * Y)",31, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::RunMaxPool2D( const int N , const int C , const int X_H , const int X_W , const int Y_H , const int Y_W , const int kernel_h , const int kernel_w , const int stride_h , const int stride_w , const int pad_t , const int pad_l , const T * X , T * Y)",43, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::RunMaxPool3D( const int N , const int C , const int X_D , const int X_H , const int X_W , const int Y_D , const int Y_H , const int Y_W , const int kernel_d , const int kernel_h , const int kernel_w , const int stride_d , const int stride_h , const int stride_w , const int pad_p , const int pad_t , const int pad_l , const T * X , T * Y)",53, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::AveragePoolFunctor<CPUContext>::GlobalPoolingForward<float,StorageOrder::NCHW>( const int N , const int C , const int HxW , const float * X , float * Y , CPUContext * context) const",14, 61, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::AveragePoolFunctor<CPUContext>::GlobalPoolingForward<float,StorageOrder::NHWC>( const int N , const int C , const int HxW , const float * X , float * Y , CPUContext * context) const",22, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::MaxPoolFunctor<CPUContext>::GlobalPoolingForward<float,StorageOrder::NCHW>( const int N , const int C , const int HxW , const float * X , float * Y , CPUContext * context) const",13, 61, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::MaxPoolFunctor<CPUContext>::GlobalPoolingForward<float,StorageOrder::NHWC>( const int N , const int C , const int HxW , const float * X , float * Y , CPUContext * context) const",23, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::AveragePoolDocGenerator( const char * dim)",31, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op.cc,"caffe2::MaxPoolDocGenerator( const char * dim)",21, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/thresholded_relu_op.cc,"caffe2::ThresholdedReluOp<float,CPUContext>::RunOnDevice()",19, 68, 2, 0
repos/cpp/pytorch/caffe2/operators/thresholded_relu_op.cc,"caffe2::ThresholdedReluGradientOp<float,CPUContext>::RunOnDevice()",21, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/thresholded_relu_op.cc,"caffe2::GetThresholdedReluGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/transpose_op_cudnn.cc,"caffe2::CuDNNTransposeOp::CuDNNTransposeOp( Args && ... args)",16, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/transpose_op_cudnn.cc,"caffe2::CuDNNTransposeOp::~CuDNNTransposeOp()",4, 57, 4, 0
repos/cpp/pytorch/caffe2/operators/transpose_op_cudnn.cc,"caffe2::CuDNNTransposeOp::RunOnDevice()",24, 74, 4, 0
repos/cpp/pytorch/caffe2/operators/transpose_op_cudnn.cc,"caffe2::CuDNNTransposeOp::DoRunWithType()",69, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/sequence_ops.cc,"caffe2::GatherPaddingOp<CPUContext>::GatherPadding( const int outer_size , const int lengths_size , const int block_size , const int pad_width , const T * in_ptr , const int * lengths_ptr , T * padding_start_ptr , T * padding_end_ptr)",40, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/sequence_ops.cc,"caffe2::RemovePaddingOp<CPUContext>::DoRunWithType()",48, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/sequence_ops.cc,"caffe2::AddPaddingOp<CPUContext>::MakePadding( const T * in_ptr , T * out_ptr , const int32_t * lengths_ptr , int32_t lengths_size , int32_t outer_size , const T * padding_start_ptr , const T * padding_end_ptr , int64_t block_size)",58, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/sequence_ops.cc,"caffe2::PadEmptySamplesOp<CPUContext>::RunOnDevice()",68, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/sequence_ops.cc,"caffe2::GetAddPaddingGradient::GetGradientDefs()",24, 77, 10, 0
repos/cpp/pytorch/caffe2/operators/sequence_ops.cc,"caffe2::GetRemovePaddingGradient::GetGradientDefs()",10, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/bbox_transform_op.cc,"caffe2::BBoxTransformOp<float,CPUContext>::RunOnDevice()",97, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/affine_channel_op.cc,"caffe2::AffineChannelScaleBiasBackwardNCHW( const int N , const int C , const int HxW , const T * dY , const T * X , T * dscale , T * dbias)",24, 52, 4, 0
repos/cpp/pytorch/caffe2/operators/affine_channel_op.cc,"caffe2::AffineChannelScaleBiasBackwardNHWC( const int N , const int C , const int HxW , const T * dY , const T * X , T * dscale , T * dbias)",13, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/affine_channel_op.cc,"caffe2::AffineChannelGradientOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",38, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/affine_channel_op.cc,"caffe2::AffineChannelGradientOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",37, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/affine_channel_op.cc,"caffe2::GetAffineChannelGradient::GetGradientDefs()",18, 61, 8, 0
repos/cpp/pytorch/caffe2/operators/lengths_tile_op.cc,"caffe2::LengthsTileOp<CPUContext>::RunOnDevice()",40, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/lengths_tile_op.cc,"caffe2::GetLengthsTileGradient::GetGradientDefs()",11, 54, 8, 0
repos/cpp/pytorch/caffe2/operators/swish_op.cc,"caffe2::SwishFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",6, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/swish_op.cc,"caffe2::SwishGradientOp<CPUContext>::DoRunWithType()",23, 76, 2, 0
repos/cpp/pytorch/caffe2/operators/swish_op.cc,"caffe2::GetSwishGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/counter_ops.cc,"caffe2::operator ... ')
for i in range(5):
    workspace.RunOperatorOnce(countup_op)
    print(' 'previous_count' after CountUp : ', workspace.FetchBlob(' previous_count '))

workspace.RunOperatorOnce(retrievecount_op)
print(' 'count' value after CountUp test : ', workspace.FetchBlob(' count '))


// Test CountDown operator
print(' \ nTesting CountDown operator ... ')
for i in range(11):
    workspace.RunOperatorOnce(countdown_op)
    workspace.RunOperatorOnce(retrievecount_op)
    print(' 'count' value after CountDown : { } \ t 'done' value : { } '.format(workspace.FetchBlob(' count '), workspace.FetchBlob(' done ')))
```

**Result**

```
'counter' pointer: counter, a C++ native class of type std::__1::unique_ptr<caffe2::Counter<long long>, std::__1::default_delete<caffe2::Counter<long long> > >.
Initial 'count': 5
Initial 'done' value: False

Testing CountUp operator...
'previous_count' after CountUp: 5
'previous_count' after CountUp: 6
'previous_count' after CountUp: 7
'previous_count' after CountUp: 8
'previous_count' after CountUp: 9
'count' value after CountUp test: 10

Testing CountDown operator...
'count' value after CountDown: 9	'done' value: False
'count' value after CountDown: 8	'done' value: False
'count' value after CountDown: 7	'done' value: False
'count' value after CountDown: 6	'done' value: False
'count' value after CountDown: 5	'done' value: False
'count' value after CountDown: 4	'done' value: False
'count' value after CountDown: 3	'done' value: False
'count' value after CountDown: 2	'done' value: False
'count' value after CountDown: 1	'done' value: False
'count' value after CountDown: 0	'done' value: False
'count' value after CountDown: -1	'done' value: True
```

</details>

)DOC' ; namespace { class CounterSerializer : public BlobSerializerBase { public : CounterSerializer()",62, 161, 0, 0
repos/cpp/pytorch/caffe2/operators/counter_ops.cc,"caffe2::CounterSerializer::~CounterSerializer()",1, 35, 2, 0
repos/cpp/pytorch/caffe2/operators/counter_ops.cc,"caffe2::CounterSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , SerializationAcceptor acceptor)",19, 74, 8, 0
repos/cpp/pytorch/caffe2/operators/counter_ops.cc,"caffe2::CounterDeserializer::Deserialize( const BlobProto & proto , Blob * blob)",13, 77, 4, 0
repos/cpp/pytorch/caffe2/operators/erf_op.cc,"caffe2::ErfGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 85, 2, 0
repos/cpp/pytorch/caffe2/operators/erf_op.cc,"caffe2::GetErfGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/sparse_to_dense_mask_op.cc,"caffe2::GetSparseToDenseMaskGradient::GetGradientDefs()",10, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_test.cc,"caffe2::BatchMatMulOpTest::SetUp()",8, 53, 4, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_test.cc,"caffe2::BatchMatMulOpTest::AddConstInput( const std :: vector<int64_t> & dims , const float value , const string & name)",13, 52, 4, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_test.cc,"caffe2::BatchMatMulOpTest::VerifyOutput( const std :: vector<int64_t> & dims , const float value) const",13, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_test.cc,"caffe2::TEST_F( BatchMatMulOpTest , BatchMatMulOpNormalTest)",8, 64, 2, 0
repos/cpp/pytorch/caffe2/operators/batch_matmul_op_test.cc,"caffe2::TEST_F( BatchMatMulOpTest , BatchMatMulOpBroadcastTest)",11, 64, 2, 0
repos/cpp/pytorch/caffe2/operators/cosh_op.cc,"caffe2::CoshGradientFunctor<CPUContext>::Forward( const std :: vector<int> & , const std :: vector<int> & X_dims , const T * dY , const T * X , T * dX , CPUContext *) const",14, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/cosh_op.cc,"caffe2::GetCoshGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::BooleanMaskLengthsOp::BooleanMaskLengthsOp( Args && ... args)",2, 58, 6, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::BooleanMaskLengthsOp::RunOnDevice()",3, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::BooleanMaskLengthsOp::DoRunWithType()",25, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::BooleanMaskOp<CPUContext>::RunOnDevice()",62, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::MaskWithFunctor( size_t N , size_t M , int B , const float * in , Functor fn , float fill_val , float * out)",39, 73, 10, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::RepeatedMaskWithFunctor( size_t N , size_t M , int D , const float * in , Functor fn , float fill_val , float * out)",17, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::SequenceFunctor::SequenceFunctor( const int * sl , const size_t len)",2, 60, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::SequenceFunctor::operator ( )( int i , int j , float)",4, 50, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::WindowFunctor::WindowFunctor( const int * c , int r)",1, 62, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::WindowFunctor::operator ( )( int i , int j , float)",3, 50, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::UpperFunctor::operator ( )( int i , int j , float)",3, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::LowerFunctor::operator ( )( int i , int j , float)",3, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::UpperDiagFunctor::operator ( )( int i , int j , float)",3, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::LowerDiagFunctor::operator ( )( int i , int j , float)",3, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::SequenceMaskOp<CPUContext>::RunOnDevice()",3, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::SequenceMaskOp<CPUContext>::DoRunWithType()",121, 80, 11, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::GetSequenceMaskGradient::GetGradientDefs()",23, 54, 4, 0
repos/cpp/pytorch/caffe2/operators/boolean_mask_ops.cc,"caffe2::GetSequenceMaskGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::RunFullyConnectedOpOnCUDADevice( const bool float16_compute , FullyConnectedOp * op)",43, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::RunFullyConnectedGradientOpOnCUDADevice( const bool float16_compute , FullyConnectedGradientOp * op)",55, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedOp<CUDAContext>::RunOnDevice()",3, 66, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedOp<CUDAContext,DefaultEngine,false>::RunOnDevice()",6, 66, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedGradientOp<CUDAContext>::RunOnDevice()",3, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedGradientOp<CUDAContext,DefaultEngine,false>::RunOnDevice()",6, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedOp<CUDAContext,TensorCoreEngine>::RunOnDevice()",3, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedOp<CUDAContext,TensorCoreEngine,false>::RunOnDevice()",6, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedGradientOp<CUDAContext,TensorCoreEngine>::RunOnDevice()",4, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op_gpu.cc,"caffe2::FullyConnectedGradientOp<CUDAContext,TensorCoreEngine,false>::RunOnDevice()",7, 57, 4, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::StringJoinOp<CPUContext>::DoRunWithType()",39, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::StartsWith::StartsWith( OperatorBase & op)",2, 68, 6, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::StartsWith::operator ( )( const std :: string & str)",4, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::EndsWith::EndsWith( OperatorBase & op)",2, 68, 6, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::EndsWith::operator ( )( const std :: string & str)",4, 73, 4, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::Prefix::Prefix( OperatorBase & op)",2, 59, 6, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::Prefix::operator ( )( const std :: string & str)",3, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::Suffix::Suffix( OperatorBase & op)",2, 59, 6, 0
repos/cpp/pytorch/caffe2/operators/string_ops.cc,"caffe2::Suffix::operator ( )( const std :: string & str)",3, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/hard_sigmoid_op.cc,"caffe2::HardSigmoidFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",8, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/hard_sigmoid_op.cc,"caffe2::HardSigmoidGradientFunctor<CPUContext>::Forward( const std :: vector<int> & Y_dims , const std :: vector<int> & , const T * Y , const T * dY , T * dX , CPUContext *) const",15, 72, 10, 0
repos/cpp/pytorch/caffe2/operators/hard_sigmoid_op.cc,"caffe2::CostInferenceForHardSigmoid( const OperatorDef & def , const vector<TensorShape> & in)",7, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/hard_sigmoid_op.cc,"caffe2::GetHardSigmoidGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::RunChannelShuffleNCHW( const int N , const int G , const int K , const int HxW , const T * X , T * Y , CPUContext * context)",25, 74, 12, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::RunChannelShuffleNHWC( const int N , const int G , const int K , const int HxW , const T * X , T * Y , CPUContext * context)",18, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",15, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",16, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleGradientOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",15, 79, 0, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::ChannelShuffleGradientOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",16, 79, 0, 0
repos/cpp/pytorch/caffe2/operators/channel_shuffle_op.cc,"caffe2::GetChannelShuffleGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/prelu_op.cc,"caffe2::runNeonPrelu( float * out , const float * in , int size , float w)",80, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/prelu_op.cc,"caffe2::PReluOp<float,CPUContext>::RunOnDevice()",70, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/prelu_op.cc,"caffe2::PReluGradientOp<float,CPUContext>::RunOnDevice()",83, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/prelu_op.cc,"caffe2::GetPReluGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/conditional_op.cc,"caffe2::ConditionalOp<CPUContext>::RunOnDevice()",38, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/relu_n_op.cc,"caffe2::ReluNFunctor<CPUContext>::operator ( )( const int N , const T * X , T * Y , CPUContext *) const",6, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/relu_n_op.cc,"caffe2::ReluNGradientFunctor<CPUContext>::Forward( const std :: vector<int> & Y_dims , const std :: vector<int> & , const T * Y , const T * dY , T * dX , CPUContext *) const",15, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/relu_n_op.cc,"caffe2::CostInferenceForReluN( const OperatorDef & def , const vector<TensorShape> & in)",7, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/relu_n_op.cc,"caffe2::GetReluNGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/weighted_multi_sampling_op.cc,"caffe2::WeightedMultiSamplingOp<Context>::RunOnDevice()",39, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op.cc,"caffe2::FCGradientShapeInference( const OperatorDef & def , const vector<TensorShape> & in , bool pretransposed_weight)",23, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op.cc,"caffe2::CostInferenceForFCGradient( const OperatorDef & def , const vector<TensorShape> & in , bool pretransposed_weight)",40, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/fully_connected_op.cc,"caffe2::GetFCGradient::GetGradientDefs()",9, 73, 4, 0
repos/cpp/pytorch/caffe2/operators/inference_lstm_op.cc,"caffe2::InferenceLSTMOp::RunOnDevice()",38, 72, 4, 0
repos/cpp/pytorch/caffe2/operators/workspace_ops.cc,"caffe2::GetAllBlobNamesOp::GetAllBlobNamesOp( const OperatorDef & operator_def , Workspace * ws)",4, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/workspace_ops.cc,"caffe2::GetAllBlobNamesOp::RunOnDevice()",7, 91, 4, 0
repos/cpp/pytorch/caffe2/operators/pool_op_util.cc,"caffe2::pool_op_util::AvgPoolNeon4x4p0s0Plane( int inputH , int inputW , const float * input , float * output)",92, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_util.cc,"caffe2::pool_op_util::MaxPoolNeon2x2p0s0Plane( int inputH , int inputW , const float * input , float * output)",72, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_util.cc,"caffe2::pool_op_util::IsNeon4x4p0s0Eligible( const int input_h , const int input_w , const int output_h , const int output_w , const int kh , const int kw , const int stride_h , const int stride_w , const int pad_t , const int pad_l , const int pad_b , const int pad_r , const int dilation_h , const int dilation_w , const float * X , float * Y)",58, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op_util.cc,"caffe2::pool_op_util::IsNeon2x2p0s0Eligible( const int input_h , const int input_w , const int output_h , const int output_w , const int kh , const int kw , const int stride_h , const int stride_w , const int pad_t , const int pad_l , const int pad_b , const int pad_r , const int dilation_h , const int dilation_w , const float * X , float * Y)",58, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/pool_op_util.cc,"caffe2::pool_op_util::RunNeonAveragePool4x4p0s0NCHW( int N , int C , int H , int W , const float * X , float * Y)",28, 51, 6, 0
repos/cpp/pytorch/caffe2/operators/pool_op_util.cc,"caffe2::pool_op_util::RunNeonMaxPool2x2p0s0NCHW( int N , int C , int H , int W , const float * X , float * Y)",28, 51, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_mobile_test.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Workspace * ws)",12, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_mobile_test.cc,"caffe2::AddNoiseInput( const vector<int64_t> & shape , const string & name , Workspace * ws)",16, 50, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_mobile_test.cc,"caffe2::relativeError( float a , float b)",3, 65, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_mobile_test.cc,"caffe2::compare( int N , int inputC , int H , int W , int outputC , int kernelH , int kernelW , int strideH , int strideW , int padT , int padL , int padB , int padR , int adjH , int adjW , float maxRelErr , float absErrForRelErrFailure)",114, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_mobile_test.cc,"randInt( int a , int b)",6, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_transpose_op_mobile_test.cc,"TEST( ConvTransposeMobile , Test)",28, 49, 4, 0
repos/cpp/pytorch/caffe2/operators/cbrt_op.cc,"caffe2::CbrtGradientFunctor<CPUContext>::Forward( const std :: vector<int> & dY_dims , const std :: vector<int> & , const T * dY , const T * Y , T * dX , CPUContext *) const",13, 72, 2, 0
repos/cpp/pytorch/caffe2/operators/cbrt_op.cc,"caffe2::GetCbrtGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/pack_rnn_sequence_op.cc,"caffe2::GetPackRNNSequenceGradient::GetGradientDefs()",8, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/pack_rnn_sequence_op.cc,"caffe2::GetUnpackRNNSequenceGradient::GetGradientDefs()",8, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_div_gradient_op.cc,"caffe2::ComputeDivGradient( const int ndim , const int * A_dims , const int * B_dims , const int * C_dims , const TGrad * dC , const TIn * B , const TOut * C , TGrad * dA , TGrad * dB , CPUContext * context)",34, 73, 6, 0
repos/cpp/pytorch/caffe2/operators/elementwise_div_gradient_op.cc,"caffe2::DivFunctor<CPUContext>::Backward( const std :: vector<int> & A_dims , const std :: vector<int> & B_dims , const TGrad * dC , const TIn * , const TIn * B , const TOut * C , TGrad * dA , TGrad * dB , CPUContext * context) const",68, 68, 8, 0
repos/cpp/pytorch/caffe2/operators/elementwise_div_gradient_op.cc,"caffe2::BinaryElementwiseWithArgsGradientOp<NumericTypes,CPUContext,BinaryFunctorWithDefaultCtor<DivFunctor<CPUContext>>,SameTypeAsInput,SameTypeAsInput>::BinaryElementwiseWithArgsGradientOp( Args && ... args)",34, 77, 12, 0
repos/cpp/pytorch/caffe2/operators/elementwise_div_gradient_op.cc,"caffe2::BinaryElementwiseWithArgsGradientOp<NumericTypes,CPUContext,BinaryFunctorWithDefaultCtor<DivFunctor<CPUContext>>,SameTypeAsInput,SameTypeAsInput>::RunOnDevice()",3, 63, 4, 0
repos/cpp/pytorch/caffe2/operators/elementwise_div_gradient_op.cc,"caffe2::BinaryElementwiseWithArgsGradientOp<NumericTypes,CPUContext,BinaryFunctorWithDefaultCtor<DivFunctor<CPUContext>>,SameTypeAsInput,SameTypeAsInput>::DoRunWithType()",83, 79, 14, 0
repos/cpp/pytorch/caffe2/operators/elementwise_div_gradient_op.cc,"caffe2::GetDivGradient::GetGradientDefs()",7, 59, 8, 0
repos/cpp/pytorch/caffe2/operators/sin_op.cc,"caffe2::SinGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 66, 6, 0
repos/cpp/pytorch/caffe2/operators/sin_op.cc,"caffe2::GetSinGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::Tokenizer::Tokenizer( const std :: vector<char> & delims , char escape)",8, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::Tokenizer::reset()",5, 26, 0, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::Tokenizer::next( char * start , char * end , TokenizedString & tokenized)",55, 75, 0, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::FileReader::FileReader( const std :: string & path , size_t bufferSize)",9, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::FileReader::reset()",6, 77, 8, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::FileReader::~FileReader()",5, 28, 0, 0
repos/cpp/pytorch/caffe2/operators/text_file_reader_utils.cc,"caffe2::FileReader::operator ( )( CharRange & range)",15, 69, 8, 0
repos/cpp/pytorch/caffe2/operators/box_with_nms_limit_op.cc,"caffe2::BoxWithNMSLimitOp<CPUContext>::RunOnDevice()",229, 79, 12, 0
repos/cpp/pytorch/caffe2/operators/weighted_sample_op.cc,"caffe2::WeightedSampleOp<float,CPUContext>::RunOnDevice()",62, 104, 10, 0
repos/cpp/pytorch/caffe2/operators/load_save_op_gpu.cc,"caffe2::LoadOp<CUDAContext>::SetCurrentDevice( BlobProto * proto)",8, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/flatten_op.cc,"caffe2::GetFlattenGradient::GetGradientDefs()",4, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/slice_op.cc,"caffe2::GetSliceGradient::GetGradientDefs()",15, 56, 10, 0
repos/cpp/pytorch/caffe2/operators/deform_conv_gradient_op.cc,"caffe2::GetDeformConvGradient::GetGradientDefs()",40, 72, 8, 0
repos/cpp/pytorch/caffe2/operators/jsd_op.cc,"caffe2::kLOG_THRESHOLD()",3, 42, 0, 0
repos/cpp/pytorch/caffe2/operators/jsd_op.cc,"caffe2::logit( float p)",6, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/jsd_op.cc,"caffe2::entropy( float p)",8, 58, 2, 0
repos/cpp/pytorch/caffe2/operators/jsd_op.cc,"caffe2::BernoulliJSDOp<float,CPUContext>::RunOnDevice()",18, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/jsd_op.cc,"caffe2::BernoulliJSDGradientOp<float,CPUContext>::RunOnDevice()",20, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/jsd_op.cc,"caffe2::GetBernoulliJSDGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/acos_op.cc,"caffe2::AcosGradientFunctor<CPUContext>::Forward( const std :: vector<int> & X_dims , const std :: vector<int> & , const T * X , const T * dY , T * dX , CPUContext *) const",14, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/acos_op.cc,"caffe2::GetAcosGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/locally_connected_op_util.cc,"caffe2::lc_op_util::SetColumnBufferShape( const int N , const int kernel_size , const int output_image_size , const std :: vector<int> & output_image_dims , const StorageOrder order , std :: vector<int> * column_slice_dims , std :: vector<int> * column_dims , std :: vector<int> * column_transposed_dims , std :: vector<int> * column_axes)",33, 74, 45, 0
repos/cpp/pytorch/caffe2/operators/locally_connected_op_util.cc,"caffe2::lc_op_util::SetYBufferShape( const int N , const int M , const int output_image_size , const StorageOrder order , std :: vector<int> * Y_dims , std :: vector<int> * Y_transposed_dims , std :: vector<int> * Y_axes)",17, 69, 40, 0
repos/cpp/pytorch/caffe2/operators/replace_nan_op.cc,"caffe2::ReplaceNaNOp<CPUContext>::ReplaceNaN( const T & value , const int64_t size , const T * X , T * Y)",13, 43, 0, 0
repos/cpp/pytorch/caffe2/operators/accuracy_op.cc,"caffe2::AccuracyOp<float,CPUContext>::RunOnDevice()",39, 74, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_shared_gpu.cc,"caffe2::createSharedBuffer<CUDAContext>( Workspace * ws)",6, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/conv_op_shared_gpu.cc,"caffe2::runWithSharedBuffer<CUDAContext>( Workspace * ws , std :: function<void(Tensor*buffer)> f)",12, 77, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_mul_gradient_op.cc,"caffe2::ComputeMulGradient( const int ndim , const int * A_dims , const int * B_dims , const int * C_dims , const TGrad * dC , const TIn * A , const TIn * B , TGrad * dA , TGrad * dB , CPUContext * context)",30, 73, 6, 0
repos/cpp/pytorch/caffe2/operators/elementwise_mul_gradient_op.cc,"caffe2::MulFunctor<CPUContext>::Backward( const std :: vector<int> & A_dims , const std :: vector<int> & B_dims , const TGrad * dC , const TIn * A , const TIn * B , const TOut * , TGrad * dA , TGrad * dB , CPUContext * context) const",42, 68, 8, 0
repos/cpp/pytorch/caffe2/operators/elementwise_mul_gradient_op.cc,"caffe2::GetMulGradient::GetGradientDefs()",7, 56, 2, 0
repos/cpp/pytorch/caffe2/operators/space_batch_op.cc,"caffe2::GetSpaceToBatchGradient::GetGradientDefs()",4, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/space_batch_op.cc,"caffe2::GetBatchToSpaceGradient::GetGradientDefs()",4, 75, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_max_ops.cc,"caffe2::MaxReduceDimsOp<float,CPUContext,true>::Compute( int rows , int cols , const float * data , const int32_t * lengths_data , float * out_data)",15, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_max_ops.cc,"caffe2::MaxReduceDimsOp<float,CPUContext,false>::Compute( int rows , int cols , const float * data , const int32_t * lengths_data , float * out_data)",15, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_max_ops.cc,"caffe2::MaxReduceDimsGradientOp<float,CPUContext,true>::Compute( int rows , int cols , const float * dYdata , const float * Xdata , const float * Ydata , const int32_t * lengths_data , float * dXdata)",19, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_max_ops.cc,"caffe2::MaxReduceDimsGradientOp<float,CPUContext,false>::Compute( int rows , int cols , const float * dYdata , const float * Xdata , const float * Ydata , const int32_t * lengths_data , float * dXdata)",19, 65, 0, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_max_ops.cc,"caffe2::GetReduceFrontMaxGradient::GetGradientDefs()",8, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_max_ops.cc,"caffe2::GetReduceBackMaxGradient::GetGradientDefs()",8, 70, 8, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::resizeNearestNCHW2x( int batch_size , int num_channels , int input_height , int input_width , const float * input , float * output)",47, 69, 10, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::ResizeNearestOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",49, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::ResizeNearestOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",45, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::ResizeNearestOp<float,CPUContext>::RunOnDevice()",10, 57, 0, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::ResizeNearestGradientOp<float,CPUContext>::RunOnDeviceWithOrderNCHW()",48, 79, 10, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::ResizeNearestGradientOp<float,CPUContext>::RunOnDeviceWithOrderNHWC()",53, 81, 6, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::ResizeNearestGradientOp<float,CPUContext>::RunOnDevice()",10, 65, 0, 0
repos/cpp/pytorch/caffe2/operators/resize_op.cc,"caffe2::GetResizeNearestGradient::GetGradientDefs()",15, 64, 6, 0
repos/cpp/pytorch/caffe2/operators/last_n_window_collector.cc,"caffe2::LastNWindowCollectorOp::LastNWindowCollectorOp( Args && ... args)",6, 74, 12, 0
repos/cpp/pytorch/caffe2/operators/last_n_window_collector.cc,"caffe2::LastNWindowCollectorOp::RunOnDevice()",9, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/last_n_window_collector.cc,"caffe2::LastNWindowCollectorOp::collect()",99, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/pow_op.cc,"caffe2::EigenPowFunctor::Run( size_t n , const T1 * a , const T2 * b , T2 e , R * out , CPUContext *)",15, 71, 2, 0
repos/cpp/pytorch/caffe2/operators/pow_op.cc,"caffe2::EigenPowFunctor::RunWithBroadcast( const T1 * a , const T2 * b , R * out , size_t pre , size_t n , CPUContext *)",18, 72, 8, 0
repos/cpp/pytorch/caffe2/operators/pow_op.cc,"caffe2::EigenPowFunctor::RunWithBroadcast2( const T1 * a , const T2 * b , R * out , size_t pre , size_t n , size_t post , CPUContext *)",23, 72, 10, 0
repos/cpp/pytorch/caffe2/operators/pow_op.cc,"caffe2::GetPowGradient::GetGradientDefs()",206, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/pow_op.cc,"caffe2::GetPowGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/gather_op.cc,"caffe2::GetGatherGradient::GetGradientDefs()",49, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/clip_op.cc,"caffe2::ClipOp<float,CPUContext>::RunOnDevice()",10, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/clip_op.cc,"caffe2::ClipGradientOp<float,CPUContext>::RunOnDevice()",15, 66, 4, 0
repos/cpp/pytorch/caffe2/operators/clip_op.cc,"caffe2::GetClipGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/spatial_batch_norm_op.cc,"caffe2::CostInferenceForSpatialBN( const OperatorDef & def , const vector<TensorShape> & in)",13, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/multi_class_accuracy_op.cc,"caffe2::MultiClassAccuracyOp<float,CPUContext>::RunOnDevice()",47, 62, 0, 0
repos/cpp/pytorch/caffe2/operators/instance_norm_gradient_op.cc,"caffe2::InstanceNormGradientOp<T,Context>::RunOnDeviceWithOrderNHWC()",129, 79, 8, 0
repos/cpp/pytorch/caffe2/operators/instance_norm_gradient_op.cc,"caffe2::InstanceNormGradientOp<T,Context>::RunOnDeviceWithOrderNCHW()",107, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/instance_norm_gradient_op.cc,"caffe2::GetInstanceNormGradient::GetGradientDefs()",14, 52, 4, 0
repos/cpp/pytorch/caffe2/operators/prepend_dim_op.cc,"caffe2::GetPrependDimGradient::GetGradientDefs()",4, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/prepend_dim_op.cc,"caffe2::GetPrependDimGradient::CopyArguments() const",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/elementwise_logical_ops.cc,"caffe2::IsMemberOfValueHolder::get<int32_t>()",3, 69, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_logical_ops.cc,"caffe2::IsMemberOfValueHolder::get<int64_t>()",3, 69, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_logical_ops.cc,"caffe2::IsMemberOfValueHolder::get<bool>()",3, 63, 0, 0
repos/cpp/pytorch/caffe2/operators/elementwise_logical_ops.cc,"caffe2::IsMemberOfValueHolder::get<string>()",3, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/stump_func_op.cc,"caffe2::StumpFuncOp<float,float,CPUContext>::RunOnDevice()",11, 73, 4, 0
repos/cpp/pytorch/caffe2/operators/stump_func_op.cc,"caffe2::StumpFuncIndexOp<float,int64_t,CPUContext>::RunOnDevice()",23, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/integral_image_op.cc,"caffe2::IntegralImageOp<float,CPUContext>::RunOnDevice()",52, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/integral_image_op.cc,"caffe2::IntegralImageGradientOp<float,CPUContext>::RunOnDevice()",48, 76, 6, 0
repos/cpp/pytorch/caffe2/operators/integral_image_op.cc,"caffe2::GetIntegralImageGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_mean_ops.cc,"caffe2::SumReduceDimsOp<CPUContext,true,true>::Compute( int rows , int cols , const T * in_data , const int32_t * lengths_data , T * out_data)",15, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_mean_ops.cc,"caffe2::SumReduceDimsOp<CPUContext,false,true>::Compute( int rows , int cols , const T * in_data , const int32_t * lengths_data , T * out_data)",16, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_mean_ops.cc,"caffe2::SumReduceDimsGradientOp<CPUContext,true,true>::Compute( int rows , int cols , const T * dYdata , const int * lengths_data , T * dXdata)",18, 63, 0, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_mean_ops.cc,"caffe2::SumReduceDimsGradientOp<CPUContext,false,true>::Compute( int rows , int cols , const T * dYdata , const int * lengths_data , T * dXdata)",18, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_mean_ops.cc,"caffe2::GetReduceFrontMeanGradient::GetGradientDefs()",8, 72, 8, 0
repos/cpp/pytorch/caffe2/operators/reduce_front_back_mean_ops.cc,"caffe2::GetReduceBackMeanGradient::GetGradientDefs()",8, 71, 8, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/flatten_cpu.cc,"caffe2::flatten_op_cpu_impl( const at :: Tensor & input_ , const at :: Tensor & output_ , int64_t axis)",16, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/averaged_loss_cpu.cc,"caffe2::averaged_loss_op_cpu_impl( const at :: Tensor & X_ , const at :: Tensor & sum_ , Cache * state)",28, 44, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/mul_cpu.cc,"caffe2::mul_op_cpu_impl( const at :: Tensor & A_ , const at :: Tensor & B_ , const at :: Tensor & C_ , bool legacy_broadcast , int64_t axis)",59, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/relu_cpu.cc,"caffe2::relu_op_cpu_impl( const at :: Tensor & input_ , const at :: Tensor & output_)",30, 79, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/expand_dims_cpu.cc,"caffe2::expand_dims_op_cpu_impl( const at :: Tensor & input_ , const at :: Tensor & output_ , ArrayRef<int64_t> dims , Cache * cache)",40, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/filler_cpu.cc,"caffe2::filler_init( ArrayRef<at::Tensor> inputs , const at :: Tensor & output_ , ArrayRef<int64_t> shape , ArrayRef<int64_t> extra_shape , bool input_as_shape)",31, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/filler_cpu.cc,"caffe2::given_tensor_fill_op_cpu_impl( ArrayRef<at::Tensor> inputs , const at :: Tensor & output_ , ArrayRef<int64_t> shape , ArrayRef<int64_t> extra_shape , bool input_as_shape , const at :: Tensor & values_)",24, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/filler_cpu.cc,"caffe2::constant_fill_op_cpu_impl( ArrayRef<at::Tensor> inputs , const at :: Tensor & output_ , ArrayRef<int64_t> shape , ArrayRef<int64_t> extra_shape , bool input_as_shape , int64_t dtype , c10 :: Scalar value)",39, 68, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/filler_cpu.cc,"caffe2::uniform_fill_op_cpu_impl( ArrayRef<at::Tensor> inputs , const at :: Tensor & output_ , ArrayRef<int64_t> shape , ArrayRef<int64_t> extra_shape , bool input_as_shape , double min , double max)",33, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc,"caffe2::sparse_lengths_sum_op_cpu_impl_( const at :: Tensor & dataInput_ , const at :: Tensor & indicesInput_ , const at :: Tensor & lengthsInput_ , const at :: Tensor & output_)",45, 79, 6, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc,"caffe2::sparse_lengths_sum_op_cpu_impl( const at :: Tensor & dataInput , const at :: Tensor & indicesInput , const at :: Tensor & lengthsInput , const at :: Tensor & output)",11, 135, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc,"caffe2::sparse_lengths_sum_op_cpu( const at :: Tensor & dataInput , const at :: Tensor & indicesInput , const at :: Tensor & lengthsInput , const at :: Tensor & output)",11, 126, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sigmoid_cpu.cc,"caffe2::sigmoid_op_cpu_impl( const at :: Tensor & input_ , const at :: Tensor & output_)",13, 56, 6, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/cast_cpu.cc,"caffe2::do_cast_( const Tensor & input , const Tensor & output)",9, 59, 0, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/cast_cpu.cc,"caffe2::cast_op_cpu_impl( const at :: Tensor & input_ , const at :: Tensor & output_ , int64_t to_)",52, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/cast_cpu.cc,"caffe2::cast_op_cpu( const at :: Tensor & input , const at :: Tensor & output , int64_t to)",11, 110, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/stop_gradient_cpu.cc,"caffe2::stop_gradient_op_cpu_impl( const at :: Tensor & input_ , const at :: Tensor & output_)",9, 37, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/batch_gather_cpu.cc,"caffe2::batch_gather_op_cpu_impl( const at :: Tensor & data_ , const at :: Tensor & indices_ , const at :: Tensor & output_)",42, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/batch_gather_cpu.cc,"caffe2::batch_gather_op_cpu( const at :: Tensor & data , const at :: Tensor & indices , const at :: Tensor & output)",9, 104, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/concat_cpu.cc,"caffe2::concat_op_cpu_impl( ArrayRef<at::Tensor> inputs , const at :: Tensor & output_ , const at :: Tensor & split_ , int64_t axis , int64_t add_axis)",91, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/batch_matmul_cpu.cc,"caffe2::batch_matmul_op_cpu_impl( const at :: Tensor & A_ , const at :: Tensor & B_ , const at :: Tensor & Y_ , int64_t trans_a , int64_t trans_b , int64_t broadcast , Cache * cache)",252, 99, 8, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc,"caffe2::sigmoid_partition( float lgt)",4, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc,"caffe2::sigmoid_xent_forward( float lgt , float tgt)",3, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc,"caffe2::sigmoid_xent_forward_with_log_d_trick( float lgt , float tgt)",3, 75, 0, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc,"caffe2::unjoined_sigmoid_xent_forward( float lgt , float tgt)",4, 67, 0, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc,"caffe2::sigmoid_cross_entropy_with_logits_op_cpu_impl( const at :: Tensor & logits_ , const at :: Tensor & targets_ , const at :: Tensor & out_ , bool log_D_trick , bool unjoined_lr_loss)",44, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/fc_cpu.cc,"caffe2::fc_op_cpu_impl( const at :: Tensor & X_ , const at :: Tensor & W_ , const at :: Tensor & b_ , const at :: Tensor & Y_ , int64_t axis , int64_t axis_w , Cache * cache)",106, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/enforce_finite_cpu.cc,"caffe2::enforce_finite_op_impl_cpu( const at :: Tensor & input_)",14, 64, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/cpu/add_cpu.cc,"caffe2::add_op_cpu_impl( const at :: Tensor & A_ , const at :: Tensor & B_ , const at :: Tensor & C_ , bool legacy_broadcast , int64_t axis)",59, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/layer_norm.cc,"AxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/layer_norm.cc,"AxisParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/layer_norm.cc,"EpsilonParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/layer_norm.cc,"EpsilonParameter::default_value()",3, 43, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"ShapeParameter::parse( const caffe2 :: ArgumentHelper & helper)",3, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"ExtraShapeParameter::parse( const caffe2 :: ArgumentHelper & helper)",3, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"InputAsShapeParameter::parse( const caffe2 :: ArgumentHelper & helper)",3, 68, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"DTypeParameter::parse( const caffe2 :: ArgumentHelper & helper)",18, 78, 6, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"ValueParameter::parse( const caffe2 :: ArgumentHelper & helper)",16, 67, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"MinParameter::parse( const caffe2 :: ArgumentHelper & helper)",3, 61, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"MaxParameter::parse( const caffe2 :: ArgumentHelper & helper)",3, 61, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"ValuesParameter::parse( const caffe2 :: ArgumentHelper & helper)",25, 74, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/filler.cc,"ValuesParameter::ExtractValues( const caffe2 :: ArgumentHelper & helper)",12, 69, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/expand_dims.cc,"DimsParameter::parse( const caffe2 :: ArgumentHelper & helper)",3, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/mul.cc,"LegacyBroadcastParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/mul.cc,"LegacyBroadcastParameter::default_value()",3, 42, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/mul.cc,"AxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/mul.cc,"AxisParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc,"LogDTrickParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc,"LogDTrickParameter::default_value()",3, 42, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc,"UnjoinedLRLossParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc,"UnjoinedLRLossParameter::default_value()",3, 42, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/cast.cc,"ToParameter::parse( const caffe2 :: ArgumentHelper & helper)",4, 56, 4, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/batch_matmul.cc,"TransAParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/batch_matmul.cc,"TransAParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/batch_matmul.cc,"TransBParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/batch_matmul.cc,"TransBParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/batch_matmul.cc,"BroadcastParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/batch_matmul.cc,"BroadcastParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/fc.cc,"AxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/fc.cc,"AxisParameter::default_value()",3, 45, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/fc.cc,"AxisWParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/fc.cc,"AxisWParameter::default_value()",3, 45, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/concat.cc,"AxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/concat.cc,"AxisParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/concat.cc,"AddAxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/concat.cc,"AddAxisParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/add.cc,"LegacyBroadcastParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/add.cc,"LegacyBroadcastParameter::default_value()",3, 42, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/add.cc,"AxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/add.cc,"AxisParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/flatten.cc,"AxisParameter::name()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/operators/experimental/c10/schemas/flatten.cc,"AxisParameter::default_value()",3, 41, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_relu_op.cc,"caffe2::CostInferenceForRelu( const OperatorDef & def , const vector<TensorShape> & in)",7, 67, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , ReLU)",21, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , DISABLED_LeakyReLU)",24, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Softmax)",24, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Sigmoid)",24, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , MaxPool)",28, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , AveragePool)",28, 80, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , ResizeNearest)",31, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , ChannelShuffle)",37, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Concat)",38, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Add)",26, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , SumRelu)",28, 75, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::setq( int8 :: Int8TensorCPU * dst , const std :: vector<float> & vs)",12, 80, 16, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::biassetq( int8 :: Int8TensorCPU * dst , const std :: vector<float> & vs)",12, 80, 16, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Conv)",69, 75, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Grouped1x1Conv)",65, 74, 36, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Conv2)",55, 75, 10, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , DepthwiseConv)",57, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , DepthwiseConv3x3)",60, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , DepthwiseConv5x5)",60, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , ConvTranspose)",51, 75, 10, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , FC)",50, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , GivenTensorFill)",33, 74, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , GivenIntTensorFill)",33, 74, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , QuantDeQuant)",19, 70, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Reshape)",18, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Flatten)",18, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_test.cc,"caffe2::TEST( Int8 , Slice)",34, 73, 2, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_average_pool_op.cc,"caffe2::AveragePoolDocGenerator( const char * dim , bool relu_fused = false)",28, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/quantized/init_qnnpack.cc,"caffe2::initQNNPACK()",7, 77, 6, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_max_pool_op.cc,"caffe2::MaxPoolDocGenerator( const char * dim , bool relu_fused = false)",28, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_conv_op.cc,"caffe2::ConvDocGenerator( const char * dim , bool relu_fused = false)",40, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/quantized/int8_roi_align_op_test.cc,"caffe2::TEST( Int8RoIAlign , RoIAlign)",55, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor_gpu.cc,"caffe2::createRNNExecutor<CUDAContext>( const NetDef & step_net_def , std :: map<string,string> & recurrent_input_map , std :: string timestep_blob , ArgumentHelper arg_helper)",15, 91, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor_gpu.cc,"caffe2::CUDARecurrentNetworkExecutor::~CUDARecurrentNetworkExecutor()",7, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor_gpu.cc,"caffe2::CUDARecurrentNetworkExecutor::_ExecRange( int from , int to)",99, 78, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor_gpu.cc,"caffe2::CUDARecurrentNetworkExecutor::Run( int T)",8, 54, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor_gpu.cc,"caffe2::CUDARecurrentNetworkExecutor::RunBackwards( int T)",8, 57, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor.cc,"caffe2::createRNNExecutor<CPUContext>( const NetDef & step_net_def , std :: map<string,string> & recurrent_input_map , std :: string timestep_blob , ArgumentHelper rnn_args)",16, 77, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor.cc,"caffe2::ThreadedRecurrentNetworkExecutor::Run( int T)",22, 55, 6, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor.cc,"caffe2::ThreadedRecurrentNetworkExecutor::RunBackwards( int T)",22, 61, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor.cc,"caffe2::ThreadedRecurrentNetworkExecutor::RunOp( OpTask job , int)",63, 78, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor.cc,"caffe2::ThreadedRecurrentNetworkExecutor::WorkerFunction()",41, 79, 4, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_executor.cc,"caffe2::ThreadedRecurrentNetworkExecutor::_Exec()",31, 80, 4, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_op.cc,"caffe2::GetRecurrentNetworkGradient::GetGradientDefs()",39, 80, 8, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_op.cc,"caffe2::detail::GetRecurrentMapping( const std :: vector<detail::Link> & links , bool backward)",25, 73, 6, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_op.cc,"caffe2::detail::PrependOps( std :: vector<OperatorDef> ops , NetDef * netdef)",10, 64, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_op.cc,"caffe2::detail::AddApplyLinkOps( const vector<Link> & links , std :: string timestep , const DeviceOption & device_option , NetDef * netdef)",44, 76, 4, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_op.cc,"caffe2::detail::extractLinks( OperatorBase * op , const std :: string & internalArg , const std :: string & externalArg , const std :: string & offsetArg , const std :: string & windowArg , std :: vector<detail::Link> * links)",42, 76, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_network_op.cc,"caffe2::detail::extractNetDef( const OperatorDef & op , const std :: string & argName)",19, 81, 8, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::detail::TensorDescriptors<T>::TensorDescriptors( size_t n , const std :: vector<int> & dim , const std :: vector<int> & stride)",16, 60, 4, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::detail::TensorDescriptors<T>::~TensorDescriptors()",5, 45, 0, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::RecurrentBaseOp<T>::~RecurrentBaseOp()",6, 62, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::RecurrentBaseOp<T>::initialize( const Tensor & input , Tensor * dropoutStates , Tensor * output , Tensor * hiddenOutput , Tensor * cellOutput)",153, 81, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::RecurrentOp<T>::RunOnDevice()",91, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::RecurrentGradientOp<T>::RunOnDevice()",86, 80, 2, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::RecurrentParamAccessOp<T,mode>::RunOnDevice()",114, 81, 4, 0
repos/cpp/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc,"caffe2::GetRecurrentGradient::GetGradientDefs()",23, 76, 8, 0
repos/cpp/pytorch/caffe2/onnx/backend_rep.cc,"caffe2::onnx::Caffe2BackendRep::CheckInit()",8, 57, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend_rep.cc,"caffe2::onnx::Caffe2BackendRep::Run( const caffe2 :: Predictor :: TensorList & inputs , caffe2 :: Predictor :: TensorList * outputs)",6, 49, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend_rep.cc,"caffe2::onnx::Caffe2BackendRep::RunMap( const caffe2 :: Predictor :: TensorMap & inputs , caffe2 :: Predictor :: TensorList * outputs)",6, 48, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnxifi_graph_info.cc,"caffe2::onnx::OnnxBackendGraphMap::lookup( const std :: string & key)",8, 80, 0, 0
repos/cpp/pytorch/caffe2/onnx/onnxifi_graph_info.cc,"caffe2::onnx::OnnxBackendGraphMap::insert( const std :: string & key , std :: function<SharedPtrBackendGraphInfo()> creator)",17, 69, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnxifi_graph_info.cc,"caffe2::onnx::OnnxBackendGraphMap::remove( const std :: string & key)",14, 64, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnxifi_graph_info.cc,"caffe2::onnx::getOnnxBackendGraphMap()",4, 53, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::ApplyTrans( std :: unordered_map<std::string,AttributeProto> * attrs , bool global , const std :: string & k , int dim = 2 , const std :: string & ks = '')",54, 81, 6, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::DimProd( const caffe2 :: TensorShape & shape , int start , int end)",7, 72, 0, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::CreateOnnxShapeTensor( std :: shared_ptr<DummyName> dummy , const std :: vector<int64_t> & shape)",12, 51, 6, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::SsaName( const std :: string & n , int version)",3, 57, 0, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::AddShapeNode( const std :: string & input , const std :: string & output)",7, 78, 0, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::Caffe2TypeToOnnxType( caffe2 :: TensorProto :: DataType t)",22, 62, 0, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::SsaRewrite( caffe2 :: NetDef * init_net , caffe2 :: NetDef * pred_net)",84, 81, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::get_renamed_operators() const",17, 79, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::get_renamed_attrs() const",5, 75, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::get_per_op_renamed_attrs() const",11, 81, 32, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::get_special_operators() const",35, 81, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CopyCaffe2ArgToOnnxAttr( AttributeProto * attr , const std :: string & op_type , const caffe2 :: Argument & arg)",36, 72, 6, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::IsBlackListed( const caffe2 :: Argument & arg)",24, 80, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::Caffe2OpToOnnxNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",17, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CommonCaffe2OpToOnnxNodes( const caffe2 :: OperatorDef & def)",23, 77, 6, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateArgMaxMinOpNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",18, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateBinaryElementwiseOpNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",42, 80, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateCastNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",92, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateElementwiseLinearNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",54, 79, 8, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateConvPoolNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",84, 97, 8, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateLrnNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",14, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateConcatNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",77, 81, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateMergeDimNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",33, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateChannelShuffleNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",47, 80, 6, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateReduceMeanNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",66, 82, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateUpsampleNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",51, 79, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateSliceNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",42, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateReshapeNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",39, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::CreateGemmNodes( const caffe2 :: OperatorDef & def , const std :: unordered_map<std::string,caffe2::TensorShape> & shapes)",97, 74, 4, 0
repos/cpp/pytorch/caffe2/onnx/onnx_exporter.cc,"caffe2::onnx::OnnxExporter::InitOpToTensorProto( const caffe2 :: OperatorDef & op , TensorProto * tensor)",59, 78, 4, 0
repos/cpp/pytorch/caffe2/onnx/helper.cc,"caffe2::onnx::DummyName::NewDummyName()",9, 65, 4, 0
repos/cpp/pytorch/caffe2/onnx/helper.cc,"caffe2::onnx::DummyName::Reset( const std :: unordered_set<std::string> & used_names)",4, 75, 0, 0
repos/cpp/pytorch/caffe2/onnx/helper.cc,"caffe2::onnx::ExtraTypeProto( const :: ONNX_NAMESPACE :: TensorProto & tensor)",11, 51, 4, 0
repos/cpp/pytorch/caffe2/onnx/helper.cc,"caffe2::onnx::MakeNode( const std :: string & type , const std :: vector<std::string> & inputs , const std :: vector<std::string> & outputs , const std :: vector<AttributeProto> & attributes , const std :: string & name)",22, 51, 4, 0
repos/cpp/pytorch/caffe2/onnx/device.cc,"caffe2::onnx::Device::Device( const std :: string & spec)",5, 50, 2, 0
repos/cpp/pytorch/caffe2/onnx/onnxifi_init.cc,"caffe2::onnx::initOnnxifiLibrary()",11, 78, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::AlmostEqual( double a , double b)",4, 40, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::TryConvertingTensorRawValues( const TensorProto & onnx_tensor , :: google :: protobuf :: RepeatedField<T> * field)",18, 81, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::IsOperator( const std :: string & op_type)",7, 67, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::GetDeviceOption( const Device & onnx_device)",9, 72, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OptimizeOnnx( const ModelProto & input , bool init)",12, 66, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::LookUpWithDefault( const std :: unordered_map<T,U> & map , const T & key , const U & default_value)",11, 41, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::UpdateNames( std :: shared_ptr<DummyName> dummy , const caffe2 :: OperatorDef & op)",8, 84, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::BuildOperator( caffe2 :: OperatorDef * c2_op , const std :: string & op_type , const std :: vector<std::string> & inputs , const std :: vector<std::string> & outputs , const std :: vector<caffe2::Argument> & args)",19, 49, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::BuildOperator( caffe2 :: OperatorDef * c2_op , const std :: string & op_type , const std :: vector<std::string> & inputs , const std :: vector<std::string> & outputs)",8, 57, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::CopyOnnxAttrValueToCaffe2Arg( caffe2 :: Argument * arg , const AttributeProto & attr)",24, 62, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::OnnxAttributes( const NodeProto & node)",5, 56, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::get( const std :: string & key) const",9, 60, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::get( const std :: string & key) const",9, 58, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::get( const std :: string & key) const",10, 71, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::get( const std :: string & key) const",9, 70, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::get( const std :: string & key) const",9, 52, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::get( const std :: string & key) const",9, 71, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::OnnxAttributes::OnnxAttrToCaffe2Arg( std :: function<std::string(const std::string&)> mapper) const",26, 81, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::get_broken_operators() const",4, 72, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::get_rnn_operators() const",6, 74, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::get_renamed_operators() const",23, 79, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::get_renamed_attrs() const",5, 75, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::get_per_op_renamed_attrs() const",11, 81, 32, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::get_special_operators() const",32, 80, 14, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateArgMaxMin( OnnxNode * onnx_node , const ConversionContext & ctx)",10, 59, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateCast( OnnxNode * onnx_node , const ConversionContext & ctx)",66, 72, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateConstant( OnnxNode * onnx_node , const ConversionContext & ctx)",12, 78, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateConstantOfShape( OnnxNode * onnx_node , const ConversionContext & ctx)",22, 94, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateConvPoolOpBase( OnnxNode * onnx_node , const ConversionContext & ctx)",30, 80, 12, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateReshape( OnnxNode * onnx_node , const ConversionContext & ctx)",10, 58, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateRandomNormal( OnnxNode * onnx_node , const ConversionContext & ctx)",23, 69, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateReciprocal( OnnxNode * onnx_node , const ConversionContext & ctx)",17, 78, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateGather( OnnxNode * onnx_node , const ConversionContext & ctx)",31, 68, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateGemm( OnnxNode * onnx_node , const ConversionContext & ctx)",113, 83, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreatePad( OnnxNode * onnx_node , const ConversionContext & ctx)",42, 81, 13, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateConcat( OnnxNode * onnx_node , const ConversionContext & ctx)",10, 58, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateLogSoftmax( OnnxNode * onnx_node , const ConversionContext & ctx)",21, 77, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateSlice( OnnxNode * onnx_node , const ConversionContext & ctx)",147, 80, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::PreprocessSliceIndexTensor( OnnxNode * onnx_node , Caffe2Ops & ret , std :: string indices_tensor , std :: string axes_tensor , std :: string rank_tensor , std :: string zero_tensor , std :: string one_tensor , int default_value)",56, 115, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateDynamicSlice( OnnxNode * onnx_node , const ConversionContext & ctx)",90, 91, 55, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateBatchNormalization( OnnxNode * onnx_node , const ConversionContext & ctx)",20, 87, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateSplit( OnnxNode * onnx_node , const ConversionContext & ctx)",11, 59, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateMatMul( OnnxNode * onnx_node , const ConversionContext & ctx)",17, 58, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateUpsample( OnnxNode * onnx_node , const ConversionContext & ctx)",57, 93, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateDropout( OnnxNode * onnx_node , const ConversionContext & ctx)",11, 62, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CreateLRN( OnnxNode * onnx_node , const ConversionContext & ctx)",17, 58, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::AllNamesInGraph( const GraphProto & graph)",20, 58, 0, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CommonOnnxNodeToCaffe2Ops( OnnxNode * onnx_node , const ConversionContext & ctx)",49, 81, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::ConvertNode( const std :: string & node_str , const ConversionContext & ctx)",11, 71, 2, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::CheckOpSchemaArguments( const caffe2 :: OpSchema & schema , const caffe2 :: OperatorDef & op)",26, 117, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::OnnxNodeToCaffe2Ops( const ModelProto & init_model , const ModelProto & pred_model , const ConversionContext & ctx , OnnxNode * onnx_node)",23, 96, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::OnnxToCaffe2( caffe2 :: NetDef * init_net , caffe2 :: NetDef * pred_net , const ModelProto & onnx_model , const std :: string & device , int opset_version , bool include_initializers , const std :: vector<Caffe2Ops> & extras)",105, 81, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::Prepare( const std :: string & onnx_model_str , const std :: string & device , const std :: vector<Caffe2Ops> & extras)",64, 80, 12, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::ConvertIntegralValueToCaffe2( caffe2 :: OperatorDef * c2_op , caffe2 :: Argument * c2_values , const TensorProto & onnx_tensor)",22, 86, 4, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::ConvertIntegralValueToCaffe2<::google::protobuf::int64>( caffe2 :: OperatorDef * c2_op , caffe2 :: Argument * c2_values , const TensorProto & onnx_tensor)",10, 94, 60, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::ConvertIntegralValueToCaffe2<::google::protobuf::uint64>( caffe2 :: OperatorDef * c2_op , caffe2 :: Argument * c2_values , const TensorProto & onnx_tensor)",15, 96, 62, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::BuildTensorFillingOp( caffe2 :: OperatorDef * c2_op , const TensorProto & onnx_tensor , const std :: string & output_name , const std :: string & shape_name)",127, 95, 6, 0
repos/cpp/pytorch/caffe2/onnx/backend.cc,"caffe2::onnx::Caffe2Backend::SupportOp( const std :: string type) const",3, 62, 0, 0
repos/cpp/pytorch/caffe2/onnx/ssa_test.cc,"TEST( SsaTest , ConvReluInplace)",30, 63, 2, 0
repos/cpp/pytorch/caffe2/onnx/ssa_test.cc,"TEST( SsaTest , FC_FC_FC_InPlace_Output)",42, 63, 2, 0
repos/cpp/pytorch/caffe2/onnx/torch_ops/schema.cc,"PyTorchSchemasRegisterer::PyTorchSchemasRegisterer()",7, 75, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state_nomni.cc,"pybind11::detail::type_caster<std::vector<nom::repr::NNGraph::NodeRef>>::cast( const std :: vector<nom::repr::NNGraph::NodeRef> & src , return_value_policy , handle parent)",6, 78, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state_nomni.cc,"pybind11::detail::type_caster<std::vector<nom::repr::NNGraph::NodeRef>>::cast( const std :: vector<nom::repr::NNGraph::NodeRef> * src , return_value_policy pol , handle parent)",6, 59, 6, 0
repos/cpp/pytorch/caffe2/python/pybind_state_nomni.cc,"caffe2::python::NNPrinter( typename nom :: repr :: NNGraph :: NodeRef node)",14, 75, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state_nomni.cc,"caffe2::python::GraphPrinter( typename Graph :: NodeRef node)",6, 80, 0, 0
repos/cpp/pytorch/caffe2/python/pybind_state_nomni.cc,"caffe2::python::addNomnigraphMethods( pybind11 :: module & m)",475, 81, 16, 0
repos/cpp/pytorch/caffe2/python/pybind_state_dlpack.cc,"caffe2::python::CaffeToDLDeviceType( int device_type)",8, 65, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_dlpack.cc,"caffe2::python::CaffeToDLType( const TypeMeta & meta)",15, 59, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_dlpack.cc,"caffe2::python::DLTypeToCaffe( const DLDataType & dl_type)",38, 80, 8, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFetcher::type_transform( const itensor & atensor)",15, 52, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFetcher::Fetch( const Blob & blob)",8, 57, 6, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFetcher::FetchTensor( const itensor & atensor , bool force_copy)",41, 80, 6, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFeeder::type_transform( const TypeMeta & meta)",12, 60, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFeeder::FeedTensor( const DeviceOption & option , PyArrayObject * original_array , itensor * tensor)",40, 78, 8, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFeeder::ZeroDim( PyArrayObject * array)",8, 63, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state_ideep.cc,"caffe2::python::IDeepFeeder::Feed( const DeviceOption & option , PyArrayObject * original_array , Blob * blob , bool in_place)",38, 94, 34, 0
repos/cpp/pytorch/caffe2/python/pybind_state_hip.cc,"caffe2::python::addHIPGlobalMethods( py :: module & m)",18, 62, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state_hip.cc,"caffe2::python::addHIPObjectMethods( py :: module & m)",32, 80, 10, 0
repos/cpp/pytorch/caffe2/python/pybind_state_hip.cc,"caffe2::python::PYBIND11_MODULE( caffe2_pybind11_state_hip , m)",11, 78, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_gpu.cc,"caffe2::python::addCUDAGlobalMethods( py :: module & m)",84, 111, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_gpu.cc,"caffe2::python::addCUDAObjectMethods( py :: module & m)",32, 81, 10, 0
repos/cpp/pytorch/caffe2/python/pybind_state_gpu.cc,"caffe2::python::PYBIND11_MODULE( caffe2_pybind11_state_gpu , m)",11, 78, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state_int8.cc,"caffe2::python::Int8TensorFetcher::Fetch( const Blob & blob)",25, 79, 8, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::BlobFetcherBase::~BlobFetcherBase()",1, 39, 0, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::BlobFeederBase::~BlobFeederBase()",1, 37, 0, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::GetCurrentWorkspace()",3, 35, 0, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::StringFetcher::Fetch( const Blob & blob)",3, 48, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::CaffeToNumpyType( const TypeMeta & meta)",22, 57, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::NumpyTypeToCaffe( int numpy_type)",29, 65, 35, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::DefinitionGetter( const Registry * registry)",4, 81, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::switchWorkspaceInternal( const std :: string & name , bool create_if_missing)",13, 80, 0, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::python_detail::gRegistry()",5, 47, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::python_detail::getOpFunc( const std :: string & token)",9, 81, 6, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::python_detail::getGradientFunc( const std :: string & token)",3, 56, 0, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::python_detail::fetchBlob( Workspace * ws , const std :: string & name)",15, 68, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::GetPythonGradient::GetGradientDefs()",42, 81, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::BackgroundPlan::BackgroundPlan( Workspace * ws , PlanDef def)",1, 69, 2, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::BackgroundPlan::run()",4, 81, 8, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::BackgroundPlan::isDone()",5, 63, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::BackgroundPlan::isSucceeded()",4, 29, 4, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::addObjectMethods( py :: module & m)",683, 85, 14, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::addGlobalMethods( py :: module & m)",809, 85, 8, 0
repos/cpp/pytorch/caffe2/python/pybind_state.cc,"caffe2::python::PYBIND11_MODULE( caffe2_pybind11_state , m)",9, 66, 2, 0
repos/cpp/pytorch/caffe2/python/mpi_python.cc,"caffe2::PYBIND11_MODULE( mpi_utils , m)",36, 59, 4, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreSetOp::StoreSetOp( const OperatorDef & operator_def , Workspace * ws)",5, 81, 10, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreSetOp::RunOnDevice()",7, 73, 6, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreGetOp::StoreGetOp( const OperatorDef & operator_def , Workspace * ws)",5, 71, 0, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreGetOp::RunOnDevice()",7, 75, 2, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreAddOp::StoreAddOp( const OperatorDef & operator_def , Workspace * ws)",6, 71, 0, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreAddOp::RunOnDevice()",8, 73, 6, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreWaitOp::StoreWaitOp( const OperatorDef & operator_def , Workspace * ws)",3, 73, 0, 0
repos/cpp/pytorch/caffe2/distributed/store_ops.cc,"caffe2::StoreWaitOp::RunOnDevice()",17, 76, 8, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::RedisStoreHandler( std :: string & host , int port , std :: string & prefix)",14, 60, 2, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::~RedisStoreHandler()",3, 42, 0, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::compoundKey( const std :: string & name)",3, 70, 0, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::set( const std :: string & name , const std :: string & data)",20, 80, 0, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::get( const std :: string & name , const std :: chrono :: milliseconds & timeout)",13, 79, 2, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::add( const std :: string & name , int64_t value)",9, 73, 0, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::check( const std :: vector<std::string> & names)",21, 75, 2, 0
repos/cpp/pytorch/caffe2/distributed/redis_store_handler.cc,"caffe2::RedisStoreHandler::wait( const std :: vector<std::string> & names , const std :: chrono :: milliseconds & timeout)",19, 75, 4, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::encodeName( const std :: string & name)",13, 81, 4, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::FileStoreHandler( const std :: string & path , const std :: string & prefix)",16, 61, 4, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::~FileStoreHandler()",1, 41, 0, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::realPath( const std :: string & path)",11, 66, 0, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::tmpPath( const std :: string & name)",3, 65, 0, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::objectPath( const std :: string & name)",3, 68, 0, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::set( const std :: string & name , const std :: string & data)",17, 79, 0, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::get( const std :: string & name , const std :: chrono :: milliseconds & timeout)",21, 75, 8, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::add( const std :: string & , int64_t)",6, 62, 2, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::check( const std :: vector<std::string> & names)",22, 70, 0, 0
repos/cpp/pytorch/caffe2/distributed/file_store_handler.cc,"caffe2::FileStoreHandler::wait( const std :: vector<std::string> & names , const std :: chrono :: milliseconds & timeout)",17, 75, 4, 0
repos/cpp/pytorch/caffe2/distributed/py_export.cc,"caffe2::python::PYBIND11_MODULE( python , m)",9, 61, 2, 0
repos/cpp/pytorch/caffe2/distributed/store_handler.cc,"caffe2::StoreHandler::~StoreHandler()",4, 59, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy.cc,"caffe2::TypedAxpy<float,float>( int N , const float a , const float * x , float * y)",5, 79, 0, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy.cc,"caffe2::TypedAxpyHalffloat__base( int N , const float a , const at :: Half * x , float * y)",22, 64, 4, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy.cc,"caffe2::TypedAxpy<at::Half,float>( int N , const float a , const at :: Half * x , float * y)",9, 47, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy.cc,"caffe2::TypedAxpy_uint8_float__base( int N , const float a , const std :: uint8_t * x , float * y)",9, 34, 0, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy.cc,"caffe2::TypedAxpy<std::uint8_t,float>( int N , const float a , const std :: uint8_t * x , float * y)",8, 50, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy_avx.cc,"caffe2::TypedAxpyHalffloat__avx_f16c( int N , const float a , const at :: Half * x , float * y)",35, 81, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad_avx.cc,"caffe2::adagrad_update__avx_f16c( int N , const float * w , const float * g , const float * h , float * nw , float * nh , float epsilon , float decay , float lr)",32, 77, 8, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad_avx.cc,"caffe2::adagrad_update_prefetch__avx_f16c( int N , const float * w , const float * w_n , const float * g , const float * h , const float * h_n , float * nw , float * nw_n , float * nh , float * nh_n , float epsilon , float lr)",21, 62, 6, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad_avx.cc,"caffe2::adagrad_fp16_update_prefetch__avx_f16c( int N , const at :: Half * w , const at :: Half * w_n , const float * g , const at :: Half * h , const at :: Half * h_n , at :: Half * nw , at :: Half * nw_n , at :: Half * nh , at :: Half * nh_n , float epsilon , float lr)",49, 77, 4, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad_avx.cc,"caffe2::rowwise_adagrad_update__avx_f16c( int N , float * w , float * w_n , const float * g , float * h , float * h_n , float epsilon , float lr)",14, 79, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::adagrad_update__base( int N , const float * w , const float * g , const float * h , float * nw , float * nh , float epsilon , float decay , const float lr)",12, 81, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::adagrad_update_prefetch__base( int N , const float * w , const float * , const float * g , const float * h , const float * , float * nw , float * , float * nh , float * , float epsilon , float lr)",20, 63, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::adagrad_fp16_update_prefetch__base( int N , const at :: Half * w , const at :: Half * , const float * g , const at :: Half * h , const at :: Half * , at :: Half * nw , at :: Half * , at :: Half * nh , at :: Half * , float epsilon , float lr)",15, 80, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::rowwise_adagrad_update__base( int N , float * w , float * w_n , const float * g , float * h , float * h_n , float epsilon , float lr)",14, 79, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::adagrad_update( int N , const float * w , const float * g , const float * h , float * nw , float * nh , float epsilon , float decay , float lr)",13, 71, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::adagrad_update_prefetch( int N , const float * w , const float * w_n , const float * g , const float * h , const float * h_n , float * nw , float * nw_n , float * nh , float * nh_n , float epsilon , float lr)",47, 38, 4, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::adagrad_fp16_update_prefetch( int N , const at :: Half * w , const at :: Half * w_n , const float * g , const at :: Half * h , const at :: Half * h_n , at :: Half * nw , at :: Half * nw_n , at :: Half * nh , at :: Half * nh_n , float epsilon , float lr)",42, 41, 4, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::rowwise_adagrad_update( int N , float * w , float * w_n , const float * g , float * h , float * h_n , float epsilon , float lr)",15, 74, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::sparse_adagrad( int num_rows , int block_size , uint64_t param_size , const float * w , const float * g , const float * h , const int32_t * indices , float * nw , float * nh , float epsilon , float lr)",39, 30, 6, 0
repos/cpp/pytorch/caffe2/perfkernels/adagrad.cc,"caffe2::sparse_adagrad( int num_rows , int block_size , uint64_t param_size , const float * w , const float * g , const float * h , const int64_t * indices , float * nw , float * nh , float epsilon , float lr)",39, 30, 6, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_float_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",374, 77, 10, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_float_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 79, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_float_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 78, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_float_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",374, 77, 10, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_float_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 79, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_float_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 78, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_half_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",493, 80, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_half_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 78, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_half_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 77, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_half_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",493, 80, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_half_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 78, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_half_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 77, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",517, 81, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 81, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 80, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",517, 81, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 81, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,"caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int64_t * indices , const int * lengths , const float * weights , bool normalize_by_lengths , float * out)",23, 80, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup.cc,"caffe2::Fused8BitRowwiseEmbeddingLookupGenericSlow( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const InType * input , const IndexType * indices , const int * lengths , const float * weights , bool normalize_by_lengths , OutType * out)",60, 80, 10, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_float_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",375, 77, 10, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_float_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 63, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_float_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 62, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_float_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",375, 77, 10, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_float_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 63, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_float_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const float * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 62, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_half_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",494, 80, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_half_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 62, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_half_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 61, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_half_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",494, 80, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_half_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 62, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_half_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const at :: Half * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 61, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_uint8_t_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",508, 81, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 65, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int32_t_uint8_t_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 64, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_uint8_t_float__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",508, 81, 16, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 65, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup_avx2.cc,"caffe2::EmbeddingLookup_int64_t_uint8_t_float_true__avx2_fma( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const uint8_t * input , const int64_t * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , float * out)",25, 64, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/math_cpu_base.cc,"caffe2::math::quantize_and_compress__base( const float * input_data , uint8_t * output_data , uint64_t input_size , uint64_t bitwidth , bool random , const float * random_buffer)",70, 81, 59, 0
repos/cpp/pytorch/caffe2/perfkernels/math_cpu_base.cc,"caffe2::math::quantize_and_compress( const float * input_data , uint8_t * output_data , uint64_t input_size , uint64_t bitwidth , bool random , const float * random_buffer)",24, 34, 4, 0
repos/cpp/pytorch/caffe2/perfkernels/math_cpu_base.cc,"caffe2::math::decompress_and_dequantize__base( const uint8_t * input_data , float * output_data , uint64_t input_size)",32, 81, 58, 0
repos/cpp/pytorch/caffe2/perfkernels/math_cpu_base.cc,"caffe2::math::decompress_and_dequantize( const uint8_t * input_data , float * output_data , uint64_t input_size)",7, 75, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy_avx2.cc,"caffe2::TypedAxpyHalffloat__avx2_fma( int N , const float a , const at :: Half * x , float * y)",34, 81, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/typed_axpy_avx2.cc,"caffe2::TypedAxpy_uint8_float__avx2_fma( int N , const float a , const std :: uint8_t * x , float * y)",35, 81, 2, 0
repos/cpp/pytorch/caffe2/perfkernels/embedding_lookup.cc,"caffe2::EmbeddingLookupGenericSlow( const int64_t block_size , const int64_t output_size , const int64_t index_size , const int64_t data_size , const InType * input , const IndexType * indices , const int * lengths , const float * weights , const float * scale_bias , bool normalize_by_lengths , OutType * out)",57, 77, 4, 0
repos/cpp/pytorch/caffe2/perfkernels/math_cpu_avx2.cc,"caffe2::math::quantize_and_compress__avx2( const float * input_data , uint8_t * output_data , uint64_t input_size , uint64_t bitwidth , bool random , const float * random_buffer)",148, 81, 59, 0
repos/cpp/pytorch/caffe2/perfkernels/math_cpu_avx2.cc,"caffe2::math::decompress_and_dequantize__avx2( const uint8_t * input_data , float * output_data , uint64_t input_size)",47, 81, 58, 0
repos/cpp/pytorch/caffe2/experiments/operators/funhash_op.cc,"caffe2::GetFunHashGradient::GetGradientDefs()",13, 61, 8, 0
repos/cpp/pytorch/caffe2/experiments/operators/sparse_funhash_op.cc,"caffe2::GetSparseFunHashGradient::GetGradientDefs()",15, 61, 8, 0
repos/cpp/pytorch/caffe2/experiments/operators/tt_pad_op.cc,"caffe2::GetTTPadGradient::GetGradientDefs()",8, 51, 2, 0
repos/cpp/pytorch/caffe2/experiments/operators/fully_connected_op_decomposition.cc,"caffe2::GetFCDecompGradient::GetGradientDefs()",8, 72, 4, 0
repos/cpp/pytorch/caffe2/experiments/operators/fully_connected_op_prune.cc,"caffe2::GetFCPruneGradient::GetGradientDefs()",7, 73, 8, 0
repos/cpp/pytorch/caffe2/experiments/operators/tt_contraction_op.cc,"caffe2::GetTTContractionGradient::GetGradientDefs()",8, 51, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCFunction::MaxPoolRTCFunction()",1, 70, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCFunction::KernelName( Args ...)",3, 40, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCFunction::MaxPoolGradientRTCFunction()",1, 78, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCFunction::KernelName( Args ...)",3, 40, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCFunction::GetSource( const int output_size , const int channels , const int height , const int width , const int pooled_height , const int pooled_width , const int kernel_h , const int kernel_w , const int stride_h , const int stride_w , const int pad_t , const int pad_l)",22, 80, 6, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCFunction::GetSource( const int output_size , const int num , const int channels , const int height , const int width , const int pooled_height , const int pooled_width , const int kernel_h , const int kernel_w , const int stride_h , const int stride_w , const int pad_t , const int pad_l)",23, 77, 6, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCOp::MaxPoolRTCOp( const OperatorDef & operator_def , Workspace * ws)",5, 74, 8, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCOp::~MaxPoolRTCOp()",1, 30, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCOp::RunOnDeviceWithOrderNCHW()",38, 83, 4, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolRTCOp::RunOnDeviceWithOrderNHWC()",4, 45, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCOp::MaxPoolGradientRTCOp( const OperatorDef & operator_def , Workspace * ws)",5, 74, 8, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCOp::~MaxPoolGradientRTCOp()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCOp::RunOnDeviceWithOrderNCHW()",42, 72, 4, 0
repos/cpp/pytorch/caffe2/cuda_rtc/pool_op_rtc_gpu.cc,"caffe2::MaxPoolGradientRTCOp::RunOnDeviceWithOrderNHWC()",4, 45, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,"caffe2::ElementwiseRTCFunction::ElementwiseRTCFunction()",1, 74, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,"caffe2::ElementwiseRTCFunction::KernelName( Args ...)",3, 40, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,"caffe2::ElementwiseRTCFunction::GetSource( int input_size , int output_size , const string command_string)",23, 68, 8, 0
repos/cpp/pytorch/caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,"caffe2::ElementwiseRTCOp::ElementwiseRTCOp( const OperatorDef & operator_def , Workspace * ws)",7, 78, 4, 0
repos/cpp/pytorch/caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,"caffe2::ElementwiseRTCOp::~ElementwiseRTCOp()",1, 34, 2, 0
repos/cpp/pytorch/caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,"caffe2::ElementwiseRTCOp::RunOnDevice()",36, 75, 18, 0
repos/cpp/pytorch/caffe2/serialize/inline_container_test.cc,"caffe2::serialize::TEST( PyTorchStreamWriterAndReader , SaveAndLoad)",47, 77, 2, 0
repos/cpp/pytorch/caffe2/serialize/istream_adapter.cc,"caffe2::serialize::IStreamAdapter::IStreamAdapter( std :: istream * istream)",1, 77, 0, 0
repos/cpp/pytorch/caffe2/serialize/istream_adapter.cc,"caffe2::serialize::IStreamAdapter::size() const",11, 48, 2, 0
repos/cpp/pytorch/caffe2/serialize/istream_adapter.cc,"caffe2::serialize::IStreamAdapter::read( uint64_t pos , void * buf , size_t n , const char * what) const",8, 81, 0, 0
repos/cpp/pytorch/caffe2/serialize/istream_adapter.cc,"caffe2::serialize::IStreamAdapter::validate( const char * what) const",5, 56, 0, 0
repos/cpp/pytorch/caffe2/serialize/istream_adapter.cc,"caffe2::serialize::IStreamAdapter::~IStreamAdapter()",1, 37, 0, 0
repos/cpp/pytorch/caffe2/serialize/file_adapter.cc,"caffe2::serialize::FileAdapter::FileAdapter( const std :: string & file_name)",7, 75, 2, 0
repos/cpp/pytorch/caffe2/serialize/file_adapter.cc,"caffe2::serialize::FileAdapter::size() const",3, 35, 0, 0
repos/cpp/pytorch/caffe2/serialize/file_adapter.cc,"caffe2::serialize::FileAdapter::read( uint64_t pos , void * buf , size_t n , const char * what) const",4, 78, 0, 0
repos/cpp/pytorch/caffe2/serialize/file_adapter.cc,"caffe2::serialize::FileAdapter::~FileAdapter()",1, 31, 0, 0
repos/cpp/pytorch/caffe2/serialize/read_adapter_interface.cc,"caffe2::serialize::ReadAdapterInterface::~ReadAdapterInterface()",1, 49, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::istream_read_func( void * pOpaque , mz_uint64 file_ofs , void * pBuf , size_t n)",4, 84, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::basename( const std :: string & name)",20, 55, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::read( uint64_t pos , char * buf , size_t n)",3, 70, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader( const std :: string & file_name)",5, 71, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader( std :: istream * in)",5, 59, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader( std :: unique_ptr<ReadAdapterInterface> in)",5, 71, 4, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::init()",62, 83, 6, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::valid( const char * what)",6, 90, 4, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::getPadding( size_t cursor , const std :: string & filename , size_t size)",22, 98, 2, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::getFileID( const std :: string & name)",10, 86, 2, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::getRecord( const std :: string & name)",12, 90, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::read_le_16( uint8_t * buf)",3, 42, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::getRecordOffset( const std :: string & name)",14, 92, 2, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamReader::~PyTorchStreamReader()",4, 46, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::ostream_write_func( void * pOpaque , mz_uint64 file_ofs , const void * pBuf , size_t n)",18, 91, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamWriter::PyTorchStreamWriter( std :: string file_name , std :: ostream * out)",27, 101, 4, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamWriter::writeRecord( const std :: string & name , const void * data , size_t size)",24, 96, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamWriter::writeEndOfFile()",9, 45, 0, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamWriter::valid( const char * what)",9, 90, 4, 0
repos/cpp/pytorch/caffe2/serialize/inline_container.cc,"caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter()",5, 46, 0, 0
repos/cpp/pytorch/caffe2/utils/fixed_divisor_test.cc,"caffe2::CompareDivMod( int32_t v , int32_t divisor)",17, 74, 6, 0
repos/cpp/pytorch/caffe2/utils/fixed_divisor_test.cc,"caffe2::TEST( FixedDivisorTest , FixedDivisorInt32Test)",46, 64, 2, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST( MathTest , GemmNoTransNoTrans)",74, 60, 6, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST( MathTest , GemmNoTransTrans)",73, 60, 6, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::GemmBatchedTest::SetUp()",15, 78, 8, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::GemmBatchedTest::RunGemmBatched( const float alpha , const float beta)",27, 61, 2, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::GemmBatchedTest::RunGemmStridedBatched( const float alpha , const float beta)",24, 68, 2, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::GemmBatchedTest::VerifyOutput( const float value) const",5, 60, 6, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST_P( GemmBatchedTest , GemmBatchedFloatTest)",8, 48, 0, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST_P( GemmBatchedTest , GemmStridedBatchedFloatTest)",8, 55, 0, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST( MathTest , GemvNoTrans)",65, 60, 6, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST( MathTest , GemvTrans)",65, 60, 6, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST( MathTest , FloatToHalfConversion)",13, 55, 2, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::BroadcastTest::SetUp()",3, 53, 4, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::BroadcastTest::RunBroadcastTest( const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<float> & X_data , const std :: vector<float> & Y_data)",28, 78, 4, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST_F( BroadcastTest , BroadcastFloatTest)",11, 76, 2, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::RandFixedSumTest::SetUp()",3, 53, 4, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST_F( RandFixedSumTest , UpperBound)",5, 56, 6, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TransposeTest::SetUp()",3, 53, 4, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TransposeTest::RunTransposeTest( const std :: vector<int> & X_dims , const std :: vector<int> & axes , const std :: vector<float> & X_data , const std :: vector<float> & Y_data)",31, 78, 4, 0
repos/cpp/pytorch/caffe2/utils/math_test.cc,"caffe2::TEST_F( TransposeTest , TransposeFloatTest)",23, 70, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"dummy_thread( void *)",5, 28, 0, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"forkAndPipe( std :: string & stderrBuffer , std :: function<void(void)> callback)",68, 80, 8, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGABRT8)",3, 44, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGINT8)",3, 42, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGILL8)",3, 42, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGFPE8)",3, 42, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGBUS8)",3, 42, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGSEGV8)",3, 44, 2, 0
repos/cpp/pytorch/caffe2/utils/fatal_signal_asan_no_sig_test.cc,"TEST( fatalSignalTest , SIGABRT8_NOPRINT)",3, 53, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::DeviceTypeName( const int32_t & d)",3, 61, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::DeviceId( const DeviceOption & option)",13, 80, 6, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::IsSameDevice( const DeviceOption & lhs , const DeviceOption & rhs)",7, 81, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::IsCPUDeviceType( int device_type)",9, 51, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::IsGPUDeviceType( int device_type)",7, 51, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ReadStringFromFile( const char * filename , string * str)",14, 72, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::WriteStringToFile( const string & str , const char * filename)",10, 77, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::IfstreamInputStream::IfstreamInputStream( const string & filename)",2, 67, 6, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::IfstreamInputStream::~IfstreamInputStream()",1, 43, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::IfstreamInputStream::Read( void * buffer , int size)",7, 49, 4, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ProtoDebugString( const MessageLite & proto)",9, 63, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ParseProtoFromLargeString( const string & str , MessageLite * proto)",9, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ReadProtoFromBinaryFile( const char * filename , MessageLite * proto)",12, 69, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::WriteProtoToBinaryFile( const MessageLite & , const char *)",5, 40, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::TextFormat::ParseFromString( const string & spec , Message * proto)",18, 85, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ProtoDebugString( const Message & proto)",3, 59, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ParseProtoFromLargeString( const string & str , Message * proto)",7, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ReadProtoFromTextFile( const char * filename , Message * proto)",9, 78, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::WriteProtoToTextFile( const Message & proto , const char * filename)",9, 69, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ReadProtoFromBinaryFile( const char * filename , MessageLite * proto)",20, 78, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::WriteProtoToBinaryFile( const MessageLite & proto , const char * filename)",14, 79, 6, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ArgumentHelper::ArgumentHelper( const OperatorDef & def)",20, 81, 6, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ArgumentHelper::ArgumentHelper( const NetDef & netdef)",9, 74, 8, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::ArgumentHelper::HasArgument( const string & name) const",3, 72, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::SupportsLosslessConversion( const InputType & value)",3, 74, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::operator ==( const NetDef & l , const NetDef & r)",3, 57, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::operator < <( std :: ostream & output , const NetDef & n)",4, 66, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::MakeArgument( const string & name , const MessageLite & value)",6, 81, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::HasOutput( const OperatorDef & op , const std :: string & output)",8, 78, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::HasInput( const OperatorDef & op , const std :: string & input)",8, 76, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetArgumentIndex( const google :: protobuf :: RepeatedPtrField<Argument> & args , const string & name)",12, 62, 4, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetArgument( const OperatorDef & def , const string & name)",14, 49, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetArgument( const NetDef & def , const string & name)",12, 80, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetFlagArgument( const google :: protobuf :: RepeatedPtrField<Argument> & args , const string & name , bool default_value)",13, 79, 8, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetFlagArgument( const OperatorDef & def , const string & name , bool default_value)",6, 58, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetFlagArgument( const NetDef & def , const string & name , bool default_value)",3, 77, 0, 0
repos/cpp/pytorch/caffe2/utils/proto_utils.cc,"caffe2::GetMutableArgument( const string & name , const bool create_if_missing , OperatorDef * def)",18, 50, 2, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"rotl32( uint32_t x , int8_t r)",3, 47, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"rotl64( uint64_t x , int8_t r)",3, 47, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"getblock32( const uint32_t * p , int i)",3, 61, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"getblock64( const uint64_t * p , int i)",3, 61, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"fmix32( uint32_t h)",9, 43, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"fmix64( uint64_t k)",9, 43, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"caffe2::MurmurHash3_x86_32( const void * key , int len , uint32_t seed , void * out)",57, 78, 0, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"caffe2::MurmurHash3_x86_128( const void * key , const int len , uint32_t seed , void * out)",174, 67, 2, 0
repos/cpp/pytorch/caffe2/utils/murmur_hash3.cc,"caffe2::MurmurHash3_x64_128( const void * key , const int len , const uint32_t seed , void * out)",124, 63, 2, 0
repos/cpp/pytorch/caffe2/utils/simple_queue_test.cc,"caffe2::ConsumerFunction( int thread_idx)",7, 70, 4, 0
repos/cpp/pytorch/caffe2/utils/simple_queue_test.cc,"caffe2::ProducerFunction( int thread_idx , int start , int count)",6, 73, 4, 0
repos/cpp/pytorch/caffe2/utils/simple_queue_test.cc,"caffe2::TEST( SimpleQueueTest , SingleProducerSingleConsumer)",9, 54, 0, 0
repos/cpp/pytorch/caffe2/utils/simple_queue_test.cc,"caffe2::TEST( SimpleQueueTest , SingleProducerDoubleConsumer)",11, 54, 0, 0
repos/cpp/pytorch/caffe2/utils/simple_queue_test.cc,"caffe2::TEST( SimpleQueueTest , DoubleProducerDoubleConsumer)",12, 54, 0, 0
repos/cpp/pytorch/caffe2/utils/simple_queue_test.cc,"caffe2::TEST( SimpleQueueDeathTest , CannotAddAfterQueueFinished)",6, 58, 0, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::ProxyPrinter::DoRunWithType()",4, 38, 4, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::ProxyPrinter::Print()",16, 59, 4, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::SmartTensorPrinter::SmartTensorPrinter( const std :: string & tensor_name)",2, 71, 0, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::SmartTensorPrinter::SmartTensorPrinter( const std :: string & tensor_name , const std :: string & file_name)",4, 48, 4, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::SmartTensorPrinter::SmartTensorPrinter( const std :: string & tensor_name , const std :: string & file_name , int limit)",5, 55, 4, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::SmartTensorPrinter::Print( const Tensor & tensor)",7, 55, 0, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::SmartTensorPrinter::DefaultTensorPrinter()",10, 75, 6, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer.cc,"caffe2::SmartTensorPrinter::PrintTensor( const Tensor & tensor)",3, 61, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"handleSignal( int signal)",17, 61, 4, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"hookupHandler()",19, 55, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"unhookHandler()",19, 59, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"getPreviousSigaction( int signum)",8, 78, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"getSignalName( int signum)",8, 78, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"unwinder( struct _Unwind_Context * context , void * userInfo)",5, 80, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"getBacktrace()",5, 40, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"printBlobSizes()",4, 63, 6, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"printStacktrace()",39, 72, 6, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"callPreviousSignalHandler( struct sigaction * action , int signum , siginfo_t * info , void * ctx)",14, 55, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"stacktraceSignalHandler( bool needsLock)",14, 76, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"fatalSignalHandler( int signum)",46, 78, 6, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"stacktraceSignalHandler( int signum , siginfo_t * info , void * ctx)",9, 75, 4, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"installFatalSignalHandlers()",25, 79, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"uninstallFatalSignalHandlers()",22, 79, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::SignalHandler( SignalHandler :: Action SIGINT_action , SignalHandler :: Action SIGHUP_action)",9, 41, 4, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::~SignalHandler()",3, 34, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::GotSIGINT()",6, 45, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::GotSIGHUP()",6, 45, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::CheckForSignals()",9, 57, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::setPrintStackTracesOnFatalSignal( bool print)",7, 52, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::printStackTracesOnFatalSignal()",4, 76, 2, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::internal::Caffe2InitFatalSignalHandler( int * , char ** *)",6, 51, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::SignalHandler( SignalHandler :: Action SIGINT_action , SignalHandler :: Action SIGHUP_action)",3, 44, 4, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::~SignalHandler()",1, 35, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::GotSIGINT()",3, 34, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::GotSIGHUP()",3, 34, 0, 0
repos/cpp/pytorch/caffe2/utils/signal_handler.cc,"caffe2::SignalHandler::CheckForSignals()",3, 57, 0, 0
repos/cpp/pytorch/caffe2/utils/string_utils.cc,"caffe2::split( char separator , const std :: string & string)",9, 76, 0, 0
repos/cpp/pytorch/caffe2/utils/string_utils.cc,"caffe2::trim( const std :: string & str)",8, 47, 2, 0
repos/cpp/pytorch/caffe2/utils/string_utils.cc,"caffe2::editDistance( const std :: string & s1 , const std :: string & s2 , size_t max_distance)",18, 69, 2, 0
repos/cpp/pytorch/caffe2/utils/string_utils.cc,"caffe2::editDistanceHelper( const char * s1 , size_t s1_len , const char * s2 , size_t s2_len , std :: vector<size_t> & current , std :: vector<size_t> & previous , std :: vector<size_t> & previous1 , size_t max_distance)",66, 80, 6, 0
repos/cpp/pytorch/caffe2/utils/proto_utils_test.cc,"caffe2::TEST( ProtoUtilsTest , IsSameDevice)",16, 50, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_utils_test.cc,"caffe2::TEST( ProtoUtilsTest , SimpleReadWrite)",8, 66, 2, 0
repos/cpp/pytorch/caffe2/utils/cpuid_test.cc,"caffe2::TEST( CpuIdTest , ShouldAlwaysHaveMMX)",3, 39, 0, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer_test.cc,"caffe2::my_to_string( const T & value)",3, 43, 0, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer_test.cc,"caffe2::my_to_string<std::string>( const std :: string & value)",3, 66, 0, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer_test.cc,"caffe2::expect_stderr_contains( const std :: vector<T> & values)",7, 73, 4, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer_test.cc,"caffe2::printTensorAndCheck( const std :: vector<T> & values)",9, 77, 6, 0
repos/cpp/pytorch/caffe2/utils/smart_tensor_printer_test.cc,"caffe2::TEST( SmartTensorPrinterTest , SimpleTest)",4, 77, 2, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::executeGpuBinaryOpTest( int shapex0 , int shapex1 , int shapey , std :: function<float(int)> input0 , std :: function<float(int)> input1 , std :: function<void(int N0,int N1,const float*src0,const float*src1,float*dst,CUDAContext*context)> operation , std :: function<float(int)> correct_output)",63, 70, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST( MathUtilGPUTest , testAddStripedBatch)",54, 66, 6, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST( MathUtilGPUTest , testReduceMin)",34, 75, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST( MathUtilGPUTest , testReduceMax)",34, 75, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST( MathUtilGPUTest , testCopyVector)",17, 70, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::GemmBatchedGPUTest::SetUp()",22, 76, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::GemmBatchedGPUTest::RunGemmBatched( const float alpha , const float beta)",27, 61, 2, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::GemmBatchedGPUTest::RunGemmStridedBatched( const float alpha , const float beta)",24, 68, 2, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::GemmBatchedGPUTest::VerifyOutput( const float value) const",6, 63, 6, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST_P( GemmBatchedGPUTest , GemmBatchedGPUFloatTest)",11, 54, 0, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST_P( GemmBatchedGPUTest , GemmStridedBatchedGPUFloatTest)",11, 61, 0, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::BroadcastGPUTest::SetUp()",11, 55, 4, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::BroadcastGPUTest::SetUpData( const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<float> & X_data)",10, 66, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::BroadcastGPUTest::VerifyResult( const std :: vector<float> & expected_output)",9, 69, 6, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::BroadcastGPUTest::RunBroadcastTest( const std :: vector<int> & X_dims , const std :: vector<int> & Y_dims , const std :: vector<float> & X_data , const std :: vector<float> & Y_data)",17, 42, 6, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST_F( BroadcastGPUTest , BroadcastGPUFloatTest)",14, 76, 2, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TransposeGPUTest::SetUp()",11, 55, 4, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TransposeGPUTest::SetUpData( const std :: vector<int> & X_dims , const std :: vector<int> & axes , const std :: vector<float> & X_data)",15, 66, 8, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TransposeGPUTest::VerifyResult( const std :: vector<float> & expected_output)",9, 69, 6, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TransposeGPUTest::RunTransposeTest( const std :: vector<int> & X_dims , const std :: vector<int> & axes , const std :: vector<float> & X_data , const std :: vector<float> & Y_data)",16, 46, 4, 0
repos/cpp/pytorch/caffe2/utils/math_gpu_test.cc,"caffe2::TEST_F( TransposeGPUTest , TransposeGPUFloatTest)",26, 70, 2, 0
repos/cpp/pytorch/caffe2/utils/cpuid.cc,"caffe2::GetCpuId()",4, 32, 2, 0
repos/cpp/pytorch/caffe2/utils/cpuid.cc,"caffe2::CpuId::CpuId()",64, 75, 4, 0
repos/cpp/pytorch/caffe2/utils/bench_utils.cc,"caffe2::wipe_cache()",78, 73, 4, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Gemm<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int M , const int N , const int K , const float alpha , const float * A , const float * B , const float beta , float * C , CPUContext * context , TensorProto :: DataType math_type)",56, 66, 10, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::GemmEx<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int M , const int N , const int K , const float alpha , const float * A , const int lda , const float * B , const int ldb , const float beta , float * C , const int ldc , CPUContext *)",70, 75, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Gemv<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const int M , const int N , const float alpha , const float * A , const float * x , const float beta , float * y , CPUContext * context , TensorProto :: DataType math_type)",36, 80, 4, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Gemm<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int M , const int N , const int K , const float alpha , const float * A , const float * B , const float beta , float * C , CPUContext * , TensorProto :: DataType)",31, 53, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::GemmEx<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int M , const int N , const int K , const float alpha , const float * A , const int lda , const float * B , const int ldb , const float beta , float * C , const int ldc , CPUContext *)",31, 43, 0, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Gemv<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const int M , const int N , const float alpha , const float * A , const float * x , const float beta , float * y , CPUContext * , TensorProto :: DataType)",13, 76, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::GemmBatched<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const float alpha , const float ** A , const float ** B , const float beta , float ** C , CPUContext * context , TensorProto :: DataType)",44, 76, 8, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::GemmStridedBatched<float,CPUContext>( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const float alpha , const float * A , const int A_stride , const float * B , const int B_stride , const float beta , float * C , const int C_stride , CPUContext * context , TensorProto :: DataType)",58, 67, 8, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::BroadcastImpl( const int X_ndim , const int * X_dims , const int Y_ndim , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",28, 79, 4, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Not<bool,CPUContext>( const int N , const bool * x , bool * y , CPUContext *)",9, 39, 0, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::RowwiseBinaryOp( const int rows , const int cols , const BinaryOperator & op , const TIn * A , const TIn * B , TOut * C)",16, 55, 6, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::ColwiseBinaryOp( const int rows , const int cols , const BinaryOperator & op , const TIn * A , const TIn * B , TOut * C)",16, 55, 6, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::BroadcastBinaryOpImpl( const int ndim , const int * A_dims , const int * B_dims , const int * C_dims , const BinaryOperator & op , const TIn * A , const TIn * B , TOut * C)",19, 77, 4, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::generate_stack_distance( std :: vector<Ind_t> & cum_val , std :: vector<Val_t> & cum_dis , std :: vector<Ind_t> & cum_map , Ind_t max_i , Ind_t i , Context_t * context)",49, 80, 5, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::generate_trace_lru( std :: vector<Ind_t> & uni_ref , std :: vector<Ind_t> & cum_val , std :: vector<Val_t> & cum_dis , std :: vector<Ind_t> & cum_map , Context_t * context , Ind_t cache_line_size , Ind_t n , Type min , Type max , Type * syn_ref)",70, 81, 4, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::RandGaussian<float,CPUContext>( const size_t n , const float mean , const float std , float * r , CPUContext * context)",11, 59, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::SumSqr<float,CPUContext>( const int N , const float * x , float * y , CPUContext * , Tensor *)",8, 55, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Select<float,CPUContext>( const int N , const int D , const float * x , const int * idx , float * y , CPUContext *)",12, 43, 0, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::CopyMatrix<CPUContext>( const size_t itemsize , const int M , const int N , const void * A , const int lda , void * B , const int ldb , CPUContext * , TypeMeta :: Copy copy)",38, 81, 10, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Im2ColNd<float,CPUContext,StorageOrder::NCHW>( const int N , const int img_size , const int col_size , const int * img_shape , const int * col_shape , const int * kernel_shape , const int * stride , const int * dilation , const int * pad , const float * img_data , float * col_data , CPUContext * , const int)",55, 76, 8, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Col2ImNd<float,CPUContext,StorageOrder::NCHW>( const int N , const int img_size , const int col_size , const int * img_shape , const int * col_shape , const int * kernel_shape , const int * stride , const int * dilation , const int * pad , const float * col_data , float * img_data , CPUContext * , const int)",28, 65, 0, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Im2Col<float,CPUContext,StorageOrder::NCHW>( const int C , const int H , const int W , const int kernel_h , const int kernel_w , const int dilation_h , const int dilation_w , const int pad_t , const int pad_l , const int pad_b , const int pad_r , const int stride_h , const int stride_w , const float * img_data , float * col_data , CPUContext * context , const int)",64, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Im2Col<float,CPUContext,StorageOrder::NHWC>( const int C , const int H , const int W , const int kernel_h , const int kernel_w , const int dilation_h , const int dilation_w , const int pad_t , const int pad_l , const int pad_b , const int pad_r , const int stride_h , const int stride_w , const float * img_data , float * col_data , CPUContext * context , const int groups)",104, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Im2Col3dNHWCImpl( const int C , const int T , const int H , const int W , const int kernel_t , const int kernel_h , const int kernel_w , const int dilation_t , const int dilation_h , const int dilation_w , const int pad_p , const int pad_t , const int pad_l , const int pad_n , const int pad_b , const int pad_r , const int stride_t , const int stride_h , const int stride_w , const TData * img_data , TData * col_data , const int groups)",75, 81, 26, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Im2ColNd<float,CPUContext,StorageOrder::NHWC>( const int N , const int , const int , const int * img_shape , const int * col_shape , const int * kernel_shape , const int * stride , const int * dilation , const int * pad , const float * img_data , float * col_data , CPUContext * , const int groups)",44, 76, 8, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Col2Im<float,CPUContext,StorageOrder::NCHW>( const int C , const int H , const int W , const int kernel_h , const int kernel_w , const int dilation_h , const int dilation_w , const int pad_t , const int pad_l , const int pad_b , const int pad_r , const int stride_h , const int stride_w , const float * col_data , float * img_data , CPUContext * context , const int)",65, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Col2Im<float,CPUContext,StorageOrder::NHWC>( const int C , const int H , const int W , const int kernel_h , const int kernel_w , const int dilation_h , const int dilation_w , const int pad_t , const int pad_l , const int pad_b , const int pad_r , const int stride_h , const int stride_w , const float * col_data , float * img_data , CPUContext * context , const int groups)",94, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Col2Im3dNHWCImpl( const int C , const int T , const int H , const int W , const int kernel_t , const int kernel_h , const int kernel_w , const int dilation_t , const int dilation_h , const int dilation_w , const int pad_p , const int pad_t , const int pad_l , const int pad_n , const int pad_b , const int pad_r , const int stride_t , const int stride_h , const int stride_w , const TData * col_data , TData * img_data , CPUContext * context , const int groups)",72, 81, 16, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::Col2ImNd<float,CPUContext,StorageOrder::NHWC>( const int N , const int , const int , const int * img_shape , const int * col_shape , const int * kernel_shape , const int * stride , const int * dilation , const int * pad , const float * col_data , float * img_data , CPUContext * context , const int groups)",45, 76, 8, 0
repos/cpp/pytorch/caffe2/utils/math_cpu.cc,"caffe2::math::BiasCHW<float,CPUContext>( const float * bias , const float * , const int bias_channels , const int image_size , float * image , CPUContext *)",82, 81, 4, 0
repos/cpp/pytorch/caffe2/utils/cast_test.cc,"caffe2::TEST( CastTest , GetCastDataType)",28, 64, 6, 0
repos/cpp/pytorch/caffe2/utils/proto_wrap.cc,"ONNX_NAMESPACE::GetEmptyStringAlreadyInited()",3, 70, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_wrap.cc,"caffe2::GetEmptyStringAlreadyInited()",3, 70, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_wrap.cc,"caffe2::ShutdownProtobufLibrary()",3, 49, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_wrap.cc,"torch::GetEmptyStringAlreadyInited()",3, 70, 2, 0
repos/cpp/pytorch/caffe2/utils/proto_wrap.cc,"torch::ShutdownProtobufLibrary()",3, 49, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool_impl.cc,"pthreadpool_compute_1d( pthreadpool_t threadpool , pthreadpool_function_1d_t function , void * argument , size_t range)",20, 77, 4, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool_impl.cc,"pthreadpool_get_threads_count( pthreadpool_t threadpool)",3, 77, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool_impl.cc,"pthreadpool_create( size_t threads_count)",6, 81, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool_impl.cc,"pthreadpool_destroy( pthreadpool_t pthreadpool)",7, 60, 8, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::defaultThreadPool()",55, 77, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::ThreadPool( int numThreads)",3, 66, 4, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::~ThreadPool()",1, 29, 0, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::getNumThreads() const",4, 54, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::setMinWorkSize( size_t size)",4, 54, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::run( const std :: function<void(int,size_t)> & fn , size_t range)",52, 81, 0, 0
repos/cpp/pytorch/caffe2/utils/threadpool/ThreadPool.cc,"caffe2::ThreadPool::withPool( const std :: function<void(WorkersPool*)> & f)",4, 72, 0, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"divide_round_up( size_t dividend , size_t divisor)",7, 72, 0, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"min( size_t a , size_t b)",3, 47, 0, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"compute_1d_tiled( void * context_ , size_t linear_index)",7, 89, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"pthreadpool_compute_1d_tiled( pthreadpool_t threadpool , pthreadpool_function_1d_tiled_t function , void * argument , size_t range , size_t tile)",24, 108, 4, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"compute_2d( void * context_ , size_t linear_index)",9, 89, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"pthreadpool_compute_2d( struct pthreadpool * threadpool , pthreadpool_function_2d_t function , void * argument , size_t range_i , size_t range_j)",24, 109, 4, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"compute_2d_tiled( void * context_ , size_t linear_index)",14, 101, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"pthreadpool_compute_2d_tiled( pthreadpool_t threadpool , pthreadpool_function_2d_tiled_t function , void * argument , size_t range_i , size_t range_j , size_t tile_i , size_t tile_j)",34, 125, 4, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"compute_3d_tiled( void * context_ , size_t linear_index)",21, 101, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"pthreadpool_compute_3d_tiled( pthreadpool_t threadpool , pthreadpool_function_3d_tiled_t function , void * argument , size_t range_i , size_t range_j , size_t range_k , size_t tile_i , size_t tile_j , size_t tile_k)",53, 77, 4, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"compute_4d_tiled( void * context_ , size_t linear_index)",34, 101, 2, 0
repos/cpp/pytorch/caffe2/utils/threadpool/pthreadpool.cc,"pthreadpool_compute_4d_tiled( pthreadpool_t threadpool , pthreadpool_function_4d_tiled_t function , void * argument , size_t range_i , size_t range_j , size_t range_k , size_t range_l , size_t tile_i , size_t tile_j , size_t tile_k , size_t tile_l)",63, 81, 8, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IncreaseIndexInDims( const int n , const int * dims , int * index)",10, 69, 0, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::GetIndexFromDims( const int n , const int * dims , const int * index)",9, 71, 0, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsIdentityPermutation( const int n , const int * perm)",8, 59, 0, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::CheckReduceDims( const int ndim , const int * X_dims , const int * Y_dims)",8, 77, 0, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsRowwiseReduce( const int ndim , const int * A_dims , const int * B_dims , int * rows , int * cols)",20, 54, 2, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsColwiseReduce( const int ndim , const int * A_dims , const int * B_dims , int * rows , int * cols)",20, 56, 2, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsBothEndsReduce( const int ndim , const int * A_dims , const int * B_dims , int * pre , int * mid , int * nxt)",26, 42, 2, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::ComputeBroadcastBinaryOpDims( const int A_ndim , const int * A_dims , const int B_ndim , const int * B_dims , int * A_broadcast_dims , int * B_broadcast_dims , int * C_broadcast_dims)",24, 80, 6, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsRowwiseBroadcastBinaryOp( const int ndim , const int * A_dims , const int * B_dims , int * rows , int * cols , bool * broadcast_1st)",38, 70, 8, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsColwiseBroadcastBinaryOp( const int ndim , const int * A_dims , const int * B_dims , int * rows , int * cols , bool * broadcast_1st)",40, 70, 8, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsBothEndsBroadcastBinaryOp( const int ndim , const int * A_dims , const int * B_dims , int * pre , int * mid , int * nxt , bool * broadcast_1st)",54, 68, 8, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::IsBatchTranspose2D( const int ndim , const int * axes)",11, 67, 2, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::ComputeTransposeAxesForReduceOp( const int num_dims , const int num_reduce_axes , const int * reduce_axes , int * transpose_axes)",18, 65, 2, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::ComputeTransposeAxesForReduceOp( const int ndim , const int * dims , int * axes)",15, 57, 2, 0
repos/cpp/pytorch/caffe2/utils/math/utils.cc,"caffe2::math::utils::ComputeTransposedStrides( const int ndim , const int * dims , const int * axes , int * strides)",15, 40, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ColwiseReduceMean( const int rows , const int cols , const T alpha , const T * X , T * Y , CPUContext * context)",9, 80, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ColwiseReduceL1( const int rows , const int cols , const T alpha , const T * X , T * Y , CPUContext * context)",15, 55, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ColwiseReduceL2( const int rows , const int cols , const T alpha , const T * X , T * Y , CPUContext *)",15, 46, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsReduceMin( const int M , const int N , const int K , const T alpha , const T * X , T * Y , CPUContext * context)",18, 63, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsReduceMax( const int M , const int N , const int K , const T alpha , const T * X , T * Y , CPUContext * context)",18, 63, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsReduceSum( const int M , const int N , const int K , const T alpha , const T * X , T * Y , CPUContext * context)",16, 80, 8, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsReduceMean( const int M , const int N , const int K , const T alpha , const T * X , T * Y , CPUContext * context)",16, 80, 8, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsReduceL1( const int M , const int N , const int K , const T alpha , const T * X , T * Y , CPUContext * context)",18, 74, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsReduceL2( const int M , const int N , const int K , const T alpha , const T * X , T * Y , CPUContext *)",21, 55, 4, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceTensorImpl( const int ndim , const int * X_dims , const int * Y_dims , const Reducer & reducer , const T init , const T * X , T * Y , CPUContext * context)",21, 77, 4, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceMinImpl( const int ndim , const int * X_dims , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",21, 73, 6, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceMaxImpl( const int ndim , const int * X_dims , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",21, 73, 6, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceSumImpl( const int ndim , const int * X_dims , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",13, 79, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceMeanImpl( const int ndim , const int * X_dims , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",20, 79, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceL1Impl( const int ndim , const int * X_dims , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",21, 73, 6, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ReduceL2Impl( const int ndim , const int * X_dims , const int * Y_dims , const T alpha , const T * X , T * Y , CPUContext * context)",22, 73, 6, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::RowwiseMoments( const int rows , const int cols , const T * X , T * mean , T * var)",12, 63, 4, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::ColwiseMoments( const int rows , const int cols , const T * X , T * mean , T * var)",19, 49, 2, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::BothEndsMoments( const int M , const int N , const int K , const T * X , T * mean , T * var)",25, 49, 6, 0
repos/cpp/pytorch/caffe2/utils/math/reduce.cc,"caffe2::math::MomentsImpl( const int ndim , const int * X_dims , const int * Y_dims , const T * X , T * mean , T * var , CPUContext *)",54, 77, 4, 0
repos/cpp/pytorch/caffe2/utils/hip/math_blas_gpu_test.cc,"caffe2::TEST( MathROCBLASTest , GemmNoTransNoTrans)",96, 69, 6, 0
repos/cpp/pytorch/caffe2/utils/hip/math_blas_gpu_test.cc,"caffe2::TEST( MathROCBLASTest , GemmNoTransTrans)",95, 69, 6, 0
repos/cpp/pytorch/caffe2/utils/hip/math_blas_gpu_test.cc,"caffe2::TEST( MathROCBLASTest , GemvNoTrans)",86, 69, 6, 0
repos/cpp/pytorch/caffe2/utils/hip/math_blas_gpu_test.cc,"caffe2::TEST( MathROCBLASTest , GemvTrans)",86, 69, 6, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::has_nnpack()",6, 77, 2, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::get_nnp_convolution_algorithm( const std :: string & algo)",16, 57, 0, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::get_nnp_convolution_transform_strategy( const std :: string & kts)",10, 75, 0, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::nnpack_threadpool()",18, 79, 8, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKConvOp::NNPACKConvOp( const OperatorDef & operator_def , Workspace * ws)",17, 79, 4, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKConvOp::RunOnDeviceWithOrderNCHW()",94, 78, 4, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKMaxPoolOp::NNPACKMaxPoolOp( const OperatorDef & operator_def , Workspace * ws)",30, 81, 8, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKMaxPoolOp::RunOnDeviceWithOrderNCHW()",40, 80, 37, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKReluOp::NNPACKReluOp( const OperatorDef & operator_def , Workspace * ws)",6, 79, 4, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKReluOp::RunOnDevice()",13, 53, 4, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKLeakyReluOp::NNPACKLeakyReluOp( const OperatorDef & operator_def , Workspace * ws)",6, 79, 4, 0
repos/cpp/pytorch/caffe2/contrib/nnpack/nnpack_ops.cc,"caffe2::NNPACKLeakyReluOp::RunOnDevice()",13, 53, 4, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op_gpu.cpp,"caffe2::detail::workspaceInfo<CUDAContext>( const CUDAContext & context)",6, 72, 0, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op.cpp,"caffe2::detail::workspaceInfo<CPUContext>( const CPUContext &)",8, 75, 2, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op.cpp,"caffe2::GetCTCGradient::GetGradientDefs()",4, 66, 8, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::getDevices( const NCCLExecution & ex)",8, 55, 0, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCLContext::NCCLContext( const NCCLExecution & ex)",22, 75, 8, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCLContext::~NCCLContext()",24, 75, 5, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::gContextsMutex()",4, 38, 0, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::gContexts()",5, 77, 0, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::ncclKey( const NCCLExecution & ex)",10, 47, 0, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::getNCCLContext( const NCCLExecution & ex)",9, 58, 4, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::runNCCL( const NCCLExecution & ex , InitF && init_f , F && f)",64, 76, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCL<T>::AllReduce( const NCCLExecution & ex)",18, 73, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCL<T>::Broadcast( const NCCLExecution & ex)",17, 76, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCL<T>::Reduce( const NCCLExecution & ex)",21, 76, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCL<T>::AllGather( const NCCLExecution & ex)",35, 74, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc,"caffe2::nccl::NCCL<T>::ReduceScatter( const NCCLExecution & ex)",22, 74, 8, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::getNCCLElements( OperatorBase * op , const CUDAContext & context)",27, 79, 2, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::AllInputsAre( OperatorBase * op)",10, 50, 4, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLAllreduceOp::NCCLAllreduceOp( const OperatorDef & operator_def , Workspace * ws)",2, 66, 2, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLAllreduceOp::RunOnDevice()",14, 72, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLAllreduceOp::ShapeInference( const OperatorDef & def , const std :: vector<TensorShape> & in)",25, 66, 12, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLAllreduceOp::CostInference( const OperatorDef & def , const vector<TensorShape> & inputs)",14, 74, 4, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLBroadcastOp::RunOnDevice()",13, 72, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLReduceOp::RunOnDevice()",15, 54, 4, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLAllGatherOp::RunOnDevice()",13, 72, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::NCCLReduceScatterOp::RunOnDevice()",11, 76, 6, 0
repos/cpp/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,"caffe2::ncclOpDevInfer( const OperatorDef & def)",11, 80, 0, 0
repos/cpp/pytorch/caffe2/contrib/gloo/reduce_scatter_ops.cc,"caffe2::gloo::ReduceScatterOp<Context>::initializeHalvingDoubling()",17, 80, 4, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc,"caffe2::gloo::initializeAlgorithm( bool gpu_direct_ , std :: shared_ptr<::gloo::Context> context , std :: vector<T*> ptrs , size_t size)",19, 72, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc,"caffe2::gloo::getAllrduceBcubeBase( int nodes)",24, 75, 5, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc,"caffe2::gloo::AllreduceOp<Context>::initializeBcube()",23, 74, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc,"caffe2::gloo::AllreduceOp<Context>::initializeHalvingDoubling()",19, 82, 6, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc,"caffe2::gloo::AllreduceOp<Context>::initializeRingFull()",19, 71, 6, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc,"caffe2::gloo::AllreduceOp<Context>::initializeRingChunked()",19, 78, 6, 0
repos/cpp/pytorch/caffe2/contrib/gloo/common.cc,"caffe2::gloo::signalFailure( Blob * status_blob , std :: exception &)",5, 70, 0, 0
repos/cpp/pytorch/caffe2/contrib/gloo/common.cc,"caffe2::gloo::createDevice( const createDeviceAttr attr)",26, 66, 4, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops.cc,"getAllrduceBcubeBase( int nodes)",24, 75, 5, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops.cc,"caffe2::gloo::AllreduceOp<Context>::initializeBcube()",18, 73, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops.cc,"caffe2::gloo::AllreduceOp<Context>::initializeHalvingDoubling()",13, 76, 4, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops.cc,"caffe2::gloo::AllreduceOp<Context>::initializeRingFull()",13, 73, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allreduce_ops.cc,"caffe2::gloo::AllreduceOp<Context>::initializeRingChunked()",13, 73, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/common_world_ops_gpu.cc,"caffe2::gloo::CreateCommonWorld<CUDAContext>::initializeForContext()",8, 69, 6, 0
repos/cpp/pytorch/caffe2/contrib/gloo/py_export.cc,"gloo::python::PYBIND11_MODULE( python , m)",4, 53, 2, 0
repos/cpp/pytorch/caffe2/contrib/gloo/broadcast_ops.cc,"caffe2::gloo::BroadcastOp<Context>::initializeAlgorithm()",20, 80, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/broadcast_ops_gpu.cc,"caffe2::gloo::BroadcastOp<Context>::initializeAlgorithm()",20, 80, 8, 0
repos/cpp/pytorch/caffe2/contrib/gloo/allgather_ops.cc,"caffe2::gloo::AllgatherOp<Context>::initializeAlgorithm()",29, 65, 4, 0
repos/cpp/pytorch/caffe2/contrib/gloo/store_handler.cc,"caffe2::gloo::StoreHandlerWrapper::set( const std :: string & key , const std :: vector<char> & data)",6, 53, 2, 0
repos/cpp/pytorch/caffe2/contrib/gloo/store_handler.cc,"caffe2::gloo::StoreHandlerWrapper::get( const std :: string & key)",4, 69, 0, 0
repos/cpp/pytorch/caffe2/contrib/gloo/store_handler.cc,"caffe2::gloo::StoreHandlerWrapper::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",5, 48, 4, 0
repos/cpp/pytorch/caffe2/contrib/gloo/common_world_ops.cc,"caffe2::gloo::CreateCommonWorld<CPUContext>::initializeForContext()",3, 61, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContextSingleton::OpenCLContextSingleton()",21, 66, 2, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContextSingleton::getInstance()",7, 64, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContext::New( size_t nbytes)",10, 70, 2, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContext::Delete( void * ptr)",3, 40, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContext::GetSingleton()",3, 63, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContext::BuildKernel( const char * src , std :: string additional_options , const char * fn_name)",22, 110, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::OpenCLContext::BuildArgumentList( std :: vector<std::pair<std::string,std::string>> args)",7, 102, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::EventCreateOPENCL( const DeviceOption & , Event *)",1, 81, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::EventRecordOPENCL( Event * , const void * , const char *)",4, 33, 4, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::EventWaitOPENCL( const Event * , void *)",1, 71, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::EventFinishOPENCL( const Event *)",1, 53, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context.cc,"caffe2::EventResetOPENCL( Event *)",1, 46, 0, 0
repos/cpp/pytorch/caffe2/contrib/opencl/context_test.cc,"caffe2::TEST( ContextTest , BasicInit)",3, 31, 0, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileInitializeOp::CudaProfileInitializeOp( const OperatorDef & operator_def , Workspace * ws)",20, 75, 8, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileInitializeOp::~CudaProfileInitializeOp()",3, 40, 2, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileInitializeOp::Run( int = 0)",6, 76, 8, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileStartOp::CudaProfileStartOp( const OperatorDef & operator_def , Workspace * ws)",2, 69, 2, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileStartOp::Run( int = 0)",4, 58, 2, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileStopOp::CudaProfileStopOp( const OperatorDef & operator_def , Workspace * ws)",2, 68, 2, 0
repos/cpp/pytorch/caffe2/contrib/prof/cuda_profile_ops.cc,"caffe2::CudaProfileStopOp::Run( int = 0)",4, 58, 2, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::InferShapes( Workspace * ws , NetDef * pred_net , CaffeMap<std::string,TensorShape> * shape_hints_ordered)",20, 75, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::DumpModel( const :: ONNX_NAMESPACE :: ModelProto & model , const std :: string & fname)",5, 86, 0, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::CPUTensorToTensorProto( const TensorCPU & cpu_tensor , :: ONNX_NAMESPACE :: TensorProto * t)",29, 63, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::BlobToTensorProto( const std :: string & name , Workspace * ws , CUDAContext * context , :: ONNX_NAMESPACE :: TensorProto * t)",32, 64, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::ConvertToValueInfo( const std :: vector<std::string> & names , const std :: unordered_map<std::string,TensorShape> & shape_hints)",24, 79, 10, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::FillModelInfo( :: ONNX_NAMESPACE :: ModelProto * model)",7, 64, 2, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::BuildInitializationList( Workspace * ws , :: ONNX_NAMESPACE :: GraphProto * g , std :: unordered_set<std::string> * initialization_list)",24, 75, 2, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::AddTrtOptions( OperatorDef * op , const std :: unordered_map<std::string,std::vector<int>> & output_size_hints)",27, 72, 6, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::BuildTrtOpLazy( const std :: string & onnx_model_str , const std :: unordered_map<std::string,std::vector<int>> & output_size_hints , const std :: unordered_set<std::string> & initialization_list , const caffe2 :: NetDef & net)",40, 80, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::BuildTrtOp( const std :: string & onnx_model_str , const std :: unordered_map<std::string,std::vector<int>> & output_size_hints)",36, 82, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::SubnetToTrtOp( const caffe2 :: NetDef & net , Workspace * ws , onnx :: OnnxExporter * exporter , std :: unordered_map<std::string,TensorShape> * shape_hints)",142, 83, 12, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::SsaRewriteAndMapNames( Workspace * ws , NetDef * pred_net , const std :: unordered_map<std::string,TensorShape> & input_shape_hints)",28, 79, 0, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::PruneUnusedWeights( Workspace * ws , const NetDef & pred_net)",20, 75, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_tranformer.cc,"caffe2::TensorRTTransformer::Transform( Workspace * ws , NetDef * pred_net , const std :: unordered_map<std::string,TensorShape> & input_shape_hints)",58, 82, 6, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_op_trt.cc,"caffe2::CheckDims( const nvinfer1 :: Dims & nv_dims , at :: ArrayRef<int64_t> c2_dims)",27, 53, 8, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_op_trt.cc,"caffe2::TensorRTOp::TensorRTOp( const OperatorDef & operator_def , Workspace * ws)",81, 90, 6, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_op_trt.cc,"caffe2::TensorRTOp::MaybeAdjustOutputShape( int output_idx , std :: vector<int64_t> * dims)",26, 79, 8, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/tensorrt_op_trt.cc,"caffe2::TensorRTOp::RunOnDevice()",74, 79, 4, 0
repos/cpp/pytorch/caffe2/contrib/tensorrt/trt_utils.cc,"caffe2::tensorrt::BuildTrtEngine( const std :: string & onnx_model_str , TrtLogger * logger , size_t max_batch_size , size_t max_workspace_size , bool debug_builder)",36, 81, 2, 0
repos/cpp/pytorch/caffe2/contrib/shm_mutex/shm_mutex.cc,"ShmProcessMutexCheck::getInstance()",4, 60, 0, 0
repos/cpp/pytorch/caffe2/contrib/shm_mutex/shm_mutex.cc,"ShmProcessMutexCheck::addLock( const std :: string & name)",5, 62, 0, 0
repos/cpp/pytorch/caffe2/contrib/shm_mutex/shm_mutex.cc,"ShmProcessMutexCheck::removeLock( const std :: string & name)",4, 65, 0, 0
repos/cpp/pytorch/caffe2/contrib/aten/aten_op_cuda.cc,"caffe2::ATenOp<CUDAContext>::backend() const",3, 51, 0, 0
repos/cpp/pytorch/caffe2/contrib/aten/aten_op.cc,"caffe2::ATenOp<CPUContext>::backend() const",3, 50, 0, 0
repos/cpp/pytorch/caffe2/contrib/aten/aten_op.cc,"caffe2::math::Set<at::Half,CPUContext>( const std :: int64_t , const at :: Half h , at :: Half * v , CPUContext * c)",7, 32, 0, 0
repos/cpp/pytorch/caffe2/image/image_input_op_gpu.cc,"caffe2::ImageInputOp<CUDAContext>::ApplyTransformOnGPU( const std :: vector<std::int64_t> & dims , const c10 :: Device & type)",27, 81, 8, 0
repos/cpp/pytorch/caffe2/image/image_input_op.cc,"caffe2::ImageInputOp<CPUContext>::ApplyTransformOnGPU( const std :: vector<std::int64_t> & , const c10 :: Device &)",5, 52, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/norm_minimization_avx2.cc,"dnnlowp::internal::L2MinimizationKernelAVX2( int precision , float * bins , int nbins , float bin_width , float dst_bin_width , int start_bin)",124, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::QuantizeWeight( const Blob & blob , int kernel_dim , int M , vector<TensorQuantizationParams> & qparams , vector<typename make_signed<T>::type> & W_quantized , dnnlowp :: QuantizationFactory * qfactory)",45, 67, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ComputeColumnOffsets( int num_rows , int num_cols , const T * W , const vector<TensorQuantizationParams> & qparams , vector<int32_t> & col_offsets)",20, 63, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ExtractOutlierMatrix( int groups , int kernel_dim , int M , int nbits_in_non_outlier , vector<int8_t> & W_quantized)",49, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::QuantizeConvBias( const Blob & blob , int M , const TensorQuantizationParams & in_qparams , const vector<TensorQuantizationParams> & filter_qparams , vector<int32_t> & b_quantized)",38, 78, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::FullyConnectedDNNLowPPackWeightOp::FullyConnectedDNNLowPPackWeightOp( const OperatorDef & operator_def , Workspace * ws)",15, 76, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::FullyConnectedDNNLowPPackWeightOp::RunOnDevice()",81, 78, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ConvDNNLowPPackWeightOp::ConvDNNLowPPackWeightOp( const OperatorDef & operator_def , Workspace * ws)",11, 76, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ConvDNNLowPPackWeightOp::TakeDepthWise3x3FastPath_()",13, 75, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ConvDNNLowPPackWeightOp::TakeDepthWise3x3x3FastPath_()",19, 81, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ConvDNNLowPPackWeightOp::TakeGConvFastPath_()",20, 78, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_op.cc,"caffe2::ConvDNNLowPPackWeightOp::RunOnDevice()",113, 79, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram_test.cc,"TEST( DynamicHistogram , HistSimilar)",63, 72, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op_avx2.cc,"caffe2::max_pool_avx2( const uint8_t * Xdata , int n , int height , int width , int channels , int pooled_height , int pooled_width , int kernel_h , int kernel_w , int stride_h , int stride_w , int pad_t , int pad_l , uint8_t * Ydata)",59, 79, 20, 0
repos/cpp/pytorch/caffe2/quantization/server/sigmoid.cc,"dnnlowp::Sigmoid<T>::Sigmoid( double max_abs_err)",13, 74, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/sigmoid.cc,"dnnlowp::Sigmoid<T>::Compute( T x) const",6, 39, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_linear_dnnlowp_op.cc,"caffe2::ElementwiseLinearDNNLowPOp<T>::ElementwiseLinearDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",5, 65, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_linear_dnnlowp_op.cc,"caffe2::ElementwiseLinearDNNLowPOp<T>::RunOnDevice()",59, 75, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_linear_dnnlowp_op.cc,"caffe2::ElementwiseLinearDNNLowPOp<T>::GetQuantizationParameters_()",29, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::Histogram::Add( float f , uint64_t cnt)",10, 71, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::Histogram::Add( const float * f , int len)",44, 78, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::Histogram::Finalize()",15, 73, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::DynamicHistogram::DynamicHistogram( int nbins)",6, 46, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::DynamicHistogram::Add( float f)",37, 73, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::DynamicHistogram::Add( const float * f , int len)",44, 75, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/dynamic_histogram.cc,"dnnlowp::DynamicHistogram::Finalize()",52, 83, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/relu_dnnlowp_op_avx2.cc,"caffe2::internal::ReluAVX2<uint8_t>( const int N , const int zero_point , const uint8_t * X , uint8_t * Y)",18, 78, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/relu_dnnlowp_op_avx2.cc,"caffe2::internal::ReluAVX2<uint16_t>( const int N , const int zero_point , const uint16_t * X , uint16_t * Y)",18, 79, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::GroupNormDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",16, 76, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::RunOnDevice()",8, 68, 38, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::GetQuantizationParameters()",15, 72, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::QuantizeGamma()",36, 68, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::QuantizeGammaImpl()",21, 59, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::QuantizeBeta()",44, 80, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::QuantizedGroupMomentsNCHW( const int N , const int G , const int K , const int HxW , const T * X , int32_t * mu , int32_t * rsig)",39, 81, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::QuantizedGroupMomentsNHWC( const int N , const int G , const int K , const int HxW , const T * X , int32_t * mu , int32_t * rsig)",43, 81, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::DequantizedGroupMomentsNCHW( const int N , const int G , const int K , const int HxW , const T * X , float * mu , float * rsig)",26, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::DequantizedGroupMomentsNHWC( const int N , const int G , const int K , const int HxW , const T * X , float * mu , float * rsig)",25, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::RunOnDeviceWithOrderNCHW()",65, 77, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::RunOnDeviceWithOrderNHWC()",66, 77, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::ComputeQuantizedInvStd( const int N , const float * var , float * rsig , int32_t * rsig_quantized)",21, 70, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::ComputeQuantizedFusedParams( const int N , const int G , const int K , const int32_t * mu , const int32_t * rsig , const int32_t * gamma , const int32_t * beta , int32_t * scale , int32_t * bias)",52, 78, 14, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::ComputeDequantizedFusedParams( const int N , const int G , const int K , const float * mu , const float * rsig , const float * gamma , const float * beta , float * scale , float * bias)",25, 72, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::AffineBatchChannelQuantizedNCHW( const int N , const int C , const int HxW , const T * X , const int32_t * scale , const int32_t * bias , T * Y)",28, 80, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::AffineBatchChannelQuantizedNHWC( const int N , const int C , const int HxW , const T * X , const int32_t * scale , const int32_t * bias , T * Y)",33, 80, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::AffineBatchChannelDequantizedNCHW( const int N , const int C , const int HxW , const float * X , const float * scale , const float * bias , float * Y)",14, 66, 7, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op.cc,"caffe2::GroupNormDNNLowPOp<T>::AffineBatchChannelDequantizedNHWC( const int N , const int C , const int HxW , const float * X , const float * scale , const float * bias , float * Y)",20, 71, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/batch_permutation_dnnlowp_op.cc,"caffe2::BatchPermutationDNNLowPOp<T>::RunOnDevice()",48, 80, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/relu_dnnlowp_op.cc,"caffe2::ReluDNNLowPOp<T>::RunOnDevice()",66, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_relu_op.cc,"caffe2::SumReluOp::SumReluOp( const OperatorDef & operator_def , Workspace * ws)",2, 60, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_relu_op.cc,"caffe2::SumReluOp::DoRunWithType()",12, 68, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_relu_op.cc,"caffe2::SumReluOp::RunOnDevice()",12, 67, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_add_dnnlowp_op.cc,"caffe2::AddDNNLowPOp::AddDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",2, 70, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_add_dnnlowp_op.cc,"caffe2::AddDNNLowPOp::RunOnDevice()",97, 81, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_add_dnnlowp_op.cc,"caffe2::AddDNNLowPOp::GetQuantizationParameters_()",27, 78, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc,"caffe2::internal::SegmentMomentsAVX2<uint8_t>( const int N , const uint8_t * src , int64_t * sum , int64_t * sumsq)",31, 77, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc,"caffe2::internal::VectorMomentsAVX2<uint8_t>( const int N , const uint8_t * src , int64_t * sum , int64_t * sumsq)",15, 61, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc,"caffe2::internal::ComputeQuantizedFusedParamsAVX2( const int N , const int G , const int K , const int32_t X_zero_point , const int32_t * mu , const int32_t * rsig , const int32_t * gamma , int32_t * scale , int32_t * bias)",41, 76, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc,"caffe2::internal::AffineBatchChannelAndRequantizeNCHWAVX2<uint8_t>( const int N , const int C , const int HxW , const fbgemm :: RequantizationParams & params , const uint8_t * X , const int32_t * scale , const int32_t * bias , uint8_t * Y)",36, 78, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc,"caffe2::internal::AffineBatchChannelAndRequantizeNHWCAVX2<uint8_t>( const int N , const int C , const int HxW , const fbgemm :: RequantizationParams & params , const uint8_t * X , const int32_t * scale , const int32_t * bias , uint8_t * Y)",40, 81, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/transpose.cc,"fbgemm::transpose_4rows( int N , const std :: uint8_t * src , std :: uint8_t * dst)",55, 78, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/resize_nearest_dnnlowp_op.cc,"caffe2::ResizeNearestDNNLowPOp<T>::RunOnDevice()",46, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxObserver::OutputMinMaxObserver( OperatorBase * op)",7, 75, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxObserver::~OutputMinMaxObserver()",14, 68, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::FindMinMax( const T * data , float * min , float * max , int len)",7, 66, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::FindMinMax<float>( const float * data , float * min , float * max , int len)",3, 77, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxObserver::Stop()",65, 80, 18, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxNetObserver::OutputMinMaxNetObserver( NetBase * subject , const string & out_file_name , int dump_freq)",18, 73, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxNetObserver::DumpAndReset_( const std :: string & out_file_name , bool print_total_min_max)",35, 73, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxNetObserver::~OutputMinMaxNetObserver()",32, 71, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::OutputMinMaxNetObserver::Stop()",20, 74, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HistogramObserver::HistogramObserver( OperatorBase * op , shared_ptr<Info> info)",2, 78, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HistogramObserver::Stop()",54, 80, 18, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HistogramNetObserver::HistogramNetObserver( NetBase * subject , const string & out_file_name , int nbins , int dump_freq , bool mul_nets)",30, 79, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HistogramNetObserver::DumpAndReset_( const string & out_file_name , bool print_total_min_max)",57, 80, 21, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HistogramNetObserver::~HistogramNetObserver()",3, 48, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HistogramNetObserver::Stop()",20, 74, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HasDNNLowPEngine_( const OperatorDef & op_def)",7, 59, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::HasDNNLowPEngine_( const OperatorBase & op)",3, 56, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::RegisterQuantizationParamsNetObserver::RegisterQuantizationParamsNetObserver( NetBase * subject , const string & min_max_file_name , bool is_weight , const string & qparams_output_file_name)",78, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/activation_distribution_observer.cc,"caffe2::RegisterQuantizationParamsWithHistogramNetObserver::RegisterQuantizationParamsWithHistogramNetObserver( NetBase * subject , const string & histogram_file_name , bool is_weight , const string & qparams_output_file_name)",111, 80, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,"caffe2::fp32_to_bfp16( const float * source , size_t size , float * dest)",16, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,"caffe2::fp32_to_bfp24( const float * source , size_t size , float * dest)",16, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,"caffe2::fp32_to_bfp14( const float * source , size_t size , float * dest)",16, 68, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,"caffe2::fp32_to_bfp16_scalar( const float * source , size_t size , float * dest)",6, 75, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,"caffe2::fp32_to_fp16( const float * source , size_t size , float * dest)",12, 72, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,"caffe2::fp32_to_bfp16_round( const float * source , size_t size , float * dest)",23, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/channel_shuffle_dnnlowp_op.cc,"caffe2::ChannelShuffleDNNLowPOp<T>::ChannelShuffleDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",9, 76, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/channel_shuffle_dnnlowp_op.cc,"caffe2::ChannelShuffleDNNLowPOp<T>::RunOnDevice()",4, 68, 38, 0
repos/cpp/pytorch/caffe2/quantization/server/channel_shuffle_dnnlowp_op.cc,"caffe2::ChannelShuffleDNNLowPOp<T>::RunOnDeviceWithOrderNCHW()",39, 78, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/channel_shuffle_dnnlowp_op.cc,"caffe2::ChannelShuffleDNNLowPOp<T>::RunOnDeviceWithOrderNHWC()",50, 78, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fc_fake_lowp_test.cc,"isclose( float x , float y)",7, 53, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fc_fake_lowp_test.cc,"isrelclose( float x , float y)",9, 79, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fc_fake_lowp_test.cc,"mse( std :: array<float,N> & a1 , std :: array<float,N> & a2)",8, 65, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/fc_fake_lowp_test.cc,"test_case( const float v , const float v_fp16 , const float v_bfp16)",32, 80, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/fc_fake_lowp_test.cc,"test_vector_case( const float v , const float v_fp16 , const float v_bfp16)",25, 80, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fc_fake_lowp_test.cc,"TEST( FP16Quant , fp32_to_fp16)",43, 80, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_dnnlowp_op_avx2.cc,"caffe2::internal::ElementWiseSumAVX2( const T * input0 , const T * input1 , T * output , int len , float a_scale , int32_t a_zero_point , float b_scale , int32_t b_zero_point , float c_scale , int32_t c_zero_point)",138, 81, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::LSTMUnitDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",7, 73, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::~LSTMUnitDNNLowPOp()",6, 69, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::Fp32Op_()",7, 74, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::InputTensorCPU_( int idx)",5, 66, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::OutputTensorCPU_( int idx)",7, 75, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnit( int N , int D , int t , const T * H_prev , const T * C_prev , const T * X , const int32_t * seqLengths , bool drop_states , T * C , T * H , const int32_t forget_bias , const Sigmoid<T> & sigmoid , const Tanh<T> & tanh , const TensorQuantizationParams & X_qparams , const TensorQuantizationParams & C_in_qparams , const TensorQuantizationParams & C_out_qparams , const TensorQuantizationParams & H_in_qparams , const TensorQuantizationParams & H_out_qparams , QuantizationFactory * qfactory)",135, 81, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::GetQuantizationParameters_()",42, 79, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/lstm_unit_dnnlowp_op.cc,"caffe2::LSTMUnitDNNLowPOp<T>::RunOnDevice()",112, 77, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/spatial_batch_norm_dnnlowp_op.cc,"caffe2::SpatialBNDNNLowPOp<T>::SpatialBNDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",17, 77, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/spatial_batch_norm_dnnlowp_op.cc,"caffe2::SpatialBNDNNLowPOp<T>::ComputeFusedParam_( const int C , const float * scale , const float * bias , const float * mean , const float * var , float * alpha , float * beta)",19, 71, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/spatial_batch_norm_dnnlowp_op.cc,"caffe2::SpatialBNDNNLowPOp<T>::RunOnDevice()",82, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/l2_minimization_test.cc,"TEST_P( ChooseQuantizationTest , L2MinimizationTest)",43, 79, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/l1_minimization_example.cc,"main( int argc , const char * argv [ ])",57, 79, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/batch_matmul_dnnlowp_op.cc,"caffe2::BatchMatMulDNNLowPOp<T>::BatchMatMulDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",9, 74, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/batch_matmul_dnnlowp_op.cc,"caffe2::BatchMatMulDNNLowPOp<T>::RunOnDevice()",664, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/kl_minimization_example.cc,"main( int argc , const char * argv [ ])",42, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op.cc,"caffe2::FullyConnectedFakeLowpFPOp<Q,Context,Engine,TransposeWeight>::DoRunWithType()",124, 77, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_fake_lowp_op.cc,"caffe2::FullyConnectedGradientFakeLowpFPOp<Q,Context,Engine,TransposeWeight>::DoRunWithType()",123, 79, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::HasDNNLowPEngine_( const OperatorDef & op_def)",7, 59, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::HasDNNLowPEngine_( const OperatorBase & op)",3, 56, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::PropagateOutputTensorQuantizationParams( OperatorBase * op , int idx , const TensorQuantizationParams & qparams)",10, 64, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::GetInputTensorQuantizationParamsOf( OperatorBase * op , int idx , const QuantizationFactory * qfactory , bool is_weight)",26, 75, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::OutputArgumentIdxString_( int idx)",3, 50, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::OutputScaleArgumentName( int idx)",3, 57, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::OutputZeroPointArgumentName( int idx)",3, 62, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::SetStaticQuantizationParams_( OperatorDef * op_def , int output_index , const TensorQuantizationParams & qparams)",9, 78, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::SetStaticQuantizationParams( OperatorBase * op , int output_index , const TensorQuantizationParams & qparams)",10, 69, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::HasStaticQuantization( const caffe2 :: OperatorBase * op , int output_index)",7, 46, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::GetStaticQuantizationParamsOf( const caffe2 :: OperatorBase * op , int idx)",14, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::QuantizeInputIfNeeded( OperatorBase * op , int input_index , const TensorQuantizationParams & qparams , vector<T> & temp)",17, 68, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::RowWiseQuantizeInputIfNeeded( OperatorBase * op , int input_index , const std :: vector<TensorQuantizationParams> & qparams , vector<T> & temp)",26, 68, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::MeasureQuantizationError( const float * actual , const float * ref , size_t len , QuantizationErrorStats * stat)",18, 41, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::ReportQuantizationError( const OperatorBase * op , const QuantizationErrorStats & stat)",25, 81, 14, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::GetQuantizationFactoryOf_( const OperatorDef & op_def)",71, 80, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::GetQuantizationFactoryOf( const OperatorBase * op)",4, 58, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::AdjustOutputTensorQuantizationParamsWithFollowedBy( OperatorBase * op , const string & followed_by)",27, 79, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::ParseDNNLowPOperatorArguments( OperatorBase * op , bool * dequantize_output , bool * measure_quantization_error , string * followed_by)",58, 81, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.cc,"dnnlowp::AddScaleZeroOffsetArgumentsWithHistogram( NetDef net_def , const string & histogram_file_name)",91, 79, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_relu_op.cc,"caffe2::ConvReluOp<T,Context>::RunOnDeviceWithOrderNCHW()",29, 78, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_relu_op.cc,"caffe2::ConvReluOp<T,Context>::RunOnDeviceWithOrderNHWC()",29, 78, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/requantization_test.cc,"TEST( Requantization , BatchRequantizationUnitTest)",56, 81, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/requantization_test.cc,"TEST( Requantization , RequantizationUnitTest)",82, 79, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/sigmoid_test.cc,"TEST( Sigmoid , SigmoidUnitTest)",32, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_dnnlowp_op.cc,"caffe2::FullyConnectedDNNLowPOp<T>::FullyConnectedDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",24, 77, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_dnnlowp_op.cc,"caffe2::FullyConnectedDNNLowPOp<T>::RunOnDevice()",494, 81, 24, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_dnnlowp_op.cc,"caffe2::FullyConnectedDNNLowPOp<T>::GetQuantizationParameters_()",279, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh_test.cc,"TEST( Tanh , TanhUnitTest)",35, 78, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/pybind.cc,"PYBIND11_MODULE( dnnlowp_pybind11 , m)",82, 79, 37, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_dnnlowp_acc16_op.cc,"caffe2::FullyConnectedDNNLowPAcc16Op::FullyConnectedDNNLowPAcc16Op( const OperatorDef & operator_def , Workspace * ws)",10, 70, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/fully_connected_dnnlowp_acc16_op.cc,"caffe2::FullyConnectedDNNLowPAcc16Op::RunOnDevice()",253, 81, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_mul_dnnlowp_op.cc,"caffe2::MulDNNLowPOp::MulDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",2, 70, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_mul_dnnlowp_op.cc,"caffe2::MulDNNLowPOp::RunOnDevice()",68, 77, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_mul_dnnlowp_op.cc,"caffe2::MulDNNLowPOp::GetQuantizationParameters_()",16, 74, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePool::initialize()",3, 30, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePool::process( const int x_col , const int y_col , ConstEigenMatrixMap<float> & x_mat , EigenMatrixMap<float> & y_mat)",7, 42, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePool::process( const T & x_data , T & y_data)",3, 52, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePool::finalize( const int size , T & y_data)",3, 52, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPool::initialize()",3, 45, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPool::process( const int x_col , const int y_col , ConstEigenMatrixMap<float> & x_mat , EigenMatrixMap<float> & y_mat)",7, 68, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPool::process( const T & x_data , T & y_data)",5, 52, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPool::finalize( const int , T &)",1, 61, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePoolDnnLowPOp::AveragePoolDnnLowPOp( const OperatorDef & operator_def , Workspace * ws)",15, 81, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePoolDnnLowPOp::RunOnDeviceWithOrderNCHW()",136, 81, 22, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::AveragePoolDnnLowPOp::RunOnDeviceWithOrderNHWC()",132, 80, 16, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPoolDnnLowPOp::MaxPoolDnnLowPOp( const OperatorDef & operator_def , Workspace * ws)",15, 81, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPoolDnnLowPOp::RunOnDeviceWithOrderNCHW()",144, 81, 22, 0
repos/cpp/pytorch/caffe2/quantization/server/pool_dnnlowp_op.cc,"caffe2::MaxPoolDnnLowPOp::RunOnDeviceWithOrderNHWC()",169, 81, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/quantize_dnnlowp_op.cc,"caffe2::QuantizeDNNLowPOp<T>::QuantizeDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",5, 60, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/quantize_dnnlowp_op.cc,"caffe2::QuantizeDNNLowPOp<T>::RunOnDevice()",38, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh_dnnlowp_op.cc,"caffe2::TanhFunctor::TanhFunctor()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh_dnnlowp_op.cc,"caffe2::TanhFunctor::operator ( )( const int n , const T * x , T * y)",5, 58, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh_dnnlowp_op.cc,"caffe2::TanhFunctor::GetOutputQuantizationParams() const",3, 65, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_dnnlowp_op.cc,"caffe2::SumDNNLowPOp<T,ReluFused>::SumDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",4, 42, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_dnnlowp_op.cc,"caffe2::SumDNNLowPOp<T,ReluFused>::RunOnDevice()",165, 81, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_dnnlowp_op.cc,"caffe2::SumDNNLowPOp<T,ReluFused>::GetQuantizationParameters_()",30, 80, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/norm_minimization.cc,"dnnlowp::GetNorm( float begin , float end , float density , NormMinimization :: Kind kind)",25, 78, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/norm_minimization.cc,"dnnlowp::NormMinimization::NonlinearQuantizationParamsSearch( const Histogram & hist , bool preserve_sparsity , int precision)",128, 78, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/norm_minimization.cc,"dnnlowp::NormMinimization::ChooseQuantizationParams( const Histogram & hist , bool preserve_sparsity , int precision)",138, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/concat_dnnlowp_op.cc,"caffe2::ConcatDNNLowPOp<T>::ConcatDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",15, 70, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/concat_dnnlowp_op.cc,"caffe2::ConcatDNNLowPOp<T>::RunOnDevice()",143, 81, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/concat_dnnlowp_op.cc,"caffe2::ConcatDNNLowPOp<T>::GetQuantizationParameters_()",15, 75, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/dequantize_dnnlowp_op.cc,"caffe2::DequantizeDNNLowPOp<T>::DequantizeDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",13, 79, 11, 0
repos/cpp/pytorch/caffe2/quantization/server/dequantize_dnnlowp_op.cc,"caffe2::DequantizeDNNLowPOp<T>::RunOnDevice()",19, 68, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/l2_minimization_approx_example.cc,"main( int argc , const char * argv [ ])",57, 79, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/fbgemm_pack_matrix_cache.cc,"caffe2::GetOrCreateFbgemmPackBMatrix( fbgemm :: matrix_op_t trans , int32_t m , int32_t n , const void * orig_data , const int8_t * quantized_data , int32_t ld)",47, 80, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/sigmoid_dnnlowp_op.cc,"caffe2::SigmoidFunctor::SigmoidFunctor()",1, 44, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/sigmoid_dnnlowp_op.cc,"caffe2::SigmoidFunctor::operator ( )( const int n , const T * x , T * y)",5, 58, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/sigmoid_dnnlowp_op.cc,"caffe2::SigmoidFunctor::GetOutputQuantizationParams() const",3, 65, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp_partition.cc,"caffe2::GetWorkPerThread_( size_t work , int nthreads , int work_align)",4, 77, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp_partition.cc,"caffe2::Get1DPartition( size_t work , int nthreads , int tid , int work_align)",6, 76, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp_partition.cc,"caffe2::Get1DPartitionOf2D( int m , int n , int nthreads , int tid , int * m_begin , int * m_end , int * n_begin , int * n_end , int n_align)",36, 80, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::StringToKind( const string & s)",26, 73, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::GetDefaultInstance()",43, 80, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::QuantizationFactory( int activation_precision , int weight_precision , int requantization_multiplier_precision , int eltwise_quantize_precision , bool preserve_activation_sparsity , bool preserve_weight_sparsity , bool force_scale_power_of_two , QuantizationKind activation_kind , QuantizationKind weight_kind)",19, 81, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::ChooseQuantizationParams( const Histogram & hist , QuantizationKind kind , int precision , bool preserve_sparsity) const",27, 81, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::ChooseQuantizationParams( const Histogram & hist , bool is_weight) const",17, 72, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::ChooseQuantizationParams( const float * values , int len , QuantizationKind kind , int precision , bool preserve_sparsity) const",28, 79, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::ChooseQuantizationParams( const float * values , int len , bool is_weight) const",20, 72, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/dnnlowp.cc,"dnnlowp::QuantizationFactory::ChooseRequantizationMultiplier( float real_multiplier , TensorQuantizationParams target_qparams) const",15, 74, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/elementwise_sum_benchmark.cc,"main( int argc , const char * argv [ ])",27, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::ConvDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",21, 75, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::~ConvDNNLowPOp()",1, 49, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::FilterQuantizationParams( int group_id)",3, 70, 0, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::RequantizationParams( int group_id)",3, 69, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::TakeDepthWise3x3FastPath_()",9, 80, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::TakeDepthWise3x3x3FastPath_()",16, 81, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::TakeGConvFastPath_()",22, 75, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::KernelDim_()",24, 67, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::IsConvGEMM_() const",16, 79, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::NoIm2ColNHWC_()",18, 70, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::PreComputeRowColumnOffsets_()",22, 79, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::QuantizeBias_()",58, 80, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::QuantizeWeight_()",151, 81, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::GetQuantizationParameters_()",63, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::RunOnDeviceEpilogueNCHW_( const T * col_buffer_data , int32_t * Y_int32 , T * Y_data , size_t i_offset , int group_id)",44, 77, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::RunOnDeviceWithOrderNCHW()",156, 78, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::RunOnDeviceEpilogueNHWC_( const T * col_buffer_data , int32_t * Y_int32)",147, 81, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::PartitionGroupedNHWCConv_( int * group_begin , int * group_end , int * i_begin , int * i_end , int num_groups , int m , int nthreads , int thread_id)",23, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::Im2ColNHWC_( Tensor * col_buffer)",80, 75, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::conv_nhwc_ref_( int group_id , int num_groups , int i_begin , int i_end , int M , int kernel_dim , const T * col_buffer , const T_signed * W , int32_t * Y)",24, 74, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::DispatchFBGEMM_( PackAMatrix & packA , vector<int32_t> * Y_int32 , uint8_t * Y_uint8_data)",35, 53, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::ConvNHWCCore_( const T * col_buffer_data , vector<int32_t> * Y_int32)",359, 81, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_op.cc,"caffe2::ConvDNNLowPOp<T,ReluFused>::RunOnDeviceWithOrderNHWC()",140, 81, 14, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh.cc,"dnnlowp::GetSaturationRegionBegin_( double max_abs_err)",9, 70, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh.cc,"dnnlowp::GetPassRegionEnd_( TensorQuantizationParams in_qparams , TensorQuantizationParams out_qparams , double max_abs_err , int num_in_bits)",32, 66, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh.cc,"dnnlowp::Tanh<T>::Tanh( double max_abs_err)",47, 80, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh.cc,"dnnlowp::sgn( T val)",3, 38, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/tanh.cc,"dnnlowp::Tanh<T>::Compute( T x) const",27, 72, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/kl_minimization.cc,"dnnlowp::KLDivergenceMinimization::ChooseQuantizationParams( const Histogram & hist , bool preserve_sparsity , int precision)",165, 80, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/l2_minimization_example.cc,"main( int argc , const char * argv [ ])",57, 79, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/p99_example.cc,"main( int argc , const char * argv [ ])",38, 80, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/p99.cc,"dnnlowp::P99::ChooseQuantizationParams( const Histogram & hist , bool preserve_sparsity , int)",52, 77, 2, 0
repos/cpp/pytorch/caffe2/quantization/server/utility_dnnlowp_ops.cc,"caffe2::GatherDNNLowPOp<T>::GatherDNNLowPOp( const OperatorDef & operator_def , Workspace * ws)",5, 60, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/utility_dnnlowp_ops.cc,"caffe2::GatherDNNLowPOp<T>::~GatherDNNLowPOp()",5, 71, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/utility_dnnlowp_ops.cc,"caffe2::GatherDNNLowPOp<T>::RunOnDevice()",52, 79, 8, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::ConvDNNLowPAcc16Op<ReluFused>::ConvDNNLowPAcc16Op( const OperatorDef & operator_def , Workspace * ws)",16, 80, 4, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::ConvDNNLowPAcc16Op<ReluFused>::GetQuantizationParameters_()",151, 81, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::ConvDNNLowPAcc16Op<ReluFused>::RunOnDeviceWithOrderNCHW()",182, 81, 14, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::conv_nhwc_acc16_ref_( int num_groups , int N , int output_image_size , int M , int kernel_dim , const uint8_t * col_buffer , const int8_t * W , int32_t * Y , OperatorBase * op)",81, 80, 10, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::ConvDNNLowPAcc16Op<ReluFused>::DispatchFBGEMM_( fbgemm :: PackAWithRowOffset<uint8_t,int16_t> & packA , const uint8_t * col_buffer_data , vector<int32_t> * Y_int32 , uint8_t * Y_uint8_data)",58, 81, 12, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::ConvDNNLowPAcc16Op<ReluFused>::ConvOutlier_( const uint8_t * col_buffer , vector<int32_t> * Y_int32)",44, 75, 6, 0
repos/cpp/pytorch/caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,"caffe2::ConvDNNLowPAcc16Op<ReluFused>::RunOnDeviceWithOrderNHWC()",181, 81, 12, 0
repos/cpp/pytorch/caffe2/transforms/single_op_transform.cc,"caffe2::SingleOpTransform::PatternRule( const Graph & g , const std :: vector<int> & subgraph , int idx)",9, 42, 4, 0
repos/cpp/pytorch/caffe2/transforms/single_op_transform.cc,"caffe2::SingleOpTransform::ValidatorRule( const Graph & , const std :: vector<int> & subgraph)",8, 40, 4, 0
repos/cpp/pytorch/caffe2/transforms/single_op_transform.cc,"caffe2::SingleOpTransform::ReplaceRule( const std :: vector<int> & subgraph , Graph * g_ptr)",8, 46, 2, 0
repos/cpp/pytorch/caffe2/transforms/conv_to_nnpack_transform_test.cc,"caffe2::TEST( ConvToNNPackTest , TestSimple)",27, 80, 2, 0
repos/cpp/pytorch/caffe2/transforms/common_subexpression_elimination_test.cc,"caffe2::TEST( CommonSubexpressionEliminationTest , TestSimple)",33, 78, 2, 0
repos/cpp/pytorch/caffe2/transforms/common_subexpression_elimination_test.cc,"caffe2::TEST( CommonSubexpressionEliminationTest , TestFromExternal)",31, 78, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform.cc,"caffe2::PatternNetTransform::GetPatternTraversalOrder( const transform :: Graph & graph)",34, 79, 6, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform.cc,"caffe2::compare_ops( const OperatorDef & p_op , const OperatorDef & g_op , bool arg_match)",40, 78, 6, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform.cc,"caffe2::PatternNetTransform::PatternRule( const transform :: Graph & g , const std :: vector<int> & subgraph , int g_idx)",41, 76, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform.cc,"caffe2::PatternNetTransform::ValidatorRule( const transform :: Graph & , const std :: vector<int> & subgraph)",6, 69, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform.cc,"caffe2::PatternNetTransform::ReplaceRule( const std :: vector<int> & match , transform :: Graph * g_ptr)",120, 81, 12, 0
repos/cpp/pytorch/caffe2/transforms/common_subexpression_elimination.cc,"caffe2::are_nodes_common( const Graph & g , int model_idx , int candidate_idx)",40, 78, 2, 0
repos/cpp/pytorch/caffe2/transforms/common_subexpression_elimination.cc,"caffe2::CommonSubexpressionEliminationTransform::PatternRule( const Graph & g , const std :: vector<int> & subgraph , int idx)",11, 59, 0, 0
repos/cpp/pytorch/caffe2/transforms/common_subexpression_elimination.cc,"caffe2::CommonSubexpressionEliminationTransform::ValidatorRule( const Graph & , const std :: vector<int> & subgraph)",8, 61, 0, 0
repos/cpp/pytorch/caffe2/transforms/common_subexpression_elimination.cc,"caffe2::CommonSubexpressionEliminationTransform::ReplaceRule( const std :: vector<int> & subgraph , Graph * g_ptr)",75, 81, 6, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::DummyCounterOp::Run( int)",4, 40, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestGenerateTransform)",53, 62, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestRepeatedTransform)",41, 64, 4, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestHardTransform)",57, 81, 6, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestGeneralStringMatching)",27, 68, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestDeviceOptionMatching)",31, 73, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestEngineMatching)",30, 73, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestSingularArgumentMatching)",93, 81, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestNonStrictTopographicTransform)",44, 69, 2, 0
repos/cpp/pytorch/caffe2/transforms/pattern_net_transform_test.cc,"caffe2::TEST( PatternNetTransformTest , TestMultiInputOutputTransform)",34, 81, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/net_gl.cc,"caffe2::GLNet::GLNet( const std :: shared_ptr<const NetDef> & net_def , Workspace * ws)",40, 100, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/net_gl.cc,"caffe2::GLNet::Run()",43, 90, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/net_gl.cc,"caffe2::GLNet::RunAsync()",3, 25, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/net_gl.cc,"caffe2::PairLargerThan( const std :: pair<A,B> & x , const std :: pair<A,B> & y)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/net_gl.cc,"caffe2::GLNet::TEST_Benchmark( const int warmup_runs , const int main_runs , const bool run_individual)",113, 97, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/context.cc,"caffe2::GLContext::GLContext()",7, 57, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/context.cc,"caffe2::EventCreateOPENGL( const DeviceOption & , Event *)",2, 58, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/context.cc,"caffe2::EventRecordOPENGL( Event * , const void * , const char *)",2, 72, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/context.cc,"caffe2::EventWaitOPENGLOPENGL( const Event * , void *)",1, 79, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/context.cc,"caffe2::EventFinishOPENGL( const Event *)",1, 54, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/context.cc,"caffe2::EventResetOPENGL( Event *)",1, 47, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::analyzeNet( const NetDef & net)",25, 68, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::insertCopyFromGLOp( NetDef & predictNet , const std :: string & cpu_blob)",7, 82, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::insertInputOutputCopyOps( const NetDef & def , std :: unordered_set<std::string> & cpuOp)",80, 100, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::tryFuseAdjacentOps( const OperatorDef & currentOp , const OperatorDef & nextOp , OperatorDef * fusedOp , std :: unordered_set<std::string> & glOps)",29, 98, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::runOpenGLFusion( const NetDef & def , std :: unordered_set<std::string> & glOps)",35, 96, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::dumpDefForOpenGL( const NetDef & d)",5, 79, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::rewritePredictNetForOpenGL( const NetDef & predictNet , bool runFusion , std :: unordered_set<std::string> cpuOps)",20, 118, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/core/rewrite_net.cc,"caffe2::tryConvertToOpenGL( const NetDef & predictNet , NetDef * glPredictNet , bool runFusion , std :: unordered_set<std::string> cpuOps)",17, 87, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_context_test.cc,"caffe2::TEST( OPENGLContextTest , Initialization)",4, 42, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_softmax_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , Softmax)",22, 73, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_activation_ops_test.cc,"caffe2::TEST( OPENGLOperatorTest , Sigmoid)",19, 73, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_activation_ops_test.cc,"caffe2::TEST( OPENGLOperatorTest , ReLU)",19, 70, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_activation_ops_test.cc,"caffe2::TEST( OPENGLOperatorTest , SigmoidTwice)",24, 75, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_resize_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , ResizeNearest)",29, 79, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_conv_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , Conv)",42, 84, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_conv_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , ConvReluConv)",60, 86, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_conv_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , ConvBenchmark)",48, 111, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_elementwise_sum_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , Sum)",21, 78, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_alignment_test.cc,"caffe2::TEST( OPENGLOperatorTest , ConvMaxPoolConv)",68, 89, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_alignment_test.cc,"caffe2::TEST( OPENGLOperatorTest , ConvReluConv)",59, 86, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_alignment_test.cc,"caffe2::TEST( OPENGLOperatorTest , ConvAddConv)",59, 85, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_spatial_batch_norm_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , SpatialBN)",29, 123, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_pool_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , AveragePool)",27, 77, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_pool_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , MaxPool)",27, 73, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_pool_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , AverageGlobalPool)",27, 77, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_concat_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , Concat)",43, 78, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_copy_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , CopyFromGL)",36, 79, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_fully_connected_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , FC)",30, 86, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/test/gl_norm_planar_yuv_op_test.cc,"caffe2::TEST( OPENGLOperatorTest , NormPlanarYUV)",25, 110, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/softmax_op.cc,"caffe2::GLSoftmaxOp::GLSoftmaxOp( const OperatorDef & operator_def , Workspace * ws)",2, 62, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/softmax_op.cc,"caffe2::GLSoftmaxOp::~GLSoftmaxOp()",1, 37, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/softmax_op.cc,"caffe2::GLSoftmaxOp<T>::RunOnDevice()",29, 73, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/reshape_op.cc,"caffe2::GLReshapeOp::GLReshapeOp( const OperatorDef & operator_def , Workspace * ws)",2, 62, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/reshape_op.cc,"caffe2::GLReshapeOp::~GLReshapeOp()",1, 37, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/reshape_op.cc,"caffe2::GLReshapeOp<T>::RunOnDevice()",9, 62, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/conv_op.cc,"caffe2::GLConvOp::GLConvOp( const OperatorDef & operator_def , Workspace * ws)",8, 69, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/conv_op.cc,"caffe2::GLConvOp::~GLConvOp()",1, 17, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/conv_op.cc,"caffe2::GLConvOp<T>::RunOnDevice()",80, 94, 20, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/concat_op.cc,"caffe2::GLConcatOp::GLConcatOp( const OperatorDef & operator_def , Workspace * ws)",2, 61, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/concat_op.cc,"caffe2::GLConcatOp::~GLConcatOp()",1, 36, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/concat_op.cc,"caffe2::GLConcatOp<T>::RunOnDevice()",90, 71, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/norm_planar_yuv_op.cc,"caffe2::GLNormalizePlanarYUVOp::GLNormalizePlanarYUVOp( const OperatorDef & operator_def , Workspace * ws)",2, 73, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/norm_planar_yuv_op.cc,"caffe2::GLNormalizePlanarYUVOp::~GLNormalizePlanarYUVOp()",1, 48, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/norm_planar_yuv_op.cc,"caffe2::GLNormalizePlanarYUVOp<T>::RunOnDevice()",47, 118, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/copy_op.cc,"caffe2::CopyFromGLOp::CopyFromGLOp( const OperatorDef & operator_def , Workspace * ws)",2, 63, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/copy_op.cc,"caffe2::CopyFromGLOp::~CopyFromGLOp()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/copy_op.cc,"caffe2::CopyFromGLOp<T>::RunOnDevice()",48, 99, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/elementwise_sum_op.cc,"caffe2::GLSumOp::GLSumOp( const OperatorDef & operator_def , Workspace * ws)",2, 58, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/elementwise_sum_op.cc,"caffe2::GLSumOp::~GLSumOp()",1, 33, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/elementwise_sum_op.cc,"caffe2::GLSumOp<T>::RunOnDevice()",32, 129, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/activation_ops.cc,"caffe2::GLReluOp<T>::RunOnDevice()",46, 72, 10, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/activation_ops.cc,"caffe2::GLSigmoidOp<T>::RunOnDevice()",47, 76, 10, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLAveragePoolOp::GLAveragePoolOp( const OperatorDef & operator_def , Workspace * ws)",3, 66, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLAveragePoolOp::~GLAveragePoolOp()",1, 24, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLMaxPoolOp::GLMaxPoolOp( const OperatorDef & operator_def , Workspace * ws)",3, 62, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLMaxPoolOp::~GLMaxPoolOp()",1, 20, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLAveragePoolOp<DataType>::RunOnDeviceWithOrderNCHW()",66, 85, 41, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLMaxPoolOp<DataType>::RunOnDeviceWithOrderNCHW()",63, 85, 41, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLAveragePoolOp<DataType>::RunOnDeviceWithOrderNHWC()",3, 61, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/pool_op.cc,"caffe2::GLMaxPoolOp<DataType>::RunOnDeviceWithOrderNHWC()",3, 57, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/fully_connected_op.cc,"caffe2::GLFullyConnectedOp::GLFullyConnectedOp( const OperatorDef & operator_def , Workspace * ws)",2, 69, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/fully_connected_op.cc,"caffe2::GLFullyConnectedOp::~GLFullyConnectedOp()",1, 44, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/fully_connected_op.cc,"caffe2::GLFullyConnectedOp<T>::RunOnDevice()",51, 78, 21, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/resize_op.cc,"caffe2::GLResizeNearestOp::GLResizeNearestOp( const OperatorDef & operator_def , Workspace * ws)",13, 81, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/resize_op.cc,"caffe2::GLResizeNearestOp::~GLResizeNearestOp()",1, 43, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/resize_op.cc,"caffe2::GLResizeNearestOp<T>::RunOnDevice()",36, 160, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/spatial_batch_norm_op.cc,"caffe2::GLSpatialBNOp::GLSpatialBNOp( const OperatorDef & operator_def , Workspace * ws)",7, 81, 8, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/spatial_batch_norm_op.cc,"caffe2::GLSpatialBNOp::~GLSpatialBNOp()",1, 39, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/arm-compute/operators/spatial_batch_norm_op.cc,"caffe2::GLSpatialBNOp<T>::RunOnDevice()",56, 83, 21, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::AddNoiseInput( const vector<int64_t> & shape , const string & name , Workspace * ws)",14, 86, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::compareMaxPooling( int N , int C , int H , int W , int kernelH , int kernelW , int strideH , int strideW , int padT , int padL , int padB , int padR , float maxRelErr = 1 . 0e - 5f , float absErrForRelErrFailure = 1 . 0e - 5f)",70, 99, 10, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::randInt( int a , int b)",5, 56, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::runMaxPool( int kernel , int stride , int pad)",9, 85, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::TEST( PoolOp , MaxPool2x2s2p0Randomized)",5, 41, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::TEST( PoolOp , MaxPool4x4s3p2Randomized)",5, 41, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::TEST( PoolOp , MaxPool2x2s2p0Special)",10, 73, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/pool_test.cc,"caffe2::TEST( PoolOp , MaxPoolFullyRandomized)",18, 81, 8, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/ios_caffe.cc,"MakeCaffe2Predictor( const std :: string & init_net_str , const std :: string & predict_net_str , bool disableMultithreadProcessing , bool allowMetalOperators , std :: string & errorMessage)",24, 83, 8, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/ios_caffe.cc,"GenerateStylizedImage( std :: vector<float> & originalImage , const std :: string & init_net_str , const std :: string & predict_net_str , int height , int width , std :: vector<float> & dataOut)",25, 63, 27, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/ios_caffe_predictor.cc,"Caffe2IOSPredictor::NewCaffe2IOSPredictor( const caffe2 :: NetDef & init_net , const caffe2 :: NetDef & predict_net , bool disableMultithreadProcessing , bool allowMetalOperators)",24, 97, 62, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/ios_caffe_predictor.cc,"Caffe2IOSPredictor::Caffe2IOSPredictor( const caffe2 :: NetDef & init_net , const caffe2 :: NetDef & predict_net , bool disableMultithreadProcessing , bool usingMetalOperators)",14, 84, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/ios_caffe_predictor.cc,"Caffe2IOSPredictor::run( const Tensor & inData , Tensor & outData , std :: string & errorMessage)",22, 97, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/resize_test.cc,"caffe2::AddNoiseInput( const vector<int64_t> & shape , const string & name , Workspace * ws)",14, 86, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/resize_test.cc,"caffe2::compareResizeNeareast( int N , int C , int H , int W , float wscale , float hscale)",48, 66, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/resize_test.cc,"caffe2::randInt( int a , int b)",5, 56, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ios/resize_test.cc,"caffe2::TEST( ResizeNearestOp , ResizeNearest2x)",9, 51, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op.cc,"caffe2::SNPEOp::SNPEOp( const OperatorDef & def , Workspace * ws)",57, 113, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op.cc,"caffe2::SNPEOp::RunOnDevice()",45, 97, 8, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,"caffe2::AddConstInput( const vector<int64_t> & shape , const float value , const string & name , Workspace * ws)",13, 62, 31, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,"caffe2::AddNoiseInput( const vector<int64_t> & shape , const string & name , Workspace * ws)",15, 50, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,"caffe2::snpe_run( int iters , Workspace & ws)",28, 88, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,"caffe2::caffe2_run( int iters , Workspace & ws)",33, 83, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,"main( int argc , char ** argv)",54, 92, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,"main()",3, 13, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"SNPEContext::SNPEContext( const std :: vector<uint8_t> & buffer , const char * input_name , bool enable_logging = false)",30, 103, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"SNPEContext::getInputDims() const",1, 106, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"SNPEContext::run( const float * inputData , size_t count)",19, 96, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"SNPEContext::copyOutputTo( float * outputData)",5, 104, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"snpe_has_gpu()",3, 84, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"snpe_create( const uint8_t * container , size_t size , const char * input_name)",4, 83, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"snpe_destroy( void * ctx)",1, 61, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"snpe_get_input_dims( void * ctx , size_t const ** dims , size_t * size)",5, 73, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"snpe_run( void * ctx , const float * inputData , size_t inputSize , size_t const ** outputDims , size_t * outputSize)",12, 76, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_ffi.cc,"snpe_copy_output_to( void * ctx , float * outputData)",3, 57, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/snpe/snpe_globals.cc,"caffe2::gSNPELocation()",7, 38, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::checkError( const TensorCPU & t1 , const TensorCPU & t2 , float error)",36, 74, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::test_relu( int N , int C , int H , int W)",36, 68, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::test_conv_NHWC( int N , int C , int H , int W , int K , int kernel , int pad_t , int pad_l , int pad_b , int pad_r , int stride_h , int stride_w)",107, 68, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::test_depthwise_conv_NHWC( int N , int C , int H , int W , int D , int kernel , int pad_t , int pad_l , int pad_b , int pad_r , int stride_h , int stride_w)",216, 68, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::test_pooling( std :: string type , int N , int C , int H , int W , int kernel , int pad_t , int pad_l , int pad_b , int pad_r , int stride_h , int stride_w)",89, 68, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::test_softmax( int N , int C , int H = 1 , int W = 1)",41, 68, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::TEST( NNApi , TestConv)",21, 66, 14, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::TEST( NNApi , Depthwise)",19, 74, 12, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::TEST( NNApi , TestRelu)",4, 30, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::TEST( NNApi , TestAveragePool)",17, 74, 12, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::TEST( NNApi , TestMaxPool)",17, 70, 12, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_test.cc,"caffe2::TEST( NNApi , TestSoftmax)",8, 47, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"reportError( int result_code)",20, 46, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::loadNNApiLibrary()",3, 60, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::~NNApi()",14, 61, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::run( const TensorVector & inputs , TensorVector * outputs)",27, 74, 8, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::getConvPoolArgs( const ArgumentHelper & helper , ConvPoolArgs & args)",41, 81, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addPooling( const OperatorDef & op , OperationCode op_code , bool fuse_relu)",82, 81, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addConv( const OperatorDef & op , bool fuse_relu)",185, 81, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addRelu( const OperatorDef & op)",24, 81, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addSoftmax( const OperatorDef & op)",39, 81, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addScalarOperand( int32_t val)",21, 80, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addFloatOperand( float val)",21, 80, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::addTensorOperand( const std :: string & blob , OperandCode type , std :: vector<uint32_t> & dims , float scale , int32_t zero_point)",30, 75, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi.cc,"caffe2::NNApi::init( const TensorVector & inputs , TensorVector * outputs)",179, 79, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_benchmark.cc,"caffe2::benchmark_conv_caffe2( Workspace * ws , int N , int C , int H , int W , int K , int kernel , int group , int warmup = 5 , int run = 10 , std :: string engine = 'NNPACK')",86, 69, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_benchmark.cc,"caffe2::benchmark_conv_nnapi( Workspace * ws , int N , int C , int H , int W , int K , int kernel , int group , int warmup = 5 , int run = 10)",90, 69, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_benchmark.cc,"caffe2::benchmark_conv_nnapi_int8( Workspace * ws , int N , int C , int H , int W , int K , int kernel , int group , int warmup = 5 , int run = 10)",131, 77, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/nnapi/nnapi_benchmark.cc,"main( int argc , char ** argv)",126, 79, 8, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::conv( const ConvArgs & args , const TensorCPU & X , const TensorCPU & W , const TensorCPU * b , TensorCPU * Y)",51, 96, 26, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::randInt( int a , int b)",5, 56, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::genTensor11( std :: vector<int64_t> shape)",13, 62, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::genTensorUniform11( std :: vector<int64_t> shape)",13, 59, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::genTensor0123( std :: vector<int64_t> shape)",13, 55, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::TEST( ULP , QPadZero)",37, 79, 10, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::gemmNT( int M , int N , int K , const float * A , const float * B , float * C)",11, 84, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::qgemmNT( int M , int N , int K , const uint8_t * A , const uint8_t * B , float * C)",16, 89, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::gemmTest( int64_t M , int64_t N , int64_t K)",19, 88, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::TEST( QConv , GemmTest)",8, 26, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::TEST( QConv , ConvTest)",19, 64, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::ConvTest2b1b( int IC , int KH , int KW , int H , int W , int OC , int N , ConvArgs args)",105, 100, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::ca( size_t pad = 0 , size_t stride = 1)",10, 49, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::TEST( QConv , 2b1bConvTest)",22, 51, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::TEST( QConv , 2b1bInputPackingTest)",7, 46, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_test.cc,"caffe2::TEST( QConv , 2b1bConvTestRandomized)",87, 48, 17, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::quantize2bNeon( size_t QC , const float * __restrict__ Xdata , float offset , float inter_center_distance , std :: array<uint8_t*,k2b1bXBits> XQdata)",74, 100, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::uniformQuantize2b1bNeon( QConvState * state , const TensorCPU & X , const std :: vector<std::unique_ptr<TensorCPU>> & XQ , float offset , float inter_center_distance)",62, 98, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::uniformQuantize2b1bNeonPacked( QConvState * state , const TensorCPU & X , const std :: vector<std::unique_ptr<TensorCPU>> & XQ , float offset , float inter_center_distance)",70, 101, 12, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::qpack_tiles( QConvState * state , const TensorCPU & X , size_t axis , TensorCPU * XP)",48, 86, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::qgess_packed( const uint8_t * __restrict__ Ablock , const uint8_t * __restrict__ Bblock , float * __restrict__ Cblock , const size_t Cstride , const size_t QK , const size_t Nstart , F && f)",68, 85, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::qgemm_nt_packed( QConvState * state , const TensorCPU & A , const TensorCPU & B , TensorCPU * C , F && f = F())",70, 92, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::run2b1bConvIm2ColGEMM( QConvState * state , const ConvArgs & args , const TensorCPU & X , TensorCPU * Y)",116, 101, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp_neon.cc,"caffe2::run2b1bConvNeon( QConvState * state , const ConvArgs & args , const TensorCPU & X , TensorCPU * Y)",7, 98, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::uniformQuantize2b1b( const TensorCPU & X , const std :: vector<std::unique_ptr<TensorCPU>> & XQ , float offset , float inter_center_distance)",45, 76, 25, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::qconv( const ConvArgs & args , const TensorCPU & X , const TensorCPU & W , const TensorCPU * b , TensorCPU * Y)",55, 98, 14, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::qpad_zero( const ConvArgs & args , const TensorCPU & X , TensorCPU * Y)",22, 79, 31, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::signQuantize( const TensorCPU & X , TensorCPU * XQ)",24, 55, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::filterNormalization11( const TensorCPU & WQ , TensorCPU * WQN)",17, 90, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::filterNormalizationL1( const TensorCPU & W , TensorCPU * WL1)",14, 65, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::qim2col( const ConvArgs & args , const TensorCPU & XQ , const TensorCPU & WQ , TensorCPU * XQcol)",63, 97, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::create2b1bConvState( Workspace * ws , const TensorCPU & W , const TensorCPU * b)",39, 76, 2, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::run2b1bConvGeneric( QConvState * state , const ConvArgs & args , const TensorCPU & X , TensorCPU * Y)",24, 101, 0, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::run2b1bUnification( QConvState * state , size_t N , size_t C , const float * WQNVdata , const float * YQs0Vdata , const float * YQs1Vdata , size_t YQstride , float * Ydata , size_t Ystride , const float * bias)",26, 95, 6, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::QConvOp::QConvOp( const OperatorDef & operator_def , Workspace * ws)",7, 100, 4, 0
repos/cpp/pytorch/caffe2/mobile/contrib/ulp2/ulp.cc,"caffe2::QConvOp::RunOnDeviceWithOrderNHWC()",20, 77, 4, 0
repos/cpp/pytorch/caffe2/sgd/lars_op.cc,"caffe2::LarsOp<float,CPUContext>::ComputeLearningRate( const float * wd , const float * trust , const float * lr_max , float offset , float lr_min , float * X_norm , float * dX_norm , float * lr_rescaled)",16, 60, 4, 0
repos/cpp/pytorch/caffe2/sgd/iter_op.cc,"caffe2::MutexSerializer::Serialize( const void * pointer , TypeMeta typeMeta , const string & name , BlobSerializerBase :: SerializationAcceptor acceptor)",12, 71, 2, 0
repos/cpp/pytorch/caffe2/sgd/iter_op.cc,"caffe2::MutexDeserializer::Deserialize( const BlobProto & , Blob * blob)",4, 81, 0, 0
repos/cpp/pytorch/caffe2/sgd/ftrl_op.cc,"caffe2::sgn( const T x)",3, 42, 2, 0
repos/cpp/pytorch/caffe2/sgd/ftrl_op.cc,"caffe2::ftrl_compute( const T w , const T n , const T z , const T g , T & nw , T & nn , T & nz , const FtrlParams<T> & params)",21, 74, 8, 0
repos/cpp/pytorch/caffe2/sgd/ftrl_op.cc,"caffe2::ftrl_update( int N , const T * w , const T * nz , const T * g , T * new_w , T * new_nz , const FtrlParams<T> & params , Context *)",23, 44, 2, 0
repos/cpp/pytorch/caffe2/sgd/ftrl_op.cc,"caffe2::FtrlOp<T,Context>::RunOnDevice()",21, 78, 4, 0
repos/cpp/pytorch/caffe2/sgd/ftrl_op.cc,"caffe2::SparseFtrlOp<T>::DoRun()",48, 72, 2, 0
repos/cpp/pytorch/caffe2/sgd/adagrad_op.cc,"caffe2::CostInferenceForSparseAdagrad( const OperatorDef & , const vector<TensorShape> & inputs)",26, 76, 6, 0
repos/cpp/pytorch/caffe2/sgd/rmsprop_op.cc,"caffe2::rmsprop_update<CPUContext>( int N , const float * g , const float * ms , const float * mom , float * ng , float * nms , float * nmom , float decay , float momentum , float epsilon , const float * lr , CPUContext *)",25, 74, 2, 0
repos/cpp/pytorch/caffe2/sgd/gftrl_op.cc,"caffe2::gftrl_compute( const T & w , const T & n , const T & z , const T & g , T & nw , T & nn , T & nz , const T & z_norm , const int OutputDim , const GFtrlParams<T> & params)",23, 74, 8, 0
repos/cpp/pytorch/caffe2/sgd/gftrl_op.cc,"caffe2::gftrl_update( int OutputDim , int InputDim , const T * w , const T * nz , const T * g , T * new_w , T * new_nz , const GFtrlParams<T> & params , Context *)",37, 72, 6, 0
repos/cpp/pytorch/caffe2/sgd/gftrl_op.cc,"caffe2::GFtrlOp<T,Context>::RunOnDevice()",23, 78, 4, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue_db.cc,"caffe2::db::CreateBlobsQueueDBOp::CreateBlobsQueueDBOp( const OperatorDef & operator_def , Workspace * ws)",2, 71, 2, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue_db.cc,"caffe2::db::CreateBlobsQueueDBOp::RunOnDevice()",11, 79, 8, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::BlobsQueue( Workspace * ws , const std :: string & queueName , size_t capacity , size_t numBlobs , bool enforceUniqueName , const std :: vector<std::string> & fieldNames)",31, 81, 6, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::blockingRead( const std :: vector<Blob*> & inputs , float timeout_secs)",47, 77, 2, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::tryWrite( const std :: vector<Blob*> & inputs)",18, 79, 2, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::blockingWrite( const std :: vector<Blob*> & inputs)",19, 79, 2, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::close()",6, 41, 2, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::canWrite()",7, 62, 2, 0
repos/cpp/pytorch/caffe2/queue/blobs_queue.cc,"caffe2::BlobsQueue::doWrite( const std :: vector<Blob*> & inputs)",13, 78, 6, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::concat( CPUContext & context , const std :: vector<std::vector<TensorCPU>> & inputs , const std :: vector<TensorCPU*> & outputs)",56, 73, 4, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::split( CPUContext & context , const std :: vector<const TensorCPU*> & inputs)",32, 72, 10, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::RebatchingQueue( size_t capacity , size_t numBlobs)",2, 68, 4, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::~RebatchingQueue()",3, 38, 0, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::canRead() const",3, 40, 0, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::dequeue( CPUContext & context , size_t numElements , const std :: vector<TensorCPU*> & outputs)",42, 71, 6, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::canWrite() const",3, 41, 0, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::enqueueOne( CPUContext & , const std :: vector<const TensorCPU*> & inputs)",13, 54, 2, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::enqueueMany( CPUContext & context , const std :: vector<const TensorCPU*> & inputs)",9, 54, 2, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::enqueue( std :: vector<std::vector<TensorCPU>> splittedInputs)",29, 80, 8, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::capacity() const",3, 43, 0, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::numBlobs() const",3, 43, 0, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::isClosed() const",4, 41, 0, 0
repos/cpp/pytorch/caffe2/queue/rebatching_queue.cc,"caffe2::RebatchingQueue::close()",9, 43, 4, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::LevelDBCursor( leveldb :: DB * db)",4, 57, 6, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::~LevelDBCursor()",1, 31, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::Seek( const string & key)",1, 62, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::SupportsSeek()",1, 48, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::SeekToFirst()",1, 56, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::Next()",1, 42, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::key()",1, 60, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::value()",1, 64, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBCursor::Valid()",1, 51, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBTransaction::LevelDBTransaction( leveldb :: DB * db)",4, 59, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBTransaction::~LevelDBTransaction()",3, 35, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBTransaction::Put( const string & key , const string & value)",3, 62, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDBTransaction::Commit()",7, 80, 4, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDB::LevelDB( const string & source , Mode mode)",15, 75, 4, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDB::Close()",1, 41, 2, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDB::NewCursor()",3, 50, 4, 0
repos/cpp/pytorch/caffe2/db/leveldb.cc,"caffe2::db::LevelDB::NewTransaction()",3, 55, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::MDB_CHECK( int mdb_status)",3, 71, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::LMDBCursor( MDB_env * mdb_env)",7, 69, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::~LMDBCursor()",5, 39, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::Seek( const string & key)",17, 65, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::SupportsSeek()",1, 48, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::SeekToFirst()",1, 55, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::Next()",1, 47, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::key()",3, 81, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::value()",4, 64, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::Valid()",1, 43, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBCursor::SeekLMDB( MDB_cursor_op op)",9, 78, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBTransaction::LMDBTransaction( MDB_env * mdb_env)",5, 60, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBTransaction::~LMDBTransaction()",4, 41, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBTransaction::Commit()",7, 60, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDB::~LMDB()",3, 21, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDB::Close()",6, 31, 6, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDB::NewCursor()",3, 46, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDB::NewTransaction()",3, 54, 2, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDB::LMDB( const string & source , Mode mode)",18, 78, 4, 0
repos/cpp/pytorch/caffe2/db/lmdb.cc,"caffe2::db::LMDBTransaction::Put( const string & key , const string & value)",8, 68, 0, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::ProtoDBCursor( const TensorProtos * proto)",2, 52, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::~ProtoDBCursor()",1, 31, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::Seek( const string &)",3, 64, 4, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::SeekToFirst()",1, 45, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::Next()",1, 36, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::key()",1, 65, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::value()",4, 78, 6, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBCursor::Valid()",1, 66, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBTransaction::ProtoDBTransaction( TensorProtos * proto)",6, 51, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBTransaction::~ProtoDBTransaction()",3, 35, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBTransaction::Put( const string & key , const string & value)",15, 65, 6, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDBTransaction::Commit()",1, 28, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDB::ProtoDB( const string & source , Mode mode)",9, 75, 10, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDB::~ProtoDB()",3, 24, 2, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDB::Close()",5, 47, 6, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDB::NewCursor()",3, 48, 4, 0
repos/cpp/pytorch/caffe2/db/protodb.cc,"caffe2::db::ProtoDB::NewTransaction()",3, 54, 2, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::CreateAndFill( const string & db_type , const string & name)",18, 71, 0, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::TestCursor( Cursor * cursor)",28, 73, 2, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::DBSeekTestWrapper( const string & db_type)",11, 60, 4, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::TEST( DBSeekTest , RocksDB)",3, 32, 2, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::TEST( DBSeekTest , LevelDB)",3, 32, 2, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::TEST( DBSeekTest , LMDB)",3, 29, 2, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::TEST( DBReaderTest , Reader)",64, 70, 2, 0
repos/cpp/pytorch/caffe2/db/db_test.cc,"caffe2::db::TEST( DBReaderShardedTest , Reader)",44, 80, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::ZmqDBCursor( const string & source)",10, 56, 8, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::~ZmqDBCursor()",8, 57, 4, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::Seek( const string &)",2, 63, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::SeekToFirst()",1, 51, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::Next()",8, 63, 4, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::key()",1, 41, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::value()",1, 45, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::Valid()",1, 41, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDBCursor::Prefetch()",16, 74, 6, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDB::ZmqDB( const string & source , Mode mode)",4, 71, 4, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDB::~ZmqDB()",1, 23, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDB::Close()",1, 27, 2, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDB::NewCursor()",3, 46, 4, 0
repos/cpp/pytorch/caffe2/db/zmqdb.cc,"caffe2::db::ZmqDB::NewTransaction()",4, 77, 4, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setOperatorDef( const caffe2 :: OperatorDef & opDef)",4, 74, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::hasOperatorDef() const",3, 48, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getOperatorDef() const",6, 76, 6, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getMutableOperatorDef()",6, 76, 6, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setDeviceOption( const caffe2 :: DeviceOption & devOpt)",3, 77, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::hasDeviceOption() const",3, 49, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getDeviceOption() const",6, 78, 6, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getMutableDeviceOption()",6, 78, 6, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setDevice( std :: string device)",3, 55, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getDevice() const",3, 56, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setDeviceType( int device)",3, 51, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getDeviceType() const",3, 46, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setParallelization( Caffe2Annotation :: ParallelizationScheme s , int num)",6, 47, 4, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getParallelizationScheme() const",3, 53, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getParallelization() const",3, 51, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setKeyNode( NNGraph :: NodeRef n)",3, 56, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getKeyNode() const",4, 63, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setLengthNode( NNGraph :: NodeRef n)",3, 59, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getLengthNode() const",4, 68, 2, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::setComponentLevels( std :: vector<std::string> components)",3, 81, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::getComponentLevels() const",3, 72, 0, 0
repos/cpp/pytorch/caffe2/opt/annotations.cc,"caffe2::Caffe2Annotation::classof( const Annotation * A)",3, 54, 0, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"fakeNet()",28, 45, 4, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"TEST( Converter , DeclareExport)",39, 78, 4, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"TEST( Distributed , InsertDeviceOptions)",17, 76, 6, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"TEST( Distributed , InsertDeviceOptionsFailureCase)",14, 66, 2, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"TEST( Converter , InjectDataEdgeIndicators)",34, 78, 2, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"TEST( Converter , OverloadedConvertToNNModule)",15, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/distributed_test.cc,"TEST( Converter , OverloadedConvertToNNModuleFailure)",12, 66, 2, 0
repos/cpp/pytorch/caffe2/opt/mobile_test.cc,"TEST( MobileTest , Convolution)",32, 66, 6, 0
repos/cpp/pytorch/caffe2/opt/backend_transformer_base.cc,"caffe2::AnnotateOpIndex( NetDef * net)",6, 42, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_transformer_base.cc,"caffe2::BackendTransformerBase::getModelId( const NetDef & net)",9, 72, 6, 0
repos/cpp/pytorch/caffe2/opt/backend_transformer_base.cc,"caffe2::BackendTransformerBase::wrapShapeInfoIntoTensorProto( const std :: string & name , const ShapeInfo & shape_info) const",11, 66, 0, 0
repos/cpp/pytorch/caffe2/opt/backend_transformer_base.cc,"caffe2::BackendTransformerBase::ssaRewriteAndMapNames( Workspace * ws , NetDef * pred_net , const std :: unordered_map<std::string,TensorShape> & input_shape_hints)",27, 80, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_transformer_base.cc,"caffe2::BackendTransformerBase::inferShapes( Workspace * ws , NetDef * pred_net , const std :: unordered_map<std::string,TensorShape> & shape_hints_mapped , const BoundShapeSpec & spec)",36, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::ConvertToVec( const :: google :: protobuf :: RepeatedField<::google::protobuf::int64> & in)",9, 78, 4, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::SizeFromDim( const TensorShape & shape , int axis)",7, 58, 0, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::SizeToDim( const TensorShape & shape , int axis)",8, 56, 0, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::EnsureShapeNames( std :: unordered_map<std::string,ShapeInfo> * info)",5, 74, 0, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferBoundShapeAndType( const NetDef & net , const std :: unordered_map<std::string,ShapeInfo> & info)",32, 67, 8, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::CheckAndSetTensorShapeAndType( const std :: string & name , ShapeInfo :: DimType t , std :: vector<int64_t> bound_dims , TensorProto :: DataType type)",44, 79, 4, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::InferOutput( const OperatorDef & op , const std :: vector<TensorShape> & input_shapes)",7, 64, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferGivenTensorFill( const OperatorDef & op)",8, 75, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferLengthsRangeFill( const OperatorDef & op)",17, 80, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferSparseLengthsSum( const OperatorDef & op)",60, 77, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferReshape( const OperatorDef & op)",7, 71, 4, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferConcat( const OperatorDef & op)",56, 73, 10, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferFC( const OperatorDef & op)",70, 81, 4, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inferencer.cc,"caffe2::BoundShapeInferencer::InferCommonOp( const OperatorDef & op)",39, 80, 4, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::OptimizeForIdeep( repr :: NNModule * nn , caffe2 :: Workspace * ws , bool training_mode)",6, 58, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::getBlob( repr :: NNGraph :: NodeRef node , caffe2 :: Workspace * ws)",5, 74, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::getTensor( Blob * blob)",7, 44, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::getOpDef( const repr :: NeuralNetOperator & nnOp)",7, 75, 0, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::getMutableOpDef( repr :: NeuralNetOperator & nnOp)",7, 74, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::isOpType( const repr :: NNGraph :: NodeRef & nodeRef , string typeName)",8, 72, 0, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::isOnIdeepDevice( const repr :: NeuralNetOperator & nnOp)",5, 75, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::shouldFuseConv( const repr :: Conv & conv)",3, 65, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::removeStopGradientForInference( repr :: NNModule * nn)",27, 78, 4, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::resetConvForFusion( repr :: NNGraph :: NodeRef convNode , int fusion_type)",35, 76, 0, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::fuseConvBNAndAffChHelperForIdeep( repr :: NNModule * nn , caffe2 :: Workspace * ws)",130, 87, 4, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::fuseConvBNAndAffChForIdeep( repr :: NNModule * nn , caffe2 :: Workspace * ws)",4, 77, 0, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::fuseConvSumForIdeep( repr :: NNModule * nn , caffe2 :: Workspace * ws)",95, 85, 4, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::fuseActivationForIdeep( repr :: NNModule * nn)",6, 78, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::enforceFusionInplaceForIdeep( repr :: NNModule * nn)",54, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::setPoolingInferenceMode( repr :: NNModule * nn)",28, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/optimize_ideep.cc,"caffe2::opt::OptimizeForIdeep( repr :: NNModule * nn , caffe2 :: Workspace * ws , bool training_mode)",21, 38, 2, 0
repos/cpp/pytorch/caffe2/opt/distributed.cc,"caffe2::setDeviceOption( NNGraph :: NodeRef n , caffe2 :: DeviceOption & d)",7, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/distributed.cc,"caffe2::addBlobDeviceOptions( std :: map<std::string,caffe2::DeviceOption> blobMap , NNModule * nn)",55, 72, 8, 0
repos/cpp/pytorch/caffe2/opt/distributed.cc,"caffe2::injectDataEdgeIndicators( nom :: repr :: NNModule * nn)",15, 81, 4, 0
repos/cpp/pytorch/caffe2/opt/distributed.cc,"caffe2::removeDataEdgeIndicators( nom :: repr :: NNModule * nn)",14, 57, 0, 0
repos/cpp/pytorch/caffe2/opt/distributed.cc,"caffe2::convertToNNModule( caffe2 :: NetDef & net , std :: map<std::string,caffe2::DeviceOption> blobMap)",8, 59, 4, 0
repos/cpp/pytorch/caffe2/opt/dead_code_elim_test.cc,"TEST( DeadCodeElim , BasicElim)",15, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/dead_code_elim_test.cc,"TEST( DeadCodeElim , BasicNoElim)",16, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/dead_code_elim_test.cc,"TEST( DeadCodeElim , PartiallyUsedNoElim)",18, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/device_test.cc,"TEST( DeviceTest , InsertCopies)",62, 79, 8, 0
repos/cpp/pytorch/caffe2/opt/optimizer.cc,"caffe2::opt::workspaceOptimizations( nom :: repr :: NNModule * nn , Workspace * ws , int level)",9, 81, 0, 0
repos/cpp/pytorch/caffe2/opt/optimizer.cc,"caffe2::opt::graphOptimzations( nom :: repr :: NNModule * nn , int level)",12, 61, 0, 0
repos/cpp/pytorch/caffe2/opt/optimizer.cc,"caffe2::opt::optimize( NetDef net , Workspace * ws , int level)",6, 56, 0, 0
repos/cpp/pytorch/caffe2/opt/optimizer.cc,"caffe2::opt::optimize( NetDef net , int level)",5, 41, 0, 0
repos/cpp/pytorch/caffe2/opt/shape_info.cc,"caffe2::getShapeInfoFromBlob( const Blob * blob)",8, 57, 2, 0
repos/cpp/pytorch/caffe2/opt/shape_info.cc,"caffe2::operator ==( const ShapeInfo & lhs , const ShapeInfo & rhs)",4, 70, 6, 0
repos/cpp/pytorch/caffe2/opt/dead_code_elim.cc,"caffe2::opt::deadCodeElim( NNModule * nn)",28, 68, 8, 0
repos/cpp/pytorch/caffe2/opt/fusion.cc,"caffe2::opt::fuseConvBNHelper( repr :: NNModule * nn , caffe2 :: Workspace * ws)",98, 81, 0, 0
repos/cpp/pytorch/caffe2/opt/fusion.cc,"caffe2::opt::fuseConvBN( nom :: repr :: NNModule * nn , caffe2 :: Workspace * ws)",4, 66, 0, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"AddConv( caffe2 :: NetDef * net , int tick)",8, 50, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"Supports( const caffe2 :: OperatorDef & op)",4, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"Transform( const caffe2 :: NetDef & net)",18, 54, 0, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"TEST( BackendCuttingTest , unit)",12, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"TEST( BackendCuttingTest , line)",24, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"TEST( BackendCuttingTest , convergedPaths)",30, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting_test.cc,"TEST( BackendCuttingTest , skipPath)",32, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/mobile.cc,"caffe2::opt::addNNPACK( repr :: NNModule * nn , bool low_memory)",51, 80, 4, 0
repos/cpp/pytorch/caffe2/opt/mobile.cc,"caffe2::opt::isNNPACKConvReluEfficient( const std :: string & algo , const repr :: Conv & conv)",20, 64, 2, 0
repos/cpp/pytorch/caffe2/opt/mobile.cc,"caffe2::opt::fuseNNPACKConvRelu( repr :: NNModule * nn)",38, 80, 4, 0
repos/cpp/pytorch/caffe2/opt/device.cc,"getInputEdges( const NNGraph :: SubgraphType & sg , const NNGraph & g)",17, 75, 10, 0
repos/cpp/pytorch/caffe2/opt/device.cc,"getOutputEdges( const NNGraph :: SubgraphType & sg , const NNGraph & g)",19, 65, 8, 0
repos/cpp/pytorch/caffe2/opt/device.cc,"caffe2::opt::insertCopies( NNModule * nn , std :: function<bool(NNGraph::NodeRef)> supported , std :: function<NNGraph::NodeRef(NNGraph&)> copyToFn , std :: function<NNGraph::NodeRef(NNGraph&)> copyFromFn)",96, 77, 8, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::GroupAnnotation::GroupAnnotation( int i , int g = - 1)",1, 65, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::ShowNode( NodeRef node)",13, 78, 8, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::DumpGraph( NNGraph * g)",32, 81, 10, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::VisitorContext::VisitorContext( std :: function<bool(const caffe2::OperatorDef&)> func)",2, 71, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::GetInfo( std :: unordered_map<NodeRef,GroupAnnotation> & infos , NodeRef node)",7, 80, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::GetInfo( const std :: unordered_map<NodeRef,GroupAnnotation> & infos , NodeRef node)",8, 76, 6, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::Explore( const std :: vector<NodeRef> & current_frontier , VisitorContext * context)",41, 80, 10, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::TransformSubgraph::TransformSubgraph( std :: vector<NodeRef> && f , std :: vector<NodeRef> && n , int id , bool need)",9, 35, 6, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::TransformSubgraph::TransformSubgraph( TransformSubgraph && rhs)",7, 67, 8, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::TransformSubgraph::operator =( TransformSubgraph && rhs)",9, 67, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::TransformSubgraph::Print() const",11, 42, 6, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::ConvertToC2Net( const TransformSubgraph & sub , const std :: unordered_map<NodeRef,GroupAnnotation> & infos)",26, 80, 10, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::DetectBoundaryReferences( TransformSubgraph * subgraph , const std :: unordered_map<NodeRef,GroupAnnotation> & infos , const std :: unordered_set<std::string> & original_external_output)",39, 81, 4, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::ReplaceSubgraph( const TransformSubgraph & subgraph , caffe2 :: NetDef & net_opt , NNGraph * g)",49, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::PruneUnrefereredNodes( NNModule * nn)",25, 60, 4, 0
repos/cpp/pytorch/caffe2/opt/backend_cutting.cc,"caffe2::opt::OptimizeForBackend( caffe2 :: NetDef & net , std :: function<bool(const caffe2::OperatorDef&)> supports , std :: function<caffe2::NetDef(const caffe2::NetDef&)> transform_func , bool debug)",79, 81, 8, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"getStrides( std :: map<std::string,caffe2::Argument> argMap)",11, 78, 0, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"getPads( std :: map<std::string,caffe2::Argument> argMap)",9, 75, 0, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"getDilations( std :: map<std::string,caffe2::Argument> argMap)",9, 80, 0, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"getGroup( std :: map<std::string,caffe2::Argument> & argMap)",7, 72, 4, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::Converter::getArgumentsFromOperator( caffe2 :: OperatorDef op)",8, 77, 0, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::getLayout( std :: map<std::string,caffe2::Argument> argMap)",13, 55, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::Converter::convertToOperatorDef( const nom :: repr :: NeuralNetOperator * nnOp)",13, 78, 6, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::getKernelShape( std :: map<std::string,caffe2::Argument> argMap)",21, 74, 4, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::ConvConverter::convertToNeuralNetOperator( const OperatorDef & op)",15, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::ConvConverter::~ConvConverter()",1, 31, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::ClipConverter::convertToNeuralNetOperator( const OperatorDef & op)",18, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::ClipConverter::~ClipConverter()",1, 31, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::AveragePoolConverter::convertToNeuralNetOperator( const OperatorDef & op)",8, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::AveragePoolConverter::~AveragePoolConverter()",1, 38, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::MaxPoolConverter::convertToNeuralNetOperator( const OperatorDef & op)",8, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::MaxPoolConverter::~MaxPoolConverter()",1, 34, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::ConcatConverter::convertToNeuralNetOperator( const OperatorDef & op)",19, 78, 6, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::ConcatConverter::~ConcatConverter()",1, 33, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::FCConverter::convertToNeuralNetOperator( const OperatorDef & op)",20, 76, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::FCConverter::~FCConverter()",1, 29, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::convertToNeuralNetOperator( const caffe2 :: OperatorDef & op)",31, 80, 8, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::convertToNNModule( const caffe2 :: NetDef & net , bool strict , std :: vector<repr::NNGraph::NodeRef> * opNodeVec)",98, 81, 8, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::convertToOperatorDef( const repr :: NNGraph :: NodeRef & instrNode)",28, 75, 4, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::getOrAddCaffe2Annotation( nom :: repr :: NNGraph :: NodeRef & instrNode)",14, 66, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::convertToCaffe2Proto( repr :: NNModule & m)",4, 57, 0, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::mergeExternalTensors( const std :: unordered_set<repr::NNGraph::NodeRef> & currExternal , const std :: vector<std::string> & oldExternal)",27, 83, 8, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::convertToCaffe2Proto( repr :: NNModule & m , const caffe2 :: NetDef & oldNet)",88, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::pushOpToFront( caffe2 :: OperatorDef & op , caffe2 :: NetDef * net)",9, 68, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::injectDataEdgeIndicators( caffe2 :: NetDef * net)",16, 54, 2, 0
repos/cpp/pytorch/caffe2/opt/converter.cc,"caffe2::removeDataEdgeIndicators( caffe2 :: NetDef * net)",16, 68, 2, 0
repos/cpp/pytorch/caffe2/opt/converter_nomigraph_test.cc,"TEST( Converter , Basic)",21, 64, 10, 0
repos/cpp/pytorch/caffe2/opt/converter_nomigraph_test.cc,"TEST( Converter , UnknownType)",9, 68, 6, 0
repos/cpp/pytorch/caffe2/opt/converter_nomigraph_test.cc,"fakeNet()",11, 40, 6, 0
repos/cpp/pytorch/caffe2/opt/converter_nomigraph_test.cc,"TEST( Converter , ExternalInputs)",10, 78, 2, 0
repos/cpp/pytorch/caffe2/opt/converter_nomigraph_test.cc,"TEST( Converter , ExternalOutputs)",10, 80, 2, 0
repos/cpp/pytorch/caffe2/opt/converter_nomigraph_test.cc,"TEST( Converter , InjectDataEdgeIndicators)",33, 74, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::stripShapeInfoMap( const ShapeInfoMap & info_map)",8, 64, 0, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::onnxifiDataType( caffe2 :: TensorProto :: DataType t)",20, 63, 4, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::convertToValueInfo( const std :: vector<std::string> & names , const std :: unordered_map<std::string,TensorShape> & shape_hints , const std :: unordered_map<std::string,::ONNX_NAMESPACE::TypeProto> & extra_shape_hints)",30, 76, 6, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::GetWeightsAndInputs( const NetDef & net , const std :: unordered_set<std::string> & weights_in_ws , const std :: vector<std::string> & extra_weights , std :: unordered_set<std::string> * initialization_list , std :: vector<std::string> * total_inputs_vec)",40, 58, 4, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::FillModelInfo( :: ONNX_NAMESPACE :: ModelProto * model)",7, 64, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::MakeSeqSizeBlob( const std :: string & blob_name)",3, 60, 0, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::MakeOutputForAdjustBatchOp( const std :: string & input)",3, 67, 0, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::MakeInputForAdjustBatchOp( const std :: string & output)",3, 67, 0, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::MakeAdjustBatchOp( const std :: string & input_blob , const std :: string & output_blob , int max_batch_size , const std :: string & real_batch_size_blob , bool adjust_to_max_batch_size)",22, 56, 6, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::ToHashSet( const :: google :: protobuf :: RepeatedPtrField<string> & strs)",4, 64, 4, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::GetBlob1stDimSize( const ShapeInfo & shape_info , const string & blob_name)",9, 43, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::AddAdjustBatchOps( const ShapeInfoMap & shape_hints , NetDef * onnxifi_net , vector<OperatorDef> * input_ops , vector<OperatorDef> * output_ops)",101, 81, 10, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::ComposeResultNet( const vector<OperatorDef> & input_ops , const vector<OperatorDef> & output_ops , const OperatorDef & onnxifi_op)",15, 54, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::OnnxifiTransformer( const OnnxifiTransformerOptions & opts)",14, 78, 0, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::~OnnxifiTransformer()",7, 81, 4, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::BuildOnnxifiOp( const std :: string & onnx_model_str , const std :: unordered_map<std::string,TensorShape> & output_shape_hints , const std :: unordered_set<std::string> & initialization_list , const std :: vector<std::string> & external_inputs , const std :: vector<std::string> & external_outputs)",65, 76, 4, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::SubnetToOnnxifiOpViaC2( const caffe2 :: NetDef & net , const std :: unordered_set<std::string> & weights_in_ws , const ShapeInfoMap & shape_hints)",104, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::SubnetToOnnxifiOpViaOnnx( const caffe2 :: NetDef & net , const std :: unordered_set<std::string> & weights_in_ws , Workspace * ws , onnx :: OnnxExporter * exporter , ShapeInfoMap * shape_hints)",135, 79, 4, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::supportOpOnnx( const caffe2 :: OperatorDef & op , onnx :: OnnxExporter * exporter , const std :: unordered_set<int> & blacklisted_ops , onnxBackendID backend_id) const",99, 81, 6, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::supportOpC2( const caffe2 :: OperatorDef & op , const ShapeInfoMap & shape_hints , const std :: unordered_set<int> & blacklisted_ops , onnxBackendID backend_id) const",66, 81, 6, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::tieGatherAndSparseLengthsWeightedSumOps( const NetDef & net , const ShapeInfoMap & shape_hints , std :: unordered_set<int> * blacklisted_ops) const",34, 80, 10, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::applyFilteringRules( const NetDef & net , const ShapeInfoMap & shape_hints , std :: unordered_set<int> * blacklisted_ops) const",6, 78, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::getBackendId()",20, 78, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::TransformViaC2( NetDef * pred_net , const std :: unordered_set<std::string> & weights , const std :: unordered_set<int> & blacklisted_ops , const ShapeInfoMap & shape_hints)",19, 73, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::TransformViaOnnx( Workspace * ws , NetDef * pred_net , const std :: unordered_set<std::string> & weights , const std :: unordered_set<int> & blacklisted_ops , ShapeInfoMap * shape_hints)",27, 80, 2, 0
repos/cpp/pytorch/caffe2/opt/onnxifi_transformer.cc,"caffe2::OnnxifiTransformer::transform( Workspace * ws , NetDef * pred_net , const std :: vector<std::string> & weight_names , const std :: unordered_map<std::string,TensorShape> & input_shape_hints , const std :: unordered_set<int> & blacklisted_ops)",68, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"makeTensorInfo( ShapeInfo :: DimType t , const std :: vector<int64_t> & dims , TensorProto :: DataType dtype = TensorProto_DataType_FLOAT)",13, 64, 4, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"verifyShapeInfo( const ShapeInfoMap & info , const std :: string & name , ShapeInfo :: DimType t , const std :: vector<int64_t> & dims , TensorProto :: DataType dtype = TensorProto_DataType_FLOAT)",18, 64, 4, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , SparseLengthsSum)",28, 79, 6, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , SparseLengthsSumFused8BitRowwise)",38, 81, 10, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , LengthsRangeFill)",38, 46, 0, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , Reshape)",39, 80, 6, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , ConcatMissingInput)",24, 78, 6, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , FC)",29, 81, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , UnsupportedFC)",12, 79, 2, 0
repos/cpp/pytorch/caffe2/opt/bound_shape_inference_test.cc,"TEST( BoundShapeInference , Combo0)",37, 80, 6, 0
repos/cpp/pytorch/caffe2/opt/sink.cc,"caffe2::opt::sinkMaxPool( nom :: repr :: NNModule * nn)",32, 71, 4, 0
repos/cpp/pytorch/caffe2/predictor/predictor_config.cc,"caffe2::getNet( const MetaNetDef & def , const std :: string & name)",8, 71, 0, 0
repos/cpp/pytorch/caffe2/predictor/predictor_config.cc,"caffe2::getBlobs( const MetaNetDef & def , const std :: string & name)",10, 69, 0, 0
repos/cpp/pytorch/caffe2/predictor/predictor_config.cc,"caffe2::makePredictorConfig( const MetaNetDef & def , Workspace * parent , bool run_init)",19, 79, 0, 0
repos/cpp/pytorch/caffe2/predictor/predictor_config.cc,"caffe2::makePredictorConfig( const NetDef & init_net , const NetDef & run_net , Workspace * parent , bool run_init , int optimization)",35, 81, 6, 0
repos/cpp/pytorch/caffe2/predictor/predictor_test.cc,"caffe2::randomTensor( const std :: vector<int64_t> & dims , CPUContext * ctx)",10, 70, 6, 0
repos/cpp/pytorch/caffe2/predictor/predictor_test.cc,"caffe2::parseNetDef( const std :: string & value)",8, 48, 6, 0
repos/cpp/pytorch/caffe2/predictor/predictor_test.cc,"caffe2::parseMetaNetDef( const std :: string & value)",8, 55, 0, 0
repos/cpp/pytorch/caffe2/predictor/predictor_test.cc,"caffe2::PredictorTest::SetUp()",8, 79, 8, 0
repos/cpp/pytorch/caffe2/predictor/predictor_test.cc,"caffe2::TEST_F( PredictorTest , SimpleBatchSized)",13, 62, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor_test.cc,"caffe2::TEST_F( PredictorTest , SimpleBatchSizedMapInput)",14, 62, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor_utils.cc,"caffe2::predictor_utils::getNet( const MetaNetDef & def , const std :: string & name)",10, 40, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor_utils.cc,"caffe2::predictor_utils::extractMetaNetDef( db :: Cursor * cursor , const std :: string & key)",23, 70, 4, 0
repos/cpp/pytorch/caffe2/predictor/predictor_utils.cc,"caffe2::predictor_utils::runGlobalInitialization( std :: unique_ptr<db::DBReader> db , Workspace * master)",33, 80, 6, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::enforceIsTensor( Workspace * ws , const std :: string & name)",6, 73, 6, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::getBlob( Workspace * ws , const std :: string & name)",6, 58, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::getTensor( Workspace * ws , const std :: string & name)",3, 66, 0, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::Predictor::Predictor( const NetDef & init_net , const NetDef & run_net , Workspace * parent , bool run_init , int optimization)",12, 37, 4, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::Predictor::Predictor( PredictorConfig config)",12, 77, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::Predictor::operator ( )( const TensorList & inputs , TensorList * outputs)",22, 77, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::Predictor::run_map_workspace( const TensorMap & inputs)",20, 80, 10, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::Predictor::operator ( )( const TensorMap & inputs , TensorList * outputs)",12, 77, 2, 0
repos/cpp/pytorch/caffe2/predictor/predictor.cc,"caffe2::Predictor::operator ( )( const TensorMap & inputs , TensorMap * outputs)",12, 74, 0, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::DataNetFiller::fill_parameter( Workspace * ws) const",8, 73, 2, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::DataNetFiller::fill_input_internal( TensorList_t * input_data) const",8, 74, 0, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::fill_with_type( const TensorFiller & filler , const std :: string & type , TensorCPU * output)",28, 73, 4, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::DataRandomFiller::DataRandomFiller( const NetDef & run_net , const std :: vector<std::vector<std::vector<int64_t>>> & input_dims , const std :: vector<std::vector<std::string>> & input_types)",87, 80, 4, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::DataRandomFiller::fill_parameter( Workspace * ws) const",10, 61, 0, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::DataRandomFiller::fill_input_internal( TensorList_t * input_data) const",8, 78, 4, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::TestDataRandomFiller::TestDataRandomFiller( const NetDef & net , const std :: vector<std::vector<std::vector<int64_t>>> & inputDims , const std :: vector<std::vector<std::string>> & inputTypes)",75, 77, 10, 0
repos/cpp/pytorch/caffe2/predictor/emulator/data_filler.cc,"caffe2::emulator::TestDataRandomFiller::fillInputToWorkspace( Workspace * workspace) const",9, 78, 0, 0
repos/cpp/pytorch/caffe2/predictor/emulator/benchmark.cc,"caffe2::emulator::BenchmarkRunner::benchmark( const BenchmarkParam & param)",26, 79, 4, 0
repos/cpp/pytorch/caffe2/observers/profile_observer.cc,"caffe2::ProfileOperatorObserver::Dump() const",43, 78, 2, 0
repos/cpp/pytorch/caffe2/observers/profile_observer.cc,"caffe2::ProfileOperatorObserver::Start()",3, 40, 0, 0
repos/cpp/pytorch/caffe2/observers/profile_observer.cc,"caffe2::ProfileOperatorObserver::Stop()",4, 51, 2, 0
repos/cpp/pytorch/caffe2/observers/profile_observer.cc,"caffe2::ProfileOperatorObserver::rnnCopy( OperatorBase * subject , int rnn_order) const",7, 78, 0, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountOperatorObserver::RunCountOperatorObserver( OperatorBase * op , RunCountNetObserver * netObserver)",6, 77, 2, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountNetObserver::debugInfo()",9, 67, 2, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountNetObserver::Start()",1, 37, 0, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountNetObserver::Stop()",1, 36, 0, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountOperatorObserver::Start()",3, 41, 0, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountOperatorObserver::Stop()",1, 41, 0, 0
repos/cpp/pytorch/caffe2/observers/runcnt_observer.cc,"caffe2::RunCountOperatorObserver::rnnCopy( OperatorBase * subject , int rnn_order) const",6, 79, 0, 0
repos/cpp/pytorch/caffe2/observers/time_observer_test.cc,"caffe2::SleepOp::Run( int)",6, 66, 4, 0
repos/cpp/pytorch/caffe2/observers/time_observer_test.cc,"caffe2::CreateNetTestHelper( Workspace * ws)",19, 57, 0, 0
repos/cpp/pytorch/caffe2/observers/time_observer_test.cc,"caffe2::TEST( TimeObserverTest , Test3Seconds)",17, 68, 2, 0
repos/cpp/pytorch/caffe2/observers/time_observer.cc,"caffe2::TimeObserver::Start()",4, 39, 2, 0
repos/cpp/pytorch/caffe2/observers/time_observer.cc,"caffe2::TimeObserver::Stop()",5, 80, 2, 0
repos/cpp/pytorch/caffe2/observers/time_observer.cc,"caffe2::TimeOperatorObserver::Start()",4, 39, 2, 0
repos/cpp/pytorch/caffe2/observers/time_observer.cc,"caffe2::TimeOperatorObserver::Stop()",6, 60, 2, 0
repos/cpp/pytorch/caffe2/observers/time_observer.cc,"caffe2::TimeOperatorObserver::rnnCopy( OperatorBase * subject , int rnn_order) const",6, 75, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/conv_op.cc,"caffe2::initNNPACK()",8, 79, 8, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/conv_op.cc,"caffe2::NNPACKConvOp::NNPACKConvOp( const OperatorDef & operator_def , Workspace * ws)",21, 73, 8, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/conv_op.cc,"caffe2::NNPACKConvOp::getConvolutionAlgorithm() const",40, 76, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/conv_op.cc,"caffe2::NNPACKConvOp::getConvolutionTransformStrategy() const",9, 59, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/conv_op.cc,"caffe2::NNPACKConvOp::getActivationType() const",11, 69, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/conv_op.cc,"caffe2::NNPACKConvOp::RunOnDeviceWithOrderNCHW()",275, 108, 12, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::AddNoiseInput( const vector<int64_t> & shape , const string & name , Workspace * ws)",17, 76, 6, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::relativeError( float a , float b)",3, 65, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::compare( int N , int inputC , int H , int W , int outputC , int kernelH , int kernelW , int strideH , int strideW , int padT , int padL , int padB , int padR , int group , const std :: string & algorithm , const std :: string & convolutionTransformStrategy , const std :: string & activation , float maxRelErr , float absErrForRelErrFailure)",150, 81, 14, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::randInt( int a , int b)",6, 56, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::runConv( int kernelH , int kernelW , int strideH , int strideW , int group = 1 , std :: string algo = '' , int planesIn = randInt(1,6) , int planesOut = randInt(1,6) , int n = randInt(1,2) , std :: string convolutionTransformStrategy = 'COMPUTE' , std :: string activation = 'identity')",41, 58, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_3x3s1)",5, 37, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_3x3s1_precompute)",16, 38, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_3x3s1_FP16)",5, 45, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_3x3s1_FP16_precompute)",16, 43, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_NxNs1)",6, 37, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_1x1s1)",9, 70, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , ConvRelu_1x1s1)",20, 46, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_1x1s1_precompute)",10, 80, 8, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_NxNs_grouped)",10, 57, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_NxNs_grouped_precompute)",10, 71, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_NxNsW)",7, 45, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , ConvRelu_NxNsW)",7, 80, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/nnpack/nnpack_test.cc,"caffe2::TEST( NNPACK , Conv_HxWsHxW)",9, 49, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::winograd_f2k3_input_transform_inplace__neon( float32x4_t * d0 , float32x4_t * d1 , float32x4_t * d2 , float32x4_t * d3)",15, 64, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::winograd_f2k3_output_transform_inplace__neon( float32x4_t * m0 , float32x4_t * m1 , float32x4_t * m2 , float32x4_t * m3)",8, 65, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::vmuladdq_f32( float32x4_t c , float32x4_t a , float32x4_t b)",7, 60, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::vmulsubq_f32( float32x4_t c , float32x4_t a , float32x4_t b)",7, 60, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::winograd_f2k3_kernel_transform__neon( const float32x4_t g0 , const float32x4_t g1 , const float32x4_t g2 , float32x4_t * transform0 , float32x4_t * transform1 , float32x4_t * transform2 , float32x4_t * transform3)",15, 63, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::v4f_transpose4x4__neon( float32x4x4_t m)",5, 70, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::runDepthwise3x3Conv( const DepthwiseArgs & args , const float * input , const float * kernel , const float * bias , float * output)",125, 77, 12, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_store_f32( void * address , psimd_f32 value)",3, 71, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_load_f32( const void * address)",3, 64, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_splat_f32( float c)",3, 53, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_interleave_lo_f32( psimd_f32 a , psimd_f32 b)",3, 78, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_interleave_hi_f32( psimd_f32 a , psimd_f32 b)",3, 78, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_concat_lo_f32( psimd_f32 a , psimd_f32 b)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_concat_hi_f32( psimd_f32 a , psimd_f32 b)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_interleave_lo_f32( psimd_f32 a , psimd_f32 b)",3, 78, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_interleave_hi_f32( psimd_f32 a , psimd_f32 b)",3, 78, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_concat_lo_f32( psimd_f32 a , psimd_f32 b)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_concat_hi_f32( psimd_f32 a , psimd_f32 b)",3, 74, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::psimd_transpose4x4_f32( const psimd_f32 row0 , const psimd_f32 row1 , const psimd_f32 row2 , const psimd_f32 row3 , psimd_f32 * col0 , psimd_f32 * col1 , psimd_f32 * col2 , psimd_f32 * col3)",18, 65, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::winograd_f2k3_input_transform( const psimd_f32 d0 , const psimd_f32 d1 , const psimd_f32 d2 , const psimd_f32 d3 , psimd_f32 * transform0 , psimd_f32 * transform1 , psimd_f32 * transform2 , psimd_f32 * transform3)",14, 50, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::winograd_f2k3_kernel_transform( const psimd_f32 g0 , const psimd_f32 g1 , const psimd_f32 g2 , psimd_f32 * transform0 , psimd_f32 * transform1 , psimd_f32 * transform2 , psimd_f32 * transform3)",15, 60, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::winograd_f2k3_output_transform( const psimd_f32 m0 , const psimd_f32 m1 , const psimd_f32 m2 , const psimd_f32 m3 , psimd_f32 * output0 , psimd_f32 * output1)",10, 51, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::runDepthwise3x3Conv( const DepthwiseArgs & args , const float * input , const float * kernel , const float * bias , float * output)",84, 80, 6, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::Depthwise3x3ConvOp::Depthwise3x3ConvOp( const OperatorDef & operator_def , Workspace * ws)",11, 69, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,"caffe2::Depthwise3x3ConvOp::RunOnDeviceWithOrderNCHW()",96, 85, 10, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,"caffe2::AddNoiseInput( const vector<int64_t> & shape , const string & name , Workspace * ws)",17, 76, 6, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,"caffe2::relativeError( float a , float b)",3, 65, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,"caffe2::compare( int N , int inputC , int H , int W , int outputC , int kernelH , int kernelW , int strideH , int strideW , int padT , int padL , int padB , int padR , int group , float maxRelErr , float absErrForRelErrFailure)",121, 81, 10, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,"caffe2::randInt( int a , int b)",6, 56, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,"caffe2::runConv( int kernelH , int kernelW , int strideH , int strideW , int group = 1 , int planesIn = randInt(1,6) , int planesOut = randInt(1,6) , int n = randInt(1,2))",35, 51, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,"caffe2::TEST( DEPTHWISE3x3 , Conv)",6, 67, 4, 0
repos/cpp/pytorch/caffe2/share/contrib/zstd/quant_decomp_zstd_op.cc,"caffe2::GetMutableData( int type_index , TensorCPU * tensor)",14, 74, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/zstd/quant_decomp_zstd_op.cc,"caffe2::GetCompressedPtr( const TensorCPU & compressed , size_t * out_size)",18, 81, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/zstd/quant_decomp_zstd_op.cc,"caffe2::GetTensorsProto( const TensorCPU & compressed)",7, 60, 0, 0
repos/cpp/pytorch/caffe2/share/contrib/zstd/quant_decomp_zstd_op.cc,"caffe2::Decompress( const TensorProto & compressed , TensorCPU * outDecomp)",16, 79, 2, 0
repos/cpp/pytorch/caffe2/share/contrib/zstd/quant_decomp_zstd_op.cc,"caffe2::QuantDecompZstdOp::RunOnDevice()",28, 71, 4, 0
repos/cpp/pytorch/torch/abi-check.cpp,"main()",7, 39, 2, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"THPException_init( PyObject * module)",6, 99, 2, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::replaceAll( std :: string & str , const std :: string & old_str , const std :: string & new_str)",8, 64, 2, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::processErrorMsg( std :: string str)",76, 74, 4, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::formatMessage( const char * format , va_list fmt_args)",10, 73, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::IndexError::IndexError( const char * format , ...)",6, 50, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::TypeError::TypeError( const char * format , ...)",6, 48, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::ValueError::ValueError( const char * format , ...)",6, 50, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_New()",12, 85, 4, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_NewWithGenerator( at :: Generator & cdata)",9, 63, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_dealloc( THPGenerator * self)",7, 53, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",15, 91, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_getState( THPGenerator * self)",10, 90, 2, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_setState( THPGenerator * self , PyObject * _new_state)",19, 109, 2, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_manualSeed( THPGenerator * self , PyObject * seed)",11, 78, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_seed( THPGenerator * self)",6, 56, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_initialSeed( THPGenerator * self)",6, 63, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_init( PyObject * module)",9, 74, 2, 0
repos/cpp/pytorch/torch/csrc/stub.cpp,"init_C()",4, 24, 0, 0
repos/cpp/pytorch/torch/csrc/stub.cpp,"PyInit__C()",4, 27, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_New( void * data , void(*destructor)(void*))",15, 68, 2, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_check( PyObject * obj)",4, 53, 2, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_get( PyObject * obj)",4, 38, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",8, 89, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_dealloc( THPWrapper * self)",5, 49, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_init( PyObject * module)",8, 48, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_getCallable( PyObject * arg , PyObject ** result)",6, 61, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackSize( PyObject * arg)",10, 75, 4, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_tryUnpackLongs( PyObject * arg , THLongStoragePtr & result)",18, 83, 6, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackLongs( PyObject * arg)",19, 93, 8, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_tryUnpackLongVarArgs( PyObject * args , int ignore_first , THLongStoragePtr & result)",22, 97, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_checkIntTuple( PyObject * arg)",12, 59, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackIntTuple( PyObject * arg)",11, 68, 4, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_setError( const char * format , ...)",11, 58, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_addPyMethodDefs( std :: vector<PyMethodDef> & vector , PyMethodDef * methods)",14, 86, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"classOrTypename( PyObject * obj)",6, 52, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_dispatchStateless( PyObject * tensor , const char * name , PyObject * args , PyObject * kwargs)",20, 86, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_invalidArguments( PyObject * given_args , PyObject * given_kwargs , const char * function_name , size_t num_options , ...)",12, 77, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THPGenerator>::free()",4, 40, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"setBackCompatBroadcastWarn( bool warn)",3, 45, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"getBackCompatBroadcastWarn()",3, 36, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"setBackCompatKeepdimWarn( bool warn)",3, 43, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"getBackCompatKeepdimWarn()",3, 34, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"maybeThrowBackCompatKeepdimWarn( char * func)",9, 112, 8, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THTensor>::free()",5, 38, 4, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THPStorage>::free()",4, 38, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_initNames( PyObject * self , PyObject * arg)",24, 73, 4, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_initExtension( PyObject * _unused , PyObject * shm_manager_path)",29, 99, 4, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfCsrcASAN( PyObject * module , PyObject * arg)",7, 81, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfCsrcUBSAN( PyObject * module , PyObject * arg)",7, 82, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfATenASAN( PyObject * module , PyObject * arg)",5, 90, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getNumThreads( PyObject * module)",4, 60, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setNumThreads( PyObject * module , PyObject * arg)",8, 78, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDefaultTensorType( PyObject * _unused , PyObject * type)",7, 77, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDefaultDtype( PyObject * _unused , PyObject * dtype)",7, 73, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_safeCall( PyObject * _unused , PyObject * args , PyObject * kwargs)",19, 82, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_addDocStr( PyObject * _unused , PyObject * args)",50, 80, 8, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_inferSize( PyObject * _unused , PyObject * args)",16, 79, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBackcompatBroadcastWarn( PyObject * module , PyObject * arg)",6, 89, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getBackcompatBroadcastWarn( PyObject * module)",5, 72, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBackcompatKeepdimWarn( PyObject * module , PyObject * arg)",6, 87, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getBackcompatKeepdimWarn( PyObject * module)",5, 70, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_hasDistributed( PyObject * _unused)",8, 54, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"DLPack_Capsule_Destructor( PyObject * data)",14, 91, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_toDLPack( PyObject * _unused , PyObject * data)",8, 75, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_fromDLPack( PyObject * _unused , PyObject * data)",27, 91, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setUserEnabledCuDNN( PyObject * _unused , PyObject * arg)",7, 74, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_userEnabledCuDNN( PyObject * _unused)",5, 62, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDeterministicCuDNN( PyObject * _unused , PyObject * arg)",7, 80, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_deterministicCuDNN( PyObject * _unused)",5, 64, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBenchmarkCuDNN( PyObject * _unused , PyObject * arg)",7, 76, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_benchmarkCuDNN( PyObject * _unused)",5, 60, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setFlushDenormal( PyObject * _unused , PyObject * arg)",8, 73, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getDefaultDtype( PyObject * _unused , PyObject * arg)",8, 72, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_isDefaultTypeCuda( PyObject * _unused , PyObject * arg)",8, 74, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THCUDNN_cudnn_version( PyObject * self , PyObject * args)",4, 72, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THCUDNN_methods()",3, 33, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"warning_handler( const c10 :: SourceLocation & source_location , const char * msg)",8, 56, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"initModule()",138, 106, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"pytorch_duplicate_guard()",8, 68, 4, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"call_duplicate_guard::call_duplicate_guard()",1, 56, 2, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_New( at :: Layout layout , const std :: string & name)",11, 68, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_repr( THPLayout * self)",4, 42, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_init( PyObject * module)",10, 79, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_New( const at :: Device & device)",9, 57, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_repr( THPDevice * self)",10, 58, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_str( THPDevice * self)",6, 49, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",32, 88, 6, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_type( THPDevice * self)",9, 49, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_index( THPDevice * self)",10, 53, 4, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_hash( THPDevice * self)",6, 114, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_rc( PyObject * a , PyObject * b , int op)",33, 60, 6, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_reduce( THPDevice * self)",24, 89, 4, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_init( PyObject * module)",10, 79, 2, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes16( void * ptr)",15, 64, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes32( void * ptr)",17, 73, 3, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes64( void * ptr)",22, 87, 2, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt16LE( const uint8_t * data)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt16BE( const uint8_t * data)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt32LE( const uint8_t * data)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt32BE( const uint8_t * data)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt64LE( const uint8_t * data)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt64BE( const uint8_t * data)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_nativeByteOrder()",5, 61, 2, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt16Buffer( int16_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",7, 94, 4, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt32Buffer( int32_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",7, 94, 4, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt64Buffer( int64_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",7, 94, 4, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeHalfBuffer( THHalf * dst , const uint8_t * src , THPByteOrder order , size_t len)",10, 91, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeBoolBuffer( bool * dst , const uint8_t * src , THPByteOrder order , size_t len)",6, 89, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeFloatBuffer( float * dst , const uint8_t * src , THPByteOrder order , size_t len)",10, 91, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeDoubleBuffer( double * dst , const uint8_t * src , THPByteOrder order , size_t len)",10, 93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt16Buffer( uint8_t * dst , const int16_t * src , THPByteOrder order , size_t len)",10, 93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt32Buffer( uint8_t * dst , const int32_t * src , THPByteOrder order , size_t len)",10, 93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt64Buffer( uint8_t * dst , const int64_t * src , THPByteOrder order , size_t len)",10, 93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeFloatBuffer( uint8_t * dst , const float * src , THPByteOrder order , size_t len)",10, 91, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeDoubleBuffer( uint8_t * dst , const double * src , THPByteOrder order , size_t len)",10, 93, 0, 0
repos/cpp/pytorch/torch/csrc/Storage.cpp,"THPPointer<THStorage>::free()",5, 37, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialRead<int>( int fildes , void * buf , size_t nbytes)",3, 67, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialRead<PyObject*>( PyObject * fildes , void * buf , size_t nbytes)",10, 79, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialWrite<int>( int fildes , void * buf , size_t nbytes)",3, 68, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialWrite<PyObject*>( PyObject * fildes , void * buf , size_t nbytes)",3, 80, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"isUnsupportedOperation()",7, 78, 2, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonReadBuffered( PyObject * fildes , void * buf , size_t raw_nbytes)",30, 100, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonIO( PyObject * fildes , void * buf , size_t nbytes , bool is_read)",26, 100, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonReadInto( PyObject * fildes , void * buf , size_t nbytes)",3, 85, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonWrite( PyObject * fildes , void * buf , size_t nbytes)",3, 82, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doRead( io fildes , void * raw_buf , size_t nbytes)",30, 96, 4, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doWrite( io fildes , void * raw_buf , size_t nbytes)",23, 83, 4, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_New( const at :: ScalarType & type)",9, 60, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_New( const at :: ScalarType & type)",9, 60, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_str( THPFInfo * self)",5, 49, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_str( THPIInfo * self)",5, 49, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",27, 96, 10, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",19, 88, 8, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_compare( THPDTypeInfo * a , THPDTypeInfo * b , int op)",17, 75, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_bits( THPDTypeInfo * self , void *)",4, 64, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_eps( THPFInfo * self , void *)",8, 68, 16, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_max( THPFInfo * self , void *)",6, 76, 8, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_min( THPFInfo * self , void *)",6, 79, 8, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_max( THPFInfo * self , void *)",5, 69, 4, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_min( THPFInfo * self , void *)",5, 72, 4, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_tiny( THPFInfo * self , void *)",6, 76, 8, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_init( PyObject * module)",16, 76, 2, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"setSignalHandler( int signal , void(*handler)(int,siginfo_t*,void*) , struct sigaction * old_sa_ptr)",11, 120, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"handler_SIGTERM( int sig , siginfo_t * info , void * ctx)",14, 80, 2, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerSignalHandlers( PyObject * module , PyObject * arg)",9, 86, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_errorIfAnyWorkerFails( PyObject * module)",43, 100, 6, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerPIDs( PyObject * module , PyObject * args)",27, 95, 4, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_removeWorkerPIDs( PyObject * module , PyObject * loader_id)",13, 94, 4, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerSignalHandlers( PyObject * module , PyObject * _ignored)",3, 91, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerPIDs( PyObject * module , PyObject * _ignored)",3, 81, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_removeWorkerPIDs( PyObject * module , PyObject * _ignored)",3, 84, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_errorIfAnyWorkerFails( PyObject * module , PyObject * _ignored)",3, 89, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_New( at :: ScalarType scalar_type , const std :: string & name)",11, 77, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_is_floating_point( THPDtype * self)",8, 53, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_reduce( THPDtype * self)",8, 75, 2, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_repr( THPDtype * self)",4, 42, 2, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_init( PyObject * module)",10, 77, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::get_backend( bool is_cuda , bool is_sparse)",15, 56, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::get_type( const std :: string & name , bool is_cuda , bool is_sparse)",7, 76, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getPyTypeObject( const at :: Storage & storage)",11, 59, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerStoragePyTypeObject( PyTypeObject * pytype , const std :: string & name , bool is_cuda , bool is_sparse)",8, 110, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerDtypeObject( THPDtype * dtype , at :: ScalarType scalarType)",3, 71, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerLayoutObject( THPLayout * layout , at :: Backend backend)",3, 68, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getVariableType( at :: ScalarType scalarType , const THPLayout & layout , const at :: Device & device)",17, 121, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getDtype( at :: ScalarType scalarType)",7, 61, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getLayout( at :: Backend backend)",7, 60, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getDeviceType( const at :: Type & type)",3, 74, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::createPyObject( const at :: Storage & storage)",8, 117, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::isStorage( PyObject * obj)",4, 56, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::createStorage( PyObject * obj)",9, 72, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_New( const torch :: autograd :: Variable & var)",17, 88, 4, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_NewFromSizes( int dim , const int64_t * sizes)",7, 69, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"isTracedZeroDimVar( PyObject * item)",5, 67, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",31, 88, 26, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_repr( THPSize * self)",14, 70, 4, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"wrap_tuple_fn( Args ... args)",9, 89, 4, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_numel( THPSize * self)",10, 67, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_init( PyObject * module)",10, 73, 2, 0
repos/cpp/pytorch/torch/csrc/onnx/init.cpp,"torch::onnx::initONNXBindings( PyObject * module)",33, 81, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_initProcessGroup( PyObject * _unused , PyObject * args)",29, 156, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_destroyProcessGroup( PyObject * _unused)",9, 62, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_registerStream( PyObject * _unused , PyObject * _stream)",10, 75, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_getRank( PyObject * _unused)",6, 48, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_getNumProcesses( PyObject * _unused)",6, 56, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_makeDescriptor( PyObject * obj)",4, 63, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_unpackRequest( PyObject * obj)",4, 56, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_getReduceOp( PyObject * obj)",9, 61, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_getGroup( PyObject * obj)",11, 78, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_clearGroupCache( PyObject * _unused , PyObject * args)",16, 84, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_isend( PyObject * _unused , PyObject * args)",19, 90, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_irecv( PyObject * _unused , PyObject * args)",19, 91, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_send( PyObject * _unused , PyObject * args)",18, 89, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_recvAnySource( PyObject * _unused , PyObject * _tensor)",17, 79, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_recv( PyObject * _unused , PyObject * args)",20, 90, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allReduceMultiGPU( PyObject * _unused , PyObject * args)",52, 78, 28, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_reduceMultiGPU( PyObject * _unused , PyObject * args)",56, 77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_broadcastMultiGPU( PyObject * _unused , PyObject * args)",53, 78, 28, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allGatherMultiGPU( PyObject * _unused , PyObject * args)",80, 81, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allReduce( PyObject * _unused , PyObject * args)",18, 106, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_reduce( PyObject * _unused , PyObject * args)",21, 86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_broadcast( PyObject * _unused , PyObject * args)",20, 86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allGather( PyObject * _unused , PyObject * args)",49, 77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_gatherSend( PyObject * _unused , PyObject * args)",19, 86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_gatherRecv( PyObject * _unused , PyObject * args)",48, 77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_scatterSend( PyObject * _unused , PyObject * args)",48, 77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_scatterRecv( PyObject * _unused , PyObject * args)",20, 86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_barrier( PyObject * _unused , PyObject * _group)",10, 66, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_newGroup( PyObject * _unused , PyObject * args)",45, 80, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_requestIsCompleted( PyObject * _unused , PyObject * _req)",11, 88, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_requestWait( PyObject * _unused , PyObject * _req)",15, 81, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_initExtension( PyObject * _unused , PyObject * args)",37, 119, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_methods()",3, 36, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::copyBroadcastTensorsToReplicas( const std :: vector<std::vector<at::Tensor>> & broadcastTensors , std :: vector<std::vector<at::Tensor>> & replicaData)",12, 80, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::bucketTensors( std :: vector<at::Tensor> & tensors , int64_t bucketSize , bool fineGrained)",14, 68, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::distBroadcastCoalesced( ProcessGroup & processGroup , std :: vector<at::Tensor> & tensors , int64_t bufferSize , bool fineGrained)",40, 81, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::syncParams( ProcessGroup & processGroup , std :: vector<std::vector<at::Tensor>> & parameterData , std :: vector<std::vector<at::Tensor>> & bufferData , const std :: vector<int64_t> & devices , int64_t broadcastBucketSize , bool broadcastBuffers)",32, 80, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::queueReduction( ProcessGroup & processGroup , std :: vector<std::vector<at::Tensor>> & gradsBatch , const std :: vector<int64_t> & devices)",45, 76, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::syncReduction( std :: shared_ptr<ProcessGroup::Work> & reductionWork , std :: vector<at::Tensor> & gradsBatch , at :: Tensor & gradsBatchCoalesced)",31, 78, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/init.cpp,"torch::distributed::c10d::c10d_init( PyObject * _unused)",447, 81, 14, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/init.cpp,"torch::distributed::c10d::python_functions()",3, 34, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"get_python_engine()",3, 37, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::thread_init( int device)",8, 74, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::thread_on_exception( FunctionTask & task , std :: exception & e)",7, 80, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::make_anomaly_metadata()",3, 73, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::execute( const edge_list & roots , const variable_list & inputs , bool keep_graph , bool create_graph , const edge_list & outputs)",13, 78, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"_maybe_reinitialize_engine_after_fork()",13, 79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_run_backward( THPEngine * self , PyObject * args , PyObject * kwargs)",100, 96, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_queue_callback( PyObject * self , PyObject * _callback)",13, 101, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_is_checkpoint_valid( PyObject * self)",9, 58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",4, 78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"child_atfork()",3, 31, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_initModule( PyObject * module)",14, 79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::cudaCheck( cudaError_t result , const char * file , int line)",7, 80, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::record( int * device , CUDAEventStub * event , int64_t * cpu_ns)",7, 77, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::elapsed( CUDAEventStub event , CUDAEventStub event2)",7, 70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::nvtxMarkA( const char * name)",3, 46, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::nvtxRangePushA( const char * name)",3, 51, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::nvtxRangePop()",3, 33, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::onEachDevice( std :: function<void(int)> op)",9, 60, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::synchronize()",3, 32, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::CUDAMethods::enabled()",3, 28, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler_cuda.cpp,"torch::autograd::profiler::RegisterCUDAMethods::RegisterCUDAMethods()",4, 35, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::registerCUDAMethods( CUDAStubs * stubs)",3, 55, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::getEventList()",9, 62, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::mark( std :: string name , bool include_cuda)",14, 62, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::c_str( const char * str)",1, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::c_str( std :: string & str)",1, 60, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::pushRangeImpl( T name , const char * msg = '' , int64_t sequence_nr = - 1)",20, 73, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::pushRange( std :: string name)",3, 35, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::popRange()",14, 42, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( Function * fn)",9, 86, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( std :: string name)",3, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( const char * name)",3, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( const char * name , int64_t current_sequence_nr)",4, 78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::enableProfiler( ProfilerState new_state)",28, 111, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::disableProfiler()",27, 78, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::Event::record( bool record_cuda)",7, 52, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::Event::cuda_elapsed_us( const Event & e)",9, 65, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordProfile::RecordProfile( std :: ostream & out)",4, 48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordProfile::RecordProfile( const std :: string & filename)",4, 58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordProfile::init()",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordProfile::~RecordProfile()",13, 54, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordProfile::processEvents( const std :: vector<Event*> & events)",33, 71, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/saved_variable.cpp,"torch::autograd::SavedVariable::SavedVariable( const Variable & variable , bool is_output)",18, 75, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/saved_variable.cpp,"torch::autograd::SavedVariable::unpack( std :: shared_ptr<Function> saved_for) const",45, 79, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/grad_mode.cpp,"torch::autograd::GradMode::is_enabled()",3, 30, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/grad_mode.cpp,"torch::autograd::GradMode::set_enabled( bool enabled)",3, 43, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp,"torch::autograd::PyAnomalyMetadata::store_stack()",16, 73, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp,"torch::autograd::PyAnomalyMetadata::print_stack()",29, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_NewWithVar( PyTypeObject * type , Variable var)",17, 80, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_Wrap( Variable var)",13, 83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_traverse( THPVariable * self , visitproc visit , void * arg)",26, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_clear( THPVariable * self)",12, 66, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_dealloc( THPVariable * self)",7, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",9, 89, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_make_subclass( PyObject * _ignored , PyObject * args , PyObject * kwargs)",16, 99, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_cdata( THPVariable * self)",7, 63, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_version( THPVariable * self)",7, 53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_grad_fn( THPVariable * self)",10, 53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_grad_fn( THPVariable * self , PyObject * obj)",9, 91, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_leaf( THPVariable * self)",6, 56, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_data( THPVariable * self)",15, 113, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_data( THPVariable * self , PyObject * data)",12, 100, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_grad( THPVariable * self)",6, 50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_grad( THPVariable * self , PyObject * py_grad)",37, 107, 7, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_volatile( THPVariable * self)",6, 80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_volatile( THPVariable * self , PyObject * obj)",4, 63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_output_nr( THPVariable * self)",7, 69, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_requires_grad( THPVariable * self)",6, 59, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_requires_grad( THPVariable * self , PyObject * obj)",18, 90, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_name( THPVariable * self)",6, 58, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_backwards_hooks( THPVariable * self)",10, 61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_backwards_hooks( THPVariable * self , PyObject * obj)",17, 76, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_base( THPVariable * self)",9, 50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_shape( THPVariable * self)",6, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_cuda( THPVariable * self)",7, 56, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_sparse( THPVariable * self)",7, 58, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_dtype( THPVariable * self)",7, 77, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_layout( THPVariable * self)",6, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_device( THPVariable * self)",5, 58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"torch::autograd::initTensorImplConversion( PyObject * module)",19, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_initModule( PyObject * module)",14, 77, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::peek_at_next_sequence_nr()",3, 48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::get_next_sequence_nr()",3, 45, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::name() const",3, 46, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::metadata()",6, 78, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::gatherFunctions( Function * func , std :: vector<std::shared_ptr<Function>> & stack)",13, 53, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::deleteFunction( Function * function)",16, 60, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::add( size_t pos , Variable var)",28, 130, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::device() const",8, 42, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::variables( InputBuffer && g)",4, 72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::VariableType( Context * context , TypeExtendedInterface * baseType)",6, 83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::scalarType() const",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::typeMeta() const",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::backend() const",3, 40, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allocator() const",3, 45, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::getDeviceFromPtr( void * data) const",3, 59, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",3, 117, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unsafeStorageFromTH( void * th_pointer , bool retain) const",3, 82, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::storageWithAllocator( int64_t size , Allocator * allocator) const",3, 87, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unsafeTensorFromTH( void * th_pointer , bool retain) const",3, 99, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::generator() const",3, 61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toString() const",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::elementSizeInBytes() const",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toBackend( Backend b) const",3, 63, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toScalarType( ScalarType s) const",3, 66, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::ID() const",3, 35, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::register_variable_type_for( TypeExtendedInterface * baseType)",9, 67, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableTypeRegistry::VariableTypeRegistry()",11, 108, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::VariableHooks( at :: VariableHooksArgs)",1, 42, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::registerVariableTypeFor( at :: LegacyTypeDispatch * context , at :: Backend backend , at :: ScalarType scalar_type) const",4, 134, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::getVariableTypeFromBaseType( const at :: Type & baseType) const",3, 87, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::isVariableType( const at :: Type & type)",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::getVariableTypeFromBaseType( const at :: Type & baseType)",6, 97, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::allTypesForBackends( at :: ArrayRef<at::Backend> backends)",14, 106, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allCPUTypes()",3, 68, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allCUDATypes()",4, 70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::checked_cast_variable( const Tensor & t , const char * name , int pos)",9, 128, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::checked_cast_variable( Tensor & t , const char * name , int pos)",9, 128, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( const Tensor & t , const char * name , int pos)",3, 84, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( Tensor & t , const char * name , int pos)",3, 72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( SparseTensorRef t , const char * name , int pos)",3, 86, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack_opt( const Tensor & t , const char * name , int pos)",6, 80, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( at :: TensorList tl , const char * name , int pos)",15, 113, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::backward( Tensor & self , c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",7, 70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::set_data( Tensor & self , Tensor new_data) const",3, 68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::s_copy_( Tensor & self , const Tensor & src , bool non_blocking) const",47, 147, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::_s_copy_from( const Tensor & self , const Tensor & dst , bool non_blocking) const",3, 102, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::resize_( Tensor & self , IntArrayRef size) const",16, 72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::resize_as_( Tensor & self , const Tensor & the_template) const",16, 86, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::detach( const Tensor & self) const",19, 118, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::detach_( Tensor & self) const",19, 64, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::Impl( at :: Tensor data , std :: unique_ptr<Variable::AutogradMeta> autograd_meta , bool requires_grad , Edge gradient_edge)",20, 133, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::numel() const",3, 40, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::sizes() const",3, 44, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::strides() const",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::is_contiguous() const",3, 45, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::dim() const",3, 38, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::size( int64_t d) const",3, 48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::stride( int64_t d) const",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::resize_dim( int64_t ndim)",3, 54, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_size( int64_t dim , int64_t new_size)",3, 63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_stride( int64_t dim , int64_t new_stride)",3, 67, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_storage_offset( int64_t storage_offset)",3, 66, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::slow_data() const",3, 51, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::has_storage() const",3, 43, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::storage() const",3, 53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::storage_offset() const",3, 49, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::get_device_slow() const",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::grad_accumulator() const",22, 127, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::detach_()",9, 67, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::backward( c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",15, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_data( Tensor new_data)",22, 90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::release_resources()",4, 43, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::DifferentiableViewImpl( Variable base , at :: Tensor data , Edge gradient_edge , std :: unique_ptr<Variable::DifferentiableViewMeta> autograd_meta)",12, 174, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::grad_fn() const",28, 95, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::release_resources()",5, 93, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::rebase_history( Edge gradient_edge)",18, 95, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::VariableInfo::VariableInfo( const Variable & var)",6, 48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::VariableInfo::zeros( at :: OptionalDeviceGuard & device_guard) const",5, 76, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::legacy_apply( const variable_list & inputs)",43, 82, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::apply( variable_list && inputs)",94, 99, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::is_traceable()",8, 90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::release_variables()",6, 47, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::name() const",10, 67, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::get_shared_ptr()",3, 65, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_traverse( THPFunction * self , visitproc visit , void * arg)",17, 79, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_clear( THPFunction * self)",25, 80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_dealloc( THPFunction * self)",11, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",14, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_mark_dirty( THPFunction * self)",24, 75, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_wrap_outputs( THPFunction * self , PyObject * inputs_tuple , PyObject * raw_output , PyObject * outputs , bool is_executable)",95, 112, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_save_variables( THPFunction * self)",29, 78, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_parse_non_differentiable( THPFunction * self)",19, 78, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"unpack_input( PyObject * args)",36, 99, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_assert_not_tracing( const char * name , const variable_list & input_vars)",8, 85, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trace_pre_record( PyObject * op_obj , PyObject * input_objects , const variable_list & input_vars)",30, 72, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trace_post_record( Node * node , PyObject * op_obj , const variable_list & input_vars , PyObject * output_objects , bool is_inplace , bool unpack_output)",34, 81, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"process_outputs( PyObject * op_obj , THPFunction * grad_fn , const UnpackedInput & unpacked , PyObject * inputs , THPObjectPtr && raw_output , bool is_executable , Node * node)",43, 97, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_do_forward( THPFunction * self , PyObject * _inputs)",30, 90, 51, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_apply( PyObject * cls , PyObject * inputs)",50, 90, 51, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_prepare_grads( THPFunction * self , THPObjectPtr & raw_grads , bool is_grad_output)",31, 92, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trim_grad_input( THPFunction * self , THPObjectPtr & grad_input)",19, 74, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_do_backward( THPFunction * self , PyObject * args)",50, 84, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction__register_hook_dict( THPFunction * self , PyObject * _var)",9, 87, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_register_hook( THPFunction * self , PyObject * hook)",4, 71, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"unpack_saved_variables( THPFunction * self , const std :: function<PyObject*(const Variable&)> & unpack_fn)",27, 65, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_saved_tensors( THPFunction * self , void * _unused)",8, 70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_saved_variables( THPFunction * self , void * _unused)",11, 72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_next_functions( THPFunction * self , void * _unused)",18, 76, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_metadata( THPFunction * self , void * _unused)",7, 83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getObject( PyObject * obj , void * _unused)",9, 52, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"setObject( PyObject * obj , PyObject * value , void * _unused)",10, 63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getMember( PyObject * obj , void * _unused)",4, 52, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getImplMember( PyObject * obj , void * _unused)",4, 56, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getRequiresGrad( PyObject * obj , void * _unused)",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_initModule( PyObject * module)",8, 77, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"Decref::operator ( )( PyFunction * p) const",4, 41, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_asFunction( THPFunction * self)",9, 70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_legacy_variable.cpp,"torch::autograd::THPVariable_pynew( PyTypeObject * type , PyObject * args , PyObject * kwds)",65, 100, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_legacy_variable.cpp,"torch::autograd::init_legacy_variable( PyObject * module)",10, 68, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::FunctionTask::FunctionTask( GraphTask * base , std :: shared_ptr<Function> fn , InputBuffer inputs)",4, 82, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::CompareFunctionTaskTime::operator ( )( FunctionTask const & t1 , FunctionTask const & t2)",9, 70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::ExecInfo::Capture::Capture( int input_idx , int output_idx)",1, 95, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::ExecInfo::should_execute() const",3, 34, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::can_checkpoint()",3, 30, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::GraphTask( bool keep_graph , bool grad_mode)",6, 45, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ReadyQueue::push( FunctionTask item)",8, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ReadyQueue::pop()",7, 76, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_init( int device)",19, 68, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_main( GraphTask * graph_task)",43, 90, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_on_exception( FunctionTask & task , std :: exception & e)",10, 82, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_pre_hooks( Function & fn , variable_list inputs)",6, 74, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_post_hooks( Function & fn , variable_list outputs , const variable_list & inputs)",6, 105, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::is_compatible_type( const at :: Type & expected , const at :: Type & actual)",6, 83, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::validate_outputs( const edge_list & edges , variable_list & grads , const F & format_error)",45, 100, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_function( FunctionTask & task)",45, 102, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::evaluate_function( FunctionTask & task)",87, 97, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::compute_dependencies( Function * root , GraphTask & task)",19, 77, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::ClearCallbacks( std :: vector<std::function<void()>> & callbacks , std :: mutex & callbacks_lock)",4, 64, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::~ClearCallbacks()",1, 33, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::clear()",4, 54, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::execute( const edge_list & roots , const variable_list & inputs , bool keep_graph , bool create_graph , const edge_list & outputs)",65, 91, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::get_base_engine()",4, 35, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::set_default_engine_stub( EngineStub stub)",3, 48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::get_default_engine()",3, 39, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::queue_callback( std :: function<void()> callback)",4, 62, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::is_checkpoint_valid()",3, 37, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::ready_queue( int device)",3, 54, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::start_threads()",12, 72, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::init_to_execute( Function & graph_root , const edge_list & outputs)",63, 94, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_length( PyObject * self)",9, 61, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::count_specified_dimensions( PyObject * index)",19, 142, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::invalid_index( PyObject * obj)",5, 76, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySlice( const Variable & self , int64_t dim , PyObject * slice , bool ensure_view = false)",21, 105, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySelect( const Variable & self , int64_t dim , int64_t index , int64_t real_dim = 0)",16, 100, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::sequenceToVariable( const at :: Type & type , PyObject * seq)",4, 79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::valueToTensor( const at :: Type & type , PyObject * value)",12, 90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::boolToIndexingTensor( const Variable & self , bool value)",8, 95, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySlicing( const Variable & self , PyObject * index , variable_list & outIndices)",58, 114, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::typeConvertIndices( const Variable & self , const variable_list & indices)",12, 100, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::dispatch_index( const Variable & self , const variable_list & indices)",6, 85, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::dispatch_index_put_( Variable & self , const variable_list & indices , const Variable & value)",6, 107, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::treatSequenceAsTuple( PyObject * index)",39, 99, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::wrapTuple( PyObject * index)",10, 84, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_getitem( PyObject * self , PyObject * index)",34, 72, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::slicePrefix1sSize( IntArrayRef sizes)",11, 58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::copy_to( Variable dst , const Variable & src)",6, 80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_setitem( PyObject * self , PyObject * index , PyObject * py_value)",50, 103, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_call( PyObject * self , PyObject * args , PyObject * kwargs)",44, 82, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_traverse( PyObject * self , visitproc visit , void * arg)",15, 72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_clear( PyObject * self)",10, 55, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_dealloc( PyObject * self)",6, 48, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_next_functions( THPCppFunction * self , PyObject * hook)",19, 78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_metadata( THPCppFunction * self , void * _unused)",7, 84, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_requires_grad( THPCppFunction * self)",3, 63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_register_hook_dict( PyObject * self , PyObject * _var)",12, 85, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_register_hook( PyObject * self , PyObject * hook)",5, 71, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_name( PyObject * self)",4, 48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::_initFunctionPyTypeObject( PyTypeObject & type , const char * name , PyGetSetDef * function_properties , PyMethodDef * function_methods)",18, 83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::DefaultFunctionType::DefaultFunctionType()",4, 70, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::functionToPyObject( const std :: shared_ptr<Function> & cdata)",37, 69, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::registerCppFunction( const std :: type_info & type , PyTypeObject * pytype)",5, 79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::registerFunctionHook( Function & fn , PyObject * hook)",25, 90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::PyFunctionPreHook( PyObject * dict , int value_idx)",6, 68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::~PyFunctionPreHook()",4, 42, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::operator ( )( const variable_list & values)",21, 81, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::PyFunctionPostHook( PyObject * dict)",3, 70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::~PyFunctionPostHook()",4, 44, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::operator ( )( const variable_list & _outputs , const variable_list & _inputs)",22, 70, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"wrap_variables( const variable_list & c_variables)",12, 66, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"unwrap_variables( PyObject * py_variables)",17, 68, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"check_result( PyObject * prev , PyObject * result , PyObject * hook)",22, 87, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"check_single_result( PyObject * _original , PyObject * _result , PyObject * hook)",44, 90, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"hook_name( PyObject * hook)",7, 63, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"THPAutograd_initExtension( PyObject * _unused)",45, 81, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::set_grad_enabled( PyObject * _unused , PyObject * arg)",9, 79, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::is_grad_enabled( PyObject * _unused , PyObject * arg)",9, 70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::set_anomaly_mode_enabled( PyObject * _unused , PyObject * arg)",9, 79, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::is_anomaly_mode_enabled( PyObject * _unused , PyObject * arg)",9, 78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::python_functions()",3, 34, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/utils.cpp,"torch::autograd::wrap_outputs( const variable_list & inputs , tensor_list && outputs , const function_constructor & ctr)",27, 82, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/utils.cpp,"torch::autograd::check_input_variables( const char * name , const variable_list & inputs , int args , int required_args)",18, 105, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/basic_ops.cpp,"torch::autograd::Error::apply( variable_list && inputs)",3, 61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/basic_ops.cpp,"torch::autograd::DelayedError::apply( variable_list && inputs)",11, 80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp,"torch::autograd::AccumulateGrad::AccumulateGrad( Variable variable_)",5, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp,"torch::autograd::AccumulateGrad::apply( variable_list && grads)",52, 90, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Scatter::Scatter( std :: vector<at::Device> devices , const c10 :: optional<std::vector<int64_t>> & chunk_sizes , int64_t dim , const c10 :: optional<std::vector<c10::optional<at::cuda::CUDAStream>>> & streams , bool unsqueeze_scalars)",11, 84, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Scatter::apply( variable_list && inputs)",33, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Gather::Gather( const at :: Device & destination_device , int64_t dim)",2, 66, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Gather::apply( variable_list && inputs)",54, 73, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"DelayedErrorCtor::operator ( )( PyObject * args)",10, 46, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"NoCtor::operator ( )( PyObject * args)",3, 50, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"addClass( PyObject * module , PyTypeObject & type , const char * name , PyGetSetDef * function_properties = nullptr , PyMethodDef * function_methods = nullptr)",8, 91, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"getTupleAttr( PyObject * obj , void * _unused)",14, 58, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"getValueAttr( PyObject * obj , void * _unused)",8, 53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"accumulateGradVar( PyObject * _self , void * _unused)",6, 67, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"THPAutograd_initFunctions()",33, 111, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopyBackwards::apply( variable_list && grads)",19, 68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::CopySlices( const Variable & base_var , at :: TensorGeometry view_ , std :: shared_ptr<Function> fn_)",18, 73, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::apply( variable_list && inputs)",34, 81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::release_variables()",3, 39, 0, 0
repos/cpp/pytorch/torch/csrc/multiprocessing/init.cpp,"torch::multiprocessing::multiprocessing_init( PyObject * _unused)",18, 71, 2, 0
repos/cpp/pytorch/torch/csrc/multiprocessing/init.cpp,"torch::multiprocessing::python_functions()",3, 34, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::throw_nccl_error( ncclResult_t status)",5, 72, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::NcclCommList( const std :: vector<int> & devices)",4, 78, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::~NcclCommList()",24, 75, 5, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::ref() const",3, 56, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_get_communicators( TensorList inputs)",10, 74, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_get_data_type( const Tensor & t)",23, 59, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_check_inputs( TensorList inputs , TensorList outputs , int input_multiplier , int output_multiplier)",66, 98, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::is_available( TensorList tensors)",19, 45, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::version()",9, 60, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::get_max_count()",3, 25, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::broadcast( TensorList tensors , const stream_list & streams , const comm_list & user_comms)",39, 76, 40, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::reduce( const std :: vector<at::Tensor> & inputs , std :: vector<at::Tensor> & outputs , int32_t root , int32_t op , const stream_list & streams , const comm_list & user_comms)",46, 88, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::reduce( std :: vector<at::Tensor> & inputs , int32_t root , int32_t op , const stream_list & streams , const comm_list & user_comms)",8, 69, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/utils.cpp,"THPUtils_PySequence_to_CUDAStreamList( PyObject * obj)",25, 108, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setDevice( int device)",4, 38, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setDevice_wrap( PyObject * self , PyObject * arg)",11, 77, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDevice_wrap( PyObject * self)",8, 53, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDeviceCount_wrap( PyObject * self)",11, 58, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCurrentStream_wrap( PyObject * , PyObject * device_index)",10, 79, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDefaultStream_wrap( PyObject * , PyObject * device_index)",10, 79, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setStream_wrap( PyObject * self , PyObject * obj)",18, 68, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_isDriverSufficient( PyObject * self)",9, 57, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDriverVersion( PyObject * self)",12, 65, 20, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCompiledVersion( PyObject * self)",4, 57, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getRNGState( PyObject * _unused)",10, 83, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setRNGState( PyObject * _unused , PyObject * obj)",13, 157, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_manualSeed( PyObject * _unused , PyObject * seed)",9, 76, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_manualSeedAll( PyObject * _unused , PyObject * seed)",9, 76, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_seed( PyObject * _unused)",6, 53, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_seedAll( PyObject * _unused)",6, 56, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_initialSeed( PyObject * _unused)",6, 60, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaHostAllocator( PyObject * _unused)",7, 68, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaSynchronize( PyObject * _unused)",7, 57, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaSleep( PyObject * _unused , PyObject * cycles)",8, 86, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaLockMutex( PyObject * module)",20, 79, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaUnlockMutex( PyObject * module)",7, 64, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_emptyCache( PyObject * _unused)",7, 52, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_memoryAllocated( PyObject * _unused , PyObject * arg)",9, 91, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_maxMemoryAllocated( PyObject * _unused , PyObject * arg)",9, 91, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_resetMaxMemoryAllocated( PyObject * _unused , PyObject * arg)",9, 94, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_memoryCached( PyObject * _unused , PyObject * arg)",9, 85, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_maxMemoryCached( PyObject * _unused , PyObject * arg)",9, 85, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_resetMaxMemoryCached( PyObject * _unused , PyObject * arg)",9, 91, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"bindCudaDeviceProperties( PyObject * module)",22, 101, 13, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_initExtension( PyObject * self)",45, 77, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_useNccl()",6, 52, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCurrentBlasHandle_wrap( PyObject * self)",7, 64, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_methods()",3, 36, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"torch::cuda::initModule( PyObject * module)",3, 36, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_version( PyObject * self , PyObject * args)",3, 68, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_unique_id( PyObject * self , PyObject * args)",7, 70, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_nccl_comm( PyObject * capsule)",7, 68, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"destroy_nccl_comm( PyObject * capsule)",16, 73, 3, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_streams( PyObject * obj , size_t size)",11, 101, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_comms( PyObject * obj , size_t size)",23, 77, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_init_rank( PyObject * self , PyObject * args)",25, 75, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_reduce( PyObject * self , PyObject * args)",36, 102, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_all_reduce( PyObject * self , PyObject * args)",55, 90, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_broadcast( PyObject * self , PyObject * args)",26, 78, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_all_gather( PyObject * self , PyObject * args)",61, 90, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_reduce_scatter( PyObject * self , PyObject * args)",53, 90, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"extract_tensors( PyObject * obj)",18, 77, 10, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",34, 59, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_dealloc( THCPStream * self)",4, 51, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_get_device( THCPStream * self)",5, 60, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_get_cuda_stream( THCPStream * self)",5, 65, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_get_priority( THCPStream * self)",5, 62, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_priority_range()",8, 67, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_query( THCPStream * self)",5, 55, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_synchronize( THCPStream * self)",6, 61, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_eq( THCPStream * self , THCPStream * other)",5, 71, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_init( PyObject * module)",12, 69, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"warp_size()",3, 34, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"torch::cuda::unique_type_checker::show( const at :: Type & t)",5, 33, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"torch::cuda::broadcast( const Tensor & tensor , IntArrayRef devices)",208, 102, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",30, 66, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_from_ipc_handle( PyTypeObject * type , PyObject * args , PyObject * kwargs)",32, 72, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_dealloc( THCPEvent * self)",4, 49, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_get_cuda_event( THCPEvent * self)",5, 62, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_get_device( THCPEvent * self)",9, 63, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_record( THCPEvent * self , THCPStream * stream)",6, 74, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_wait( THCPEvent * self , THCPStream * stream)",6, 72, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_query( THCPEvent * self)",5, 53, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_elapsed_time( THCPEvent * self , THCPEvent * other)",5, 79, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_synchronize( THCPEvent * self)",6, 59, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_ipc_handle( THCPEvent * self)",7, 75, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Event.cpp,"THCPEvent_init( PyObject * module)",11, 67, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/python_comm.cpp,"torch::cuda::python::initCommMethods( PyObject * module)",52, 85, 12, 0
repos/cpp/pytorch/torch/csrc/utils/object_ptr.cpp,"THPPointer<PyObject>::free()",4, 36, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_layouts.cpp,"torch::utils::initializeLayouts()",23, 90, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::StridedData::StridedData( const Tensor & tensor)",4, 57, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::StridedData::step( int dim)",3, 55, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::recursive_apply( IntArrayRef sizes , ScalarType scalarType , int64_t dim , PyObject * fn , std :: array<StridedData,N> strided_data)",25, 85, 28, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::apply_( Tensor & self , PyObject * fn)",8, 67, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::map_( Tensor & self , const Tensor & other_ , PyObject * fn)",14, 74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::map2_( Tensor & self , const Tensor & x_ , const Tensor & y_ , PyObject * fn)",18, 125, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::take_tensors( TensorList tensors , size_t size_limit , bool fine_grained)",55, 76, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::reorder_tensors_like( std :: vector<Tensor> & tensors , TensorList order)",17, 76, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::get_indices( const at :: Tensor & t)",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::get_values( const at :: Tensor & t)",3, 45, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::flatten_sparse_tensors( at :: TensorList tensors)",5, 83, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::unflatten_sparse_tensors( const at :: Tensor & flat_indices , const at :: Tensor & flat_values , at :: TensorList tensors)",17, 85, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::maybe_initialize_cuda( const Type & type)",5, 47, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::maybe_initialize_cuda( const Device device)",5, 50, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_zeros( const Type & type , optional<Device> device , IntArrayRef sizes)",5, 86, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_ones( const Type & type , optional<Device> device , IntArrayRef sizes)",5, 85, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_full( const Type & type , Scalar fill_value , optional<Device> device , IntArrayRef sizes)",5, 104, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_sizes( const Type & type , optional<Device> device , IntArrayRef sizes)",5, 86, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_storage( const Type & type , Storage storage)",5, 61, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_tensor( const Type & type , const Tensor & other)",6, 87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::compute_sizes( PyObject * seq)",20, 100, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::infer_scalar_type( PyObject * obj)",51, 98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::recursive_store( char * data , IntArrayRef sizes , IntArrayRef strides , int64_t dim , ScalarType scalarType , int elementSize , PyObject * obj)",23, 87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::internal_new_from_data( const Type & type , c10 :: optional<Device> device_opt , PyObject * data , bool copy_variables , bool copy_numpy , bool type_inference)",47, 131, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_from_data_copy( const Type & type , c10 :: optional<Device> device , PyObject * data)",6, 83, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_new_from_sequence( const Type & type , c10 :: optional<Device> device , PyObject * data)",9, 88, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::check_legacy_ctor_device( const Type & type , c10 :: optional<Device> device)",8, 80, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_sparse_tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",40, 98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_sparse_tensor_new( const Type & type , PyObject * args , PyObject * kwargs)",45, 101, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::typeWithDefault( PythonArgs & r , int64_t dtype_idx , int64_t device_idx , const Type & type)",6, 102, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",45, 98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_tensor_new( const Type & type , PyObject * args , PyObject * kwargs)",45, 98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::indexing_tensor_from_data( const Type & type , c10 :: optional<Device> device , PyObject * data)",14, 91, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::sparse_coo_tensor_ctor( const Type & default_type , PyObject * args , PyObject * kwargs)",35, 153, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",30, 110, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::as_tensor( const Type & type , PyObject * args , PyObject * kwargs)",15, 107, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_tensor( const Type & type , PyObject * args , PyObject * kwargs)",26, 115, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_empty( const Type & type , PyObject * args , PyObject * kwargs)",13, 108, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_full( const Type & type , PyObject * args , PyObject * kwargs)",13, 126, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_ones( const Type & type , PyObject * args , PyObject * kwargs)",13, 107, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_zeros( const Type & type , PyObject * args , PyObject * kwargs)",13, 108, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_to_numpy( const at :: Tensor & tensor)",3, 74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_from_numpy( PyObject * obj)",3, 74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::is_numpy_scalar( PyObject * obj)",3, 74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::to_numpy_shape( IntArrayRef x)",9, 61, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::to_aten_shape( int ndim , npy_intp * values)",8, 72, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_to_numpy( const at :: Tensor & tensor)",49, 93, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_from_numpy( PyObject * obj)",44, 82, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::aten_to_dtype( const ScalarType scalar_type)",14, 78, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::numpy_dtype_to_aten( int dtype)",24, 85, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::is_numpy_scalar( PyObject * obj)",4, 42, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_list.cpp,"torch::utils::recursive_to_list( char * data , IntArrayRef sizes , IntArrayRef strides , int64_t dim , ScalarType scalarType , int64_t elementSize)",19, 95, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_list.cpp,"torch::utils::tensor_to_list( const Tensor & tensor)",11, 63, 6, 0
repos/cpp/pytorch/torch/csrc/utils/structseq.cpp,"torch::utils::structseq_slice( PyStructSequence * obj , Py_ssize_t low , Py_ssize_t high)",25, 82, 0, 0
repos/cpp/pytorch/torch/csrc/utils/structseq.cpp,"torch::utils::returned_structseq_repr( PyStructSequence * obj)",47, 93, 12, 0
repos/cpp/pytorch/torch/csrc/utils/cuda_lazy_init.cpp,"torch::utils::cuda_lazy_init()",15, 82, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::TupleParser( PyObject * args , int num_args)",8, 78, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( bool & x , const std :: string & param_name)",7, 74, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( int & x , const std :: string & param_name)",7, 73, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( double & x , const std :: string & param_name)",7, 76, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( std :: vector<int> & x , const std :: string & param_name)",15, 86, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( std :: string & x , const std :: string & param_name)",7, 81, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::next_arg()",6, 46, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::invalid_type( const std :: string & expected , const std :: string & param_name)",27, 115, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::py_typename( PyObject * object)",3, 44, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SimpleType::SimpleType( std :: string & name)",1, 48, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SimpleType::is_matching( PyObject * object)",3, 48, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::MultiType::MultiType( std :: initializer_list<std::string> accepted_types)",2, 64, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::MultiType::is_matching( PyObject * object)",4, 74, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::NullableType::NullableType( std :: unique_ptr<Type> type)",1, 70, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::NullableType::is_matching( PyObject * object)",3, 59, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::TupleType::TupleType( std :: vector<std::unique_ptr<Type>> types)",2, 55, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::TupleType::is_matching( PyObject * object)",10, 63, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SequenceType::SequenceType( std :: unique_ptr<Type> type)",2, 44, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SequenceType::is_matching( PyObject * object)",9, 61, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Argument::Argument( std :: string name , std :: unique_ptr<Type> type)",2, 58, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( std :: vector<Argument> arguments , bool is_variadic , bool has_out)",2, 86, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( bool is_variadic , bool has_out)",2, 66, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( Option && other)",3, 75, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_splitString( const std :: string & s , const std :: string & delim)",11, 88, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_buildType( std :: string type_name , bool is_nullable)",24, 79, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_parseOption( const std :: string & _option_str , const std :: unordered_map<std::string,PyObject*> & kwargs)",63, 78, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_argcountMatch( const Option & option , const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",13, 62, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_formattedArgDesc( const Option & option , const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",51, 83, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_argDesc( const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",13, 68, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_tryMatchKwargs( const Option & option , const std :: unordered_map<std::string,PyObject*> & kwargs)",21, 73, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::format_invalid_args( PyObject * given_args , PyObject * given_kwargs , const std :: string & function_name , const std :: vector<std::string> & options)",80, 96, 10, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_dtypes.cpp,"torch::utils::getDtypeNames( at :: ScalarType scalarType)",34, 86, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_dtypes.cpp,"torch::utils::initializeDtypes()",28, 95, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::backend_to_string( const at :: Type & type)",9, 65, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::type_to_string( const at :: Type & type)",5, 83, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::type_from_string( const std :: string & str)",36, 103, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::all_declared_types()",18, 151, 6, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::should_allow_numbers_as_tensors( const std :: string & name)",9, 71, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::FunctionParameter( const std :: string & fmt , bool keyword_only)",49, 87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::check( PyObject * obj)",50, 96, 6, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::type_name() const",19, 65, 4, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::parse_as_integer( const std :: string & s)",8, 78, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::parse_intlist_args( const std :: string & s , int64_t size)",24, 106, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::set_default_str( const std :: string & str)",53, 86, 6, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::FunctionSignature( const std :: string & fmt)",65, 73, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::toString() const",14, 50, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::extra_args( const FunctionSignature & signature , ssize_t nargs)",12, 87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::missing_args( const FunctionSignature & signature , int idx)",21, 72, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::find_param( FunctionSignature & signature , PyObject * name)",13, 74, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::extra_kwargs( FunctionSignature & signature , PyObject * kwargs , ssize_t num_pos_args)",24, 97, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::parse( PyObject * args , PyObject * kwargs , PyObject * dst [ ] , bool raise_exception)",92, 87, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::PythonArgParser( std :: vector<std::string> fmts , bool traceable)",16, 80, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::raw_parse( PyObject * args , PyObject * kwargs , PyObject * parsed_args [ ])",17, 99, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::print_error( PyObject * args , PyObject * kwargs , PyObject * parsed_args [ ])",26, 97, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::checkListInputType( const c10 :: TypePtr & elem_type , const Node * node)",18, 97, 6, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::scalarTypeFromJitType( const c10 :: TypePtr & type)",11, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::list_size( const IValue & list)",12, 47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::compute_sizes( const IValue & seq)",15, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::checkSequenceSize( int64_t n , int64_t dim , int64_t seq_size)",5, 91, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::storeLastDimension( char * data , const std :: vector<int64_t> & sizes , const c10 :: ArrayRef<int64_t> & strides , int64_t dim , int elementSize , const std :: vector<DTYPE> & obj)",10, 123, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::storeLastDimension<bool>( char * data , const std :: vector<int64_t> & sizes , const c10 :: ArrayRef<int64_t> & strides , int64_t dim , int elementSize , const std :: vector<bool> & obj)",10, 129, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_special_ops.cpp,"torch::jit::recursiveStore( char * data , const std :: vector<int64_t> & sizes , const c10 :: ArrayRef<int64_t> & strides , int64_t dim , int elementSize , const IValue & obj)",24, 119, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::createTripCountConjunctiveCondition( Graph * g , Value * cur_trip_count , Value * max_trip_count , Value * cond)",20, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::desugarTripCounts( Block * b)",53, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::flattenIO( Graph & graph)",14, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::dropUnused( Block * b)",23, 68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::findLastUses( Graph & g)",109, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::PreprocessGraph::PreprocessGraph( Graph & g)",9, 67, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::ContainerTensor()",6, 40, 12, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::sizes() const",3, 60, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::strides() const",3, 62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::dim() const",3, 58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::storage() const",3, 62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::relativeJump( int from_inst , int to_inst)",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::CodeImpl( const std :: shared_ptr<Graph> & graph_)",4, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJumpFalse( int from_inst , int to_inst)",10, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJumpTrue( int from_inst , int to_inst)",10, 53, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJump( int from_inst , int to_inst)",7, 58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertNodesFromBlock( Block * block)",100, 81, 14, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertInstruction( Node * n)",10, 51, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertInstruction( Symbol sym , std :: shared_ptr<SourceLocation> debug_location , ArrayRef<Value*> inputs , ArrayRef<uint8_t> move_flags , ArrayRef<Value*> outputs)",24, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::moveFlags( Node * n)",3, 41, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::moveFlags( Block * b)",3, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertAssign( std :: shared_ptr<SourceLocation> debug_location , ArrayRef<Value*> inputs , ArrayRef<uint8_t> move_flags , ArrayRef<Value*> outputs)",14, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::get( const ListHandle<int> & list , int i) const",3, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::get( const ListHandle<bool> & list , int i) const",3, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listBegin( ListHandle<int> & list)",4, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listInsert( ListHandle<int> & list , int value)",7, 56, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listBegin( ListHandle<bool> & list)",4, 43, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listInsert( ListHandle<bool> & list , int value)",7, 57, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::aliasRegistersTo( ArrayRef<Value*> new_allocations , ArrayRef<Value*> existing_allocations)",11, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::getOrAllocateRegister( Value * n , bool required = false)",9, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::grad_executors()",11, 71, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::dumpInstruction( std :: ostream & out , size_t pc) const",25, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::dump( std :: ostream & out) const",6, 55, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::InterpreterStateImpl( const Code & code)",5, 46, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::intrusive_from_this()",4, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::runImpl( Stack & stack)",70, 81, 10, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::handleError( std :: string && error_msg , bool is_jit_exception)",9, 72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::getOrCreateFuture()",6, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::runAsync( Stack & stack)",5, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::run( Stack & stack)",15, 57, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::get( const ListHandle<int> & list , int i)",3, 48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::get( const ListHandle<bool> & list , int i)",3, 50, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::loadTensorsFromRegisters( const UseList & uses , Stack & stack)",11, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::operator < <( std :: ostream & out , const Code & code)",5, 64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::Code::Code( const std :: shared_ptr<Graph> & graph)",1, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::Code::grad_executors()",3, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::InterpreterState( const Code & code)",2, 64, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::run( Stack & stack)",3, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::runAsync( Stack & stack)",3, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::getFuture()",3, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::InterpreterState( c10 :: intrusive_ptr<c10::intrusive_ptr_target> pImpl_)",3, 58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterContinuation::operator ( )()",4, 55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/netdef_converter.cpp,"torch::jit::getArgKind( const caffe2 :: Argument & arg)",25, 63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/netdef_converter.cpp,"torch::jit::convertArg( const caffe2 :: Argument & arg , Node * node)",41, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/netdef_converter.cpp,"torch::jit::convertNetDefToIR( const caffe2 :: NetDef & net , Graph * g , std :: unordered_map<std::string,Value*> * valueMapPtr , const std :: string & prefix)",72, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/netdef_converter.cpp,"torch::jit::convertAttrToCaffe2Arg( const Node * node , const Symbol & name , caffe2 :: Argument * arg)",45, 72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/netdef_converter.cpp,"torch::jit::convertNodeToCaffe2Op( const Node * node , caffe2 :: NetDef * net)",16, 75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/netdef_converter.cpp,"torch::jit::convertIRToNetDef( caffe2 :: NetDef * net , const Graph & g)",15, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::getPythonInterpreterStackTrace()",13, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::createGraphByTracing( const py :: function & func , Stack trace_inputs , const py :: function & var_name_lookup_fn , bool force_outplace , const c10 :: optional<size_t> & num_real_inputs)",36, 80, 10, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::preRecordPythonTrace( THPObjectPtr pyobj , const std :: string & arg_types , at :: ArrayRef<Variable> inputs , pyobj_list scalar_args)",25, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::pythonRecordSourceLocation( Node * n)",5, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::pythonWarn( const std :: string & reason)",5, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::initPythonTracerBindings( PyObject * module)",65, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printValueRef( std :: ostream & out , const Value * n)",3, 63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const std :: vector<T> & nodes)",4, 75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printValueRefs( std :: ostream & out , const at :: ArrayRef<T> & nodes)",12, 37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const at :: ArrayRef<const Value*> & nodes)",5, 47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const at :: ArrayRef<Value*> & nodes)",3, 81, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::const_value_list_with_types::const_value_list_with_types( ArrayRef<const Value*> values , const std :: string & delim = ', ')",4, 40, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const_value_list_with_types l)",12, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printPrimList( std :: ostream & out , const std :: vector<T> & items)",11, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printStrList( std :: ostream & out , const std :: vector<std::string> & items)",12, 45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::printAttrValue( std :: ostream & out , const Symbol & name) const",56, 73, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::printAttributes( std :: ostream & out , bool ignore_subgraph = false) const",22, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::indent( std :: ostream & out , size_t level)",6, 63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::print( std :: ostream & out , size_t level , std :: vector<const Node*> * groups) const",46, 72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const Node & n)",3, 61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const Graph & g)",25, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::prettyPrint( std :: ostream & out)",5, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::dumpPretty()",4, 47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::checkSameDevice( const Node * node)",20, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::lint() const",78, 78, 10, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::lint() const",141, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::dump() const",3, 30, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::LintGraph( std :: shared_ptr<Graph> & graph)",3, 48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::Block( Graph * graph_ , Node * node_)",11, 60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::reIndexTopology()",8, 58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::cloneFrom( Block * src , std :: function<Value*(Value*)> value_map)",28, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::destroy()",14, 69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::copy()",9, 79, 8, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::mustBeNone() const",3, 33, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::uniqueNameBase() const",12, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::isValidName( const std :: string & name)",13, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::setUniqueName( const std :: string & name)",44, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::copyMetadata( Value * from)",7, 42, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::replaceFirstUseWith( Value * newValue)",7, 55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::replaceAllUsesWith( Value * newValue)",5, 50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::findArgument( const FunctionSchema & the_schema , Symbol name)",11, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::get( Symbol name) const",3, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::namedInput( Symbol name) const",3, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::matches( const char * signature_literal , at :: ArrayRef<Symbol> const_inputs) const",13, 47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::mustBeNone() const",5, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::dump() const",3, 30, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::findSchema() const",3, 44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::maybeSchema() const",8, 50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isNondeterministic() const",38, 134, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::hasSideEffects() const",12, 36, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::assignTopoPosition()",43, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::Node( Graph * graph_ , NodeKind kind_)",9, 42, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::eraseOutput( size_t i)",11, 49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addBlock()",5, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::eraseBlock( size_t i)",7, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::destroy()",13, 39, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::cloneFrom( Node * s)",7, 45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceAllUsesWith( Node * n)",7, 55, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertInput( size_t i , Value * value)",18, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addInput( Value * value)",7, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceInput( size_t i , Value * newValue)",8, 55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceInputWith( Value * from , Value * to)",12, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addOutput()",5, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertOutput( size_t i)",8, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isBeforeOrAfter( const Node * n , MoveSide moveSide) const",37, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isBefore( const Node * n) const",3, 47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isAfter( const Node * n) const",3, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertBefore( Node * n)",5, 36, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertAfter( Node * n)",12, 49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveAfter( Node * n)",4, 32, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveBefore( Node * n)",4, 33, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeInput( size_t i)",11, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeAllInputs()",7, 49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::findUseForInput( size_t i)",8, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::dropInput( size_t i)",8, 36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeFromList()",10, 33, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::fakeRange()",5, 73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::insert( Symbol opname , at :: ArrayRef<NamedValue> args , at :: ArrayRef<NamedValue> kwargs , const c10 :: optional<SourceRange> & range)",14, 47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::create( NodeKind kind , size_t num_outputs)",8, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::create( NodeKind kind , ArrayRef<Value*> inputs , size_t num_outputs)",10, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createAutogradZero()",3, 37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNone( TypePtr typ)",5, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createFusionGroup()",5, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTuple( at :: ArrayRef<Value*> values , c10 :: OptNameList field_names)",9, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleUnpack( Value * v)",8, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleIndex( Value * tup , int64_t index)",7, 59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleSlice( Value * tup , int64_t beg , int64_t end)",13, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createList( const TypePtr & elem_type , at :: ArrayRef<Value*> values)",8, 81, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createListUnpack( Value * v , size_t size)",9, 57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createDict( const TypePtr & key_type , const TypePtr & value_type , at :: ArrayRef<Value*> keys , at :: ArrayRef<Value*> values)",17, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createDictIndex( Value * dict , Value * index)",8, 66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNumToTensor( Value * value)",6, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createImplicitTensorToNum( const TypePtr & type , Value * value)",5, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createObject( const ClassTypePtr & type)",5, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createSetAttr( Value * obj , const std :: string & field , Value * newValue)",8, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createGetAttr( Value * obj , const std :: string & field)",10, 67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createClone( Node * n , const std :: function<Value*(Value*)> & value_map , bool copy_blocks)",20, 52, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::insertConstant( IValue val , const TypePtr & result_type , c10 :: optional<SourceRange> loc , c10 :: optional<ScopePtr> scope)",8, 77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::toString() const",5, 38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::~Graph()",11, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeNode( Node * n)",6, 36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeValue( Value * v)",7, 37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeBlock( Block * b)",6, 37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::createTupleUnpack( Value * v)",9, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::inlineCallTo( Graph & g , Graph & callee , ArrayRef<Value*> inputs , bool unpack_outputs)",40, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::defaultAllocPythonOp( Graph * g)",4, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::allocPythonOp( Graph * g)",3, 36, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::setAllocPythonOp( PythonOp *(*v)(Graph*g))",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/hooks_for_testing.cpp,"torch::jit::didFinishEmitModule( std :: shared_ptr<script::Module> module)",5, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/hooks_for_testing.cpp,"torch::jit::setEmitModuleHook( std :: function<void(std::shared_ptr<script::Module>module)> cb)",4, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::ModuleAccessorValue( std :: shared_ptr<script::Module> module)",2, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",18, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::OpsValue( size_t version)",1, 50, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",6, 69, 4, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::ConstantValue( IValue value)",1, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::asValue( const SourceRange & loc , script :: Method & m)",3, 71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::ConstantTableValue( ArrayRef<at::Tensor> constants)",1, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",17, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::parseVersionNumber( script :: Lexer & L)",15, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::import_methods( const std :: shared_ptr<script::Module> & mod , const std :: string & src , const std :: vector<at::Tensor> & constant_table)",44, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_interpreter.cpp,"torch::jit::createPythonOperation( const Node * op_)",43, 74, 12, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::tensorEqual( const at :: Tensor & lhs , const at :: Tensor & rhs)",3, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::tensorListEqual( const std :: vector<at::Tensor> & lhs , const std :: vector<at::Tensor> & rhs)",7, 71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::attributesEqualCSE( const Node * lhs , const Node * rhs)",54, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::HashNode::operator ( )( const Node * k) const",7, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::EqualNode::operator ( )( const Node * lhs , const Node * rhs) const",32, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::getPythonName( const PyObject * obj_)",8, 68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::printPyObject( std :: ostream & out , const THPObjectPtr & obj)",39, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::findAllNodes( c10 :: ArrayRef<torch::jit::Block*> blocks , Symbol kind , bool recurse = true)",18, 63, 8, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::findAllNodes( Block * block , Symbol kind , bool recurse = true)",7, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::findNode( c10 :: ArrayRef<torch::jit::Block*> blocks , Symbol kind , bool recurse = true)",19, 58, 8, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::findNode( Block * block , Symbol kind , bool recurse = true)",4, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::ConcretePythonOp( Graph * graph)",1, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::name() const",8, 46, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::cloneFrom( Node * other_)",11, 52, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::allocNewInstance( Graph * g)",3, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::autogradFunction() const",21, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::writeScalars( std :: ostream & out) const",10, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::pythonAllocPythonOp( Graph * g)",3, 42, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::initPythonIRBindings( PyObject * module_)",492, 81, 18, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::ExecutionPlan( std :: shared_ptr<Graph> graph)",2, 48, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::run( Stack & stack) const",3, 46, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::operator bool() const",3, 37, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::getDebugState()",6, 39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::DifferentiableGraphBackward( GraphExecutor executor , size_t capture_size)",6, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::apply( variable_list && inputs)",44, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::capture( const IValue & val , bool is_output)",9, 70, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::DifferentiableGraphOp( Gradient grad)",6, 55, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::operator ( )( Stack & stack) const",58, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::detachVariables( Stack & stack) const",15, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::captureInputs( DifferentiableGraphBackward & grad_fn , at :: ArrayRef<IValue> inputs) const",7, 60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::captureOutputs( DifferentiableGraphBackward & grad_fn , at :: ArrayRef<IValue> outputs) const",7, 60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::packGradient( Gradient gradient , Node * dnode)",14, 75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::getGradient( const Node * n)",14, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::detail::getGradExecutor( Operation & op)",6, 59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::prepareGraph( std :: shared_ptr<Graph> & graph)",5, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::countFlatInputs( const TypePtr & ptr)",13, 63, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::countFlatInputs( const std :: shared_ptr<Graph> & graph)",7, 71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::hasMutableOperators( Block * block)",11, 59, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::GraphExecutorImpl( std :: shared_ptr<Graph> graph , bool optimize)",8, 77, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::run( Stack & stack)",16, 65, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::graphFor( const Stack & stack) const",15, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getDebugState()",11, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::debugDisableAutodiffSubgraphInlining()",6, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getOrCompileFallback()",9, 53, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getOrCompile( const Stack & stack)",17, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::compileSpec( const ArgumentSpec & spec)",44, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runOptimization( std :: shared_ptr<Graph> & graph , const ArgumentSpec & spec)",20, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runNondiffOptimization( std :: shared_ptr<Graph> & graph)",3, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::needsGradient( const std :: shared_ptr<const Graph> & graph)",11, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::mayIntroduceGradient( const Block * b)",11, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runTraced( Stack & stack)",34, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::GraphExecutor( std :: shared_ptr<Graph> graph , bool optimize)",2, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::run( Stack & inputs)",3, 41, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::graph() const",3, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::graphFor( const Stack & inputs) const",3, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::getDebugState()",3, 52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::debugDisableAutodiffSubgraphInlining()",3, 61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::runRequiredPasses( const std :: shared_ptr<Graph> & g)",11, 69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::SchemaParser( const std :: string & str)",2, 73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDeclaration()",38, 77, 12, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDeclarations()",8, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseIdent()",3, 54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseArgument( size_t idx , bool is_return , bool kwarg_only)",41, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseSingleConstant( TypeKind kind)",45, 79, 10, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::convertToList( TypeKind kind , const SourceRange & range , std :: vector<IValue> vs)",16, 67, 12, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseConstantList( TypeKind kind)",11, 58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseTensorDefault( const SourceRange & range)",4, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDefaultValue( const TypePtr & arg_type , c10 :: optional<int32_t> arg_N)",40, 74, 12, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseList( int begin , int sep , int end , const std :: function<void()> & callback)",16, 47, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::registerPendingOperators()",8, 66, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::registerOperator( Operator && op)",4, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::lookupByLiteral( const char * name)",24, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::getOperators( Symbol name)",9, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::findSimilarOperators( Symbol input_op)",26, 76, 10, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getRegistry()",4, 34, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::registerOperator( Operator && op)",21, 71, 10, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getAllOperatorsFor( Symbol name)",3, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::findSimilarOperators( Symbol input_op)",3, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::sig( const char * signature)",3, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::parseSchema( const std :: string & schema)",3, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::canonicalSchemaString( const FunctionSchema & schema)",32, 66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::Operator::matches( const Node * node) const",34, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::findOperatorFor( const Node * node)",9, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getOperatorFor( const Node * node)",22, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorSet::OperatorSet( std :: initializer_list<const char*> sig_literals)",7, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorSet::find( const Node * n) const",12, 51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/constants.cpp,"torch::jit::insertConstant( Graph & g , const IValue & val , const c10 :: TypePtr & result_type , c10 :: optional<SourceRange> loc , c10 :: optional<ScopePtr> scope)",77, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/constants.cpp,"torch::jit::toIValue(const Value*v)",10, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::IRParser( const std :: string & str , torch :: jit :: Graph * graph)",4, 64, 8, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::parseIR( const std :: string & str , torch :: jit :: Graph * graph)",4, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseVarWithType()",16, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseVar()",7, 38, 4, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseOperatorOutputs( std :: vector<VarWithType> * outs)",9, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseScalarLiteral( Node * n)",33, 59, 6, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseAttr( Node * n)",67, 70, 10, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseAttrs( Node * n)",3, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseOperatorInputs( Node * n)",10, 46, 0, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseBlocks( Node * parentNode)",7, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseBlockInputs( Block * b)",9, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseBlockOutputs( Block * b)",10, 45, 0, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseBlock( Node * parentNode)",8, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseOperatorsList( Block * b)",6, 66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseOperatorName()",7, 48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseOperator( Block * b)",28, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseGraphInputs()",9, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseReturnOperator()",18, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parse()",13, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/irparser.cpp,"torch::jit::script::IRParser::parseList( int begin , int sep , int end , const std :: function<void()> & callback)",17, 45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_c10_ops.cpp,"torch::jit::unwrap( at :: Tensor && tensor)",6, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_c10_ops.cpp,"torch::jit::createOperatorFromC10( const c10 :: OperatorHandle & op)",29, 133, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_c10_ops.cpp,"torch::jit::RegistrationListener::onOperatorRegistered( const c10 :: OperatorHandle & op)",3, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_c10_ops.cpp,"torch::jit::RegistrationListener::onOperatorDeregistered( const c10 :: OperatorHandle & op)",3, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_c10_ops.cpp,"torch::jit::Registerer::Registerer()",7, 64, 4, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::push( Symbol name)",3, 66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::getRoot()",7, 44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::getDepth()",9, 44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::namesFromRoot( const std :: string & separator) const",14, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::getNodeStackTraceString( const Node * n)",9, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::validateBlock( Block * b , onnx_torch :: OperatorExportTypes operator_export_type)",51, 166, 12, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::validateGraph( const std :: shared_ptr<Graph> & graph , onnx_torch :: OperatorExportTypes operator_export_type)",6, 60, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::get_model_proto()",3, 39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeIntermediateValueInfo( onnx :: GraphProto * graph_proto , const Value * n)",3, 44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ATenTypeToOnnxType( at :: ScalarType at_type)",22, 72, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncoderBase( onnx_torch :: OperatorExportTypes operator_export_type , bool strip_doc)",11, 68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeValueInfo( onnx :: GraphProto * graph_proto , onnx :: ValueInfoProto * v , const Value * n)",20, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeGraph( onnx :: GraphProto * graph_proto , const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers)",6, 58, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeBlock( onnx :: GraphProto * graph_proto , const Block * block , const std :: vector<at::Tensor> & initializers)",102, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::AddAttribute( onnx :: NodeProto * node_proto , const jit :: Node * node , const jit :: Symbol name)",63, 66, 6, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::get_raw_data_export_map()",3, 47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::GraphEncoder( const std :: shared_ptr<Graph> & graph , int64_t onnx_opset_version , onnx_torch :: OperatorExportTypes operator_export_type , const std :: vector<at::Tensor> & initializers , bool defer_weight_export , bool strip_doc)",19, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::EncodeTensor( onnx :: TensorProto * tensor_proto , const at :: Tensor & tensor , const c10 :: optional<std::string> external_ref)",27, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::ScriptModuleSerializer( const std :: string & filename)",4, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::ScriptModuleSerializer( std :: ostream * ofs)",2, 66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::serialize( const script :: Module & module , const script :: ExtraFilesMap & extra_files)",26, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertModel( const script :: Module & module , torch :: ModelDef * model_def , const script :: ExtraFilesMap & extra_files)",18, 77, 42, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::addTensor( const at :: Tensor & tensor)",4, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertAndWriteTensor( size_t tensor_id , const at :: Tensor & tensor , torch :: TensorDef * tensor_proto , std :: unordered_map<const void*,std::string> & storageMap)",55, 80, 31, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::writeTensorTable( torch :: ModelDef * model_def)",8, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertModule( const script :: Module & module , const std :: string & prefix , const std :: string & name , torch :: ModuleDef * module_def)",42, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertParameter( const script :: NamedIValue & param , torch :: ParameterDef * param_def , bool is_parameter)",8, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::idt( size_t indent)",3, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::nlidt( size_t indent)",3, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TensorProto & tensor , std :: ostream & stream)",7, 74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TensorShapeProto & shape , std :: ostream & stream)",11, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TypeProto_Tensor & tensor_type , std :: ostream & stream)",4, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TypeProto & type , std :: ostream & stream)",3, 63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: ValueInfoProto & value_info , std :: ostream & stream)",5, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: AttributeProto & attr , std :: ostream & stream , size_t indent)",51, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: NodeProto & node , std :: ostream & stream , size_t indent)",16, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: GraphProto & graph , std :: ostream & stream , size_t indent)",25, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: OperatorSetIdProto & operator_set_id , std :: ostream & stream)",5, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: ModelProto & model , std :: ostream & stream , size_t indent)",18, 81, 9, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::prettyPrint( const onnx :: ModelProto & model)",5, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::pretty_print_onnx( const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers , int64_t onnx_opset_version , bool defer_weight_export , :: torch :: onnx :: OperatorExportTypes operator_export_type , bool google_printer)",19, 61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::export_onnx( const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers , int64_t onnx_opset_version , bool defer_weight_export , :: torch :: onnx :: OperatorExportTypes operator_export_type)",17, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ExportModule( const script :: Module & module , std :: ostream & out , const script :: ExtraFilesMap & extra_files)",7, 48, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ExportModule( const script :: Module & module , const std :: string & filename , const script :: ExtraFilesMap & extra_files)",7, 48, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::cast_handle_sequence( std :: vector<py::handle> objs)",7, 64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::flatten_rec( PyObject * obj , ParsedArgs & args)",24, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::flatten( py :: handle obj)",6, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::cast_sequence( std :: vector<py::object> objs)",7, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::unflatten_rec( ArrayRef<Variable> :: iterator & var_it , ArrayRef<Variable> :: iterator & var_it_end , std :: string :: const_iterator & desc_it)",24, 75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::unflatten( ArrayRef<Variable> vars , const IODescriptor & desc)",11, 73, 0, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::extractClosure( Value * closure)",12, 75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::originalReturnType( const TupleTypePtr & tup)",8, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::overloadedSchemaString( const FunctionSchema & schema)",14, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::isHelperFunction( const std :: string & method_name)",4, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::loadModule( const std :: shared_ptr<script::Module> & module)",50, 74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::loadFunctions()",8, 56, 8, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::gradientInfoForSchema( const FunctionSchema & schema)",21, 60, 4, 0
repos/cpp/pytorch/torch/csrc/jit/symbolic_script.cpp,"torch::jit::hasGradientInfoForSchema( const FunctionSchema & schema)",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::loadPythonClasses()",9, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::runJITCPPTests()",3, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::initJITBindings( PyObject * module)",375, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::detail::genericAddInput( Node * n , T value)",5, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::detail::badArgType( const T & v)",6, 64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::pauseTracing()",7, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::delValueTrace( const Variable & var)",14, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getValueTrace( const IValue & var)",52, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getNestedValueTrace( const IValue & v)",19, 78, 16, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getOutputTrace( const std :: shared_ptr<TracingState> & state , const Variable & var)",19, 85, 7, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getNestedOutputTrace( const std :: shared_ptr<TracingState> & state , const IValue & iv)",18, 82, 8, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::enter( Stack inputs)",44, 82, 10, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::exit( const Stack & outputs)",9, 71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::abandon()",3, 28, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setValueTrace( const IValue & v , Value * value)",38, 76, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , int64_t value)",9, 59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , c10 :: optional<int64_t> value)",9, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , bool value)",3, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , double value)",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: Scalar & value)",3, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const c10 :: optional<at::Scalar> & value)",12, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const std :: string & value)",3, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: Tensor & value)",3, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: SparseTensorRef & value)",3, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Generator * value)",9, 68, 6, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Device value)",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Layout value)",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: ScalarType value)",3, 66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const c10 :: optional<at::ScalarType> & value)",12, 74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: TensorList value , bool allow_undefined)",17, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: TensorOptions & options)",7, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: IntArrayRef value)",23, 82, 10, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const ArrayRef<double> & value)",3, 75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const std :: vector<double> & value)",6, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addOutput( Node * node , const at :: Tensor & output)",3, 55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setOutput( Value * value , const at :: Tensor & output)",6, 61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addOutput( Node * node , const std :: vector<at::Tensor> & outputs)",11, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getTracingState()",3, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setTracingState( std :: shared_ptr<TracingState> state)",3, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::TracingState::TracingState()",2, 66, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getSizeOf( const autograd :: Variable & var , int64_t dim)",18, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ensureUniqueIfOutOfPlaced( const char * name , const at :: Tensor & tensor)",20, 101, 7, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ArgumentStash::stashIntArrayRefElem( const std :: string & arg_name , size_t size , size_t idx , const Variable & var)",19, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ArgumentStash::stashValue( const std :: string & arg_name , size_t idx , const Variable & var , const TypePtr & type)",20, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::defaultRecordSourceLocation( Node * n)",1, 45, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::recordSourceLocation( Node * n)",3, 43, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setRecordSourceLocation( void(*v)(Node*))",3, 49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::defaultWarn( const std :: string & str)",3, 43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::_do_warn( const char * _reason , const char * _kind)",7, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setWarn( warn_fn_type fn)",3, 32, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::wrapDim( int64_t & dim , const std :: vector<int64_t> & sizes)",5, 64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::needTrimGrad( Node * n)",10, 96, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::isDifferentiable( Node * n)",122, 190, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::isDifferentiable( Graph & g)",6, 55, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::build_script_grad( Node * node , const ArrayRef<Value*> & grads)",37, 70, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::GradientHelper::GradientHelper( Node * n)",1, 39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::GradientHelper::gradient( ArrayRef<Value*> grad_values)",15, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::GradientHelper::gradSumToSizeOf( SymbolicVariable v , Symbol input_name)",8, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::GradientHelper::buildSymbolicGradient( const std :: vector<SymbolicVariable> & grads)",483, 199, 12, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::linearGradientForNode( Node * node , ArrayRef<Value*> grad_values)",22, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::ReverseDetails::ReverseDetails( value_map && grad_map , Block * reverse_block)",2, 71, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::createAutogradAdd( Value * a , Value * b)",4, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::addReverseInline( Gradient & grad_desc)",82, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::getReverseCaptures( Gradient & grad_desc)",24, 72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::liftConstants( Gradient & grad_desc , ReverseDetails & rev_info)",28, 75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::deduplicateSizeCaptures( Gradient & grad_desc , ReverseDetails & rev_info)",24, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::eliminateDeadCode( ReverseDetails & rev_info)",23, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::Optimize( Gradient & grad_desc , ReverseDetails & rev_info)",17, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::lambdaLiftReverse( Gradient & grad_desc , ReverseDetails & rev_info)",113, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::differentiate( std :: shared_ptr<Graph> & graph)",26, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::noop( const Node * n)",3, 41, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::checkImplicitTensorToNum( at :: Tensor t , bool toInt)",18, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listConstruct( int64_t num_inputs)",10, 69, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::floordiv( int64_t a , int64_t b)",13, 74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::to_dispatch( at :: Tensor self , c10 :: optional<at::Device> device , c10 :: optional<at::ScalarType> scalarType , bool non_blocking , bool copy)",19, 62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::normalizeIndex(int64_t idx,int64_t list_size)",7, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::getItem(TList&list,int64_t idx)",8, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::getBoolItem(const std::vector<bool>&list,int64_t idx)",8, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listAppend(Stack&stack)",10, 31, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listPop(Stack&stack)",18, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listPop<Shared<BoolList>>(Stack&stack)",18, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listClear(Stack&stack)",7, 30, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listInsert(Stack&stack)",22, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listRemove(Stack&stack)",16, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listRemove<Shared<TensorList>,at::Tensor>(Stack&stack)",20, 70, 6, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listExtend(const Node*node)",12, 61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listCopy(const Node*node)",11, 40, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSelect(Stack&stack)",9, 37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSelect<Shared<BoolList>>(Stack&stack)",9, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listLen(Stack&stack)",8, 45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listEq(Stack&stack)",7, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listNe(Stack&stack)",7, 50, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::tensor_list_equal(Shared<TensorList>a,Shared<TensorList>b)",19, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listEq<Shared<TensorList>>(Stack&stack)",7, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listNe<Shared<TensorList>>(Stack&stack)",7, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listList(const Node*node)",7, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listAdd(Stack&stack)",18, 71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listMulIntLeft(Stack&stack)",18, 49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listMulIntRight(Stack&stack)",18, 49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSlice(Stack&stack)",32, 62, 6, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSetItem(Stack&stack)",11, 32, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSetItem<Shared<BoolList>,bool>(Stack&stack)",17, 59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::dictSetItem(Stack&stack)",8, 55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::dictLen(Stack&stack)",5, 45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::dictKeys(Stack&stack)",10, 45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::dictValues(Stack&stack)",10, 45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::dictIndex(Stack&stack)",11, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::_output_size(const at::Tensor&input,size_t dim,const IValue&size,const IValue&scale_factors)",25, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::interpolate(const at::Tensor&input,const IValue&size,const IValue&scale_factors,const std::string&mode,c10::optional<bool>align_corners)",81, 95, 10, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::interpolate_op(const Node*n)",14, 77, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::convert_scale_factor_to_double(const IValue&int_ivalue)",18, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_nearest_op(const Node*n)",14, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_op(const Node*n)",20, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_bilinear_op(const Node*n)",14, 73, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::leaky_relu(const at::Tensor&tensor,double scalar)",3, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::cat(const std::vector<at::Tensor>&tensors)",3, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::get_first(const std::vector<std::vector<std::string>>&strings)",3, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/attributes.cpp,"torch::jit::GraphAttr::clone() const",3, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/attributes.cpp,"torch::jit::GraphsAttr::clone() const",7, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( const std :: string & filename)",4, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( std :: istream * is)",2, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( std :: unique_ptr<ReadAdapterInterface> rai)",3, 52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::deserialize( script :: ModuleLookup module_lookup , c10 :: optional<at::Device> device , script :: ExtraFilesMap & extra_files)",51, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::loadTensorTable( torch :: ModelDef * model_def)",6, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::loadTensor( const torch :: TensorDef & tensor_proto , std :: unordered_map<std::string,at::Storage> & storageMap)",70, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::convertModule( const torch :: ModuleDef & module_def)",28, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( script :: ModuleLookup module_lookup , std :: istream & in , c10 :: optional<at::Device> device , script :: ExtraFilesMap & extra_files)",8, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( script :: ModuleLookup module_lookup , const std :: string & filename , c10 :: optional<at::Device> device , script :: ExtraFilesMap & extra_files)",8, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( script :: ModuleLookup module_lookup , std :: unique_ptr<ReadAdapterInterface> rai , c10 :: optional<at::Device> device , script :: ExtraFilesMap & extra_files)",8, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( std :: istream & in , c10 :: optional<at::Device> device , script :: ExtraFilesMap & extra_files)",9, 59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( const std :: string & filename , c10 :: optional<at::Device> device , script :: ExtraFilesMap & extra_files)",8, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( std :: unique_ptr<ReadAdapterInterface> rai , c10 :: optional<c10::Device> device , script :: ExtraFilesMap & extra_files)",22, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( at :: Tensor data , at :: Tensor mask , at :: Tensor dims)",11, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( const at :: Tensor & data , int64_t batch_size)",11, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( const std :: vector<at::Tensor> & datalist , at :: Tensor dims)",32, 73, 8, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::examples()",20, 68, 8, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::initBatchTensorBindings( PyObject * module)",12, 60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::isSimpleMap( Node * node)",84, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::isDefined( Value * tensor)",9, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::isFusableBatchNorm( Node * batch_norm)",11, 192, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::broadcastSizes( at :: ArrayRef<Value*> sizes)",8, 69, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::GraphFuser( Block * block , std :: shared_ptr<Graph> graph)",2, 57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tensorInputs( Node * node)",5, 56, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::containsGradSumToSize( Node * fusion_group)",6, 65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusable( Node * node)",3, 59, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableMap( Node * node)",15, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableCatNode( Node * node)",17, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::calculatesSize( Node * node)",3, 62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::allUsersAreThisConsumerOrCalcSizes( Node * consumer , Value * producer)",10, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::getSubgraph( Node * n)",4, 47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::decomposeBatchNorm( Node * batch_norm)",52, 163, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mergeFusionGroups( Node * consumer_group , Node * producer_group)",57, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mergeNodeIntoGroup( Node * group , Node * n)",73, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::createSingletonFusionGroup( Node * n)",13, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::insertAt( Node ** insertion_point , Node * n)",4, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tryFuse( Node * consumer , Value * producer)",66, 194, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::canFuseChunk( Node * consumer , Value * producer)",23, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::findFusedChunk( Node * group , Value * input)",17, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseChunkByReusingExistingFusedChunk( Node * group , Node * chunk , Node * existingFusedChunk)",25, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseChunk( Node * consumer , Value * producer)",19, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::sortReverseTopological( ArrayRef<Value*> inputs)",13, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::scanNodeForChunks( Node * consumer)",12, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::insertExplicitBroadcast( Node * node)",15, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::promoteChunkToBroadcastingChunk( Node * chunk)",18, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tryToMoveChunk( Node * consumer , Value * producer)",139, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::scanNode( Node * consumer)",22, 80, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::replaceIntermediateBroadcastingChunks()",34, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::usedOnlyInSize( Value * v)",6, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::buildShapeExpressions( Node * fusion_group)",69, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::removeOutputsUsedOnlyInSize( Node * fusion_group)",26, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::refreshAliasDb()",3, 52, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::canFuseWithConcat( Value * producer , Node * before_check)",21, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::createFusedConcat( Node * node)",15, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseConcats()",39, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::optimizeFusedGraphs()",11, 47, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::run()",53, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::PeepholeOptimizeShapeExpressions( Block * block)",51, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::trackSingleGradSumToSizeToOutputs( Value * gradSumToSizeOutput , std :: vector<int64_t> * outputGradSumToSizes)",66, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::FuseGraph( std :: shared_ptr<Graph> & graph)",12, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::removePrintOps( Block * block)",22, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::removePrintOps( std :: shared_ptr<Graph> & graph)",3, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::ToONNX( std :: shared_ptr<Graph> & graph , :: torch :: onnx :: OperatorExportTypes operator_export_type)",9, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::BlockToONNX( Block * old_block , Block * new_block , :: torch :: onnx :: OperatorExportTypes operator_export_type , std :: unordered_map<Value*,Value*> env)",190, 90, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/erase_number_types.cpp,"torch::jit::EraseNumberTypesOnBlock( Block * block)",44, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/erase_number_types.cpp,"torch::jit::EraseNumberTypes( const std :: shared_ptr<Graph> & graph)",3, 61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/common_subexpression_elimination.cpp,"torch::jit::EliminateCommonSubexpression( Block * block , const AliasDb & aliasDb , std :: function<Node*(Node*)> parent_lookup_fn)",48, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/common_subexpression_elimination.cpp,"torch::jit::EliminateCommonSubexpression( std :: shared_ptr<Graph> & graph)",5, 67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::canRunWithAutograd( Node * node)",3, 44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::scanNode( Node * node , size_t threshold)",28, 73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::InlineAutodiffSubgraphs( Block * block , size_t threshold)",5, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::InlineAutodiffSubgraphs( std :: shared_ptr<Graph> & graph , size_t threshold)",4, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isTrueConstant( Value * val)",4, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isForLoop( Node * node)",7, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::limitedBlockSize( Block * body , int64_t limit)",13, 55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isSmallBlock( Block * body)",3, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::inlineBody( Node * loop)",34, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::repeatBody( Block * body , int64_t times)",52, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::replaceLoopCounter( Node * loop)",16, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::unroll( Node * loop)",47, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::UnrollLoops( Block * block)",14, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::UnrollLoops( std :: shared_ptr<Graph> & graph)",4, 50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_fork_wait.cpp,"torch::jit::InlineForkWait( Block * b , std :: unordered_map<Value*,Value*> & future_remap)",23, 68, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_fork_wait.cpp,"torch::jit::InlineForkWait( const std :: shared_ptr<Graph> & graph)",4, 59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::isValidArgumentForRunning( Value * v)",9, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::isValidReturnForRunning( Value * v)",4, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::ShapePropagator( std :: shared_ptr<Graph> graph)",3, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnBlock( Block * block , bool insert_expands = true)",15, 80, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::resizesInput( Node * n)",19, 77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::collectResizeSet( Block * block)",14, 77, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::setUnshapedType( Value * o)",3, 41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::setUnshapedType( Node * node)",5, 37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::wrapDim( int64_t dim , at :: IntArrayRef sizes)",6, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::jitDeviceIndexToDevice( int device)",3, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::representativeValue( Value * v)",25, 78, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::gatherTensorTypes( Node * node)",26, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::mergeTypes( ArrayRef<Value*> lhs , ArrayRef<Value*> rhs , ArrayRef<Value*> outputs)",16, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::broadcastBinary( Node * node , std :: vector<CompleteTensorTypePtr> & types , size_t idx1 , size_t idx2)",28, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::dependsOnMutation( Node * node)",27, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::canPropagateShapeByRunningIt( Node * node)",25, 59, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnNodeByRunningIt( Node * node)",26, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateCatShape( Node * cat_node)",63, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::mayAliasResizedSet( at :: ArrayRef<Value*> vs)",10, 63, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnNode( Node * node , bool insert_expands = true)",139, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::determineListSize( Value * list)",11, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateTensorShapeOnNode( Node * node , bool insert_expands)",882, 239, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateCompleteShapeOnNode( Node * node , bool insert_expands , std :: vector<CompleteTensorTypePtr> tensor_types)",227, 83, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::PropagateInputShapes( const std :: shared_ptr<Graph> & graph)",3, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( at :: ArrayRef<Value*> vals)",5, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( Block * b)",13, 51, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( const std :: shared_ptr<Graph> & graph)",3, 66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::getRequiresGrad( Value * value)",3, 37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( Value * value , bool req_value)",5, 66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( at :: ArrayRef<Value*> outputs , const std :: vector<bool> & values)",8, 47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( Node * node , const std :: vector<bool> & values)",3, 68, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::bitwiseOr( std :: vector<bool> a , const std :: vector<bool> & b)",7, 79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGradSimpleNode( Node * node)",36, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( Node * node)",34, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( Block * block)",5, 43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( std :: shared_ptr<Graph> & graph)",3, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isPrint( char s)",3, 31, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::printQuotedString( std :: ostream & stmt , const std :: string & str)",53, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isValidIdentifierChar( char c , size_t pos)",3, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isValidIdentifier( const std :: string & name)",9, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::QualifiedName( QualifiedNamePtr prefix , std :: string name)",2, 62, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::create( QualifiedNamePtr prefix , std :: string name)",4, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::create( std :: string name)",4, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::str() const",5, 28, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::emit( std :: ostream & out) const",16, 39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::createTensorToParameterNameMap( const script :: Module & module , const QualifiedNamePtr & prefix , std :: unordered_map<IValue*,QualifiedNamePtr> & result)",17, 75, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::canInline( Value * v)",25, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanValue( Node * block_point , Value * v)",17, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::previousNonConstant( Node * n)",6, 43, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanNode( Node * n)",15, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanBlock( Block * b)",6, 45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::getOrAddTensorConstant( at :: Tensor t)",14, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::buildConstantList( Node * n , std :: vector<Node*> & constants)",12, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::buildConstantList( Block * b , std :: vector<Node*> & constants)",5, 68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genNameImpl( const std :: string & candidate , std :: unordered_set<std::string> & used)",10, 61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genName( const std :: string & candidate)",3, 54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genMethodName( const std :: string & candidate)",3, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::makeValidIdentifier( const std :: string & candidate)",12, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genUniqueNameFor( Value * v)",4, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::useOf( Value * v) const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValue( Value * v , const std :: string & s)",3, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValue( Value * v , Value * w)",3, 41, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValuesToTheirUniqueNames( at :: ArrayRef<Value*> values)",5, 69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::indent()",6, 41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::WithIndented()",4, 47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::zipWith( at :: ArrayRef<T0> list_a , at :: ArrayRef<T1> list_b , F action) const",13, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printValueList( std :: ostream & stmt , at :: ArrayRef<Value*> list , const char * begin = '' , const char * end = '')",14, 33, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printDict( std :: ostream & stmt , at :: ArrayRef<Value*> key_value_pairs , const char * begin = '{' , const char * end = '}')",18, 61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printAssignment( at :: ArrayRef<Value*> lhs , at :: ArrayRef<Value*> rhs)",9, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printIf( IfView stmt)",16, 63, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::shouldEmitAsForLoop( LoopView stmt)",28, 95, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printLoop( LoopView stmt)",47, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::isLongLine( const std :: string & str)",3, 44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::isLongInline( Node * node)",3, 76, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::isNonConstantInline( Value * input)",4, 54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::splitLongInlines( at :: ArrayRef<Value*> inputs)",17, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printOutputDefinition( Node * node , const std :: string & str)",10, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printNode( Node * node , bool print_const)",62, 121, 14, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMaybeAnnotatedConstantList( std :: ostream & stmt , const char * the_type , size_t list_size , const IValue & the_list)",11, 56, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printConstant( std :: ostream & stmt , const IValue & v)",31, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printNone( std :: ostream & stmt , const Node * node)",29, 81, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printRHS( std :: ostream & stmt , Node * node)",140, 109, 14, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printBlock( Block * root , bool block_has_other_statements)",15, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printDefaultValue( const TypePtr & typ , std :: ostream & stmt , const IValue & value)",16, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printFunctionDefinition( Graph & graph , const std :: string & name , const std :: vector<c10::optional<IValue>> & defaults = { } , const std :: vector<std::string> & param_names = { })",54, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::PythonPrintPass( std :: ostream & out_ , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",7, 51, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::resultType( const Graph & graph)",8, 77, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printFunction( Graph & graph , const std :: string & name , const std :: vector<c10::optional<IValue>> & defaults = { } , const std :: vector<std::string> & param_names = { })",13, 65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMethod( script :: Method & method)",6, 73, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMethod( script :: Method & method , const std :: unordered_map<IValue*,QualifiedNamePtr> & extra_ivalue_names)",14, 75, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printModule( script :: Module & module)",15, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const Graph & graph , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",9, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const script :: Method & method , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",9, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const script :: Module & module , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",9, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::printerHasSpecialCaseFor( Symbol sym)",52, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize.cpp,"torch::jit::Canonicalize( const std :: shared_ptr<Graph> & graph , bool keep_unique_names)",38, 69, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::DeadCodeEliminator( std :: shared_ptr<Graph> graph)",2, 67, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::run( Block * block , bool recurse)",10, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::setDeleteCallback( std :: function<void(const std::unordered_set<const Value*>&)> deleteCallback)",5, 67, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::markReturnNode( Node * node)",44, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::mark( Block * block)",19, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::markIfLive( Node * node)",13, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::mark( Node * node)",26, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::sweep( Block * block , bool recurse)",22, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::hasUntrackedMutation( Node * node)",16, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::hasSideEffects( Node * node)",18, 79, 26, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::removeDeadBlockOutputs( Node * node)",15, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::removeDeadLoopOutputs( Node * node)",19, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( const std :: shared_ptr<Graph> & graph)",3, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( Block * block , bool recurse)",3, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( Block * block , std :: function<void(const std::unordered_set<const Value*>&)> cb)",7, 71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_expands.cpp,"torch::jit::RemoveExpands( Block * block)",12, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_expands.cpp,"torch::jit::RemoveExpands( const std :: shared_ptr<Graph> & graph)",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::isInplaceOp( const Node * node)",3, 55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::RemoveInplaceOps( Block * block)",27, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::RemoveInplaceOps( const std :: shared_ptr<Graph> & graph)",3, 61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimizeImpl( Block * block , bool addmm_fusion_enabled)",146, 87, 16, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimize( Block * block , bool addmm_fusion_enabled)",5, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimize( const std :: shared_ptr<Graph> & graph , bool addmm_fusion_enabled)",5, 58, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_pooling.cpp,"torch::jit::ConstantPooling( Block * block , std :: unordered_set<Node*,HashNode,EqualNode> & constants)",35, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_pooling.cpp,"torch::jit::ConstantPooling( const std :: shared_ptr<Graph> & graph)",4, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::ChunkOutput::ChunkOutput( Value * v , size_t o)",1, 57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::getChunkOutputs( Node * chunk)",25, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::CanonicalizeOps( Block * block)",70, 106, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::CanonicalizeOps( const std :: shared_ptr<Graph> & graph)",4, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp,"torch::jit::specializeAutogradZero( Graph & g)",95, 81, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::shouldAnnotate( const TypePtr & type)",10, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::shouldAnnotate( const Value * v)",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::AliasDb( std :: shared_ptr<Graph> graph)",4, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWildcard( const Node * n) const",13, 49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::isWildcard( const Value * v) const",3, 49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesTo( Node * n , const Value * v) const",22, 71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWriters( const Node * n) const",13, 48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWriters( const Value * v) const",29, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWrites( Node * n) const",13, 43, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesToInputAlias( Node * n) const",23, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWritesImpl( Block * b , ValueSet & ret , bool recurseBlocks) const",5, 81, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWritesImpl( Node * n , ValueSet & ret , bool recurseBlocks) const",18, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWrites( Block * b) const",5, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesToAlias( Node * n , const ValueSet & vs , bool recurseBlocks) const",5, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWrites( Node * n , bool recurseBlocks) const",6, 81, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getReadsImpl( Node * n , ValueSet & ret , bool recurseBlocks) const",16, 79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getReads( Node * n , bool recurseBlocks) const",5, 64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::dump() const",35, 67, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::makeAllAlias( const std :: vector<Value*> & values)",8, 64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( const std :: shared_ptr<Graph> & graph)",59, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( Block * block)",5, 38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( Node * node)",8, 48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeImpl( Node * node)",163, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::registerWrite( const Value * v , Node * n)",11, 55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeIf( Node * node)",18, 71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeLoop( Node * node)",19, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeSubgraph( Node * node)",18, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeCreator( Node * node)",5, 43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeExtractor( Node * node)",7, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeChunk( Node * node)",5, 42, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeFork( Node * node)",13, 55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeWait( Node * node)",48, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeSetAttr( Node * node)",5, 58, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeBroadcastingChunk( Node * node)",13, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::makePointerTo( const Value * from , const Value * to)",27, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::mayAlias( const Value * a , const Value * b) const",11, 69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::mapAliases( at :: ArrayRef<Value*> from , at :: ArrayRef<Value*> to)",6, 79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::giveFreshAlias( const Value * value)",13, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::isTracked( const Value * v) const",3, 48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::moveAfterTopologicallyValid( Node * n , Node * movePoint)",3, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::couldMoveAfterTopologically( Node * n , Node * movePoint)",3, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::moveBeforeTopologicallyValid( Node * n , Node * movePoint)",10, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::couldMoveBeforeTopologically( Node * n , Node * movePoint)",3, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::WorkingSet( Node * mover , const AliasDb & aliasDb)",3, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::add( Node * n)",16, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::eraseMover()",27, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::nodes()",3, 36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::dependsOn( Node * n) const",7, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::hasDataDependency( Node * n) const",7, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::hasMutabilityDependency( Node * n) const",26, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::producesFor( Node * n) const",5, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::consumesFrom( Node * n) const",6, 71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::getUsersSameBlock( Node * n) const",11, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::WorkingSet::findSameBlock( Node * target , Node * n)",17, 75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::tryMove( Node * toMove , Node * movePoint , MoveSide moveSide , bool dryRun)",90, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::move( Node * toMove , Node * movePoint , MoveSide moveSide)",10, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasUntrackedEffects( Node * node) const",9, 75, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::isBeforeSameGraph( const Node * a , const Node * b) const",20, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getLastWildcard() const",11, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::aliasAnalysisHasSpecialCaseFor( Symbol symbol)",52, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::setWildcard( const Value * v)",3, 44, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::rebuildWriteCache() const",12, 75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getMemoryLocations( const Value * v) const",11, 66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::getBatchOperator( const std :: string & name , int64_t num_inputs)",18, 73, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::inlineUnpackedCallTo( Graph & g , Graph & callee , ArrayRef<Value*> inputs)",6, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitAten( Node * n , Block * block , Block * res_block)",58, 88, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitConstant( Node * n , Block * block , Block * res_block)",6, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitNumToTensor( Node * n , Block * block , Block * res_block)",10, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitTensorToNum( Node * n , Block * block , Block * res_block)",10, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitListConstruct( Node * n , Block * block , Block * res_block)",26, 76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitIf( Node * n , Block * block , Block * res_block)",24, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitLoop( Node * n , Block * block , Block * res_block)",142, 79, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::toBatch( Block * block , Block * res_block)",73, 120, 14, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::to_batch_graph( std :: shared_ptr<Graph> graph)",25, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::initRegisterBatchOpsBindings( PyObject * module)",9, 62, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inplace_check.cpp,"torch::jit::CheckInplace( Block * block)",11, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inplace_check.cpp,"torch::jit::CheckInplace( std :: shared_ptr<Graph> & graph)",3, 51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::have_same_shape( at :: TensorList inputs)",7, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::shape_is_fast_for_reduce( const at :: Tensor & lhs , const at :: Tensor & rhs)",7, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::mm( Node * mm)",7, 34, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::transpose( Node * t , TreeToken & inp_token)",12, 63, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::add( Node * add , TreeToken & l , TreeToken & r)",12, 69, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::operator bool()",3, 29, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::removeTransposesAndGatherMatmuls()",33, 84, 14, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMMTreeReduce( Block * block)",60, 82, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::shape_is_fast_for_side( const at :: Tensor & other_side_input)",4, 66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::gatherIndependentMMUses( Value * value , AliasDb & alias_db)",42, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMMSide( Block * block , AliasDb & alias_db)",47, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::hasMutableOperators( Block * block)",11, 57, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMM( std :: shared_ptr<Graph> & graph)",13, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_grad_of.cpp,"torch::jit::LowerGradOf( Graph & g)",28, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::SubgraphSlicer( Block * block , std :: shared_ptr<Graph> graph , size_t minSubgraphSize)",7, 45, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::run( std :: vector<Node*> & diffGraphs)",53, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::inlineIfTooSmall( Node * n)",14, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::sortReverseTopological( ArrayRef<Value*> inputs)",13, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::shouldConsiderForMerge( Node * node)",10, 53, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::scanNode( Node * consumer , AliasDb & aliasDb)",20, 78, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::tryMerge( Node * consumer , Node * producer , AliasDb & aliasDb)",16, 66, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::CreateAutodiffSubgraphs( const std :: shared_ptr<Graph> & graph , size_t threshold)",7, 68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::removeTupleNodes( Node * n , bool must_remove_tuples)",33, 77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::VisitNode( Node * n , Node * insert_point)",70, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerAllTuples( Block * block)",16, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::EnsureNoTuples( ArrayRef<Value*> values)",6, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::EnsureNoTuples( Block * block)",8, 43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerAllTuples( std :: shared_ptr<Graph> & graph)",5, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerSimpleTuples( Block * block)",8, 55, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerSimpleTuples( std :: shared_ptr<Graph> & graph)",4, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::runNode( Node * n)",21, 60, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::propagateNode( Node * n)",25, 69, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::removeLoopNode( Node * n)",8, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::loopWillNotRun( Node * node)",10, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::inlineIfBody( Block * body)",16, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::inlineIf( Node * n , const AliasDb & aliasDb)",7, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::removeExtraIfOutputs( Node * n)",19, 72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::removeExtraLoopOutputs( Node * node)",22, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( Node * n , const AliasDb & aliasDb)",34, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( Block * block , const AliasDb & aliasDb)",7, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( std :: shared_ptr<Graph> & graph)",5, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,"torch::jit::PrepareDivisionForONNXOnBlock( Block * block)",28, 80, 16, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,"torch::jit::PrepareDivisionForONNX( const std :: shared_ptr<Graph> & graph)",3, 67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isRNN( const Node * node)",4, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isNopTranspose( const std :: vector<int64_t> & perm)",6, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::composeTransposes( const std :: vector<int64_t> & t1 , const std :: vector<int64_t> & t2)",12, 40, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::getBroadcastPositions( Node * node)",24, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fusibleExpandTo( at :: IntArrayRef from , at :: IntArrayRef to)",15, 82, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseBroadcast( Block * b)",46, 77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseConsecutiveTransposes( Block * b)",20, 70, 14, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::eliminateNopTranspose( Block * b)",15, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseTransposeIntoGemm( Block * b)",23, 67, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::pushPackingPastRnn( Block * b)",83, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::removeNopPacking( Block * graph)",27, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::hackFixupPadPackedShapes( Block * graph)",17, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultRNNState( Graph * graph , Node * n , int input_index)",76, 97, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultRnnHiddenState( Block * b)",18, 69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultLstmCellState( Block * b)",18, 69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isSafeToSpeculate( Node * n)",3, 41, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::speculateOps( Block * block)",25, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::replaceInputWithList( Node * node , size_t i , ArrayRef<Value*> to)",7, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::eraseListConstruct( Block * block)",61, 77, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseSplitListUnpack( Block * b)",23, 88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::removeMaxPoolUnusedOutput( Block * b)",13, 82, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::PeepholeOptimizeONNX( std :: shared_ptr<Graph> & graph)",18, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,"torch::jit::FixupONNXLoops( Block * block)",12, 45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,"torch::jit::FixupONNXLoops( std :: shared_ptr<Graph> & graph)",3, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepCopy( const IValue & self)",42, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepCopy( const Stack & stack)",8, 37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepEquals( const IValue & lhs , const IValue & rhs)",15, 71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::AliasAndIValue::AliasAndIValue( c10 :: optional<at::AliasInfo> aliasInfo , IValue iValue)",2, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkInputPreconditions( const Stack & inputs)",12, 52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkAliases( const std :: vector<AliasAndIValue> & inputs , const std :: vector<AliasAndIValue> & outputs)",22, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkWrites( const std :: vector<AliasAndIValue> & inputs , const std :: vector<IValue> & deepCopiedInputs)",12, 60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::findNodeForOp( const Graph & g , const std :: string & unqualifiedOpName)",11, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::toIValueProp( const Value * v)",40, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkAliasAnnotation( const std :: shared_ptr<Graph> & graph , std :: vector<IValue> pythonInputs , const std :: string & unqualifiedOpName)",62, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::MemoryDAG::mayAlias( Element * a , Element * b) const",3, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::MemoryDAG::mayAlias( const Element * a , const Element * b) const",3, 69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::MemoryDAG::mayAliasImpl( const Element * a , const Element * b) const",16, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::MemoryDAG::makePointerTo( Element * from , Element * to)",4, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::MemoryDAG::makeFreshValue( const Value * v)",8, 53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::Element::getMemoryLocations() const",18, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/memory_dag.cpp,"torch::jit::Element::bfs( Fn fn , BfsDirection dir) const",32, 51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::hasSubgraph( Node * n)",3, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::mergeSubgraph( Node * mergeTo , Node * mergeFrom)",15, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::getSubgraph( Node * n)",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::unmergeSubgraph( Node * subgraphNode)",12, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::mergeNodeIntoSubgraph( Node * toMerge , Node * subgraphNode)",83, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::inlineGraph( const std :: shared_ptr<Graph> & subgraph , at :: ArrayRef<Value*> outerInputs , Node * insertBefore)",30, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::createSingletonSubgraph( Node * n , Symbol subgraphKind)",8, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::SourceRangeFactory::SourceRangeFactory( std :: string source)",8, 69, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::SourceRangeFactory::create( int line , int start_col , int end_col)",10, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::wrap_list( const SourceRange & fallback_pos , std :: vector<T> && vec)",5, 75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::wrap_maybe( const SourceRange & fallback_pos , T * val)",4, 63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::initTreeViewBindings( PyObject * module)",217, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/edit_distance.cpp,"torch::jit::script::ComputeEditDistance( const char * word1 , const char * word2 , size_t maxEditDistance)",41, 63, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Refinements::setRefinement( const std :: string & name , TypeAndRange mapping)",3, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Refinements::getRefinement( const std :: string & name) const",7, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Refinements::intersectRefinements( const Refinements & other)",16, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Refinements::unionRefinements( const Refinements & other)",31, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::BoolInfo::BoolInfo( Refinements true_refinements , Refinements false_refinements)",3, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::BoolInfo::mergeOr( const BoolInfo & other)",9, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::BoolInfo::mergeAnd( const BoolInfo & other)",9, 74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::asSimple( const SugaredValuePtr & value)",6, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::meaningfulName( const std :: string & name)",13, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::Environment( Method & method , Resolver resolver , Block * b , std :: shared_ptr<Environment> next = nullptr)",9, 51, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setVariableTypeError( const std :: string & name , const std :: string & msg)",7, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findVariableTypeError( const std :: string & name)",12, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInThisFrame( const std :: string & name)",7, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInParentFrame( const std :: string & name)",3, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInAnyFrame( const std :: string & name)",8, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getValueInThisFrame( const SourceRange & loc , const std :: string & name)",3, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::createCapturedInput( Value * orig , const std :: string & name)",23, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::createCapturedInputIfNeeded( const SourceRange & loc , const std :: string & ident)",19, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::block()",3, 19, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getBlockOwningKind()",7, 45, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setVar( const SourceRange & loc , const std :: string & name , Value * value)",3, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setSugaredVar( const SourceRange & loc , const std :: string & name , SugaredValuePtr value)",56, 101, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getSugaredVar( const Ident & ident , bool required = true)",3, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getVar( const Ident & ident)",3, 65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getSugaredVar( const std :: string & ident , const SourceRange & range , bool required = true)",43, 90, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getVar( const std :: string & ident , const SourceRange & range)",3, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::deleteExtraInputs( const SourceRange & loc)",22, 71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::definedVariables()",7, 48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::materializeConstant( T val , Graph & graph , const SourceRange & r , std :: unordered_map<T,Value*> & map)",16, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::ensureInt( const SourceRange & range , Value * v)",7, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isSupportedListElementType( const TypePtr & type)",4, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::to_ir( const Def & def , Resolver resolver_ , const c10 :: optional<Self> & self , Method & method)",24, 78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::pushFrame( Block * b , bool starts_def = false)",7, 79, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::popFrame( bool ends_def = false)",8, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::runCleanupPasses( std :: shared_ptr<Graph> & to_clean)",5, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitDef( const Def & def , const c10 :: optional<Self> & self , Block * block)",18, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::evaluateDefaults( const SourceRange & r , const std :: vector<Expr> & default_types , const std :: vector<Expr> & default_exprs)",35, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::parseArgsFromDecl( const Decl & decl , const c10 :: optional<Self> & self)",53, 73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::parseReturnFromDecl( const Decl & decl)",19, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::extractSchemaFromDef( const Def & def , const c10 :: optional<Self> & self)",9, 70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitFormalArguments( const Def & def , const c10 :: optional<Self> & self , const FunctionSchema & schema , Block * block)",50, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitOutput( const SourceRange & range , const FunctionSchema & schema , Block * block)",11, 69, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStatements( const List<Stmt> & statements)",3, 65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::lambdaLift( Block * block)",28, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitClosure( const Def & def)",27, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitReturn( const Return & stmt)",41, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStatements( List<Stmt> :: const_iterator begin , List<Stmt> :: const_iterator end)",53, 78, 14, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSingleIfBranch( Block * b , const List<Stmt> & branch , const Refinements & refinements)",10, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::create( Symbol kind , const SourceRange & loc , size_t n_outputs)",4, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTernaryIf( const TernaryIf & expr)",13, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::insertRefinements( const Refinements & ref)",13, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitShortCircuitIf( const SourceRange & loc , const TreeRef & first_expr , const TreeRef & second_expr , bool is_or)",38, 76, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIfExpr( const SourceRange & range , Value * cond_value , std :: function<Value*()> true_expr , std :: function<Value*()> false_expr)",36, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitCond( const Expr & cond)",14, 72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIfElseBlocks( Value * cond_value , const If & stmt)",89, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIf( const If & stmt)",59, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitLoopCommon( SourceRange range , c10 :: optional<Expr> max_trip_count , c10 :: optional<Expr> cond , const List<Stmt> & body , c10 :: optional<Ident> itr_ident , bool in_list = false)",101, 79, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitForRange( const SourceRange & range , const Ident & target , const List<Expr> & args , const List<Stmt> & body)",12, 67, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitFor( const For & stmt)",60, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitWhile( const While & stmt)",4, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitRaise( const SourceRange & loc)",5, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAssert( const Assert & stmt)",14, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::calcNumStarredUnpack( const List<Expr> & lhs , const SourceRange & r)",27, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getAugOp( const AugAssign & stmt , bool isTensor)",15, 80, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignment( const AugAssign & stmt)",17, 58, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToSelectVar( const AugAssign & stmt)",27, 77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToVar( const AugAssign & stmt)",30, 77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToSubscript( const AugAssign & stmt)",82, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscriptAssign( const SourceRange & stmtRange , const Subscript & lhs , const Expr & rhs)",6, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscriptAssign( const SourceRange & stmtRange , const Subscript & lhs , const NamedValue & rhs)",56, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleAssign( const TupleLiteral & tl , const Expr & rhs)",58, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAssignment( const Assign & stmt)",21, 78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSelectAssign( const Assign & stmt)",8, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNodeKind( int kind , int ninputs)",54, 74, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNamedValues( const TreeList & trees , bool maybe_unpack)",19, 71, 14, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNamedValues( const List<Expr> & trees , bool maybe_unpack)",5, 64, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getValues( const TreeList & trees , bool maybe_unpack)",3, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getValues( const List<Expr> & trees , bool maybe_unpack)",3, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAttributes( const List<Attribute> & attributes)",6, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::checkApplyExpr( Apply & apply , SourceRange & loc)",11, 81, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitApplyExpr( Apply & apply , size_t n_binders)",100, 99, 14, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::findRefinements( const TreeRef & tree)",44, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitExpr( const Expr & tree , const TypePtr & type_hint = nullptr)",3, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::reverseComparision( NodeKind kind)",13, 65, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSugaredExpr( const Expr & tree , size_t n_binders , const TypePtr & type_hint = nullptr)",20, 79, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitNegate( const TreeRef & tree)",25, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitForkExpr( SourceRange loc , const std :: shared_ptr<SugaredValue> & forked , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes)",28, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSimpleExpr( const TreeRef & tree , const TypePtr & type_hint = nullptr)",145, 80, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitConst( const Const & c)",8, 66, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStringLiteral( const StringLiteral & c)",3, 65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSelect( const SourceRange & loc , Value * input , int64_t dim , Value * index)",14, 66, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSlice( const SourceRange & loc , Value * input , c10 :: optional<int64_t> dim , const SliceExpr & slice)",36, 73, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIndex( const SourceRange & loc , Value * input , at :: ArrayRef<Value*> indices)",13, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIntAndSliceIndexing( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",44, 84, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitMultidimSlicing( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",21, 73, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitBasicSlice( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",14, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getTupleIndexVal( const SourceRange & loc , const TupleTypePtr & tuple_type , Value * idx_val , bool allow_out_of_bounds)",24, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleIndex( const SourceRange & loc , Value * tuple_val , Value * idx_val)",10, 76, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitDictIndex( const SourceRange & loc , Value * dict_val , Value * key_val)",9, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleSlice( const SourceRange & loc , const NamedValue & tuple_val , const NamedValue & beg_val , const at :: optional<NamedValue> & end_val)",23, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscript( const Subscript & subscript)",6, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscript( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",13, 67, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitBasicGather( const SourceRange & loc , Value * gatherable , const List<Expr> & subscript_exprs)",26, 81, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::defineMethodsInModule( const std :: shared_ptr<Module> & m , const std :: vector<Def> & definitions , const std :: vector<Resolver> & resolvers , const c10 :: optional<Self> & self)",41, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::defineMethodsInModule( const std :: shared_ptr<Module> & m , const std :: string & source , const Resolver & resolver , const c10 :: optional<Self> & self)",15, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::lambdaLiftFork( Node * fork_node)",22, 69, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::NoneValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::PrintValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",27, 77, 19, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::builtin_cast_methods()",12, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::BuiltinFunction::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",9, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::SimpleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",64, 81, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::SimpleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint)",25, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::SimpleValue::setAttr( const SourceRange & loc , Method & m , const std :: string & field , Value * newValue)",55, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/sugared_value.cpp,"torch::jit::script::ClassValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",22, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::placeholderCreator( Method &)",3, 36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::try_emit_call_to( Graph & graph , const SourceRange & loc , Method & callee , c10 :: optional<NamedValue> self , ArrayRef<NamedValue> args , ArrayRef<NamedValue> kwargs , std :: stringstream & failure_messages , Method * caller , bool conv_tensors_to_nums)",44, 98, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Method::emit_call_to( const SourceRange & loc , Method & callee , ArrayRef<NamedValue> args , ArrayRef<NamedValue> kwargs)",21, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Method::ensure_defined()",8, 41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: Device device , at :: ScalarType dtype , bool non_blocking)",3, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: ScalarType dtype , bool non_blocking)",3, 59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: Device device , bool non_blocking)",3, 57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::save( std :: ostream & out , const ExtraFilesMap & extra_files)",3, 73, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::save( const std :: string & filename , const ExtraFilesMap & extra_files)",5, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to_impl( const c10 :: optional<at::Device> & device , const c10 :: optional<at::ScalarType> & dtype , bool non_blocking)",21, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/final_returns.cpp,"torch::jit::script::checkNoReturn( const TreeRef & ref)",7, 68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/final_returns.cpp,"torch::jit::script::makeReturnsFinal( const List<Stmt> & stmts , bool return_none)",3, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/final_returns.cpp,"torch::jit::script::makeReturnsFinal( const SourceRange & range , at :: ArrayRef<TreeRef> stmts , bool return_none)",65, 120, 16, 0
repos/cpp/pytorch/torch/csrc/jit/script/final_returns.cpp,"torch::jit::script::moveAllReturnsToEnd( const List<Stmt> & stmts)",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::unwrapOptional( TypePtr opt_type)",6, 64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::isIntOrFloatUsedAsList( const Value * value , const Argument & arg)",11, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::convertibleToList( const TypePtr & type , const TypePtr & list_type_)",18, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::tryConvertToType( const SourceRange & loc , Graph & graph , const TypePtr & concrete_type , Value * value , bool allow_conversions)",64, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::tryMatchArgument( const Argument & arg , Graph & graph , const SourceRange & loc , const NamedValue & named_value , const std :: function<std::ostream&()> & err , bool allow_conversions , TypeEnv & type_env)",41, 81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::findInputWithName( const std :: string & name , at :: ArrayRef<NamedValue> kwargs)",9, 47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::tryCreateList( const TypePtr & elem_type , Graph & graph , const SourceRange & loc , at :: ArrayRef<NamedValue> varargs , const std :: function<std::ostream&()> & err , bool convert_tensor_to_num , TypeEnv & type_env)",19, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::tryMatchSchema( const FunctionSchema & schema , const SourceRange & loc , Graph & graph , c10 :: optional<NamedValue> self , at :: ArrayRef<NamedValue> args , at :: ArrayRef<NamedValue> kwargs , std :: ostream & failure_messages , bool allow_conversions)",132, 110, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::packOutputs( Graph & g , at :: ArrayRef<Value*> values , c10 :: OptNameList field_names)",6, 90, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::emitBuiltinNode( const MatchedSchema & matched_schema , const SourceRange & loc , Graph & graph , Symbol name)",18, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::prefixLine( const std :: string & str , const std :: string & prefix)",13, 33, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_matching.cpp,"torch::jit::script::emitBuiltinCall( const SourceRange & loc , Graph & graph , Symbol name , const c10 :: optional<NamedValue> & self , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , bool required)",76, 91, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::SharedParserData::isUnary( int kind , int * prec)",8, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::SharedParserData::isBinary( int kind , int * prec)",8, 55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::stringToKind( const std :: string & str)",18, 62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::kindToString( int kind)",13, 73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::sharedParserData()",4, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/class_type.cpp,"c10::ClassType::getMethod( const std :: string & name) const",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/script_type_parser.cpp,"torch::jit::script::ident_to_type_lut()",15, 75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/script_type_parser.cpp,"torch::jit::script::subscript_to_type_fns()",59, 80, 13, 0
repos/cpp/pytorch/torch/csrc/jit/script/script_type_parser.cpp,"torch::jit::script::isTorch( const Expr & expr)",3, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/script_type_parser.cpp,"torch::jit::script::parseBroadcastList( const Expr & expr)",52, 90, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/script_type_parser.cpp,"torch::jit::script::parseBaseTypeName( const Expr & expr)",17, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/script_type_parser.cpp,"torch::jit::script::parseTypeFromExpr( const Expr & expr)",27, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::getAllBuiltinFunctionsFor( Symbol name)",22, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::loadSource( const std :: string & source)",10, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::loadBuiltinFunctions()",24, 58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::getAllBuiltinFunctionsFor( Symbol name)",4, 79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_type_parser.cpp,"torch::jit::script::SchemaTypeParser::parseBaseType()",25, 74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_type_parser.cpp,"torch::jit::script::SchemaTypeParser::parseAliasAnnotation()",46, 76, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_type_parser.cpp,"torch::jit::script::SchemaTypeParser::parseTensorDType( const std :: string & dtype)",13, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_type_parser.cpp,"torch::jit::script::SchemaTypeParser::parseRefinedTensor()",32, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_type_parser.cpp,"torch::jit::script::SchemaTypeParser::parseType()",63, 77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/schema_type_parser.cpp,"torch::jit::script::SchemaTypeParser::parseList( int begin , int sep , int end , const std :: function<void()> & callback)",16, 45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::typeString( py :: handle h)",3, 49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::toSimple( Value * v)",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::PythonValue( py :: object self)",1, 58, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::getSchema( const size_t n_args , const size_t n_binders)",49, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs_ , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",42, 76, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::kind() const",5, 63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",14, 62, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::getattr( const SourceRange & loc , const std :: string & name)",7, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonModuleValue::PythonModuleValue( py :: object mod)",1, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonModuleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",10, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::ConstantPythonTupleValue( py :: object tup)",2, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",13, 62, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::asValue( const SourceRange & loc , Method & m)",8, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantParameterList::ConstantParameterList( std :: shared_ptr<Module> module)",2, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantParameterList::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantParameterList::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",17, 73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::ModuleValue( std :: shared_ptr<Module> module)",1, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",58, 81, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",9, 60, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",21, 71, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::BooleanDispatchValue( py :: dict dispatched_fn)",2, 52, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",35, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENOverloadedFunctionValue::OverloadedFunctionValue( py :: list functions)",2, 53, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENOverloadedFunctionValue::kind() const",3, 38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENOverloadedFunctionValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",29, 72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::toSugaredValue( py :: object obj , Method & m , SourceRange loc , bool is_constant , bool is_submodule)",105, 81, 13, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::unpackVariableTensorList( std :: vector<at::Tensor> outputs)",16, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::gatherParametersAndBuffers( std :: vector<IValue*> & values , const Module & m)",15, 55, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::pythonResolver( const ResolutionCallback & rcb)",11, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::getSchemaWithNameAndDefaults( const SourceRange & range , const FunctionSchema & schema , const at :: optional<std::string> & new_name , const FunctionDefaults & default_args)",38, 76, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::initJitScriptBindings( PyObject * module)",412, 95, 16, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::mergeTypesFromTypeComment( const Decl & decl , const Decl & type_annotation_decl , bool is_method)",34, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::ParserImpl( const std :: string & str)",2, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseIdent()",7, 71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::createApply( const Expr & expr)",11, 66, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::followsTuple( int kind)",14, 39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExpOrExpTuple()",14, 61, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseBaseExp()",70, 76, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAssignmentOp()",16, 46, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseTrinary( TreeRef true_branch , const SourceRange & range , int binary_prec)",9, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExp()",3, 24, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExp( int precedence)",42, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSequence( int begin , int sep , int end , const std :: function<void()> & parse)",15, 44, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseList( int begin , int sep , int end , T(ParserImpl::*parse)())",7, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseConst()",5, 45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseConcatenatedStringLiterals()",9, 64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAttributeValue()",3, 31, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseOperatorArguments( TreeList & inputs , TreeList & attributes)",17, 72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSubscriptExp()",19, 76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSubscript( const TreeRef & value)",7, 67, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseParam( bool kwarg_only)",17, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseBareTypeAnnotation()",9, 43, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseTypeComment()",14, 72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAssign( const Expr & lhs)",15, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseStmt()",58, 79, 48, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseOptionalIdentList()",9, 64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseIf( bool expect_if = true)",21, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseWhile()",8, 59, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseFor()",11, 79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseStatements( bool expect_indent = true)",11, 55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseReturnAnnotation()",9, 65, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseParams()",13, 53, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseDecl()",9, 71, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseClass()",16, 71, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseFunction( bool is_method)",18, 80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::lexer()",3, 19, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::c( int kind , const SourceRange & range , TreeList && trees)",3, 68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::makeList( const SourceRange & range , TreeList && trees)",3, 65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::Parser( const std :: string & src)",1, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseFunction( bool is_method)",3, 48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseClass()",3, 31, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::lexer()",3, 25, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseTypeComment()",3, 36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::Check::Check( CheckType type , std :: string str , c10 :: optional<size_t> count = c10 :: nullopt)",7, 51, 6, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::operator < <( std :: ostream & out , const Check & c)",24, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::assertFind( const SourceRange & search_range , const std :: string & sub , const Check & check)",18, 80, 8, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::assertFind( const std :: shared_ptr<std::string> & file , const std :: string & sub , size_t start , const Check & check)",7, 73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::assertNotFind( const SourceRange & search_range , const std :: string & sub , const Check & check)",17, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheckImpl::run( const std :: string & test_file)",4, 56, 4, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheckImpl::addCheck( CheckType type , const std :: string & s , c10 :: optional<size_t> count = c10 :: nullopt)",20, 75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheckImpl::doCheckNot( const std :: vector<Check> & nots , const std :: shared_ptr<std::string> & file , const SourceRange & prev , const SourceRange & next)",15, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheckImpl::matchDagGroup( const std :: vector<Check> & group , const std :: shared_ptr<std::string> & test_file , const SourceRange & prev)",17, 78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheckImpl::matchGroup( const std :: vector<Check> & group , const std :: shared_ptr<std::string> & test_file , const SourceRange & prev)",58, 81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheckImpl::doChecks( const std :: shared_ptr<std::string> & test_file)",23, 75, 10, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::FileCheck()",1, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::operator < <( std :: ostream & out , const FileCheckImpl & fc)",7, 71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::~FileCheck()",7, 67, 4, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::run( const std :: string & test_file)",3, 52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::check( const std :: string & str)",4, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::check_not( const std :: string & str)",4, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::check_same( const std :: string & str)",4, 59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::check_next( const std :: string & str)",4, 59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::check_count( const std :: string & str , size_t count , bool exactly)",10, 45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/testing/file_check.cpp,"torch::jit::testing::FileCheck::check_dag( const std :: string & str)",4, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::getMapSize( const KernelSpec & spec , at :: TensorList args , at :: IntArrayRef arg_subset)",39, 77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::canRunKernel( const KernelSpec & spec , at :: TensorList args)",27, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::expandArgs( const KernelSpec & spec , std :: vector<at::Tensor> & args , std :: vector<int64_t> & map_size , bool dry_run)",32, 55, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::shouldExpandArgs( const KernelSpec & spec , std :: vector<at::Tensor> & args , std :: vector<int64_t> & map_size)",6, 61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::computeNumel( const at :: ArrayRef<int64_t> & sizes)",8, 67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::computeMapSize( const at :: Tensor & tensor , const PartitionDesc & chunkDesc)",8, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::compressContiguous( const at :: IntArrayRef & sizes , const at :: IntArrayRef & strides , const std :: vector<bool> & cont , uint32_t * c_sizes , uint32_t * c_strides)",25, 64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::launchFusion( const FusedKernel & fusion , const at :: Device device , const at :: ArrayRef<at::Tensor> & inputs , std :: vector<at::Tensor> & outputs)",117, 81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::runFusion( const int64_t key , Stack & stack)",79, 84, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/fallback.cpp,"torch::jit::fuser::runFallback( int64_t key , Stack & stack)",7, 77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::getFusionBackends()",5, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::registerFusionBackend( at :: Device :: Type backend_type , FusedKernelConstructor ctor)",6, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::hasFusionBackend( at :: Device :: Type backend_type)",4, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::getConstructor( at :: Device :: Type backend_type)",4, 78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::nCompiledKernels()",3, 32, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::debugFuser()",7, 60, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::usedInFusedChunk( const Value * input)",10, 58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::setInputChunkDescriptors( KernelSpec & spec)",15, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::getInputDependencies( const Value * output)",38, 72, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::setInputBroadcastGroups( KernelSpec & spec)",17, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::processGradSumToSize( KernelSpec & spec)",51, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::upfrontCompilation( KernelSpec & spec)",5, 51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::registerFusion( const Node * fusion_group)",21, 79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::compileKernel( const KernelSpec & spec , const ArgSpec & arg_spec , const std :: vector<int64_t> & map_size , const at :: Device device)",86, 92, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::registerFusion( const Node * fusion_group)",3, 51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::runFusion( const int64_t key , Stack & stack)",5, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::canFuseOnCPU()",4, 57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::canFuseOnGPU()",3, 56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::overrideCanFuseOnCPU( bool value)",3, 40, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::debugLaunchGraph( Graph & graph , at :: ArrayRef<at::Tensor> inputs)",21, 70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::nCompiledKernels()",3, 36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::valueName( const Value * n)",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const int64_t v)",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const bool v)",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const double v)",15, 49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarTypeName( const at :: ScalarType type)",15, 63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::calcScalarTypeName( const at :: ScalarType type)",6, 67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::variableType( const std :: shared_ptr<c10::Type> & t)",15, 74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::typeCastedValueName( const std :: shared_ptr<c10::Type> & t , const at :: ScalarType outtype , const std :: string & vn)",25, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::encodeSpecialRHS( const Node * n , TemplateEnv & env)",28, 81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::encodeRHS( const Node * n)",114, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::emitIndexingFor( std :: ostream & out , const std :: string & tensor , const int ndim , const bool last_is_cont)",23, 79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::generateKernel( const std :: string & name , const Graph & graph , const std :: vector<std::pair<const Value*,const TensorDesc>> & inputs , const std :: vector<std::pair<const Value*,const TensorDesc>> & outputs , const bool use_cuda)",157, 84, 8, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::getKernelCache()",4, 43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::debugNumCachedKernelSpecs()",5, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::normalizeGraphForCache( const std :: shared_ptr<Graph> & graph)",6, 66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::store( std :: shared_ptr<Graph> graph)",13, 67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::nolock_retrieve( KernelCacheImpl & cache , const int64_t key)",8, 50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::retrieve( const int64_t key)",5, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::lookupGraph( std :: shared_ptr<Graph> graph)",10, 70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_win.cpp,"torch::jit::fuser::cpu::DynamicLibrary::DynamicLibrary( const char * name)",4, 51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_win.cpp,"torch::jit::fuser::cpu::DynamicLibrary::sym( const char * name)",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_win.cpp,"torch::jit::fuser::cpu::DynamicLibrary::directoryOf( void * addr)",3, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_win.cpp,"torch::jit::fuser::cpu::DynamicLibrary::~DynamicLibrary()",1, 37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_unix.cpp,"torch::jit::fuser::cpu::checkDL( void * x)",7, 55, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_unix.cpp,"torch::jit::fuser::cpu::DynamicLibrary::DynamicLibrary( const char * name)",4, 57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_unix.cpp,"torch::jit::fuser::cpu::DynamicLibrary::sym( const char * name)",4, 46, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_unix.cpp,"torch::jit::fuser::cpu::DynamicLibrary::~DynamicLibrary()",5, 36, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/dynamic_library_unix.cpp,"torch::jit::fuser::cpu::DynamicLibrary::directoryOf( void * addr)",10, 54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::programExists( const std :: string & program)",6, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::CompilerConfig::CompilerConfig()",10, 41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::getConfig()",4, 37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::runCompiler( const std :: string & cpp_file , const std :: string & so_file)",19, 95, 8, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::disas( const std :: string & so_file)",7, 48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::FusedKernelCPU::FusedKernelCPU( std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",29, 80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::createFusionKernel( int16_t device , std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",18, 56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/thnvrtc.cpp,"torch_load_nvrtc()",6, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::checkCUDAVersion( const cudaDeviceProp & prop)",11, 83, 8, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::loadNVRTC()",3, 72, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::loadNVRTC()",12, 80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::nvrtc()",5, 60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::nvrtcCheck( nvrtcResult result , const char * file , int line)",7, 80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::cuCheck( CUresult result , const char * file , int line)",9, 74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::getMajorMinor( const cudaDeviceProp * const prop , int & major , int & minor)",35, 75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::FusedKernelCUDA( int16_t device , std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",78, 76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::ceilDiv( const int a , const int b)",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::launch_raw( const uint32_t numel , std :: vector<void*> & arguments) const",40, 74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::~FusedKernelCUDA()",3, 38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::createFusionKernel( int16_t device , std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",19, 56, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/jit.cpp,"torch::jit::compile( const std :: string & source)",6, 70, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::device_count()",3, 50, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::is_available()",6, 81, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::cudnn_is_available()",3, 66, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::join_name( const std :: string & name_prefix , const std :: string & name)",14, 81, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::extend( std :: vector<Tensor> & vector , const OrderedDict<std::string,Tensor> & dict)",8, 52, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::Module()",2, 78, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::Module( std :: string name)",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::name() const",24, 81, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::clone( const optional<Device> & device) const",8, 78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ModuleApplyFunction & function)",7, 79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ConstModuleApplyFunction & function) const",7, 79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const NamedModuleApplyFunction & function , const std :: string & name_prefix)",11, 76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ConstNamedModuleApplyFunction & function , const std :: string & name_prefix) const",11, 76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ModulePointerApplyFunction & function) const",7, 79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const NamedModulePointerApplyFunction & function , const std :: string & name_prefix) const",7, 57, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::parameters( bool recurse) const",9, 80, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_parameters( bool recurse) const",12, 80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::buffers( bool recurse) const",8, 79, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_buffers( bool recurse) const",12, 77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::modules( bool include_self) const",14, 80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_modules( const std :: string & name_prefix , bool include_self) const",21, 77, 12, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::children() const",3, 64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_children() const",4, 75, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::train( bool on)",6, 34, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::eval()",3, 23, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Device device , torch :: Dtype dtype , bool non_blocking)",3, 79, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Dtype dtype , bool non_blocking)",3, 57, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Device device , bool non_blocking)",3, 59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::is_training() const",3, 44, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::zero_grad()",12, 40, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::save( serialize :: OutputArchive & archive) const",13, 69, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::load( serialize :: InputArchive & archive)",13, 68, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::register_parameter( std :: string name , Tensor tensor , bool requires_grad)",13, 65, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::register_buffer( std :: string name , Tensor tensor)",9, 67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::pretty_print( std :: ostream & stream) const",3, 56, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::pretty_print_recursive( std :: ostream & stream , const std :: string & indentation) const",15, 71, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::clone_( Module & other , const optional<Device> & device)",1, 70, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply_to_submodules( const NamedModulePointerApplyFunction & function , const std :: string & name_prefix) const",9, 66, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::shared_from_this_checked() const",17, 75, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator < <( std :: ostream & stream , const nn :: Module & module)",4, 75, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator < <( serialize :: OutputArchive & archive , const std :: shared_ptr<nn::Module> & module)",7, 64, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator > >( serialize :: InputArchive & archive , const std :: shared_ptr<nn::Module> & module)",7, 66, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::Fan::Fan( Tensor & tensor)",14, 91, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::calculate_kaiming_std( Tensor tensor , double a , FanMode mode , Nonlinearity nonlinearity)",16, 53, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::calculate_gain( Nonlinearity nonlinearity , double param)",11, 65, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::constant_( Tensor tensor , Scalar value)",4, 48, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::dirac_( Tensor tensor)",27, 68, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::eye_( Tensor matrix)",6, 81, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::normal_( Tensor tensor , double mean , double std)",4, 57, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::ones_( Tensor tensor)",4, 30, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::orthogonal_( Tensor tensor , double gain)",32, 75, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::sparse_( Tensor tensor , double sparsity , double std)",21, 81, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::uniform_( Tensor tensor , double low , double high)",4, 58, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::kaiming_uniform_( Tensor tensor , double a , FanMode mode , Nonlinearity nonlinearity)",11, 67, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::kaiming_normal_( Tensor tensor , double a , FanMode mode , Nonlinearity nonlinearity)",10, 67, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::xavier_normal_( Tensor tensor , double gain)",7, 63, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::xavier_uniform_( Tensor tensor , double gain)",8, 63, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::zeros_( Tensor tensor)",4, 31, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::detail::DropoutImplBase<Derived>::DropoutImplBase( DropoutOptions options_)",5, 77, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::detail::DropoutImplBase<Derived>::reset()",1, 42, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutOptions::DropoutOptions( double rate)",1, 61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutImpl::forward( const Tensor & input)",3, 68, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutImpl::pretty_print( std :: ostream & stream) const",3, 64, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::FeatureDropoutImpl::forward( const Tensor & input)",3, 76, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::FeatureDropoutImpl::pretty_print( std :: ostream & stream) const",3, 71, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormOptions::BatchNormOptions( int64_t features)",1, 78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::BatchNormImpl( BatchNormOptions options)",3, 76, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::reset()",14, 76, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::pretty_print( std :: ostream & stream) const",7, 81, 9, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::forward( const Tensor & input)",8, 59, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::pure_forward( const Tensor & input , const Tensor & mean , const Tensor & variance)",22, 76, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearOptions::LinearOptions( int64_t in , int64_t out)",1, 78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::LinearImpl( LinearOptions options)",3, 67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::reset()",13, 79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::pretty_print( std :: ostream & stream) const",5, 77, 9, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::forward( const Tensor & input)",4, 52, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingOptions::EmbeddingOptions( int64_t count , int64_t dimension)",2, 69, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::EmbeddingImpl( EmbeddingOptions options)",3, 76, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::reset()",6, 69, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::pretty_print( std :: ostream & stream) const",4, 63, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::forward( const Tensor & input)",3, 54, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNOptionsBase::RNNOptionsBase( int64_t input_size , int64_t hidden_size)",2, 72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::RNNImplBase( const RNNOptionsBase & options_ , optional<CuDNNMode> cudnn_mode , int64_t number_of_gates)",9, 43, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::reset()",36, 75, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Device device , torch :: Dtype dtype , bool non_blocking)",7, 47, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Dtype dtype , bool non_blocking)",4, 71, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Device device , bool non_blocking)",4, 73, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::pretty_print( std :: ostream & stream) const",8, 79, 9, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::flatten_parameters()",19, 65, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::generic_forward( std :: function<RNNFunctionSignature> function , const Tensor & input , Tensor state)",23, 79, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::flat_weights() const",14, 77, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::any_parameters_alias() const",13, 80, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::RNNOptions( int64_t input_size , int64_t hidden_size)",2, 64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::tanh()",3, 42, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::relu()",3, 42, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::RNNImpl( const RNNOptions & options)",10, 76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::pretty_print( std :: ostream & stream) const",8, 79, 9, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::forward( const Tensor & input , Tensor state)",16, 64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::LSTMImpl::LSTMImpl( const LSTMOptions & options)",5, 47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::LSTMImpl::forward( const Tensor & input , Tensor state)",27, 81, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::GRUImpl::GRUImpl( const GRUOptions & options)",5, 44, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::GRUImpl::forward( const Tensor & input , Tensor state)",4, 81, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::ConvImpl( ConvOptions<D> options)",4, 55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::reset()",39, 77, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::pretty_print( std :: ostream & stream) const",7, 70, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv1dImpl::forward( const Tensor & input)",21, 50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv2dImpl::forward( const Tensor & input)",21, 50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv3dImpl::forward( const Tensor & input)",22, 50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::FunctionalImpl( Function function)",2, 50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::reset()",1, 32, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::pretty_print( std :: ostream & stream) const",3, 64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::forward( Tensor input)",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::operator ( )( Tensor input)",3, 50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/python/init.cpp,"torch::python::bind_ordered_dict( py :: module module , const char * dict_name)",20, 74, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/python/init.cpp,"torch::python::init_bindings( PyObject * module)",11, 76, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::OptimizerBase( std :: vector<Tensor> parameters)",2, 61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::add_parameters( const std :: vector<Tensor> & parameters)",3, 79, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::zero_grad()",8, 40, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::parameters() const",3, 72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::parameters()",3, 60, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::size() const",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::buffer_at( std :: vector<Tensor> & buffers , size_t index)",16, 79, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::save( serialize :: OutputArchive & archive) const",1, 69, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::load( serialize :: InputArchive & archive)",1, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::operator < <( serialize :: OutputArchive & archive , const OptimizerBase & optimizer)",6, 39, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::operator > >( serialize :: InputArchive & archive , OptimizerBase & optimizer)",6, 38, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGSOptions::LBFGSOptions( double learning_rate)",2, 49, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::gather_flat_grad()",7, 48, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::add_grad( const torch :: Tensor & step_size , const Tensor & update)",11, 77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::step( LossClosure closure)",119, 76, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::save( serialize :: OutputArchive & archive) const",3, 60, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::load( serialize :: InputArchive & archive)",3, 53, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/serialize.cpp,"torch::optim::serialize( serialize :: OutputArchive & archive , const std :: string & key , const std :: vector<int64_t> & steps)",11, 66, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/serialize.cpp,"torch::optim::serialize( serialize :: InputArchive & archive , const std :: string & key , std :: vector<int64_t> & steps)",11, 43, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSpropOptions::RMSpropOptions( double learning_rate)",2, 53, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::step()",36, 78, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::save( serialize :: OutputArchive & archive) const",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::load( serialize :: InputArchive & archive)",3, 55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::AdamOptions::AdamOptions( double learning_rate)",2, 47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::step()",38, 81, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::save( serialize :: OutputArchive & archive) const",3, 59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::load( serialize :: InputArchive & archive)",3, 52, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::AdagradOptions::AdagradOptions( double learning_rate)",2, 53, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::step()",23, 72, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::save( serialize :: OutputArchive & archive) const",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::load( serialize :: InputArchive & archive)",3, 55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGDOptions::SGDOptions( double learning_rate)",1, 80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::step()",32, 75, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::save( serialize :: OutputArchive & archive) const",3, 67, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::load( serialize :: InputArchive & archive)",3, 67, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::InputArchive()",2, 58, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::read( const std :: string & key , Tensor & tensor , bool is_buffer)",30, 72, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::read( const std :: string & key , InputArchive & archive)",8, 73, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::load_from( const std :: string & filename , c10 :: optional<torch::Device> device)",4, 62, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::load_from( std :: istream & stream , c10 :: optional<torch::Device> device)",4, 62, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::OutputArchive()",2, 58, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::write( const std :: string & key , const Tensor & tensor , bool is_buffer)",6, 55, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::write( const std :: string & key , OutputArchive & nested_archive)",5, 57, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::save_to( const std :: string & filename)",4, 59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::save_to( std :: ostream & stream)",4, 52, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::check_is_little_endian()",4, 58, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::flip_endianness( uint32_t value)",4, 68, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_int32( std :: ifstream & stream)",6, 73, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::expect_int32( std :: ifstream & stream , uint32_t expected)",8, 79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::join_paths( std :: string head , const std :: string & tail)",7, 68, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_images( const std :: string & root , bool train)",19, 76, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_targets( const std :: string & root , bool train)",15, 78, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::MNIST( const std :: string & root , Mode mode)",3, 60, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::get( size_t index)",3, 44, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::size() const",3, 39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::is_train() const",3, 40, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::images() const",3, 38, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::targets() const",3, 39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::DistributedRandomSampler( size_t size , size_t num_replicas , size_t rank , bool allow_duplicates)",12, 70, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::next( size_t batch_size)",16, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::reset( optional<size_t> new_size)",8, 66, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::populate_indices()",15, 70, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::save( serialize :: OutputArchive & archive) const",10, 79, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::load( serialize :: InputArchive & archive)",11, 72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedRandomSampler::index() const",3, 58, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::DistributedSequentialSampler( size_t size , size_t num_replicas , size_t rank , bool allow_duplicates)",11, 70, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::next( size_t batch_size)",21, 66, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::reset( optional<size_t> new_size)",9, 70, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::populate_indices()",5, 56, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::save( serialize :: OutputArchive & archive) const",7, 58, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::load( serialize :: InputArchive & archive)",5, 76, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/distributed.cpp,"torch::data::samplers::DistributedSequentialSampler::index() const",3, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::RandomSampler( int64_t size , Dtype index_dtype)",2, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::reset( optional<size_t> new_size)",7, 78, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::next( size_t batch_size)",19, 79, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::save( serialize :: OutputArchive & archive) const",10, 68, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::load( serialize :: InputArchive & archive)",12, 61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::index() const",3, 47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::SequentialSampler( size_t size)",1, 67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::reset( optional<size_t> new_size)",6, 59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::next( size_t batch_size)",11, 76, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::save( serialize :: OutputArchive & archive) const",6, 72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::load( serialize :: InputArchive & archive)",8, 65, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::index() const",3, 51, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::BatchSize( size_t size)",1, 51, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::size() const",3, 42, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::operator size_t() const",3, 46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::StreamSampler( size_t epoch_size)",1, 77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::reset( optional<size_t> new_size)",6, 55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::next( size_t batch_size)",11, 63, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::save( serialize :: OutputArchive & archive) const",7, 76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::load( serialize :: InputArchive & archive)",8, 61, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( New)( THWStorage * ptr)",12, 56, 2, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( dealloc)( THPStorage * self)",5, 51, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( newWithAllocator)( int64_t size , at :: Allocator * allocator)",9, 89, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( pynew)( PyTypeObject * type , PyObject * args , PyObject * kwargs)",102, 91, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( length)( THPStorage * self)",6, 56, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( get)( THPStorage * self , PyObject * index)",49, 102, 18, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( set)( THPStorage * self , PyObject * index , PyObject * value)",36, 90, 6, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( initCopyMethods)()",44, 115, 2, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( init)( PyObject * module)",17, 78, 2, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( postInit)( PyObject * module)",12, 97, 2, 0
repos/cpp/pytorch/torch/csrc/generic/serialization.cpp,"THPStorage_( writeFileRaw)( THWStorage * self , io fd)",44, 121, 2, 0
repos/cpp/pytorch/torch/csrc/generic/serialization.cpp,"THPStorage_( readFileRaw)( io file , THWStorage * _storage)",65, 124, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedDecref)( THPStorage * self)",14, 88, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedIncref)( THPStorage * self)",13, 88, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( __newHandle)()",12, 51, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newFilenameStorage)( ptrdiff_t size)",7, 126, 6, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( pyNewFilenameStorage)( PyObject * _unused , PyObject * args)",10, 87, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareFilename)( THPStorage * self)",33, 82, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedFilename)( PyObject * _unused , PyObject * args)",24, 111, 12, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newFdStorage)( ptrdiff_t size)",10, 100, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( pyNewFdStorage)( PyObject * _unused , PyObject * args)",10, 81, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareFd)( THPStorage * self)",28, 76, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedFd)( PyObject * _unused , PyObject * args)",30, 104, 12, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareCuda)( THPStorage * self)",39, 127, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedCuda)( PyObject * _unused , PyObject * args)",46, 91, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( weakRef)( THPStorage * self , PyObject * args)",6, 75, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newWithWeakPtr)( PyObject * _unused , PyObject * arg)",12, 74, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( freeWeakRef)( PyObject * _unused , PyObject * arg)",14, 70, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( expired)( PyObject * _unused , PyObject * arg)",8, 86, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedFd)( THPStorage * self)",13, 70, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( isShared)( THPStorage * self)",13, 69, 6, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( size)( THPStorage * self)",6, 72, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( dataPtr)( THPStorage * self)",6, 75, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( copy_)( PyObject * self , PyObject * args , PyObject * kwargs)",6, 87, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( isPinned)( THPStorage * self)",20, 99, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( elementSize)( THPStorage * self)",6, 74, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( new)( THPStorage * self)",9, 69, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( resize_)( THPStorage * self , PyObject * number_arg)",11, 79, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fill_)( THPStorage * self , PyObject * number_arg)",11, 83, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fromBuffer)( PyObject * _unused , PyObject * args , PyObject * keywds)",96, 97, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fromFile)( PyObject * _unused , PyObject * args , PyObject * keywds)",17, 93, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( writeFile)( THPStorage * self , PyObject * args)",18, 79, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( newWithFile)( PyObject * _unused , PyObject * file)",13, 71, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( setFromFile)( THPStorage * self , PyObject * args)",35, 82, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( getDevice)( THPStorage * self)",6, 77, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( _setCdata)( THPStorage * self , PyObject * new_cdata)",14, 81, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::PyTensorType::aten_type()",10, 139, 6, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::unavailable_type( const PyTensorType & type)",4, 104, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",10, 87, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_instancecheck( PyTensorType * self , PyObject * arg)",17, 78, 4, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_dtype( PyTensorType * self)",3, 52, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_layout( PyTensorType * self)",3, 53, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_is_cuda( PyTensorType * self)",7, 47, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_is_sparse( PyTensorType * self)",7, 53, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_initialize_metaclass( PyTypeObject & metaclass)",12, 65, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_initialize_tensor_type( PyTypeObject & type , const char * name , PyObject * tp_dict)",20, 97, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_module( Backend backend)",9, 63, 4, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_name( Backend backend , ScalarType scalarType)",5, 72, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_storage_obj( const Type & type)",12, 88, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_type( PyTensorType & type_obj , Backend backend , ScalarType scalarType)",9, 91, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_name( PyTensorType & type_obj , const std :: string & name)",5, 72, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_tensor_dict()",22, 77, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::initialize_aten_types( std :: vector<PyTensorType> & tensor_types)",13, 77, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::initialize_python_bindings()",28, 94, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_bind_tensor_types( const std :: vector<PyTensorType> & tensor_types)",26, 101, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::PyTensorType_Check( PyObject * obj)",7, 67, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_tensor_type( THPDtype * dtype , THPLayout * layout , bool is_cuda)",10, 89, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_set_default_tensor_type( PyObject * obj)",13, 49, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_set_default_dtype( PyObject * obj)",15, 94, 28, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_default_tensor_type( const at :: Type & type)",25, 91, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_default_tensor_type()",4, 38, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"libshm_init( const char * manager_exec_path)",2, 50, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"deleteTHManagedMapAllocator( void * ptr)",3, 53, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"THManagedMapAllocator::makeDataPtr( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",4, 126, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"THManagedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 85, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"get_alloc_info( const char * filename)",11, 73, 4, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"start_manager()",40, 87, 4, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"get_manager_socket( const std :: string & manager_handle)",10, 71, 4, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"libshm_init( const char * manager_exec_path)",3, 60, 2, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocatorInit::THManagedMapAllocatorInit( const char * manager_handle , const char * filename)",21, 103, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::THManagedMapAllocator( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",2, 122, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::close()",8, 62, 2, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"deleteTHManagedMapAllocator( void * ptr)",3, 53, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::makeDataPtr( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",4, 126, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 85, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"ClientSession::ClientSession( ManagerSocket s)",1, 66, 2, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"register_fd( int fd)",6, 27, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"unregister_fd( int fd)",7, 66, 8, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"print_init_message( const char * message)",5, 47, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"object_exists( const char * name)",9, 40, 2, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"free_used_object( const std :: string & name)",8, 65, 4, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"main( int argc , char * argv [ ])",84, 87, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_tensor( std :: shared_ptr<thd::DataChannel> data_channel)",14, 77, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_tensor_any_source( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",21, 76, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_scalar( std :: shared_ptr<thd::DataChannel> data_channel)",14, 77, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_broadcast( std :: shared_ptr<thd::DataChannel> data_channel)",12, 74, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"_test_reduce_helper( std :: shared_ptr<thd::DataChannel> data_channel , THDReduceOp op_type , int64_t init_value , int64_t expected_value)",15, 69, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_reduce( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",19, 80, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"_test_allReduce_helper( std :: shared_ptr<thd::DataChannel> data_channel , THDReduceOp op_type , int64_t init_value , int64_t expected_value)",16, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allReduce( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",17, 79, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_scatter( std :: shared_ptr<thd::DataChannel> data_channel)",18, 68, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_gather( std :: shared_ptr<thd::DataChannel> data_channel)",21, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allGather( std :: shared_ptr<thd::DataChannel> data_channel)",13, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_barrier( std :: shared_ptr<thd::DataChannel> data_channel)",20, 76, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_isend( std :: shared_ptr<thd::DataChannel> data_channel)",23, 70, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_irecv( std :: shared_ptr<thd::DataChannel> data_channel)",25, 70, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_interlaces( std :: shared_ptr<thd::DataChannel> data_channel)",34, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_broadcast_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",17, 65, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_reduce_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",24, 72, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allReduce_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",14, 76, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_scatter_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",27, 78, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_gather_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",31, 77, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allGather_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",23, 68, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_barrier_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",31, 78, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_invalid_rank( std :: shared_ptr<thd::DataChannel> data_channel)",23, 77, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_empty_group( std :: shared_ptr<thd::DataChannel> data_channel)",6, 72, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_process_not_in_group( std :: shared_ptr<thd::DataChannel> data_channel)",29, 81, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_tensors_do_not_match_group_size( std :: shared_ptr<thd::DataChannel> data_channel)",28, 70, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_tensors_are_not_the_same( std :: shared_ptr<thd::DataChannel> data_channel)",29, 81, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"run_all_tests( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",33, 59, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_tcp_master( int workers)",17, 67, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_tcp_worker( unsigned int id , int workers)",14, 70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_gloo_master( int workers)",14, 67, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_gloo_worker( unsigned int id , int workers)",16, 70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_mpi_process()",7, 78, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"main( int argc , char const * argv [ ])",66, 77, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"master()",22, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"worker( int id)",17, 70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"main()",13, 54, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_mpi_smoke.cpp,"main( int argc , char ** argv)",18, 73, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"master()",18, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"worker( int id)",15, 70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"main()",13, 54, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"test( std :: shared_ptr<thd::DataChannel> data_channel)",12, 74, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"run_all_tests( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",10, 73, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"init_gloo_master( int workers)",12, 67, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"init_gloo_worker( unsigned int id , int workers)",14, 70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"main( void)",22, 75, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/tensor_smoke.cpp,"main()",35, 66, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_accept_timeout.cpp,"master()",9, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_accept_timeout.cpp,"main()",6, 37, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDProcessGroupInit( THDChannelType channel_type , std :: string init_method = 'env://' , int world_size = - 1 , std :: string group_name = '' , int rank = - 1)",12, 75, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDProcessGroupDestroy()",8, 32, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDClearGroupCache( THDGroup group)",7, 42, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGetRank()",3, 51, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGetNumProcesses()",3, 59, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllReduceMultiGPU( THDTensorDescriptor * data , size_t len , THDReduceOp operation , THDGroup group)",8, 53, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllReduce( THDTensorDescriptor & desc , THDReduceOp operation , THDGroup group)",6, 50, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDReduceMultiGPU( THDTensorDescriptor * desc , size_t len , THDReduceOp operation , int dst_rank , THDGroup group)",9, 75, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDReduce( THDTensorDescriptor & desc , THDReduceOp operation , int dst_rank , THDGroup group)",7, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBroadcastMultiGPU( THDTensorDescriptor * desc , size_t len , int src_rank , THDGroup group)",8, 67, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBroadcast( THDTensorDescriptor & desc , int src_rank , THDGroup group)",3, 77, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDIsend( THDTensorDescriptor & desc , int dst_rank)",3, 64, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDIrecv( THDTensorDescriptor & desc , int src_rank)",3, 64, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDSend( THDTensorDescriptor & desc , int dst_rank)",3, 56, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRecvAnySource( THDTensorDescriptor & desc)",3, 50, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRecv( THDTensorDescriptor & desc , int src_rank)",3, 56, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllGatherMultiGPU( THDTensorDescriptor * output , size_t outputLen , THDTensorDescriptor * input , size_t inputLen , THDGroup group)",10, 65, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllGather( THDTensorDescriptor * output , size_t len , THDTensorDescriptor & input , THDGroup group)",8, 58, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGatherSend( THDTensorDescriptor & input , int dst_rank , THDGroup group)",4, 79, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGatherRecv( THDTensorDescriptor * output , size_t len , THDTensorDescriptor & input , THDGroup group)",8, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDScatterSend( THDTensorDescriptor * input , size_t len , THDTensorDescriptor & output , THDGroup group)",8, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDScatterRecv( THDTensorDescriptor & output , int src_rank , THDGroup group)",7, 81, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBarrier( THDGroup group)",3, 34, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDNewGroup( const int * ranks , size_t len)",8, 53, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRequest_isCompleted( THDRequest * request)",3, 51, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRequest_wait( THDRequest * request)",3, 44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::setSocketNoDelay( int socket)",5, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::getSocketPort( int fd)",19, 74, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::splitAddress( const std :: string & addr)",26, 80, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::sockaddrToString( struct sockaddr * addr)",15, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::listen( port_type port)",51, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::connect( const std :: string & address , port_type port , bool wait , int timeout)",99, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::accept( int listen_socket , int timeout)",36, 76, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::newChannel( THDChannelType type , std :: string init_method , int world_size , std :: string group_name , int rank)",38, 77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::Group()",1, 31, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::Group( std :: vector<rank_type> ranks , rank_type max_rank)",18, 78, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::~Group()",1, 32, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::size() const",3, 53, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::mustGetGroupRank( rank_type global_rank) const",13, 77, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::getGroupRank( rank_type global_rank) const",8, 70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::mustGetGlobalRank( rank_type group_rank) const",15, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::getGlobalRank( rank_type group_rank) const",7, 67, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDSetCudaStatePtr( THCState ** state)",3, 44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDRegisterCudaStream( cudaStream_t stream)",3, 50, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDGetStreamId( cudaStream_t stream)",10, 64, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannelRequest.cpp,"THDRequest_free( void * request)",3, 46, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::lockLoop( int fd , struct flock & oflock)",11, 62, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::lockFile( int fd)",8, 41, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::unlockFile( int fd)",8, 41, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::waitForGroup( std :: string file_path , std :: string group_name , std :: fstream & file)",46, 76, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::waitForData( int fd , std :: fstream & file , rank_type world_size)",18, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::parseFile( std :: fstream & file , rank_type world_size , std :: string group_name)",46, 78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::initFile( std :: string argument , int world_size_r , std :: string group_name , int assigned_rank)",82, 72, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::getRandomString()",22, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::MulticastMessage( std :: string group_name , port_type port , int rank)",6, 69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::MulticastMessage( std :: string msg)",6, 57, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::pack()",8, 66, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::isMulticastAddress( struct sockaddr * address)",16, 63, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::bindMulticastSocket( struct sockaddr * address , struct sockaddr_storage * sock_addr , int timeout_sec = 1 , int ttl = 1)",69, 80, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::getMessages( struct sockaddr * addr , rank_type world_size , std :: string group_name , std :: string packed_msg)",68, 76, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCPMaster( std :: string address , std :: string str_port , rank_type world_size , int assigned_rank)",29, 71, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCPMulticast( std :: string group_name , rank_type world_size , int assigned_rank , struct sockaddr * addr)",69, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCP( std :: string argument , int world_size_r , std :: string group_name , int rank)",48, 78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::sendPeerName( int socket)",12, 74, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::getInterfaceAddresses()",21, 78, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::discoverWorkers( int listen_socket , rank_type world_size)",15, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::discoverMaster( std :: vector<std::string> addresses , port_type port)",25, 64, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::getRank( const std :: vector<int> & ranks , int assigned_rank , size_t order)",26, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethod.cpp,"thd::getInitConfig( std :: string argument , int world_size , std :: string group_name , int rank)",17, 71, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::mustGetEnv( const char * env)",9, 65, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::loadWorkerEnv()",5, 61, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::maybeLoadEnv( const char * env_name , int value , std :: string parameter_name)",21, 58, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::initEnv( std :: string argument , int world_size_r , std :: string group_name , int rank)",30, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::log2ceil( uint32_t value)",12, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::pow2( T value)",8, 32, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::RequestTCP( QueueWorker :: Request && request)",2, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::~RequestTCP()",1, 45, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::isCompleted()",3, 49, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::wait()",3, 42, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::DataChannelTCP( InitMethod :: Config config)",2, 58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::DataChannelTCP( InitMethod :: Config config , int timeout)",28, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::~DataChannelTCP()",9, 59, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::destroy()",1, 34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::initWorker()",54, 80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::initMaster()",55, 78, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::init()",14, 76, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::getRank()",3, 38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::getNumProcesses()",3, 46, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",48, 80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",39, 77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",39, 77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",78, 79, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",51, 78, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",48, 78, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::send( Scalar & data , rank_type dst_rank)",5, 65, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::send( at :: Tensor & data , rank_type dst_rank)",5, 66, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( Scalar & data , rank_type src_rank)",5, 68, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( at :: Tensor & data)",37, 79, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( at :: Tensor & data , rank_type src_rank)",5, 69, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::isend( at :: Tensor & data , rank_type dst_rank)",7, 64, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::ireceive( at :: Tensor & data , rank_type src_rank)",7, 67, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::barrier( THDGroup group_id)",40, 81, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::newGroup( const std :: vector<rank_type> & ranks)",7, 73, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_send( const Scalar & data , rank_type dst_rank)",20, 80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_send( const at :: Tensor & data , rank_type dst_rank)",23, 80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_receive( Scalar & data , rank_type src_rank)",27, 81, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_receive( const at :: Tensor & data , rank_type src_rank)",31, 81, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_reduce( at :: Tensor & result , at :: Tensor & data , THDReduceOp operation) const",18, 60, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",8, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",8, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",9, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",8, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::clearGroupCache( THDGroup group_id)",5, 58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::RequestMPI()",1, 44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::~RequestMPI()",6, 44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::isCompleted()",5, 79, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::wait()",3, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::save_buffer( std :: shared_ptr<T> ptr)",3, 71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::save_tensor_buffer( at :: Tensor & t)",3, 69, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::new_request()",4, 57, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::DataChannelMPI()",1, 67, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::~DataChannelMPI()",9, 57, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::destroy()",1, 34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::init()",38, 105, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::getRank()",3, 38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::getNumProcesses()",3, 46, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::_newLikeFlat( std :: vector<at::Tensor> & tensors) const",13, 79, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",31, 78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",45, 77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",45, 78, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",19, 71, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",25, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",20, 75, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::send( Scalar & data , rank_type dst_rank)",9, 62, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::send( at :: Tensor & data , rank_type dst_rank)",12, 66, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( Scalar & data , rank_type src_rank)",10, 65, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( at :: Tensor & data)",15, 67, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( at :: Tensor & data , rank_type src_rank)",13, 69, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::barrier( THDGroup group_id)",7, 50, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::isend( at :: Tensor & data , rank_type dst_rank)",20, 64, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::ireceive( at :: Tensor & data , rank_type src_rank)",20, 67, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::newGroup( const std :: vector<rank_type> & ranks)",43, 81, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",8, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",8, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",9, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",8, 56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::clearGroupCache( THDGroup group_id)",5, 58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::_getNcclDataType( at :: ScalarType type)",7, 72, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::getDevicesList( const std :: string & deviceSeq)",9, 64, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::DataChannelNccl( InitMethod :: Config config , int timeout)",39, 79, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcastUniqueNcclId( ncclUniqueId * ncclId)",12, 81, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::~DataChannelNccl()",7, 80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_destroySockets()",17, 62, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::destroy()",24, 71, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_destroyNcclResources( THDGroup groupId)",21, 80, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::clearGroupCache( THDGroup groupId)",8, 58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::init()",16, 81, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::getRank()",3, 39, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::getNumProcesses()",3, 47, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_getNcclResourcePair( std :: vector<at::Tensor> & input , THDGroup groupId)",87, 81, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_tensorCheckHelper( const std :: vector<at::Tensor> & input , const std :: vector<at::Tensor> & output , size_t outputOverInput)",77, 81, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",47, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup groupId)",7, 44, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",46, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup groupId)",7, 50, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",50, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",8, 48, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",47, 80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcast( at :: Tensor & data , rank_type srcRank , THDGroup groupId)",7, 44, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::barrier( THDGroup groupId)",3, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::newGroup( const std :: vector<rank_type> & ranks)",28, 74, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_checkGroupIdValid( THDGroup groupId)",7, 66, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dstRank , THDGroup groupId)",7, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type srcRank , THDGroup groupId)",7, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::send( Scalar & data , rank_type dstRank)",3, 69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::send( at :: Tensor & data , rank_type dstRank)",3, 69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( Scalar & data , rank_type srcRank)",3, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( at :: Tensor & data)",5, 55, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( at :: Tensor & data , rank_type srcRank)",3, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::isend( at :: Tensor & data , rank_type dstRank)",5, 70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::ireceive( at :: Tensor & data , rank_type srcRank)",5, 73, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::StoreDeamon( int listen_socket)",4, 67, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::~StoreDeamon()",7, 37, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::join()",3, 34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::deamon()",42, 81, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::query( rank_type rank)",40, 74, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::checkAndUpdate( std :: vector<std::string> & keys) const",12, 80, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::Store( const std :: string & addr , port_type port , int listen_socket)",12, 73, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::~Store()",8, 59, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::set( const std :: string & key , const std :: vector<char> & data)",5, 73, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::get( const std :: string & key)",6, 55, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::wait( const std :: vector<std::string> & keys)",13, 65, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::RequestGloo( QueueWorker :: Request && request)",2, 74, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::~RequestGloo()",1, 48, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::isCompleted()",3, 51, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::wait()",3, 44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::Group::Group( const std :: string & addr , port_type port , std :: vector<rank_type> ranks , rank_type max_rank , int store_socket)",8, 54, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::DataChannelGloo( InitMethod :: Config config)",43, 79, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::~DataChannelGloo()",5, 38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::destroy()",1, 35, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::init()",17, 75, 22, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::getRank()",3, 39, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::getNumProcesses()",3, 47, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGatherT( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",34, 77, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",16, 78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",7, 70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",7, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduceT( at :: Tensor & t , THDReduceOp operation , THDGroup group_id)",20, 69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",8, 65, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",7, 71, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcastT( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",26, 75, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",8, 64, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::send( Scalar & data , rank_type dst_rank)",3, 69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::send( at :: Tensor & data , rank_type dst_rank)",3, 69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( Scalar & data , rank_type src_rank)",3, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( at :: Tensor & data)",4, 67, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( at :: Tensor & data , rank_type src_rank)",3, 72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::isend( at :: Tensor & data , rank_type dst_rank)",4, 70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::ireceive( at :: Tensor & data , rank_type src_rank)",4, 73, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",8, 57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",8, 57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",9, 57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",8, 57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::clearGroupCache( THDGroup group_id)",5, 59, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::barrier( THDGroup group_id)",9, 66, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::newGroup( const std :: vector<rank_type> & ranks)",8, 74, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getNcclDataType( at :: ScalarType type)",7, 78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getKeyFromDevices( const std :: vector<at::Device> & devices)",11, 72, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getDeviceList( const std :: vector<at::Tensor> & tensors)",8, 80, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::syncStreams( const std :: vector<at::Device> & devices , std :: vector<at::cuda::CUDAEvent> & ncclEvents , std :: vector<at::cuda::CUDAStream> & ncclStreams)",11, 74, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::WorkNCCL( const std :: vector<at::Device> & devices)",7, 77, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL()",1, 43, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::isCompleted()",3, 49, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::isSuccess() const",3, 53, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::exception() const",6, 70, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecution()",13, 61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::synchronize()",12, 78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::wait()",3, 42, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::ProcessGroupNCCL( const std :: shared_ptr<Store> & store , int rank , int size , const std :: string & groupName)",19, 78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::~ProcessGroupNCCL()",4, 54, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::broadcastUniqueNCCLID( ncclUniqueId * ncclID)",33, 74, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::getNCCLComm( const std :: string & devicesKey , const std :: vector<at::Device> & devices)",69, 81, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::tensorCheckHelper( const std :: vector<at::Tensor> & input , const std :: vector<at::Tensor> & output , int outputOverInput)",75, 81, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::allreduce( std :: vector<at::Tensor> & tensors , const AllreduceOptions & opts)",46, 69, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::broadcast( std :: vector<at::Tensor> & tensors , const BroadcastOptions & opts)",47, 69, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::reduce( std :: vector<at::Tensor> & tensors , const ReduceOptions & opts)",49, 69, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::allgather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const AllgatherOptions & opts)",73, 75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::barrier( const BarrierOptions & opts)",39, 78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::gather( std :: vector<std::vector<at::Tensor>> & , std :: vector<at::Tensor> & , const GatherOptions &)",6, 72, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::scatter( std :: vector<at::Tensor> & , std :: vector<std::vector<at::Tensor>> & , const ScatterOptions &)",6, 73, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::send( std :: vector<at::Tensor> & , int , int)",6, 70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::recv( std :: vector<at::Tensor> & , int , int)",6, 70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::recvAnysource( std :: vector<at::Tensor> & , int)",5, 70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::getGroupRank()",3, 79, 2, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::setSocketNoDelay( int socket)",5, 96, 2, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::getSocketPort( int fd)",22, 74, 6, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::sockaddrToString( struct :: sockaddr * addr)",15, 101, 4, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::listen( PortType port)",52, 88, 6, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::connect( const std :: string & address , PortType port , bool wait , const std :: chrono :: milliseconds & timeout)",104, 81, 2, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::accept( int listenSocket , const std :: chrono :: milliseconds & timeout)",39, 78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::PrefixStore( const std :: string & prefix , Store & store)",2, 66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::joinKey( const std :: string & key)",3, 59, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::joinKeys( const std :: vector<std::string> & keys)",9, 48, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::set( const std :: string & key , const std :: vector<uint8_t> & value)",5, 41, 4, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::get( const std :: string & key)",3, 64, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::add( const std :: string & key , int64_t value)",3, 66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::check( const std :: vector<std::string> & keys)",4, 64, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::wait( const std :: vector<std::string> & keys)",4, 63, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",6, 48, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::cudaAwareMpiCheck()",12, 43, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSingleTensorHelper( const at :: Tensor & tensor)",13, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSingleTensor( const std :: vector<at::Tensor> & tensors)",7, 69, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSameSizeAndType( const at :: Tensor & tensor , const std :: vector<at::Tensor> & tensors)",11, 78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::AsyncWork( at :: Tensor tensor , MPI_Request request)",4, 78, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()",8, 76, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::isCompleted()",20, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::isSuccess() const",8, 75, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::sourceRank() const",3, 53, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::wait()",13, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::populateException()",7, 81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::mpiExit()",4, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::initMPIOnce()",17, 73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::createProcessGroupMPI( std :: vector<int> ranks)",40, 76, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::ProcessGroupMPI( int rank , int size , MPI_Comm pgComm)",34, 72, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::~ProcessGroupMPI()",3, 38, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::destroy()",21, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::abort()",4, 36, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::runLoop()",29, 48, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::enqueue( std :: unique_ptr<WorkEntry> entry)",8, 62, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::broadcast( std :: vector<at::Tensor> & tensors , const BroadcastOptions & opts)",22, 65, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::allreduce( std :: vector<at::Tensor> & tensors , const AllreduceOptions & opts)",24, 65, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::reduce( std :: vector<at::Tensor> & tensors , const ReduceOptions & opts)",29, 80, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::allgather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const AllgatherOptions & opts)",45, 76, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::gather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const GatherOptions & opts)",68, 81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::scatter( std :: vector<at::Tensor> & outputTensors , std :: vector<std::vector<at::Tensor>> & inputTensors , const ScatterOptions & opts)",66, 82, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::send( std :: vector<at::Tensor> & tensors , int dstRank , int tag)",27, 61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::recv( std :: vector<at::Tensor> & tensors , int srcRank , int tag)",27, 61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::recvAnysource( std :: vector<at::Tensor> & tensors , int tag)",26, 68, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::barrier( const BarrierOptions & opts)",14, 65, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::getGroupRank()",3, 63, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::TCPStoreDaemon( int storeListenSocket)",10, 75, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::~TCPStoreDaemon()",18, 37, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::join()",3, 30, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::run()",100, 80, 8, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::stop()",7, 39, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::query( int socket)",23, 55, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::wakeupWaitingClients( const std :: string & key)",12, 68, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::setHandler( int socket)",6, 58, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::addHandler( int socket)",16, 77, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::getHandler( int socket) const",5, 52, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::checkHandler( int socket) const",14, 81, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::waitHandler( int socket)",17, 51, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::checkKeys( const std :: vector<std::string> & keys) const",5, 78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::TCPStore( const std :: string & masterAddr , PortType masterPort , int numWorkers , bool isServer)",23, 78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::~TCPStore()",9, 61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::waitForWorkers_()",24, 77, 6, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::set( const std :: string & key , const std :: vector<uint8_t> & data)",6, 79, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::get( const std :: string & key)",4, 61, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::getHelper_( const std :: string & key)",6, 68, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::add( const std :: string & key , int64_t value)",4, 63, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::addHelper_( const std :: string & key , int64_t value)",6, 70, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::check( const std :: vector<std::string> & keys)",17, 76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::wait( const std :: vector<std::string> & keys)",3, 60, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",10, 48, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::waitHelper_( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",25, 77, 32, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::GlooStore( const std :: shared_ptr<::c10d::Store> & store)",1, 76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::set( const std :: string & key , const std :: vector<char> & value)",4, 78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::get( const std :: string & key)",4, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::wait( const std :: vector<std::string> & keys)",3, 61, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",5, 59, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::toFunction( const ReduceOp & r)",16, 50, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setInputs( O & opts , std :: vector<at::Tensor> & tensors)",3, 67, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setInput( O & opts , at :: Tensor & tensor)",3, 60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setOutputs( O & opts , std :: vector<at::Tensor> & tensors)",3, 68, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setOutput( O & opts , at :: Tensor & tensor)",3, 61, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::pinnedLike( at :: Tensor & tensor)",5, 80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::initializeStreamsEvents( std :: vector<at::Tensor> & tensors , std :: vector<at::cuda::CUDAStream> & streams , std :: vector<at::cuda::CUDAEvent> & events)",20, 72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::initializeStreamsEvents( std :: vector<std::vector<at::Tensor>> & tensors , std :: vector<at::cuda::CUDAStream> & streams , std :: vector<at::cuda::CUDAEvent> & events)",32, 83, 12, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::SendWork::SendWork( at :: Tensor & tensor , std :: unique_ptr<::gloo::transport::UnboundBuffer> buffer)",4, 62, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::SendWork::wait()",13, 45, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::RecvWork( at :: Tensor & tensor , std :: unique_ptr<::gloo::transport::UnboundBuffer> buffer)",4, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::sourceRank() const",4, 53, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::wait()",13, 45, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::Options::Options()",3, 53, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::ProcessGroupGloo( const std :: shared_ptr<Store> & store , int rank , int size , Options options)",32, 80, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::~ProcessGroupGloo()",18, 49, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::nextTag()",3, 39, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::runLoop( int workerIndex)",20, 50, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::enqueue( std :: shared_ptr<AsyncWork> work)",5, 66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::AsyncBroadcastWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , uint32_t tag)",11, 53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::broadcast( at :: Tensor & tensor)",8, 61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::run()",11, 50, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::AsyncBroadcastCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , uint32_t tag)",17, 73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::run()",19, 65, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::synchronize()",9, 57, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::broadcast( std :: vector<at::Tensor> & inputs , const BroadcastOptions & opts)",40, 72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::AsyncAllreduceWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , ReduceOp reduceOp , uint32_t tag)",6, 74, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::allreduce( std :: vector<at::Tensor> & tensors)",8, 63, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::run()",11, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::getFunction( gloo :: AllreduceOptions :: Func & fn , const ReduceOp op)",3, 74, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::getFunction( const at :: ScalarType & dtype , const ReduceOp op)",7, 52, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::AsyncAllreduceCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , ReduceOp reduceOp , uint32_t tag)",16, 67, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::run()",23, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::synchronize()",8, 57, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::allreduce( std :: vector<at::Tensor> & inputs , const AllreduceOptions & opts)",39, 72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::AsyncReduceWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , ReduceOp reduceOp , uint32_t tag)",13, 53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::reduce( std :: vector<at::Tensor> & tensors)",9, 65, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::run()",3, 24, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::getFunction( gloo :: ReduceOptions :: Func & fn , const ReduceOp op)",3, 71, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::getFunction( const at :: ScalarType & dtype , const ReduceOp op)",7, 52, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::AsyncReduceCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , ReduceOp reduceOp , uint32_t tag)",18, 80, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::run()",19, 58, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::synchronize()",8, 57, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::reduce( std :: vector<at::Tensor> & inputs , const ReduceOptions & opts)",49, 69, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::AsyncAllgatherWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , uint32_t tag)",6, 72, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::allgather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs)",25, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::run()",3, 32, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::AsyncAllgatherCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , uint32_t tag)",25, 73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::run()",26, 72, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::synchronize()",8, 63, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::allgather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , const AllgatherOptions & opts)",65, 72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::AsyncGatherWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , int root , uint32_t tag)",11, 53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::gather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs)",27, 73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::run()",3, 29, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::AsyncGatherCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , int root , uint32_t tag)",26, 73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::run()",26, 72, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::synchronize()",8, 81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::gather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , const GatherOptions & opts)",56, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::AsyncScatterWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , int root , uint32_t tag)",11, 53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::scatter( std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs)",17, 66, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::run()",3, 30, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::AsyncScatterCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , int root , uint32_t tag)",27, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::run()",23, 64, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::synchronize()",8, 78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::scatter( std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , const ScatterOptions & opts)",55, 70, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::checkSingleTensor( std :: vector<at::Tensor> & tensors)",13, 78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::checkTag( int32_t tag)",6, 50, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::send( std :: vector<at::Tensor> & tensors , int dstRank , int tag)",18, 70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::recv( std :: vector<at::Tensor> & tensors , int srcRank , int tag)",18, 70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::recvAnysource( std :: vector<at::Tensor> & tensors , int tag)",27, 70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBarrierWork::AsyncBarrierWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::weak_ptr<AsyncWork>> priorWork , uint32_t tag)",5, 71, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBarrierWork::run()",13, 40, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::barrier( const BarrierOptions & opts)",19, 77, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::getGroupRank()",3, 78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::syscall( F fn)",11, 51, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::Lock( int fd , int operation)",3, 51, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::~Lock()",3, 14, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::Lock( Lock && other)",4, 32, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::unlock()",6, 22, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::flock( int operation)",4, 59, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::File( const std :: string & path , int flags , std :: chrono :: milliseconds timeout)",22, 78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::~File()",3, 18, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::lockShared()",3, 31, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::lockExclusive()",3, 31, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::seek( off_t offset , int whence)",5, 62, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::tell()",5, 59, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::size()",6, 35, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const void * buf , size_t count)",8, 62, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( void * buf , size_t count)",8, 61, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const std :: string & str)",6, 69, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const std :: vector<uint8_t> & data)",6, 70, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( std :: string & str)",7, 40, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( std :: vector<uint8_t> & data)",6, 42, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::refresh( File & file , off_t pos , std :: unordered_map<std::string,std::vector<uint8_t>> & cache)",19, 68, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::FileStore( const std :: string & path , int numWorkers)",12, 72, 8, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::~FileStore()",10, 79, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::set( const std :: string & key , const std :: vector<uint8_t> & value)",8, 81, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::get( const std :: string & key)",28, 77, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::addHelper( const std :: string & key , int64_t i)",20, 66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::add( const std :: string & key , int64_t i)",4, 60, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::check( const std :: vector<std::string> & keys)",14, 62, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::wait( const std :: vector<std::string> & keys)",3, 61, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",17, 75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::~Work()",1, 31, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::isCompleted()",4, 44, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::isSuccess() const",4, 45, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::exception() const",4, 59, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::sourceRank() const",5, 59, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::synchronize()",1, 42, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::wait()",10, 45, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::finish( std :: exception_ptr exception)",6, 64, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::ProcessGroup( int rank , int size)",1, 77, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::~ProcessGroup()",1, 33, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Store.cpp,"c10d::Store::~Store()",1, 19, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Store.cpp,"c10d::Store::setTimeout( const std :: chrono :: milliseconds & timeout)",6, 67, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::SignalTest( const std :: string & path)",1, 55, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::~SignalTest()",5, 27, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::arm( int pid , int signal)",6, 34, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::run( int rank , int size)",30, 75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testSignal( const std :: string & path , int signal)",14, 56, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::initialize( const std :: string & path , int num)",19, 74, 10, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::CollectiveTest( const std :: string & path)",1, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::CollectiveTest( CollectiveTest && other)",4, 43, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::getProcessGroup()",3, 48, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::start( int rank , int size)",13, 75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"copyTensors( const std :: vector<std::vector<at::Tensor>> & inputs)",13, 63, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testAllreduce( const std :: string & path , const at :: Backend b)",35, 71, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testBroadcast( const std :: string & path , const at :: Backend b)",55, 76, 8, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testBarrier( const std :: string & path)",15, 71, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"main( int argc , char ** argv)",53, 67, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::NCCLTestBase( const std :: string & path)",1, 57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::NCCLTestBase( NCCLTestBase && other)",4, 39, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::getProcessGroup()",3, 48, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::initialize( int rank , int size)",6, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::NCCLTest( const std :: string & path , int worldSize)",31, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::wait( std :: shared_ptr<ProcessGroup::Work> & work)",4, 57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::getTensors()",14, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::getOutputTensors()",18, 70, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::numDevices() const",3, 27, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllreduceNCCLTest::AllreduceNCCLTest( const std :: string & path , int worldSize)",2, 60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllreduceNCCLTest::run()",19, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"BroadcastNCCLTest::BroadcastNCCLTest( const std :: string & path , int worldSize)",2, 60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"BroadcastNCCLTest::run( int rootRank , int rootTensor)",22, 80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"ReduceNCCLTest::ReduceNCCLTest( const std :: string & path , int worldSize)",2, 57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"ReduceNCCLTest::run( int rootRank , int rootTensor)",22, 80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllgatherNCCLTest::AllgatherNCCLTest( const std :: string & path , int worldSize)",2, 60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllgatherNCCLTest::run()",19, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testAllreduce( const std :: string & path , int rank , int size)",22, 66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testBroadcast( const std :: string & path , int rank , int size)",29, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testReduce( const std :: string & path , int rank , int size)",30, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testAllgather( const std :: string & path , int rank , int size)",25, 66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"main( int argc , char ** argv)",25, 72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"initialize( const std :: string & path , int N , Args && ... args)",17, 78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::AsyncTest( const std :: string & path)",1, 54, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::AsyncTest( AsyncTest && other)",4, 36, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::getProcessGroup()",3, 48, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::start( int rank , int size)",12, 75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::AsyncInputIsOutputTest( const std :: string & path , int numTensors)",28, 77, 14, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::wait( std :: shared_ptr<ProcessGroup::Work> & work)",4, 57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::getTensors()",13, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncAllreduceTest::AsyncAllreduceTest( const std :: string & path , int numTensors)",2, 62, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncAllreduceTest::run()",19, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncBroadcastTest::AsyncBroadcastTest( const std :: string & path , int numTensors)",2, 62, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncBroadcastTest::run( int rootRank , int rootTensor)",22, 80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"runAsyncAllreduceTest( const std :: string & path , size_t numProcesses , size_t numTensors)",31, 79, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"runAsyncBroadcastTest( const std :: string & path , size_t numProcesses , size_t numTensors)",36, 81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"main( int argc , char ** argv)",12, 47, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"tmppath()",19, 72, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"testHelper( const std :: string prefix = '')",55, 71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"main( int argc , char ** argv)",5, 46, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/TCPStoreTest.cpp,"testHelper( const std :: string & prefix = '')",83, 81, 12, 0
repos/cpp/pytorch/torch/lib/c10d/test/TCPStoreTest.cpp,"main( int argc , char ** argv)",6, 46, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"waitWork( std :: shared_ptr<c10d::ProcessGroupMPI> pg , std :: vector<std::shared_ptr<c10d::ProcessGroup::Work>> works)",12, 69, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testAllreduce( int iter = 1000)",32, 79, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testBroadcast( int iter = 10000)",35, 79, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testReduce( int iter = 10000)",35, 76, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testAllgather( int iter = 10000)",43, 76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testGather( int iter = 10000)",49, 76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testScatter( int iter = 1)",48, 75, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testSendRecv( bool recvAnysource , int iter = 10000)",58, 73, 8, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"main( int argc , char ** argv)",23, 72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/example/allreduce.cpp,"main( int argc , char ** argv)",27, 80, 8, 0
repos/cpp/pytorch/binaries/run_plan.cc,"main( int argc , char ** argv)",16, 73, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::reportTime( std :: string type , double ts , std :: string metric , std :: string unit)",18, 80, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::splitSizes( const std :: string & arg , int * ptr0 , int * ptr1)",12, 64, 0, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::resizeImage( cv :: Mat & img)",36, 71, 4, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::cropToRec( cv :: Mat & img , int * height_ptr , int * width_ptr)",28, 74, 8, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::convertToVector( cv :: Mat & img)",55, 80, 10, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::convertOneImage( std :: string & filename , int * height_ptr , int * width_ptr)",42, 78, 6, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::getBatchSize( int num_items)",9, 41, 4, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::writeValues( std :: vector<std::vector<std::vector<float>>> & values , std :: vector<std::vector<int>> & dims)",38, 58, 4, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::convertImages( std :: string & image_file)",49, 69, 10, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::splitString( std :: string & line)",8, 56, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"caffe2::convertValues( std :: string & file_name)",44, 55, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"observerConfig()",8, 69, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"backendCudaSet( const string & backend)",15, 58, 6, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"setOperatorEngine( caffe2 :: NetDef * net_def , const string & backend)",27, 73, 0, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"fillInputBlob( shared_ptr<caffe2::Workspace> workspace , map<string,caffe2::TensorProtos> & tensor_protos_map , int iteration)",27, 73, 6, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"writeOutput( shared_ptr<caffe2::Workspace> workspace , const bool run_on_gpu , const string & output , const string & output_folder , const bool text_output , const int index , const int num_blobs)",52, 74, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"runNetwork( shared_ptr<caffe2::Workspace> workspace , caffe2 :: NetDef & net_def , map<string,caffe2::TensorProtos> & tensor_protos_map , const bool wipe_cache , const bool run_individual , const bool run_on_gpu , const bool text_output , const int warmup , const int iter , const int num_blobs , const int sleep_before_run , const int sleep_between_iteration , const int sleep_between_net_and_operator , const std :: string & output , const std :: string & output_folder)",89, 79, 6, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"benchmark( int argc , char * argv [ ] , const string & FLAGS_backend , const string & FLAGS_init_net , const string & FLAGS_input_dims , int FLAGS_iter , const string & FLAGS_net , const string & FLAGS_output , const string & FLAGS_output_folder , bool FLAGS_run_individual , int FLAGS_sleep_before_run , int FLAGS_sleep_between_iteration , int FLAGS_sleep_between_net_and_operator , bool FLAGS_text_output , int FLAGS_warmup , bool FLAGS_wipe_cache)",86, 81, 2, 0
repos/cpp/pytorch/binaries/convert_and_benchmark.cc,"main( int argc , char ** argv)",22, 44, 6, 0
repos/cpp/pytorch/binaries/tsv_2_proto.cc,"main( int argc , char ** argv)",20, 60, 4, 0
repos/cpp/pytorch/binaries/make_cifar_db.cc,"caffe2::ReadImage( std :: ifstream * file , int * label , char * buffer)",20, 77, 2, 0
repos/cpp/pytorch/binaries/make_cifar_db.cc,"caffe2::WriteToDB( const string & filename , const int num_items , const int & offset , db :: DB * db)",31, 70, 2, 0
repos/cpp/pytorch/binaries/make_cifar_db.cc,"caffe2::ConvertCIFAR()",28, 76, 4, 0
repos/cpp/pytorch/binaries/make_cifar_db.cc,"main( int argc , char ** argv)",5, 36, 2, 0
repos/cpp/pytorch/binaries/tutorial_blob.cc,"main( int argc , char ** argv)",64, 78, 6, 0
repos/cpp/pytorch/binaries/predictor_verifier.cc,"caffe2::run()",19, 79, 4, 0
repos/cpp/pytorch/binaries/predictor_verifier.cc,"main( int argc , char ** argv)",7, 52, 2, 0
repos/cpp/pytorch/binaries/make_mnist_db.cc,"caffe2::swap_endian( uint32_t val)",4, 63, 4, 0
repos/cpp/pytorch/binaries/make_mnist_db.cc,"caffe2::convert_dataset( const char * image_filename , const char * label_filename , const char * db_path , const int data_limit)",86, 78, 2, 0
repos/cpp/pytorch/binaries/make_mnist_db.cc,"main( int argc , char ** argv)",9, 36, 2, 0
repos/cpp/pytorch/binaries/speed_benchmark.cc,"main( int argc , char ** argv)",128, 80, 8, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::Converter::Converter()",19, 49, 6, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::Converter::~Converter()",5, 30, 4, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::Converter::queue( const std :: pair<std::string,int> & pair)",3, 56, 2, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::Converter::start()",3, 50, 4, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::Converter::get()",11, 47, 4, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::Converter::run()",68, 81, 10, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"caffe2::ConvertImageDataset( const string & input_folder , const string & list_filename , const string & output_db_name , const bool)",69, 80, 4, 0
repos/cpp/pytorch/binaries/make_image_db.cc,"main( int argc , char ** argv)",6, 81, 6, 0
repos/cpp/pytorch/binaries/split_db.cc,"caffe2::Split( int argc , char ** argv)",40, 81, 6, 0
repos/cpp/pytorch/binaries/split_db.cc,"main( int argc , char ** argv)",3, 36, 2, 0
repos/cpp/pytorch/binaries/convert_encoded_to_raw_leveldb.cc,"caffe2::ConvertToRawDataset( const string & input_db_name , const string & output_db_name)",96, 81, 12, 0
repos/cpp/pytorch/binaries/convert_encoded_to_raw_leveldb.cc,"main( int argc , char ** argv)",5, 74, 2, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_CUDAContextCreation( benchmark :: State & state)",7, 62, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_CUDAContextStreamAccess( benchmark :: State & state)",7, 66, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaGetDevice( benchmark :: State & state)",7, 56, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaSetDevice( benchmark :: State & state)",8, 56, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaSetAndGetDevice( benchmark :: State & state)",10, 62, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaSetSameDevice( benchmark :: State & state)",6, 60, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaStreamCreateSyncDelete( benchmark :: State & state)",9, 69, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaStreamSynchronize( benchmark :: State & state)",8, 64, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaEventRecord( benchmark :: State & state)",11, 59, 6, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_cudaStreamWaitEventThenStreamSynchronize( benchmark :: State & state)",16, 59, 6, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_CudaPointerAffinity( benchmark :: State & state)",8, 62, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"DummyEmptyOp::DummyEmptyOp( const OperatorDef & def , Workspace * ws)",2, 54, 2, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"DummyEmptyOp::RunOnDevice()",1, 44, 2, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_OperatorCreationCPU( benchmark :: State & state)",10, 62, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_OperatorCreationCUDA( benchmark :: State & state)",11, 63, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_RawAllocDeallocCPU( benchmark :: State & state)",7, 64, 4, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_TensorAllocDeallocCPU( benchmark :: State & state)",9, 64, 0, 0
repos/cpp/pytorch/binaries/core_overhead_benchmark_gpu.cc,"BM_TensorAllocDeallocCUDA( benchmark :: State & state)",10, 65, 0, 0
repos/cpp/pytorch/binaries/run_plan_mpi.cc,"main( int argc , char ** argv)",22, 76, 2, 0
repos/cpp/pytorch/binaries/convert_caffe_image_db.cc,"main( int argc , char ** argv)",55, 77, 4, 0
repos/cpp/pytorch/binaries/inspect_gpu.cc,"main( int argc , char ** argv)",28, 79, 6, 0
repos/cpp/pytorch/binaries/zmq_feeder.cc,"main( int argc , char ** argv)",33, 81, 2, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"observerConfig()",8, 69, 2, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"backendCudaSet( const string & backend)",15, 58, 6, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"setDeviceType( caffe2 :: NetDef * net_def , caffe2 :: DeviceType & run_dev)",6, 80, 4, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"setOperatorEngine( caffe2 :: NetDef * net_def , const string & backend)",19, 73, 0, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"loadInput( shared_ptr<caffe2::Workspace> workspace , const bool run_on_gpu , map<string,caffe2::TensorProtos> & tensor_protos_map , const string & input , const string & input_file , const string & input_dims , const string & input_type)",111, 81, 8, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"fillInputBlob( shared_ptr<caffe2::Workspace> workspace , map<string,caffe2::TensorProtos> & tensor_protos_map , int iteration)",27, 73, 6, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"runNetwork( shared_ptr<caffe2::Workspace> workspace , caffe2 :: NetDef & net_def , map<string,caffe2::TensorProtos> & tensor_protos_map , const bool wipe_cache , const bool run_individual , const bool run_on_gpu , const bool text_output , const int warmup , const int iter , const int num_blobs , const int sleep_before_run , const int sleep_between_iteration , const int sleep_between_net_and_operator , const std :: string & output , const std :: string & output_folder)",89, 79, 6, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"writeOutput( shared_ptr<caffe2::Workspace> workspace , const bool run_on_gpu , const string & output , const string & output_folder , const bool text_output , const int index , const int num_blobs)",52, 74, 2, 0
repos/cpp/pytorch/binaries/benchmark_helper.cc,"benchmark( int argc , char * argv [ ] , const string & FLAGS_backend , const string & FLAGS_init_net , const string & FLAGS_input , const string & FLAGS_input_dims , const string & FLAGS_input_file , const string & FLAGS_input_type , int FLAGS_iter , const string & FLAGS_net , const string & FLAGS_output , const string & FLAGS_output_folder , bool FLAGS_run_individual , int FLAGS_sleep_before_run , int FLAGS_sleep_between_iteration , int FLAGS_sleep_between_net_and_operator , bool FLAGS_text_output , int FLAGS_warmup , bool FLAGS_wipe_cache)",88, 81, 2, 0
repos/cpp/pytorch/binaries/convert_db.cc,"main( int argc , char ** argv)",20, 70, 2, 0
repos/cpp/pytorch/binaries/caffe2_benchmark.cc,"main( int argc , char ** argv)",23, 44, 6, 0
repos/cpp/pytorch/binaries/db_throughput.cc,"TestThroughputWithDB()",23, 74, 8, 0
repos/cpp/pytorch/binaries/db_throughput.cc,"TestThroughputWithReaderWorker( const DBReader * reader , int thread_id)",17, 77, 0, 0
repos/cpp/pytorch/binaries/db_throughput.cc,"TestThroughputWithReader()",12, 68, 2, 0
repos/cpp/pytorch/binaries/db_throughput.cc,"main( int argc , char ** argv)",9, 36, 2, 0
repos/cpp/pytorch/binaries/print_registered_core_operators.cc,"HasSchema( const std :: string & str)",3, 48, 0, 0
repos/cpp/pytorch/binaries/print_registered_core_operators.cc,"HasDoc( const std :: string & str)",4, 62, 2, 0
repos/cpp/pytorch/binaries/print_registered_core_operators.cc,"main( int argc , char ** argv)",36, 81, 14, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::reportTime( std :: string type , double ts , std :: string metric , std :: string unit)",18, 80, 2, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::splitSizes( const std :: string & arg , int * ptr0 , int * ptr1)",12, 64, 0, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::resizeImage( cv :: Mat & img)",36, 71, 4, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::cropToRec( cv :: Mat & img , int * height_ptr , int * width_ptr)",28, 74, 8, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::convertToVector( cv :: Mat & img)",55, 80, 10, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::convertOneImage( std :: string & filename , int * height_ptr , int * width_ptr)",42, 78, 6, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::getBatchSize( int num_items)",9, 41, 4, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::writeValues( std :: vector<std::vector<std::vector<float>>> & values , std :: vector<std::vector<int>> & dims , std :: string output_file)",43, 58, 4, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::convertImages()",50, 69, 10, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::splitString( std :: string & line)",8, 56, 2, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"caffe2::convertValues()",43, 55, 2, 0
repos/cpp/pytorch/binaries/convert_image_to_tensor.cc,"main( int argc , char ** argv)",6, 36, 2, 0
repos/cpp/pytorch/binaries/print_core_object_sizes_gpu.cc,"main( int , char **)",13, 53, 2, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::~THCIpcDeleter()",1, 35, 0, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"deleteTHCIpcDeleter( void * ptr)",3, 43, 2, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::makeDataPtr( std :: shared_ptr<void> basePtr , void * data)",7, 94, 2, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::THCIpcDeleter( std :: shared_ptr<void> basePtr)",2, 60, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimension( THCState * state , const THCTensor * self)",3, 67, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimensionLegacyNoScalars( THCState * state , const THCTensor * self)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimensionLegacyAll( THCState * state , const THCTensor * self)",3, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_size( THCState * state , const THCTensor * self , int dim)",4, 74, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_sizeLegacyNoScalars( THCState * state , const THCTensor * self , int dim)",3, 89, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_stride( THCState * state , const THCTensor * self , int dim)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_strideLegacyNoScalars( THCState * state , const THCTensor * self , int dim)",3, 91, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_new( THCState * state , caffe2 :: TypeMeta type_meta)",25, 72, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resize( THCState * state , THCTensor * self , at :: IntArrayRef size , at :: IntArrayRef stride)",10, 104, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resizeAs( THCState * state , THCTensor * self , THCTensor * src)",19, 81, 4, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resizeNd( THCState * state , THCTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",10, 118, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_set( THCState * state , THCTensor * self , THCTensor * src)",11, 69, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_setStorage( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntArrayRef size_ , at :: IntArrayRef stride_)",14, 156, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_setStorageNd( THCState * state , THCTensor * self , THCStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",26, 168, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_squeeze1d( THCState * state , THCTensor * self , THCTensor * src , int dimension)",21, 90, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_unsqueeze1d( THCState * state , THCTensor * self , THCTensor * src , int dimension)",23, 92, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_allContiguous( THCState * state , THCTensor ** inputs , int numInputs)",9, 83, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nElement( THCState * state , const THCTensor * self)",7, 71, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_retain( THCState * state , THCTensor * self)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_free( THCState * state , THCTensor * self)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_getDevice( THCState * state , const THCTensor * tensor)",4, 70, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_allSameDevice( THCState * state , THCTensor ** inputs , int numInputs)",10, 84, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_canUse32BitIndexMath( THCState * state , const THCTensor * t , ptrdiff_t max_elem)",27, 95, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_all32BitIndexable( THCState * state , THCTensor ** inputs , int numInputs)",8, 87, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_preserveReduceDimSemantics( THCState * state , THCTensor * tensor , int in_dims , int64_t dimension , int keepdim)",7, 89, 42, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"compareSizeAndStride( const void * a , const void * b)",8, 57, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_maybeOverlappingIndices( THCState * state , const THCTensor * t)",38, 78, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_resize( THCState * state , THCStorage * self , ptrdiff_t size)",38, 88, 4, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_getDevice( THCState * state , const THCStorage * storage)",3, 71, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_new( THCState * state , caffe2 :: TypeMeta data_type)",10, 61, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"destroyGenerator( THCState * state , THCGenerator * gen)",14, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"createSeed( std :: random_device & rd)",6, 64, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_init( THCState * state , int devices , int current_device)",16, 89, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_shutdown( THCState * state)",11, 56, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_rawGenerator( THCState * state)",8, 74, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_getGenerator( THCState * state)",12, 56, 4, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_generatorStates( THCState * state)",5, 62, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_seed( THCState * state)",7, 41, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_seedAll( THCState * state)",7, 44, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_manualSeed( THCState * state , uint64_t seed)",9, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_manualSeedAll( THCState * state , uint64_t seed)",11, 61, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_initialSeed( THCState * state)",5, 53, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_free( THCState * state)",4, 36, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_alloc( void)",5, 61, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaInit( THCState * state)",57, 94, 6, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaShutdown( THCState * state)",43, 78, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getPeerToPeerAccess( THCState * state , int dev , int devToAccess)",32, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getRngState( THCState * state)",4, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCudaHostAllocator( THCState * state)",4, 63, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getNumDevices( THCState * state)",4, 44, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getDeviceResourcePtr( THCState * state , int device)",11, 73, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentStreamOnDevice( THCState * state , int device)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentStream( THCState * state)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentBlasHandle( THCState * state)",20, 91, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentSparseHandle( THCState * state)",20, 91, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentDeviceScratchSpaceSize( THCState * state)",7, 81, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCudaCheck( cudaError_t err , const char * file , const int line)",13, 117, 6, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCudaCheckWarn( cudaError_t err , const char * file , const int line)",7, 119, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCublasCheck( cublasStatus_t status , const char * file , const int line)",46, 78, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCusparseCheck( cusparseStatus_t status , const char * file , const int line)",48, 82, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaMalloc( THCState * state , size_t size)",6, 58, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaFree( THCState * state , void * ptr)",3, 51, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaHostAlloc( THCState * state , size_t size)",6, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaHostRecord( THCState * state , void * ptr)",5, 80, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaMemGetInfo( THCState * state , size_t * freeBytes , size_t * totalBytes , size_t * largestBlock)",27, 108, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"BlockSize::BlockSize( size_t size , void * ptr = NULL)",1, 67, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"Block::Block( size_t size , void * ptr , bool allocated)",2, 79, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"BlockComparator( const BlockSize & a , const BlockSize & b)",8, 68, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::HostAllocator()",1, 50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::malloc( void ** ptr , size_t size)",34, 68, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::free( void * ptr)",36, 77, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::recordEvent( void * ptr , at :: cuda :: CUDAStream stream)",16, 66, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::processEvents()",31, 78, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::emptyCache()",31, 73, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::insertEvents( Block & block)",27, 80, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator_recordEvent( void * ptr , at :: cuda :: CUDAStream stream)",4, 88, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator_emptyCache()",4, 42, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostDeleter( void * ptr)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator::allocate( size_t size) const",6, 68, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator::raw_deleter() const",3, 50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"getTHCCachingHostAllocator()",3, 46, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( storage)( THCState * state , const THCTensor * self)",4, 72, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( storageOffset)( THCState * state , const THCTensor * self)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimension)( THCState * state , const THCTensor * self)",4, 67, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimensionLegacyNoScalars)( THCState * state , const THCTensor * self)",4, 82, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimensionLegacyAll)( THCState * state , const THCTensor * self)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( size)( THCState * state , const THCTensor * self , int dim)",4, 74, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( sizeLegacyNoScalars)( THCState * state , const THCTensor * self , int dim)",4, 89, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( stride)( THCState * state , const THCTensor * self , int dim)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( strideLegacyNoScalars)( THCState * state , const THCTensor * self , int dim)",4, 91, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( data)( THCState * state , const THCTensor * self)",7, 92, 4, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( new)( THCState * state)",8, 75, 4, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithTensor)( THCState * state , THCTensor * tensor)",16, 82, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , at :: IntArrayRef sizes , at :: IntArrayRef strides)",14, 151, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage1d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0)",5, 103, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage2d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1)",6, 104, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage3d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2)",7, 120, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage4d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2 , int64_t size3 , int64_t stride3)",10, 103, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize)( THCState * state , at :: IntArrayRef size , at :: IntArrayRef stride)",4, 98, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize1d)( THCState * state , int64_t size0)",4, 69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize2d)( THCState * state , int64_t size0 , int64_t size1)",4, 84, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize3d)( THCState * state , int64_t size0 , int64_t size1 , int64_t size2)",4, 99, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize4d)( THCState * state , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",4, 114, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newClone)( THCState * state , THCTensor * self)",7, 66, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newContiguous)( THCState * state , THCTensor * self)",9, 71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newSelect)( THCState * state , THCTensor * tensor , int dimension_ , int64_t sliceIndex_)",6, 106, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newNarrow)( THCState * state , THCTensor * tensor , int dimension_ , int64_t firstIndex_ , int64_t size_)",6, 121, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newTranspose)( THCState * state , THCTensor * tensor , int dimension1_ , int dimension2_)",6, 106, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newUnfold)( THCState * state , THCTensor * tensor , int dimension_ , int64_t size_ , int64_t step_)",6, 115, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newView)( THCState * state , THCTensor * tensor , at :: IntArrayRef size)",15, 124, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newFoldBatchDim)( THCState * state , THCTensor * input)",13, 87, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize)( THCState * state , THCTensor * self , at :: IntArrayRef size , at :: IntArrayRef stride)",4, 104, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resizeAs)( THCState * state , THCTensor * self , THCTensor * src)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize0d)( THCState * state , THCTensor * tensor)",4, 62, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize1d)( THCState * state , THCTensor * tensor , int64_t size0)",5, 77, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize2d)( THCState * state , THCTensor * tensor , int64_t size0 , int64_t size1)",5, 92, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize3d)( THCState * state , THCTensor * tensor , int64_t size0 , int64_t size1 , int64_t size2)",5, 107, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize4d)( THCState * state , THCTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",5, 120, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize5d)( THCState * state , THCTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3 , int64_t size4)",5, 135, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set)( THCState * state , THCTensor * self , THCTensor * src)",4, 71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntArrayRef size_ , at :: IntArrayRef stride_)",3, 160, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage1d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_)",6, 112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage2d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_)",8, 112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage3d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_)",9, 112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage4d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_ , int64_t size3_ , int64_t stride3_)",12, 112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( narrow)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t firstIndex , int64_t size)",18, 123, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( select)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t sliceIndex)",30, 109, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( transpose)( THCState * state , THCTensor * self , THCTensor * src , int dimension1 , int dimension2)",22, 111, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( unfold)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t size , int64_t step)",36, 117, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( squeeze)( THCState * state , THCTensor * self , THCTensor * src)",25, 75, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( squeeze1d)( THCState * state , THCTensor * self , THCTensor * src , int dimension)",4, 92, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( unsqueeze1d)( THCState * state , THCTensor * self , THCTensor * src , int dimension)",4, 94, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isContiguous)( THCState * state , const THCTensor * self)",4, 69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isSetTo)( THCState * state , const THCTensor * self , const THCTensor * src)",16, 86, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isSameSizeAs)( THCState * state , const THCTensor * self , const THCTensor * src)",12, 91, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nElement)( THCState * state , const THCTensor * self)",4, 71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( retain)( THCState * state , THCTensor * self)",4, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( free)( THCState * state , THCTensor * self)",4, 56, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( freeCopyTo)( THCState * state , THCTensor * self , THCTensor * dst)",7, 78, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorageNd)( THCState * state , THCTensor * self , THCStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",4, 170, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resizeNd)( THCState * state , THCTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",4, 120, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set0d)( THCState * state , THCTensor * tensor , scalar_t value)",5, 92, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get0d)( THCState * state , const THCTensor * tensor)",5, 95, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set1d)( THCState * state , THCTensor * tensor , int64_t x0 , scalar_t value)",6, 137, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get1d)( THCState * state , const THCTensor * tensor , int64_t x0)",6, 137, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set2d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , scalar_t value)",6, 134, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get2d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1)",6, 134, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set3d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , scalar_t value)",6, 155, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get3d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2)",6, 155, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set4d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3 , scalar_t value)",6, 187, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get4d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3)",6, 187, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( checkGPU)( THCState * state , unsigned int nTensors , ...)",28, 70, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( sizeDesc)( THCState * state , const THCTensor * tensor)",21, 77, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( data)( THCState * state , const THCStorage * self)",4, 69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( size)( THCState * state , const THCStorage * self)",4, 69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( elementSize)( THCState * state)",4, 46, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( set)( THCState * state , THCStorage * self , ptrdiff_t index , scalar_t value)",9, 96, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( get)( THCState * state , const THCStorage * self , ptrdiff_t index)",10, 96, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( new)( THCState * state)",9, 61, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize)( THCState * state , ptrdiff_t size)",9, 70, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithAllocator)( THCState * state , ptrdiff_t size , at :: Allocator * allocator)",10, 75, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize1)( THCState * state , scalar_t data0)",6, 71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize2)( THCState * state , scalar_t data0 , scalar_t data1)",7, 87, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize3)( THCState * state , scalar_t data0 , scalar_t data1 , scalar_t data2)",8, 103, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize4)( THCState * state , scalar_t data0 , scalar_t data1 , scalar_t data2 , scalar_t data3)",9, 119, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithMapping)( THCState * state , const char * fileName , ptrdiff_t size , int isShared)",5, 109, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithDataAndAllocator)( THCState * state , at :: DataPtr && data , ptrdiff_t size , at :: Allocator * allocator)",13, 61, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( retain)( THCState * state , THCStorage * self)",4, 60, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( free)( THCState * state , THCStorage * self)",4, 58, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorageCopy.cpp,"THCStorage_( copyCPU)( THCState * state , THCStorage * self , struct THStorage * src)",11, 84, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorageCopy.cpp,"THStorage_( copyCuda)( THCState * state , THStorage * self , struct THCStorage * src)",11, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getDLDataType( const Tensor & t)",45, 74, 6, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getDLContext( const Type & type , const int64_t & device_id)",10, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getATenDeviceType( const DLContext & ctx)",15, 93, 6, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::toScalarType( const DLDataType & dtype)",51, 91, 10, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::deleter( DLManagedTensor * arg)",3, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::toDLPack( const Tensor & src)",18, 86, 2, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::fromDLPack( const DLManagedTensor * src)",12, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/Utils.cpp,"at::_crash_if_asan( int arg)",5, 30, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::errorHandler( const char * msg , void * data)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::argErrorHandler( int arg , const char * msg , void * data)",5, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::Context()",13, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::globalContext()",4, 33, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::userEnabledCuDNN() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setUserEnabledCuDNN( bool e)",3, 44, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::deterministicCuDNN() const",3, 43, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setDeterministicCuDNN( bool b)",3, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::benchmarkCuDNN() const",3, 39, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setBenchmarkCuDNN( bool b)",3, 42, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasMKL() const",7, 31, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasOpenMP() const",7, 34, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasLAPACK() const",7, 34, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setFlushDenormal( bool on)",3, 42, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( TensorOptions options)",4, 94, 12, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( const TensorImpl * impl)",5, 122, 12, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( const Tensor & t)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getLegacyTHDispatcher( TensorOptions options)",4, 71, 12, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getLegacyTHDispatcher( const TensorImpl * impl)",5, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getCPUAllocator()",3, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::LegacyDeviceTypeInit( LegacyDeviceTypeInitArgs)",1, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initCPU() const",3, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initCUDA() const",3, 36, 4, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initHIP() const",3, 35, 4, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initComplex() const",3, 39, 4, 0
repos/cpp/pytorch/aten/src/ATen/ExpandUtils.cpp,"at::infer_size( IntArrayRef a , IntArrayRef b)",26, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/ExpandUtils.cpp,"at::inferExpandGeometry( IntArrayRef tensor_sizes , IntArrayRef tensor_strides , IntArrayRef sizes)",54, 81, 32, 0
repos/cpp/pytorch/aten/src/ATen/TensorGeometry.cpp,"at::TensorGeometry::is_contiguous() const",6, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::operator < <( std :: ostream & out , TensorGeometryArg t)",10, 69, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDim( CheckedFrom c , const TensorGeometryArg & t , int64_t dim)",5, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDimRange( CheckedFrom c , const TensorGeometryArg & t , int64_t dim_start , int64_t dim_end)",7, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkContiguous( CheckedFrom c , const TensorGeometryArg & t)",6, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllContiguous( CheckedFrom c , at :: ArrayRef<TensorArg> ts)",6, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSize( CheckedFrom c , const TensorGeometryArg & t , IntArrayRef sizes)",7, 80, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSize( CheckedFrom c , const TensorGeometryArg & t , int64_t dim , int64_t size)",7, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSame( CheckedFrom c , ArrayRef<TensorArg> tensors , void(*fn)(CheckedFrom,const TensorArg&,const TensorArg&))",11, 124, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameSize( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",7, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameSize( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkNumel( CheckedFrom c , const TensorGeometryArg & t , int64_t numel)",7, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameNumel( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",8, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameNumel( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameGPU( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",19, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameGPU( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 67, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameType( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",7, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkScalarType( CheckedFrom c , const TensorArg & t , ScalarType ty)",7, 79, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkScalarTypes( CheckedFrom c , const TensorArg & t , at :: ArrayRef<ScalarType> l)",19, 77, 6, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameType( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameDim( CheckedFrom c , const TensorGeometryArg & t1 , const TensorGeometryArg & t2)",7, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDefined( CheckedFrom c , const TensorArg & t)",6, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllDefined( CheckedFrom c , ArrayRef<TensorArg> ts)",6, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkBackend( CheckedFrom c , const Tensor & t , Backend backend)",7, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkBackend( CheckedFrom c , ArrayRef<Tensor> tensors , at :: Backend backend)",5, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::maybe_data_ptr( const Tensor & tensor)",3, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::maybe_data_ptr( const TensorArg & tensor)",3, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::geometry_is_contiguous( IntArrayRef sizes , IntArrayRef strides)",17, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/LegacyTHDispatch.cpp,"at::globalLegacyTHDispatch()",4, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::UndefinedType()",2, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::scalarType() const",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::typeMeta() const",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::backend() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::allocator() const",3, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::getDeviceFromPtr( void *) const",3, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",3, 118, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::unsafeStorageFromTH( void * th_pointer , bool retain) const",3, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::storageWithAllocator( int64_t size , Allocator * allocator) const",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::unsafeTensorFromTH( void * th_pointer , bool retain) const",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::generator() const",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toString() const",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::ID() const",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::elementSizeInBytes() const",3, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toBackend( Backend b) const",6, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toScalarType( ScalarType s) const",6, 83, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGeneral.cpp,"at::set_num_threads( int num_threads_)",4, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGeneral.cpp,"at::get_num_threads()",1, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::allocator() const",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::getDeviceFromPtr( void * data) const",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::generator() const",3, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::CPUGenerator( Context * context_)",3, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::~CPUGenerator()",4, 33, 4, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::copy( const Generator & from)",4, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::free()",4, 37, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::seed()",3, 35, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::initialSeed()",3, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::manualSeed( uint64_t seed)",4, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::manualSeedAll( uint64_t seed)",4, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::unsafeGetTH()",3, 37, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::sparseTensorIdToDeviceType( TensorTypeId type_id)",9, 90, 6, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::SparseTensorImpl( at :: TensorTypeId type_id , const caffe2 :: TypeMeta & data_type)",6, 130, 4, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::sizes() const",3, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::strides() const",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::is_contiguous() const",3, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::stride( int64_t d) const",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::resize_dim( int64_t ndim)",3, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_size( int64_t dim , int64_t new_size)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_stride( int64_t dim , int64_t new_stride)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_storage_offset( int64_t storage_offset)",3, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::dim() const",3, 40, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::maybe_zero_dim( bool condition_when_zero_dim)",7, 93, 11, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::has_storage() const",3, 45, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::storage() const",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::storage_offset() const",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_indices_and_values_unsafe( const Tensor & indices , const Tensor & values)",32, 211, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/register_symbols.cpp,"c10::InternedStrings::InternedStrings()",14, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/Range.cpp,"at::operator < <( std :: ostream & out , const Range & range)",4, 66, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyDeviceTypeInit.cpp,"at::getLegacyDeviceTypeInit()",12, 122, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/TensorImpl_test.cpp,"TEST( TensorImplTest , Caffe2Constructor)",4, 42, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::domain_prefix()",4, 60, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::symbol( const std :: string & s)",4, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::string( Symbol sym)",15, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::ns( Symbol sym)",13, 49, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::_symbol( const std :: string & s)",18, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::customString( Symbol sym)",5, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::globalStrings()",4, 43, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::fromQualString( const std :: string & s)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toUnqualString() const",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toQualString() const",3, 46, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toDisplayString() const",7, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::ns() const",3, 36, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::domainString() const",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::fromDomainAndUnqualString( const std :: string & d , const std :: string & s)",10, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::ivalue::ConstantString::create( std :: string str_)",4, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::printList( std :: ostream & out , const List & v , const std :: string start , const std :: string finish)",12, 87, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::printDict( std :: ostream & out , const Dict & v)",15, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::operator < <( std :: ostream & out , const IValue & v)",53, 72, 8, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::IValue::dump() const",3, 30, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/VariableHooksInterface.cpp,"at::detail::getVariableHooks()",15, 89, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::NonVariableTypeMode::is_enabled()",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::NonVariableTypeMode::set_enabled( bool enabled)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::NonVariableTypeMode::is_enabled()",3, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::NonVariableTypeMode::set_enabled( bool enabled)",3, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::globalLegacyTypeDispatch()",4, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::enforce_invariants()",23, 79, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::print() const",7, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::toString() const",3, 40, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::operator < <( std :: ostream & out , const Type & t)",50, 77, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::TensorType::get()",4, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::AutogradZeroTensorType::get()",4, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::NumberType::get()",4, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::IntType::get()",4, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::FloatType::get()",4, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::BoolType::get()",4, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::NoneType::get()",4, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::GeneratorType::get()",4, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::StringType::get()",4, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::DeviceObjType::get()",4, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::OptionalType::ofTensor()",4, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofTensors()",4, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofInts()",4, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofFloats()",4, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofBools()",4, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::incompleteInferTypeFrom( const IValue & value)",26, 90, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::attemptToRecoverType( const IValue & ivalue)",19, 80, 8, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::isSubvalueOf( const IValue & ivalue , TypePtr type)",36, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::tryEitherIsTheSuperType( const TypePtr & t1 , const TypePtr & t2)",9, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::unifyTypes( const TypePtr & t1 , const TypePtr & t2)",49, 91, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::matchTypeVariables( TypePtr formal , TypePtr actual , TypeEnv & type_env)",130, 109, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::evalTypeVariables( TypePtr type , std :: unordered_map<std::string,TypePtr> & type_env)",15, 112, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::typeKindToString( TypeKind kind)",8, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::Type::isSubtypeOf( const TypePtr rhs) const",6, 54, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ClassTypeRegistry::registerType( std :: string name , ClassTypePtr type)",5, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ClassTypeRegistry::getType( const std :: string & name)",7, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::getRegistry()",4, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ClassType::create( const std :: string & name , std :: shared_ptr<Module> module)",7, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ClassType::get( const std :: string & name)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"c10::operator < <( std :: ostream & out , Backend b)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::defaultfloat( std :: ios_base & __base)",4, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::FormatGuard::FormatGuard( std :: ostream & out)",4, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::FormatGuard::~FormatGuard()",3, 24, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::operator < <( std :: ostream & out , const Type & t)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printFormat( std :: ostream & stream , const Tensor & self)",87, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printIndent( std :: ostream & stream , int64_t indent)",6, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::printScale( std :: ostream & stream , double scale)",4, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printMatrix( std :: ostream & stream , const Tensor & self , int64_t linesize , int64_t indent)",50, 102, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printTensor( std :: ostream & stream , Tensor & self , int64_t linesize)",39, 73, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::print( std :: ostream & stream , const Tensor & tensor_ , int64_t linesize)",48, 104, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::detail::RegistrationListenerList::addListener( std :: unique_ptr<OpRegistrationListener> listener)",3, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::detail::RegistrationListenerList::callOnOperatorRegistered( const OperatorHandle & op)",5, 60, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::detail::RegistrationListenerList::callOnOperatorDeregistered( const OperatorHandle & op)",5, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::OpRegistrationListener::~OpRegistrationListener()",1, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::Dispatcher()",4, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::~Dispatcher()",1, 29, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::singleton()",4, 49, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::registerSchema( FunctionSchema schema)",12, 97, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::deregisterSchema( const OperatorHandle & op)",13, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::registerKernel( const OperatorHandle & op , TensorTypeId dispatch_key , KernelFunction * kernel_func , KernelCacheCreatorFunction * cache_creator_func)",4, 164, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::deregisterKernel( const OperatorHandle & op , TensorTypeId dispatch_key)",4, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp,"c10::Dispatcher::addRegistrationListener( std :: unique_ptr<OpRegistrationListener> listener)",9, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::check_dims_match_num_input_features( const char * arg_name , int64_t expected , int64_t actual)",4, 100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::repeat_if_defined( const Tensor & t , int64_t repeat)",6, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::conditional_accessor_1d( const Tensor & t)",6, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::InvStd::operator ( )( T var , double epsilon) const",7, 68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::Var::operator ( )( T var , double epsilon) const",3, 46, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu_transform_input_template( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & save_mean , const Tensor & save_invstd , const Tensor & running_mean , const Tensor & running_var , bool train , double eps)",41, 90, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu_update_stats_template( const Tensor & input , const Tensor & running_mean , const Tensor & running_var , double momentum , double eps)",48, 88, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_backward_cpu_template( const Tensor & grad_out_ , const Tensor & input , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_invstd , bool train , double eps , std :: array<bool,3> grad_input_mask)",101, 175, 68, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::_batch_norm_impl_index( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double momentum , double eps , bool cudnn_enabled)",69, 97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::_batch_norm_impl_index_backward( int64_t impl_index , const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var_transform , bool train , double epsilon , std :: array<bool,3> output_mask)",15, 158, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double momentum , double eps , bool cudnn_enabled)",7, 97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::instance_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool use_input_stats , double momentum , double eps , bool cudnn_enabled)",31, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::layer_norm( const Tensor & input , IntArrayRef normalized_shape , const Tensor & weight , const Tensor & bias , double eps , bool cudnn_enabled)",57, 85, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::group_norm( const Tensor & input , int64_t num_groups , const Tensor & weight , const Tensor & bias , double eps , bool cudnn_enabled)",44, 93, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_update_stats_cpu( const Tensor & self , const Tensor & running_mean , const Tensor & running_var , double momentum)",6, 112, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu( const Tensor & self , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool train , double momentum , double eps)",14, 175, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_backward_cpu( const Tensor & grad_out , const Tensor & self , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_invstd , bool train , double eps , std :: array<bool,3> grad_input_mask)",7, 166, 59, 0
repos/cpp/pytorch/aten/src/ATen/native/PixelShuffle.cpp,"at::native::pixel_shuffle( const Tensor & self , int64_t upscale_factor)",21, 135, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::integer_upcast( const Tensor & self , optional<ScalarType> dtype)",5, 113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::make_dim_mask( IntArrayRef dims , int64_t ndim)",11, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::allocate_reduction_result( Tensor & result , const Tensor & self , DimMask mask , bool keepdim , ScalarType dtype)",20, 68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::review_reduce_result( const Tensor & result , int ndim , DimMask mask , bool keepdim)",14, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::make_reduction( const char * name , Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim , ScalarType dtype)",27, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::n_dim_size( const Tensor & self , IntArrayRef dim)",7, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim , optional<ScalarType> dtype)",3, 91, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim , ScalarType dtype)",3, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim)",3, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim , optional<ScalarType> dtype)",11, 112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim , optional<ScalarType> dtype)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim , ScalarType dtype)",3, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim)",3, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim , optional<ScalarType> dtype)",11, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",3, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim)",3, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::get_dtype( Tensor & result , const Tensor & self , optional<ScalarType> dtype , bool promote_integers = false)",13, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype)",11, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntArrayRef dim , bool keepdim , optional<ScalarType> dtype)",5, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , ScalarType dtype)",3, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self)",3, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype)",11, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , IntArrayRef dim , bool keepdim , optional<ScalarType> dtype)",5, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , ScalarType dtype)",3, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self)",3, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype)",34, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim , ScalarType dtype)",4, 104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim)",3, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntArrayRef dim , ScalarType dtype)",3, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntArrayRef dim , bool keepdim , optional<ScalarType> dtype)",4, 107, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , optional<ScalarType> dtype)",3, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , ScalarType dtype)",3, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim , ScalarType dtype)",4, 103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntArrayRef dim , ScalarType dtype)",3, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim , ScalarType dtype)",4, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",3, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntArrayRef dim , bool keepdim , ScalarType dtype)",3, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntArrayRef dim , bool keepdim)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntArrayRef dim , ScalarType dtype)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntArrayRef dim , bool keepdim , ScalarType dtype)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntArrayRef dim , bool keepdim)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntArrayRef dim , ScalarType dtype)",3, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , bool keepdim , ScalarType dtype)",3, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , bool keepdim)",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , ScalarType dtype)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::squeeze_multiple( const Tensor & self , IntArrayRef dims)",11, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::logsumexp_out( Tensor & result , const Tensor & self , IntArrayRef dims , bool keepdim)",14, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::logsumexp( const Tensor & self , IntArrayRef dims , bool keepdim)",4, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm_out( Tensor & result , const Tensor & self , optional<Scalar> opt_p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype)",22, 96, 31, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_norm( const Tensor & self , Scalar p)",12, 98, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm_out( Tensor & result , const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , ScalarType dtype)",3, 124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm_out( Tensor & result , const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim)",3, 106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype)",5, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , ScalarType dtype)",3, 103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , optional<Scalar> p , ScalarType dtype)",3, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , Scalar p)",3, 44, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_all( Tensor & result , std :: unique_ptr<TensorIterator> & iter)",9, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all( const Tensor & self)",12, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all( const Tensor & self , int64_t dim , bool keepdim)",4, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",15, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_any( Tensor & result , std :: unique_ptr<TensorIterator> & iter)",9, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any( const Tensor & self)",12, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any( const Tensor & self , int64_t dim , bool keepdim)",4, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",15, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::min_values( const Tensor & self , IntArrayRef dims , bool keepdim)",12, 92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::max_values( const Tensor & self , IntArrayRef dims , bool keepdim)",12, 92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std_var_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool unbiased , bool keepdim , bool take_sqrt)",13, 127, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var( const Tensor & self , bool unbiased)",7, 104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var( const Tensor & self , IntArrayRef dim , bool unbiased , bool keepdim)",4, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool unbiased , bool keepdim)",3, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std( const Tensor & self , bool unbiased)",7, 104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std( const Tensor & self , IntArrayRef dim , bool unbiased , bool keepdim)",4, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool unbiased , bool keepdim)",3, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::get_target_prime( target_t * target , int64_t offset , int64_t stride , int64_t idx , int64_t BLANK)",7, 119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_cpu_template( const Tensor & log_probs , const Tensor & targets , IntArrayRef input_lengths , IntArrayRef target_lengths , int64_t BLANK)",123, 169, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_backward_cpu_template( const Tensor & grad_out , const Tensor & log_probs , const Tensor & targets , IntArrayRef input_lengths , IntArrayRef target_lengths , const Tensor & neg_log_likelihood , const Tensor & log_alpha , int64_t BLANK , bool zero_infinity)",141, 165, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_cpu( const Tensor & log_probs , const Tensor & targets , IntArrayRef input_lengths , IntArrayRef target_lengths , int64_t BLANK , bool zero_infinity)",10, 180, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_backward_cpu( const Tensor & grad , const Tensor & log_probs , const Tensor & targets , IntArrayRef input_lengths , IntArrayRef target_lengths , const Tensor & neg_log_likelihood , const Tensor & log_alpha , int64_t BLANK , bool zero_infinity)",10, 166, 0, 1
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss( const Tensor & log_probs , const Tensor & targets , IntArrayRef input_lengths , IntArrayRef target_lengths , int64_t BLANK , int64_t reduction , bool zero_infinity)",41, 175, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss( const Tensor & log_probs , const Tensor & targets , const Tensor & input_lengths , const Tensor & target_lengths , int64_t BLANK , int64_t reduction , bool zero_infinity)",10, 179, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::make_feature_noise( const Tensor & input)",11, 93, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::is_fused_kernel_acceptable( const Tensor & input , double p)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::multiply( Tensor & input , const Tensor & noise)",4, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::multiply( const Tensor & input , const Tensor & noise)",4, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::_dropout_impl( T & input , double p , bool train)",28, 92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::dropout( const Tensor & input , double p , bool train)",6, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::dropout_( Tensor & input , double p , bool train)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_dropout( const Tensor & input , double p , bool train)",3, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_dropout_( Tensor & input , double p , bool train)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::alpha_dropout( const Tensor & input , double p , bool train)",3, 66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::alpha_dropout_( Tensor & input , double p , bool train)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_alpha_dropout( const Tensor & input , double p , bool train)",3, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_alpha_dropout_( Tensor & input , double p , bool train)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::flip_cpu_kernel( const int64_t total_dims , const std :: vector<int64_t> & stride_contiguous_v , const std :: bitset<dim_bitset_size> & flip_dims_b , const Tensor & in_tensor , Tensor & out_tensor)",30, 113, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::flip_cpu( const Tensor & self , IntArrayRef dims)",28, 105, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::roll_cpu( const Tensor & self , IntArrayRef shifts , IntArrayRef dims)",29, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::rot90( const Tensor & self , int64_t k , IntArrayRef dims)",34, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Memory.cpp,"at::native::pin_memory( const Tensor & self)",9, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::linear( const Tensor & input , const Tensor & weight , const Tensor & bias)",11, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::sumproduct_pair( const Tensor & left_ , const Tensor & right_ , IntArrayRef sum_dims_ , bool keepdim)",95, 116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::einsum( std :: string eqn , TensorList tensors)",232, 191, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::_trilinear( const Tensor & i1_ , const Tensor & i2_ , const Tensor & i3_ , IntArrayRef expand1_ , IntArrayRef expand2_ , IntArrayRef expand3_ , IntArrayRef sumdim_ , int64_t unroll_dim)",68, 106, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::bilinear( const Tensor & input1 , const Tensor & input2 , const Tensor & weight , const Tensor & bias)",28, 130, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::tensordot( const Tensor & input1 , const Tensor & input2 , IntArrayRef dims1 , IntArrayRef dims2)",55, 111, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Resize.cpp,"at::native::resize_cpu_( Tensor & self , IntArrayRef size)",6, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding( const Tensor & weight , const Tensor & indices , int64_t padding_idx , bool scale_grad_by_freq , bool sparse)",16, 78, 17, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_backward( const Tensor & grad , const Tensor & indices , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq , bool sparse)",11, 70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_sparse_backward( const Tensor & grad_ , const Tensor & indices_ , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq)",36, 87, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_dense_backward_cpu( const Tensor & grad_ , const Tensor & indices , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq)",67, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_renorm_cpu_( Tensor & self , const Tensor & indices , double max_norm , double norm_type)",29, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_generate_intervals( scalar_t sample , int inputSize , int outputSize , int poolSize)",17, 81, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_out_single_batch_frame( scalar_t * input , scalar_t * output , int64_t * indices , scalar_t * randomSamples , int numPlanes , int inputW , int inputH , int outputW , int outputH , int poolSizeW , int poolSizeH)",62, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_out_frame( scalar_t * input , scalar_t * output , int64_t * indices , scalar_t * randomSamples , int numBatch , int numPlanes , int inputW , int inputH , int outputW , int outputH , int poolSizeW , int poolSizeH)",30, 76, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_out_cpu_template( const at :: Tensor & input_ , at :: Tensor & output , IntArrayRef output_size , IntArrayRef pool_size , at :: Tensor & indices , const at :: Tensor & randomSamples)",73, 76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_backward_out_single_batch_frame( scalar_t * gradInput , scalar_t * gradOutput , int64_t * indices , int numPlanes , int inputW , int inputH , int outputW , int outputH)",26, 75, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_backward_out_frame( scalar_t * gradInput , scalar_t * gradOutput , int64_t * indices , int numBatch , int numPlanes , int inputW , int inputH , int outputW , int outputH)",25, 71, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_backward_out_cpu_template( const at :: Tensor & input , const at :: Tensor & gradOutput_ , at :: Tensor & gradInput , IntArrayRef output_size , IntArrayRef pool_size , const at :: Tensor & indices)",59, 75, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_out_cpu( at :: Tensor & output , at :: Tensor & indices , const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & randomSamples)",17, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_cpu( const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & randomSamples)",17, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_backward_out_cpu( at :: Tensor & gradInput , const at :: Tensor & gradOutput_ , const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & indices)",18, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool2d.cpp,"at::native::fractional_max_pool2d_backward_cpu( const at :: Tensor & gradOutput_ , const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & indices)",17, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_out( Tensor & result , const Tensor & self , const Tensor & other , Scalar alpha)",15, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add( const Tensor & self , const Tensor & other , Scalar alpha)",10, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_( Tensor & self , const Tensor & other , Scalar alpha)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_out( Tensor & result , const Tensor & self , const Tensor & other)",12, 77, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div( const Tensor & self , const Tensor & other)",10, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_( Tensor & self , const Tensor & other)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_out( Tensor & result , const Tensor & self , const Tensor & other)",8, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul( const Tensor & self , const Tensor & other)",10, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_( Tensor & self , const Tensor & other)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_out( Tensor & result , const Tensor & self , const Tensor & other , Scalar alpha)",18, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub( const Tensor & self , const Tensor & other , Scalar alpha)",10, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_( Tensor & self , const Tensor & other , Scalar alpha)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::rsub( const Tensor & self , const Tensor & other , Scalar alpha)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::wrapped_scalar_tensor( Scalar scalar)",5, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add( const Tensor & self , Scalar other , Scalar alpha)",3, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_( Tensor & self , Scalar other , Scalar alpha)",3, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div( const Tensor & self , Scalar other)",3, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_( Tensor & self , Scalar other)",3, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul( const Tensor & self , Scalar other)",3, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_( Tensor & self , Scalar other)",3, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub( const Tensor & self , Scalar other , Scalar alpha)",3, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_( Tensor & self , Scalar other , Scalar alpha)",3, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::rsub( const Tensor & self , Scalar other , Scalar alpha)",3, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::checkLongTensor( const Tensor & tensor)",4, 110, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pack_padded_sequence( const Tensor & _input , const Tensor & _lengths , bool batch_first)",75, 115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pack_padded_sequence_backward( const Tensor & grad , at :: IntArrayRef input_size , const Tensor & _batch_sizes , bool batch_first)",24, 134, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pad_packed_sequence( const Tensor & data , const Tensor & _batch_sizes , bool batch_first , Scalar padding_value , int64_t total_length)",62, 160, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"_copy__cpu( at :: Tensor & self , const at :: Tensor & src)",7, 74, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"_copy__cpu( at :: Tensor & self , const at :: Tensor & src)",6, 91, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"copy_transpose_valid( const at :: Tensor & self , const at :: Tensor & src)",6, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_s_copy__cpu( Tensor & self , const Tensor & src , bool non_blocking)",9, 107, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_copy_same_type_transpose_( Tensor & self , const Tensor & src)",50, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_copy_same_type__cpu( Tensor & self , const Tensor & src)",42, 80, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t nplane , int64_t input_w , int64_t output_w , int64_t pad_l)",28, 54, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_out_loop( scalar_t * input_p , scalar_t * output_p , int64_t nbatch , int64_t nplane , int64_t input_w , int64_t output_w , int64_t pad_l)",16, 43, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_out_template( Tensor & output , const Tensor & input_ , IntArrayRef padding)",55, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_backward_out_frame( scalar_t * grad_input , scalar_t * grad_output , int64_t nplane , int64_t input_w , int64_t output_w , int64_t pad_l)",28, 56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_backward_out_loop( scalar_t * grad_input , scalar_t * grad_output , int64_t nbatch , int64_t nplane , int64_t input_w , int64_t output_w , int64_t pad_l)",16, 51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_backward_out_template( Tensor & grad_input , const Tensor & grad_output_ , const Tensor & input , IntArrayRef padding)",50, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t nplane , int64_t input_w , int64_t input_h , int64_t output_w , int64_t output_h , int64_t pad_l , int64_t pad_t)",42, 79, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_out_loop( scalar_t * input_p , scalar_t * output_p , int64_t nbatch , int64_t nplane , int64_t input_w , int64_t input_h , int64_t output_w , int64_t output_h , int64_t pad_l , int64_t pad_t)",17, 51, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_out_template( Tensor & output , const Tensor & input_ , IntArrayRef padding)",69, 79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_backward_out_frame( scalar_t * grad_input , scalar_t * grad_output , int64_t nplane , int64_t input_w , int64_t input_h , int64_t output_w , int64_t output_h , int64_t pad_l , int64_t pad_t)",44, 70, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_backward_out_loop( scalar_t * grad_input , scalar_t * grad_output , int64_t nbatch , int64_t nplane , int64_t input_w , int64_t input_h , int64_t output_w , int64_t output_h , int64_t pad_l , int64_t pad_t)",17, 54, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_backward_out_template( Tensor & grad_input , const Tensor & grad_output_ , const Tensor & input , IntArrayRef padding)",61, 69, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_out_cpu( Tensor & output , const Tensor & input , IntArrayRef padding)",5, 64, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_cpu( const Tensor & input , IntArrayRef padding)",5, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_backward_out_cpu( Tensor & grad_input , const Tensor & grad_output , const Tensor & input , IntArrayRef padding)",11, 46, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad1d_backward_cpu( const Tensor & grad_output , const Tensor & input , IntArrayRef padding)",9, 46, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_out_cpu( Tensor & output , const Tensor & input , IntArrayRef padding)",5, 64, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_cpu( const Tensor & input , IntArrayRef padding)",5, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_backward_out_cpu( Tensor & grad_input , const Tensor & grad_output , const Tensor & input , IntArrayRef padding)",11, 46, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReflectionPad.cpp,"at::native::reflection_pad2d_backward_cpu( const Tensor & grad_output , const Tensor & input , IntArrayRef padding)",9, 46, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::generate_intervals( scalar_t sample , int64_t inputSize , int64_t outputSize , int64_t poolSize)",17, 81, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_out_single_batch_frame( scalar_t * input , scalar_t * output , int64_t * indices , scalar_t * randomSamples , int64_t numPlanes , int64_t inputT , int64_t inputH , int64_t inputW , int64_t outputT , int64_t outputH , int64_t outputW , int64_t poolSizeT , int64_t poolSizeH , int64_t poolSizeW)",72, 79, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_out_frame( scalar_t * input , scalar_t * output , int64_t * indices , scalar_t * randomSamples , int64_t numBatch , int64_t numPlanes , int64_t inputT , int64_t inputH , int64_t inputW , int64_t outputT , int64_t outputH , int64_t outputW , int64_t poolSizeT , int64_t poolSizeH , int64_t poolSizeW)",34, 67, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_out_cpu_template( Tensor & output , Tensor & indices , const Tensor & input_ , IntArrayRef pool_size , IntArrayRef output_size , const Tensor & randomSamples)",80, 76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_backward_out_single_batch_frame( scalar_t * gradInput , scalar_t * gradOutput , int64_t * indices , int64_t numPlanes , int64_t inputT , int64_t inputH , int64_t inputW , int64_t outputT , int64_t outputH , int64_t outputW)",28, 80, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_backward_out_frame( scalar_t * gradInput , scalar_t * gradOutput , int64_t * indices , int64_t numBatch , int64_t numPlanes , int64_t inputT , int64_t inputH , int64_t inputW , int64_t outputT , int64_t outputH , int64_t outputW)",29, 71, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_backward_out_cpu_template( const Tensor & input , const Tensor & gradOutput_ , Tensor & gradInput , IntArrayRef output_size , IntArrayRef pool_size , const Tensor & indices)",64, 81, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_out_cpu( at :: Tensor & output , at :: Tensor & indices , const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & randomSamples)",16, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_cpu( const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & randomSamples)",16, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_backward_out_cpu( at :: Tensor & gradInput , const at :: Tensor & gradOutput_ , const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & indices)",16, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/FractionalMaxPool3d.cpp,"at::native::fractional_max_pool3d_backward_cpu( const at :: Tensor & gradOutput_ , const at :: Tensor & input , IntArrayRef pool_size , IntArrayRef output_size , const at :: Tensor & indices)",16, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_cuda( const Tensor & self)",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_distributed( const Tensor & self)",3, 42, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_complex( const Tensor & self)",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_floating_point( const Tensor & self)",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_signed( const Tensor & self)",8, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_sparse( const Tensor & self)",3, 37, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::type_as( const Tensor & self , const Tensor & other)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::invalid_mask( const Tensor & self , int64_t idx , const Tensor & mask , int64_t maskIdx)",7, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::checkIndexTensorTypes( TensorList indices)",10, 82, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::expandByteTensors( const Tensor & self , TensorList indices)",31, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::hasContiguousSubspace( TensorList tl)",9, 74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::transposeToFront( Tensor self , TensorList indices)",18, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::computeLinearStride( const Tensor & tensor)",8, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::unsqueezeN( const Tensor & src , int64_t before , int64_t after)",9, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::wrapIndexOnce( const Tensor & index , int64_t dim , int64_t dim_size)",13, 107, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::computeLinearIndex( const Tensor & src , TensorList indices)",53, 96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::makeLinearIndex( Tensor self , TensorList orig)",18, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::all_strides_match( TensorList tensors)",10, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::shapes_as_str( TensorList tensors)",14, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::restride_src( const Tensor & src , int64_t dims_before , int64_t dims_indexed , IntArrayRef replacement_shape)",11, 97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::reshape_indexer( const Tensor & index , int64_t dims_before , int64_t dims_after)",8, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::AdvancedIndex::AdvancedIndex( const Tensor & src , TensorList indices_list)",50, 101, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_info( Tensor self , TensorList orig)",28, 86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_index_iterator( const AdvancedIndex & info)",10, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_index_put_iterator( const AdvancedIndex & info , const Tensor & value)",15, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index( const Tensor & self , TensorList indices)",10, 108, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_put( const Tensor & self , TensorList indices , const Tensor & value , bool accumulate)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_put_( Tensor & self , TensorList indices , const Tensor & value , bool accumulate)",15, 108, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_copy_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",40, 127, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_copy( const Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_add( const Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 98, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_fill( const Tensor & self , int64_t dim , const Tensor & index , Scalar source)",3, 91, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_fill( const Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::scatter( const Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::scatter( const Tensor & self , int64_t dim , const Tensor & index , Scalar source)",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::scatter_add( const Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::masked_scatter( const Tensor & self , const Tensor & mask , const Tensor & source)",5, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::masked_fill( const Tensor & self , const Tensor & mask , Scalar source)",5, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::masked_fill( const Tensor & self , const Tensor & mask , const Tensor & source)",5, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::_gather_sparse_backward( const Tensor & self , int64_t dim , const Tensor & index , const Tensor & grad)",19, 159, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::data_ptr( const Tensor & self)",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , Storage source)",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , Storage source , int64_t storage_offset , IntArrayRef size , IntArrayRef stride)",3, 108, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , const Tensor & source)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self)",3, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::is_set_to( const Tensor & self , const Tensor & tensor)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_fill_( Tensor & self , const Tensor & mask , Scalar value)",3, 73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_fill_( Tensor & self , const Tensor & mask , const Tensor & value)",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_scatter_( Tensor & self , const Tensor & mask , const Tensor & source)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::view( const Tensor & self , IntArrayRef size)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::put_( Tensor & self , const Tensor & index , const Tensor & source , bool accumulate)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_add_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_fill_( Tensor & self , int64_t dim , const Tensor & index , Scalar value)",3, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_fill_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & value)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & src)",3, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_( Tensor & self , int64_t dim , const Tensor & index , Scalar value)",3, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_add_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & src)",3, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_( Tensor & self , Scalar other)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_( Tensor & self , const Tensor & other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_( Tensor & self , Scalar other)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_( Tensor & self , const Tensor & other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_( Tensor & self , Scalar other)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_( Tensor & self , const Tensor & other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_( Tensor & self , Scalar other)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_( Tensor & self , const Tensor & other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_( Tensor & self , Scalar other)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_( Tensor & self , const Tensor & other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_( Tensor & self , Scalar other)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_( Tensor & self , const Tensor & other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma_( Tensor & self)",3, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2_( Tensor & self , const Tensor & other)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma_( Tensor & self)",3, 45, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma_( Tensor & self , int64_t n)",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv_( Tensor & self)",3, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac_( Tensor & self)",3, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm_( Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal_( Tensor & self)",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg_( Tensor & self)",3, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_( Tensor & self , Scalar exponent)",3, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_( Tensor & self , const Tensor & exponent)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign_( Tensor & self)",3, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_( Tensor & self , Scalar other)",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_( Tensor & self , const Tensor & other)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_( Tensor & self , Scalar other)",3, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_( Tensor & self , const Tensor & other)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm_out( Tensor & result , const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm( const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul_( Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv_( Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , int64_t from , int64_t to , Generator * generator)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , int64_t to , Generator * generator)",3, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , Generator * generator)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::uniform_( Tensor & self , double from , double to , Generator * generator)",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_( Tensor & self , double mean , double std , Generator * generator)",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cauchy_( Tensor & self , double median , double sigma , Generator * generator)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::log_normal_( Tensor & self , double mean , double std , Generator * generator)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::exponential_( Tensor & self , double lambd , Generator * generator)",3, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geometric_( Tensor & self , double p , Generator * generator)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::diag_out( Tensor & result , const Tensor & self , int64_t diagonal)",3, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::diag( const Tensor & self , int64_t diagonal)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cross_out( Tensor & result , const Tensor & self , const Tensor & other , int64_t dim)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cross( const Tensor & self , const Tensor & other , int64_t dim)",3, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trace( const Tensor & self)",3, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_out( Tensor & result , const Tensor & self , Scalar other)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne( const Tensor & self , Scalar other)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne( const Tensor & self , const Tensor & other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_out( Tensor & result , const Tensor & self , Scalar other)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq( const Tensor & self , Scalar other)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq( const Tensor & self , const Tensor & other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_out( Tensor & result , const Tensor & self , Scalar other)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge( const Tensor & self , Scalar other)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge( const Tensor & self , const Tensor & other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_out( Tensor & result , const Tensor & self , Scalar other)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le( const Tensor & self , Scalar other)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le( const Tensor & self , const Tensor & other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_out( Tensor & result , const Tensor & self , Scalar other)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt( const Tensor & self , Scalar other)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt( const Tensor & self , const Tensor & other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_out( Tensor & result , const Tensor & self , Scalar other)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt( const Tensor & self , Scalar other)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt( const Tensor & self , const Tensor & other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::take_out( Tensor & result , const Tensor & self , const Tensor & index)",3, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::take( const Tensor & self , const Tensor & index)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_select_out( Tensor & result , const Tensor & self , int64_t dim , const Tensor & index)",3, 101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_select( const Tensor & self , int64_t dim , const Tensor & index)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_select_out( Tensor & result , const Tensor & self , const Tensor & mask)",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_select( const Tensor & self , const Tensor & mask)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::nonzero_out( Tensor & result , const Tensor & self)",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::nonzero( const Tensor & self)",3, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gather_out( Tensor & result , const Tensor & self , int64_t dim , const Tensor & index , bool sparse_grad)",3, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gather( const Tensor & self , int64_t dim , const Tensor & index , bool sparse_grad)",3, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul_out( Tensor & result , const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 123, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul( const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv_out( Tensor & result , const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 123, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv( const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gels_out( Tensor & X , Tensor & qr , const Tensor & self , const Tensor & A)",3, 105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gels( const Tensor & self , const Tensor & A)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trtrs_out( Tensor & X , Tensor & M , const Tensor & self , const Tensor & A , bool upper , bool transpose , bool unitriangular)",3, 153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trtrs( const Tensor & self , const Tensor & A , bool upper , bool transpose , bool unitriangular)",3, 121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::symeig_out( Tensor & e , Tensor & V , const Tensor & self , bool eigenvectors , bool upper)",3, 119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::symeig( const Tensor & self , bool eigenvectors , bool upper)",3, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eig_out( Tensor & e , Tensor & v , const Tensor & self , bool eigenvectors)",3, 104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eig( const Tensor & self , bool eigenvectors)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::svd_out( Tensor & U , Tensor & S , Tensor & V , const Tensor & self , bool some , bool compute_uv)",3, 134, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::svd( const Tensor & self , bool some , bool compute_uv)",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::potri_out( Tensor & result , const Tensor & self , bool upper)",3, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::potri( const Tensor & self , bool upper)",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pstrf_out( Tensor & u , Tensor & piv , const Tensor & self , bool upper , Scalar tol)",3, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pstrf( const Tensor & self , bool upper , Scalar tol)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::qr_out( Tensor & Q , Tensor & R , const Tensor & self)",3, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::qr( const Tensor & self)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geqrf_out( Tensor & result0 , Tensor & result1 , const Tensor & self)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geqrf( const Tensor & self)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::orgqr_out( Tensor & result , const Tensor & self , const Tensor & input2)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::orgqr( const Tensor & self , const Tensor & input2)",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ormqr_out( Tensor & result , const Tensor & self , const Tensor & input2 , const Tensor & input3 , bool left , bool transpose)",3, 132, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ormqr( const Tensor & self , const Tensor & input2 , const Tensor & input3 , bool left , bool transpose)",3, 109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrisolve_out( Tensor & result , const Tensor & self , const Tensor & LU_data , const Tensor & LU_pivots)",3, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrisolve( const Tensor & self , const Tensor & LU_data , const Tensor & LU_pivots)",3, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::multinomial_out( Tensor & result , const Tensor & self , int64_t num_samples , bool replacement , Generator * generator)",3, 127, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::multinomial( const Tensor & self , int64_t num_samples , bool replacement , Generator * generator)",3, 104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma_out( Tensor & result , const Tensor & self)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma( const Tensor & self)",3, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma_out( Tensor & result , const Tensor & self)",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma( const Tensor & self)",3, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma_out( Tensor & result , int64_t n , const Tensor & self)",3, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma( int64_t n , const Tensor & self)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv_out( Tensor & result , const Tensor & self)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv( const Tensor & self)",3, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac_out( Tensor & result , const Tensor & self)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac( const Tensor & self)",3, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::dist( const Tensor & self , const Tensor & other , Scalar p)",3, 67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal_out( Tensor & result , const Tensor & self)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal( const Tensor & self)",3, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg_out( Tensor & result , const Tensor & self)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg( const Tensor & self)",3, 40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2( const Tensor & self , const Tensor & other)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_histc_out_cpu( Tensor & result , const Tensor & self , int64_t bins , Scalar min , Scalar max)",3, 102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_histc_cpu( const Tensor & self , int64_t bins , Scalar min , Scalar max)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign_out( Tensor & result , const Tensor & self)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign( const Tensor & self)",3, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_out( Tensor & result , const Tensor & self , Scalar other)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod( const Tensor & self , Scalar other)",3, 49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod( const Tensor & self , const Tensor & other)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_out( Tensor & result , const Tensor & self , Scalar other)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder( const Tensor & self , Scalar other)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder( const Tensor & self , const Tensor & other)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min( const Tensor & self , const Tensor & other)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min( const Tensor & self)",3, 40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max( const Tensor & self , const Tensor & other)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max( const Tensor & self)",3, 40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::median( const Tensor & self)",3, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sort_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool descending)",3, 127, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sort( const Tensor & self , int64_t dim , bool descending)",3, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::argsort( const Tensor & self , int64_t dim , bool descending)",3, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::topk_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t k , int64_t dim , bool largest , bool sorted)",3, 148, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::topk( const Tensor & self , int64_t k , int64_t dim , bool largest , bool sorted)",3, 105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm_out( Tensor & result , const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm( const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",3, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::unfold( const Tensor & self , int64_t dimension , int64_t size , int64_t step)",3, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::equal( const Tensor & self , const Tensor & other)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_out( Tensor & result , const Tensor & self , const Tensor & exponent)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow( const Tensor & self , const Tensor & exponent)",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_out( Tensor & result , Scalar self , const Tensor & exponent)",3, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow( Scalar self , const Tensor & exponent)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , const Tensor & mean , double std , Generator * generator)",3, 95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( const Tensor & mean , double std , Generator * generator)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , double mean , const Tensor & std , Generator * generator)",3, 95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( double mean , const Tensor & std , Generator * generator)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , const Tensor & mean , const Tensor & std , Generator * generator)",3, 103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( const Tensor & mean , const Tensor & std , Generator * generator)",3, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::alias( const Tensor & self)",3, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_dirichlet_grad_out( Tensor & output , const Tensor & x , const Tensor & alpha , const Tensor & total)",3, 110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_dirichlet_grad( const Tensor & x , const Tensor & alpha , const Tensor & total)",3, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__and__( const Tensor & self , Scalar other)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__and__( const Tensor & self , const Tensor & other)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__or__( const Tensor & self , Scalar other)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__or__( const Tensor & self , const Tensor & other)",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__xor__( const Tensor & self , Scalar other)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__xor__( const Tensor & self , const Tensor & other)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__lshift__( const Tensor & self , Scalar other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__lshift__( const Tensor & self , const Tensor & other)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__rshift__( const Tensor & self , Scalar other)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__rshift__( const Tensor & self , const Tensor & other)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__iand__( Tensor & self , Scalar other)",3, 49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__iand__( Tensor & self , const Tensor & other)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ior__( Tensor & self , Scalar other)",3, 48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ior__( Tensor & self , const Tensor & other)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ixor__( Tensor & self , Scalar other)",3, 49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ixor__( Tensor & self , const Tensor & other)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ilshift__( Tensor & self , Scalar other)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ilshift__( Tensor & self , const Tensor & other)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__irshift__( Tensor & self , Scalar other)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__irshift__( Tensor & self , const Tensor & other)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::reorder_dimensions()",53, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::compute_result_type( at :: ArrayRef<OperandInfo> operands , const F & predicate)",16, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_types()",48, 122, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_common_type()",23, 90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compatible_stride( int element_size) const",9, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::invert_perm( IntArrayRef input) const",10, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::allocate_outputs()",17, 86, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::coalesce_dimensions()",52, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::numel() const",7, 40, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_dim_strides( int dim) const",8, 67, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_data_ptrs( ArrayRef<char*> base , IntArrayRef counter) const",10, 103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_base_ptrs() const",7, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_dim_reduced( int dim) const",8, 72, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::permute_dimensions( IntArrayRef perm)",19, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::num_output_elements() const",9, 68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::num_reduce_dims() const",9, 47, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::loop_wrapper( const loop_t & loop)",15, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::for_each( const loop_t & loop)",3, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::for_each( const loop2d_t & loop)",12, 87, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_strides() const",9, 59, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::serial_for_each( const loop_t & loop , Range range) const",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::serial_for_each( const loop2d_t & loop , Range range) const",23, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_trivial_1d() const",4, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_scalar( int arg) const",9, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_cpu_scalar( int arg) const",3, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::data_ptr( int arg) const",3, 48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::remove_operand( int arg)",3, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::replace_operand( int arg , void * data , IntArrayRef stride)",4, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::remove_dimension( int dim)",7, 58, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::narrow( int dim , int64_t start , int64_t size)",10, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::select_all_keeping_dim( int start_dim , IntArrayRef indices)",9, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::binary_op( Tensor & out , const Tensor & a , const Tensor & b)",13, 107, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::reduce_op( Tensor & out , const Tensor & a)",10, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::mark_outputs()",15, 60, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_shape()",37, 94, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::compute_stride( const Tensor & tensor , IntArrayRef shape)",17, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_strides()",7, 59, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::can_use_32bit_indexing() const",16, 62, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::split( int dim)",14, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_dim_to_split() const",17, 58, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::with_32bit_indexing() const",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::Builder::build()",23, 67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::iterator( const TensorIterator & iter)",5, 66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::operator ++()",9, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::operator *() const",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::begin() const",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::end() const",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::DimCounter( IntArrayRef shape , Range range)",16, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::is_done() const",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::increment( const std :: array<int64_t,2> & step)",25, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::max_2d_step() const",8, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::linspace_from_neg_one( const Tensor & grid , int64_t num_steps)",7, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::make_base_grid_4D( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",14, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::make_base_grid_5D( const Tensor & theta , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",16, 96, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_4D( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",10, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_5D( const Tensor & theta , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",11, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator( const Tensor & theta , IntArrayRef size)",11, 80, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_4D_backward( const Tensor & grad_grid , int64_t N , int64_t C , int64_t H , int64_t W)",13, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_5D_backward( const Tensor & grad_grid , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",14, 65, 24, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_backward( const Tensor & grad , IntArrayRef size)",12, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Scalar.cpp,"at::native::item( const Tensor & self)",11, 92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Scalar.cpp,"at::native::_local_scalar_dense_cpu( const Tensor & self)",9, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"lerp_cpu( at :: Tensor & ret , const at :: Tensor & self , const at :: Tensor & end , const at :: Tensor & weight)",10, 106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"lerp_cpu( at :: Tensor & ret , const at :: Tensor & self , const at :: Tensor & end , scalar_t weight_val)",9, 101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"at::native::lerp_cpu_tensor_out( Tensor & result , const Tensor & self , const Tensor & end , const Tensor & weight)",12, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"at::native::lerp_cpu_scalar_out( Tensor & result , const Tensor & self , const Tensor & end , Scalar weight)",10, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"at::native::lerp_cpu_tensor_( Tensor & self , const Tensor & end , const Tensor & weight)",13, 87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"at::native::lerp_cpu_scalar_( Tensor & self , const Tensor & end , Scalar weight)",11, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"at::native::lerp_cpu_tensor( const Tensor & self , const Tensor & end , const Tensor & weight)",11, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Lerp.cpp,"at::native::lerp_cpu_scalar( const Tensor & self , const Tensor & end , Scalar weight)",9, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp( const Tensor & self , optional<Scalar> min , optional<Scalar> max)",4, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp_max( const Tensor & self , Scalar max)",4, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp_min( const Tensor & self , Scalar min)",4, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp__cpu( Tensor & self , optional<Scalar> min , optional<Scalar> max)",3, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_out_cpu( Tensor & result , const Tensor & self , optional<Scalar> min , optional<Scalar> max)",16, 65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_max__cpu( Tensor & self , Scalar max)",3, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_max_out_cpu( Tensor & result , const Tensor & self , Scalar max)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_min__cpu( Tensor & self , Scalar min)",3, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_min_out_cpu( Tensor & result , const Tensor & self , Scalar min)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::fill_( Tensor & self , Scalar value)",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::fill_( Tensor & self , const Tensor & value)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::mvlgamma( const Tensor & self , int64_t p)",10, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::mvlgamma_( Tensor & self , int64_t p)",10, 85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::check1d( const char * function_name , const char * argument_name , IntArrayRef x)",9, 56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::adaptive_avg_pool1d( const Tensor & self , IntArrayRef output_size)",10, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::adaptive_max_pool1d( const Tensor & self , IntArrayRef output_size)",11, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool1d_with_indices( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",27, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::avg_pool1d( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",25, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool1d( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",11, 64, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool2d( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",11, 64, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool3d( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",11, 64, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding)",8, 76, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution_backward_input( const at :: Tensor & input , const at :: Tensor & gradOutput , const at :: Tensor & weight , IntArrayRef padding)",8, 91, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution_backward_weight( const at :: Tensor & input , at :: IntArrayRef weight_size , const at :: Tensor & gradOutput , IntArrayRef padding)",8, 92, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution_backward( const at :: Tensor & input , const at :: Tensor & gradOutput , const at :: Tensor & weight , IntArrayRef padding , std :: array<bool,3> output_mask)",9, 86, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_available()",3, 27, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::nnpack_threadpool()",26, 81, 0, 1
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_available()",9, 45, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::deallocate_workspace()",5, 44, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::allocate_workspace()",6, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::conv_output_size( IntArrayRef input_size , IntArrayRef weight_size , IntArrayRef padding)",14, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding)",161, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution_backward_input( const at :: Tensor & input , const at :: Tensor & gradOutput , const at :: Tensor & weight , IntArrayRef padding)",129, 87, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution_backward_weight( const at :: Tensor & input , IntArrayRef weight_size , const at :: Tensor & gradOutput , IntArrayRef padding)",118, 80, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/NNPACK.cpp,"at::native::_nnpack_spatial_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , std :: array<bool,3> output_mask)",24, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Itertools.cpp,"_triu_mask( int64_t n , int64_t dims , bool diagonal , TensorOptions opt)",17, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Itertools.cpp,"at::native::cartesian_prod( TensorList tensors)",13, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Itertools.cpp,"at::native::combinations( const Tensor & self , int64_t r , bool with_replacement)",11, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SummaryOps.cpp,"at::native::_bincount_cpu_template( const Tensor & self , const Tensor & weights , int64_t minlength)",40, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/SummaryOps.cpp,"at::native::_bincount_cpu( const Tensor & self , const Tensor & weights , int64_t minlength)",9, 106, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 139, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 180, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 145, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 118, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 144, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 117, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 114, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss( const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",4, 118, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_forward_out( Tensor & output , Tensor & is_target , const Tensor & self , const Tensor & target , int64_t reduction)",3, 167, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_forward( const Tensor & self , const Tensor & target , int64_t reduction)",3, 122, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction , const Tensor & is_target)",3, 185, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction , const Tensor & is_target)",3, 158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",4, 149, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 126, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_forward_out( Tensor & output , Tensor & total_weight , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 201, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_forward( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 219, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 192, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",4, 151, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_forward_out( Tensor & output , Tensor & total_weight , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 203, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_forward( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 155, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 221, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 194, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",4, 110, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",4, 112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 126, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_out( Tensor & output , const Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",3, 105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu( const Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",3, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_backward_out( Tensor & grad_input , const Tensor & grad_output , Scalar alpha , Scalar scale , Scalar input_scale , const Tensor & output)",3, 148, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_backward( const Tensor & grad_output , Scalar alpha , Scalar scale , Scalar input_scale , const Tensor & output)",3, 121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_( Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_out( Tensor & output , const Tensor & self , int64_t dim)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu( const Tensor & self , int64_t dim)",3, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , int64_t dim)",3, 111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_backward( const Tensor & grad_output , const Tensor & self , int64_t dim)",3, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_out( Tensor & output , const Tensor & self , Scalar min_val , Scalar max_val)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh( const Tensor & self , Scalar min_val , Scalar max_val)",3, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar min_val , Scalar max_val)",3, 135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_backward( const Tensor & grad_output , const Tensor & self , Scalar min_val , Scalar max_val)",3, 108, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_( Tensor & self , Scalar min_val , Scalar max_val)",3, 74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_out( Tensor & output , const Tensor & self , Scalar negative_slope)",3, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu( const Tensor & self , Scalar negative_slope)",3, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar negative_slope)",3, 128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_backward( const Tensor & grad_output , const Tensor & self , Scalar negative_slope)",3, 101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_( Tensor & self , Scalar negative_slope)",3, 74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_out( Tensor & output , const Tensor & self)",4, 91, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid( const Tensor & self)",3, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_forward_out( Tensor & output , Tensor & buffer , const Tensor & self)",3, 111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_forward( const Tensor & self)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & buffer)",3, 129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_backward( const Tensor & grad_output , const Tensor & self , const Tensor & buffer)",3, 102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_out( Tensor & output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise( const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training)",3, 176, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_backward( const Tensor & grad_output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training)",3, 149, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_( Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 132, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_out( Tensor & output , const Tensor & self , Scalar beta , Scalar threshold)",3, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus( const Tensor & self , Scalar beta , Scalar threshold)",3, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar beta , Scalar threshold , const Tensor & output)",3, 157, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_backward( const Tensor & grad_output , const Tensor & self , Scalar beta , Scalar threshold , const Tensor & output)",3, 130, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_out( Tensor & output , const Tensor & self , Scalar lambd)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink( const Tensor & self , Scalar lambd)",3, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar lambd)",3, 119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_backward( const Tensor & grad_output , const Tensor & self , Scalar lambd)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_out( Tensor & output , const Tensor & self , IntArrayRef output_size)",3, 98, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d( const Tensor & self , IntArrayRef output_size)",3, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self)",3, 114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_backward( const Tensor & grad_output , const Tensor & self)",3, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_out( Tensor & output , Tensor & indices , const Tensor & self , IntArrayRef output_size)",3, 137, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d( const Tensor & self , IntArrayRef output_size)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 138, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_out( Tensor & output , Tensor & indices , const Tensor & self , IntArrayRef output_size)",3, 137, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d( const Tensor & self , IntArrayRef output_size)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 138, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_out( Tensor & output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 170, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 147, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 211, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_backward( const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 184, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_out( Tensor & output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 170, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 147, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 211, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_backward( const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , bool ceil_mode , bool count_include_pad)",3, 184, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_out( Tensor & output , Tensor & indices , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",3, 220, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",3, 177, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode , const Tensor & indices)",3, 246, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_backward( const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode , const Tensor & indices)",3, 219, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_out( Tensor & output , Tensor & indices , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",3, 220, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices( const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode)",3, 177, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode , const Tensor & indices)",3, 246, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_backward( const Tensor & grad_output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool ceil_mode , const Tensor & indices)",3, 219, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_out( Tensor & output , const Tensor & self , const Tensor & indices , IntArrayRef output_size)",3, 115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d( const Tensor & self , const Tensor & indices , IntArrayRef output_size)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntArrayRef output_size)",3, 156, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntArrayRef output_size)",3, 129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_out( Tensor & output , const Tensor & self , const Tensor & indices , IntArrayRef output_size , IntArrayRef stride , IntArrayRef padding)",3, 156, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d( const Tensor & self , const Tensor & indices , IntArrayRef output_size , IntArrayRef stride , IntArrayRef padding)",3, 133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntArrayRef output_size , IntArrayRef stride , IntArrayRef padding)",3, 197, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntArrayRef output_size , IntArrayRef stride , IntArrayRef padding)",3, 170, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_out( Tensor & output , const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d( const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 160, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_out( Tensor & output , const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 118, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d( const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 162, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bicubic2d_out( Tensor & output , const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 117, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bicubic2d( const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bicubic2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 161, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bicubic2d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 134, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_out( Tensor & output , const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d( const Tensor & self , IntArrayRef output_size , bool align_corners)",3, 96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 163, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size , bool align_corners)",3, 136, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_out( Tensor & output , const Tensor & self , IntArrayRef output_size)",3, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d( const Tensor & self , IntArrayRef output_size)",3, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size)",3, 141, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size)",3, 114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_out( Tensor & output , const Tensor & self , IntArrayRef output_size)",3, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d( const Tensor & self , IntArrayRef output_size)",3, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size)",3, 141, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size)",3, 114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_out( Tensor & output , const Tensor & self , IntArrayRef output_size)",3, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d( const Tensor & self , IntArrayRef output_size)",3, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size)",3, 141, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_backward( const Tensor & grad_output , IntArrayRef output_size , IntArrayRef input_size)",3, 114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::sigmoid_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & output)",3, 104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::sigmoid_backward( const Tensor & grad_output , const Tensor & output)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::tanh_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & output)",3, 101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::tanh_backward( const Tensor & grad_output , const Tensor & output)",3, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",5, 235, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",3, 212, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",3, 306, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",3, 246, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation , const Tensor & columns , const Tensor & ones)",3, 372, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",3, 331, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",5, 235, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",3, 212, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",3, 312, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation)",3, 246, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation , const Tensor & finput , const Tensor & fgrad_input)",3, 378, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef dilation , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",3, 337, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",5, 175, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",3, 152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",3, 252, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",3, 186, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , const Tensor & finput , const Tensor & fgrad_input)",3, 318, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",3, 277, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",5, 175, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 207, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 184, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_forward_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 215, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 192, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_backward_out( Tensor & grad_input , Tensor & grad_weight , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 270, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , std :: array<bool,2> output_mask)",3, 251, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",3, 152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",3, 252, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding)",3, 186, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , const Tensor & finput , const Tensor & fgrad_input)",3, 318, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",3, 277, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",5, 205, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 182, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 276, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 216, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , const Tensor & columns , const Tensor & ones)",3, 342, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",3, 301, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",5, 205, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 182, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 276, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_forward( const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation)",3, 216, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , const Tensor & columns , const Tensor & ones)",3, 342, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntArrayRef kernel_size , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",3, 301, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_col2im( const Tensor & self , IntArrayRef output_size , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 155, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_col2im_out( Tensor & output , const Tensor & self , IntArrayRef output_size , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 178, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_col2im_backward( const Tensor & grad_output , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 146, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_col2im_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 173, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_im2col( const Tensor & self , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 130, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_im2col_out( Tensor & output , const Tensor & self , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_im2col_backward( const Tensor & grad_output , IntArrayRef input_size , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 170, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_im2col_backward_out( Tensor & grad_input , const Tensor & grad_output , IntArrayRef input_size , IntArrayRef kernel_size , IntArrayRef dilation , IntArrayRef padding , IntArrayRef stride)",3, 197, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_fft( const Tensor & self , const int64_t signal_ndim , const bool complex_input , const bool complex_output , const bool inverse , IntArrayRef signal_sizes , const bool normalized , const bool onesided)",108, 112, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_get_plan_cache_max_size()",3, 60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_set_plan_cache_max_size( int64_t max_size)",3, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_get_plan_cache_size()",3, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_clear_plan_cache()",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::fft( const Tensor & self , const int64_t signal_ndim , const bool normalized)",5, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::ifft( const Tensor & self , const int64_t signal_ndim , const bool normalized)",5, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::rfft( const Tensor & self , const int64_t signal_ndim , const bool normalized , const bool onesided)",6, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::irfft( const Tensor & self , const int64_t signal_ndim , const bool normalized , const bool onesided , IntArrayRef signal_sizes)",6, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::stft( const Tensor & self , const int64_t n_fft , const optional<int64_t> hop_lengthOpt , const optional<int64_t> win_lengthOpt , const Tensor & window , const bool normalized , const bool onesided)",81, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConstantPadNd.cpp,"at::native::constant_pad_nd( const Tensor & self , IntArrayRef pad , Scalar value)",68, 102, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::window_function_checks( const char * function_name , const TensorOptions & options , int64_t window_length)",20, 66, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::getFactoryType( const TensorOptions & options)",3, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar end , const TensorOptions & options)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar start , Scalar end , const TensorOptions & options)",3, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",8, 74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar end)",3, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar start , Scalar end)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::_dim_arange( const Tensor & like , int64_t dim)",3, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_cpu( IntArrayRef size , const TensorOptions & options)",22, 139, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_strided_cpu( IntArrayRef size , IntArrayRef stride , const TensorOptions & options)",6, 95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_out( Tensor & result , IntArrayRef size)",9, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_like( const Tensor & self)",3, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_like( const Tensor & self , const TensorOptions & options)",8, 85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye( int64_t n , const TensorOptions & options)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye( int64_t n , int64_t m , const TensorOptions & options)",4, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye_out_cpu( Tensor & result , int64_t n)",3, 49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye_out_cpu( Tensor & result , int64_t n , int64_t m)",20, 70, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full( IntArrayRef size , Scalar fill_value , const TensorOptions & options)",7, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_out( Tensor & result , IntArrayRef size , Scalar fill_value)",7, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_like( const Tensor & self , Scalar fill_value)",3, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_like( const Tensor & self , Scalar fill_value , const TensorOptions & options)",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace( Scalar start , Scalar end , int64_t steps , const TensorOptions & options)",8, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace( Scalar start , Scalar end , int64_t steps , const TensorOptions & options)",8, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones( IntArrayRef size , const TensorOptions & options)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_out( Tensor & result , IntArrayRef size)",3, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_like( const Tensor & self)",3, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_like( const Tensor & self , const TensorOptions & options)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::scalar_tensor( Scalar s , const TensorOptions & options)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand( IntArrayRef size , const TensorOptions & options)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand( IntArrayRef size , Generator * generator , const TensorOptions & options)",4, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_out( Tensor & result , IntArrayRef size)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_out( Tensor & result , IntArrayRef size , Generator * generator)",4, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_like( const Tensor & self)",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_like( const Tensor & self , const TensorOptions & options)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t high , IntArrayRef size , const TensorOptions & options)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t high , IntArrayRef size , Generator * generator , const TensorOptions & options)",7, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t low , int64_t high , IntArrayRef size , const TensorOptions & options)",7, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t low , int64_t high , IntArrayRef size , Generator * generator , const TensorOptions & options)",9, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t high , IntArrayRef size)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t high , IntArrayRef size , Generator * generator)",8, 45, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t low , int64_t high , IntArrayRef size)",3, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t low , int64_t high , IntArrayRef size , Generator * generator)",9, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t high)",3, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t low , int64_t high)",3, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t high , const TensorOptions & options)",6, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t low , int64_t high , const TensorOptions & options)",7, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn( IntArrayRef size , const TensorOptions & options)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn( IntArrayRef size , Generator * generator , const TensorOptions & options)",4, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_out( Tensor & result , IntArrayRef size)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_out( Tensor & result , IntArrayRef size , Generator * generator)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_like( const Tensor & self)",3, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_like( const Tensor & self , const TensorOptions & options)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_cpu( Tensor & result , int64_t n , THGenerator * generator)",19, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::get_generator( at :: Generator * gen)",5, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm( int64_t n , const TensorOptions & options)",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm( int64_t n , Generator * generator , const TensorOptions & options)",4, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_out( Tensor & result , int64_t n)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_out_cpu( Tensor & result , int64_t n , Generator * generator)",10, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",8, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range( Scalar start , Scalar end , const TensorOptions & options)",6, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tril_indices_cpu( int64_t row , int64_t col , int64_t offset , const TensorOptions & options)",44, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::triu_indices_cpu( int64_t row , int64_t col , int64_t offset , const TensorOptions & options)",35, 81, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros( IntArrayRef size , const TensorOptions & options)",4, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_out( Tensor & result , IntArrayRef size)",9, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_like( const Tensor & self)",3, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_like( const Tensor & self , const TensorOptions & options)",8, 85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::bartlett_window( int64_t window_length , const TensorOptions & options)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::bartlett_window( int64_t window_length , bool periodic , const TensorOptions & options)",19, 106, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::blackman_window( int64_t window_length , const TensorOptions & options)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::blackman_window( int64_t window_length , bool periodic , const TensorOptions & options)",16, 108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , const TensorOptions & options)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , const TensorOptions & options)",7, 57, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , double alpha , const TensorOptions & options)",8, 63, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , double alpha , double beta , const TensorOptions & options)",20, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hann_window( int64_t window_length , const TensorOptions & options)",3, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hann_window( int64_t window_length , bool periodic , const TensorOptions & options)",8, 70, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tensor_cpu( ArrayRef<T> values , const TensorOptions & options)",8, 79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tensor_cuda( ArrayRef<T> values , const TensorOptions & options)",4, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::tanh_f::operator ( )( const Tensor & t) const",1, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::relu_f::operator ( )( const Tensor & t) const",1, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedSequence::PackedSequence( Tensor _data , Tensor _batch_sizes)",2, 70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::CellParams( const Tensor & _w_ih , const Tensor & _w_hh , const Tensor & _b_ih , const Tensor & _b_hh)",2, 97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::matmul_ih( Tensor input) const",3, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::matmul_hh( Tensor h) const",3, 37, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::linear_ih( Tensor input) const",3, 42, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::linear_hh( Tensor h) const",3, 38, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::QuantizedCellParams::QuantizedCellParams( const Tensor & _w_ih , const Tensor & _w_hh , const Tensor & _b_ih , const Tensor & _b_hh , const Tensor & _packed_ih , const Tensor & _packed_hh , const Tensor & _col_offsets_ih , const Tensor & _col_offsets_hh , const Scalar & _scale_ih , const Scalar & _scale_hh , const Scalar & _zero_point_ih , const Scalar & _zero_point_hh)",12, 81, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::QuantizedCellParams::matmul_ih( Tensor input) const",3, 75, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::QuantizedCellParams::matmul_hh( Tensor h) const",3, 75, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::QuantizedCellParams::linear_ih( Tensor input) const",4, 80, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::QuantizedCellParams::linear_hh( Tensor h) const",4, 76, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::pair_vec( const std :: vector<T> & vals)",9, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::unpair_vec( std :: vector<pair_of<T>> && vals)",9, 67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gather_params( TensorList params , bool has_biases)",16, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gather_quantized_params( TensorList params)",12, 92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_as_output( const Tensor & t)",1, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_as_output( const tpair_of<Tensor> & t)",1, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::project( at :: ArrayRef<tpair_of<Tensor>> tuples)",8, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_concat( at :: ArrayRef<Tensor> hiddens)",1, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_concat( at :: ArrayRef<tpair_of<Tensor>> hiddens)",3, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_slice( const Tensor & t , int64_t start , int64_t end)",3, 67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_slice( const tpair_of<Tensor> & t , int64_t start , int64_t end)",4, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::Cell::~Cell()",1, 104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::SimpleCell::operator ( )( const Tensor & input , const Tensor & hidden , const cell_params & params) const",3, 107, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::LSTMCell::operator ( )( const Tensor & input , const hidden_type & hidden , const cell_params & params) const",25, 117, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::GRUCell::operator ( )( const Tensor & input , const hidden_type & hidden , const cell_params & params) const",20, 117, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::Layer::~Layer()",1, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::FullLayer( Cell<hidden_type,cell_params> & cell)",2, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::operator ( )( std :: vector<Tensor> step_inputs , const hidden_type & input_hidden , const cell_params & params) const",9, 136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::operator ( )( const Tensor & inputs , const hidden_type & input_hidden , const cell_params & params) const",4, 124, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::FullBidirectionalLayer( Cell<dir_hidden_type,cell_params> & cell)",2, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::operator ( )( const Tensor & input , const hidden_type & input_hidden , const param_type & params) const",13, 122, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::reverse( std :: vector<Tensor> && x) const",4, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedLayer::PackedLayer( Cell<hidden_type,cell_params> & cell)",2, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const cell_params & params) const",35, 131, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::ReversedPackedLayer::ReversedPackedLayer( Cell<hidden_type,cell_params> & cell)",2, 60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::ReversedPackedLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const cell_params & params) const",29, 131, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedBidirectionalLayer::PackedBidirectionalLayer( Cell<dir_hidden_type,cell_params> & cell)",2, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedBidirectionalLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const param_type & params) const",6, 130, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::dropout( const Tensor & input , double p)",3, 48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::dropout( const PackedSequence & input , double p)",3, 74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::apply_layer_stack( const Layer<io_type,hidden_type,weight_type> & layer , const io_type & input , const std :: vector<hidden_type> & hiddens , const std :: vector<weight_type> & weights , int64_t num_layers , double dropout_p , bool train)",22, 100, 18, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_rnn_impl( const io_type & input , const std :: vector<cell_params> & params , const std :: vector<typename CellType::hidden_type> & hiddens , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",15, 135, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_rnn_impl_with_concat( const io_type & input , const std :: vector<cell_params> & params , const std :: vector<typename CellType::hidden_type> & hiddens , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",8, 127, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_lstm_impl( const io_type & input , const std :: vector<cell_params> & params , const Tensor & hx , const Tensor & cx , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",27, 140, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm( const Tensor & _input , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",21, 98, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm( const Tensor & data , const Tensor & batch_sizes , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",18, 93, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm_cell( const Tensor & input , TensorList hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",6, 107, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gru_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",5, 86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::rnn_tanh_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",5, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::rnn_relu_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",5, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::quantized_lstm( const Tensor & _input , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",22, 98, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::prepare_quantized_lstm_hx( TensorList hx)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::prepare_quantized_hx( simple_hx_type hx)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Sorting.cpp,"at::native::dim_apply( TensorList tensors , int64_t dim , Fn f)",33, 76, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/Sorting.cpp,"at::native::quick_select_template( TensorAccessor<scalar_t,1> arr , int64_t k , Fn swap_fn)",56, 44, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Sorting.cpp,"at::native::kthvalue_out_cpu( Tensor & values , Tensor & indices , const Tensor & self , int64_t k , int64_t dim_ , bool keepdim)",54, 86, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Sorting.cpp,"at::native::kthvalue( const Tensor & self , int64_t k , int64_t dim , bool keepdim)",10, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::start_index( int a , int b , int c)",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::end_index( int a , int b , int c)",3, 53, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t sizeD , int64_t isizeH , int64_t isizeW , int64_t osizeH , int64_t osizeW , int64_t istrideD , int64_t istrideH , int64_t istrideW)",52, 87, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_cpu_template( at :: Tensor & output , at :: Tensor const & input , IntArrayRef output_size)",65, 117, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_frame( scalar_t * gradInput_p , scalar_t * gradOutput_p , int64_t sizeD , int64_t isizeH , int64_t isizeW , int64_t osizeH , int64_t osizeW)",46, 73, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_cpu_template( Tensor & gradInput , const Tensor & gradOutput_ , const Tensor & input)",55, 91, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_cpu( Tensor & output , const Tensor & input , IntArrayRef output_size)",9, 42, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_cpu( at :: Tensor const & input , IntArrayRef output_size)",9, 51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d( at :: Tensor const & input , IntArrayRef output_size)",12, 122, 7, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_cpu( Tensor & gradInput , const Tensor & gradOutput , const Tensor & input)",10, 51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_cpu( const Tensor & gradOutput , const Tensor & input)",9, 51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::make_offset2bag( const Tensor & offsets , const Tensor & indices , Tensor & offset2bag)",7, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::index_select_add( const Tensor & select_indices , const Tensor & add_indices , const Tensor & src , Tensor & output)",20, 81, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::make_bag_size( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & bag_size)",12, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::apply_bag_size( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & output , const Tensor & bag_size)",19, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::apply_bag_size_backward( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & output , const Tensor & offset2bag , const Tensor & bag_size)",17, 81, 38, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::embedding_bag_cpu_max( const Tensor & weight , const Tensor & indices , const Tensor & offset2bag , const Tensor & output , const Tensor & bag_size , const Tensor & offsets)",39, 144, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::embedding_bag( const Tensor & weight , const Tensor & indices , const Tensor & offsets , const bool scale_grad_by_freq , const int64_t mode , bool sparse)",6, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_cpu( const Tensor & weight , const Tensor & indices , const Tensor & offsets , const bool scale_grad_by_freq , const int64_t mode , bool sparse)",42, 104, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_backward( const Tensor & grad , const Tensor & indices , const Tensor & offsets , const Tensor & offset2bag , const Tensor & bag_size_ , const Tensor & max_indices_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode , bool sparse)",28, 82, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_dense_backward_cpu( const Tensor & grad_ , const Tensor & indices_ , const Tensor & offsets_ , const Tensor & offset2bag__ , const Tensor & bag_size_ , const Tensor & max_indices_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode)",98, 92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_sparse_backward( const Tensor & grad_ , const Tensor & indices , const Tensor & offsets , const Tensor & offset2bag , const Tensor & bag_size_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode)",16, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ConvolutionTBC.cpp,"at::native::conv_tbc( const Tensor & self , const Tensor & weight , const Tensor & bias , int64_t pad)",51, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConvolutionTBC.cpp,"at::native::conv_tbc_backward( const Tensor & dOutput , const Tensor & input , const Tensor & weight , const Tensor & bias , int64_t pad)",45, 154, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_cpu_template( const Tensor & self , const bool sorted , const bool return_inverse)",33, 82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu_impl( ForwardIt first , ForwardIt last , std :: vector<int64_t> & indices , Tensor inverse_indices_vec)",23, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu_template( const Tensor & self , const int64_t dim , const bool return_inverse)",49, 83, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_cpu( const Tensor & self , const bool sorted , const bool return_inverse)",5, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu( const Tensor & self , const int64_t dim , const bool sorted , const bool return_inverse)",6, 103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_out_frame( scalar_t * input_p , scalar_t * output_p , long nslices , long iwidth , long owidth , int pad_l , int pad_r)",30, 53, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_out_batch( scalar_t * input_data , scalar_t * output_data , long nslices , long iwidth , long owidth , int pad_l , int pad_r , int nbatch)",17, 91, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_out_cpu_template( Tensor & output , const Tensor & input_ , IntArrayRef paddingSize)",71, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_backward_out_frame( scalar_t * ginput_p , scalar_t * goutput_p , long nslices , long iwidth , long owidth , int pad_l , int pad_r)",30, 53, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_backward_out_batch( scalar_t * ginput_data , scalar_t * goutput_data , long nslices , long iwidth , long owidth , int pad_l , int pad_r , int nbatch)",18, 63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_backward_out_cpu_template( Tensor & gradInput , const Tensor & gradOutput_ , const Tensor & input , IntArrayRef paddingSize)",72, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t owidth , int64_t oheight , int pad_l , int pad_r , int pad_t , int pad_b)",44, 77, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_out_batch( scalar_t * input_data , scalar_t * output_data , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t owidth , int64_t oheight , int pad_l , int pad_r , int pad_t , int pad_b , int nbatch)",19, 71, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_out_cpu_template( Tensor & output , const Tensor & input_ , IntArrayRef paddingSize)",73, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_backward_out_frame( scalar_t * ginput_p , scalar_t * goutput_p , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t owidth , int64_t oheight , int pad_l , int pad_r , int pad_t , int pad_b)",44, 79, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_backward_out_batch( scalar_t * ginput_data , scalar_t * goutput_data , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t owidth , int64_t oheight , int pad_l , int pad_r , int pad_t , int pad_b , int nbatch)",19, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_backward_out_cpu_template( Tensor & gradInput , const Tensor & gradOutput_ , const Tensor & input , IntArrayRef paddingSize)",79, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::shapeCheck3d( const Tensor & input , int pleft , int pright , int ptop , int pbottom , int pfront , int pback)",36, 86, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t idepth , int64_t owidth , int64_t oheight , int64_t odepth , int pleft , int pright , int ptop , int pbottom , int pfront , int pback)",59, 72, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_out_batch( scalar_t * input_data , scalar_t * output_data , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t idepth , int64_t owidth , int64_t oheight , int64_t odepth , int pleft , int pright , int ptop , int pbottom , int pfront , int pback , int nbatch)",21, 80, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_out_cpu_template( Tensor & output , const Tensor & input_ , IntArrayRef paddingSize)",71, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_backward_out_frame( scalar_t * ginput_p , scalar_t * goutput_p , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t idepth , int64_t owidth , int64_t oheight , int64_t odepth , int pleft , int pright , int ptop , int pbottom , int pfront , int pback)",59, 72, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_backward_out_batch( scalar_t * ginput_data , scalar_t * goutput_data , int64_t nslices , int64_t iwidth , int64_t iheight , int64_t idepth , int64_t owidth , int64_t oheight , int64_t odepth , int pleft , int pright , int ptop , int pbottom , int pfront , int pback , int nbatch)",21, 82, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_backward_out_cpu_template( Tensor & gradInput , const Tensor & gradOutput_ , const Tensor & input , IntArrayRef paddingSize)",84, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_out_cpu( Tensor & output , const Tensor & input , IntArrayRef paddingSize)",9, 38, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_cpu( const Tensor & input , IntArrayRef paddingSize)",9, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_backward_out_cpu( Tensor & gradInput , const Tensor & gradOutput , const Tensor & input , IntArrayRef paddingSize)",11, 50, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad1d_backward_cpu( const Tensor & gradOutput , const Tensor & input , IntArrayRef paddingSize)",10, 50, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_out_cpu( Tensor & output , const Tensor & input , IntArrayRef paddingSize)",9, 38, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_cpu( const Tensor & input , IntArrayRef paddingSize)",9, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_backward_out_cpu( Tensor & gradInput , const Tensor & gradOutput , const Tensor & input , IntArrayRef paddingSize)",10, 50, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad2d_backward_cpu( const Tensor & gradOutput , const Tensor & input , IntArrayRef paddingSize)",10, 50, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_out_cpu( Tensor & output , const Tensor & input , IntArrayRef paddingSize)",9, 38, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_cpu( const Tensor & input , IntArrayRef paddingSize)",9, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_backward_out_cpu( Tensor & gradInput , const Tensor & gradOutput , const Tensor & input , IntArrayRef paddingSize)",10, 50, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReplicationPadding.cpp,"at::native::replication_pad3d_backward_cpu( const Tensor & gradOutput , const Tensor & input , IntArrayRef paddingSize)",10, 50, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"where_cpu( at :: Tensor & ret , const at :: Tensor & condition , const at :: Tensor & self , const at :: Tensor & other)",17, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::allclose( const Tensor & self , const Tensor & other , double rtol , double atol , bool equal_nan)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::isclose( const Tensor & self , const Tensor & other , double rtol , double atol , bool equal_nan)",18, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::isnan( const Tensor & self)",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::is_nonzero( const Tensor & self)",17, 76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::where( const Tensor & condition , const Tensor & self , const Tensor & other)",9, 93, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_s_where_cpu( const Tensor & condition , const Tensor & self , const Tensor & other)",7, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::median( const Tensor & self , int64_t dim , bool keepdim)",5, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::median_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool keepdim)",13, 97, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::mode( const Tensor & self , int64_t dim , bool keepdim)",5, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::mode_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool keepdim)",13, 95, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_max_out_cpu( Tensor & max , Tensor & max_indices , const Tensor & self , int64_t dim , bool keepdim)",14, 89, 40, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max( const Tensor & self , int64_t dim , bool keepdim)",5, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max_out( Tensor & max , Tensor & max_indices , const Tensor & self , int64_t dim , bool keepdim)",17, 94, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_min_out_cpu( Tensor & min , Tensor & min_indices , const Tensor & self , int64_t dim , bool keepdim)",14, 89, 40, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min( const Tensor & self , int64_t dim , bool keepdim)",5, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min_out( Tensor & min , Tensor & min_indices , const Tensor & self , int64_t dim , bool keepdim)",17, 94, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmax( const Tensor & self , int64_t dim , bool keepdim)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmax( const Tensor & self)",3, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmin( const Tensor & self , int64_t dim , bool keepdim)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmin( const Tensor & self)",3, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_argmax( const Tensor & self , int64_t dim , bool keepdim)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_argmin( const Tensor & self , int64_t dim , bool keepdim)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::norm_except_dim( const Tensor & v , int64_t pow , int64_t dim)",19, 97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::_weight_norm( const Tensor & v_in , const Tensor & g_in , int64_t dim)",26, 95, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::_weight_norm_differentiable_backward( const Tensor & grad_w , const Tensor & saved_v , const Tensor & saved_g , const Tensor & saved_norms , int64_t dim)",45, 101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::_type_has_native( const Type & dtype)",3, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::_has_native( const Tensor & self)",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::clone( const Tensor & self)",7, 40, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::resize_as_( Tensor & self , const Tensor & the_template)",7, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::pow_out( Tensor & result , const Tensor & self , Scalar exponent)",7, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::pow( const Tensor & self , Scalar exponent)",7, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::zero_( Tensor & self)",7, 40, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm_out( Tensor & result , const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",11, 123, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm( const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",11, 102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm_( Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",10, 98, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::host_softmax( Tensor output , const Tensor & input , const int64_t dim)",50, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::host_softmax_backward( Tensor & gI , const Tensor & grad , const Tensor & output , int64_t dim)",51, 81, 18, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax_cpu( const Tensor & input_ , const int64_t dim_ , const bool half_to_float)",23, 95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax_cpu( const Tensor & input_ , const int64_t dim_ , const bool half_to_float)",23, 95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax_backward_cpu( const Tensor & grad_ , const Tensor & output_ , int64_t dim_ , const Tensor & input_)",31, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax_backward_cpu( const Tensor & grad_ , const Tensor & output_ , int64_t dim_ , const Tensor & input_)",31, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax( const Tensor & input_ , const int64_t dim_)",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax( const Tensor & input_ , const int64_t dim_ , ScalarType dtype)",7, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax( const Tensor & input_ , const int64_t dim_)",3, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax( const Tensor & input_ , const int64_t dim_ , ScalarType dtype)",7, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::is_same_size( const Tensor & self , const Tensor & other)",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::size( const Tensor & self , int64_t dim)",5, 100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::stride( const Tensor & self , int64_t dim)",5, 100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::cudnn_is_acceptable( const Tensor & self)",16, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::detach( const Tensor & self)",5, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::detach_( Tensor & self)",5, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::contiguous( const Tensor & self)",6, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"apply_loss_reduction( const at :: Tensor & unreduced , int64_t reduction)",8, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::cosine_embedding_loss( const Tensor & input1 , const Tensor & input2 , const Tensor & target , double margin , int64_t reduction)",15, 131, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::hinge_embedding_loss( const Tensor & self , const Tensor & target , double margin , int64_t reduction)",8, 106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::triplet_margin_loss( const Tensor & anchor , const Tensor & positive , const Tensor & negative , double margin , double p , double eps , bool swap , int64_t reduction)",11, 112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::margin_ranking_loss( const Tensor & input1 , const Tensor & input2 , const Tensor & target , double margin , int64_t reduction)",4, 129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::kl_div( const Tensor & input , const Tensor & target , int64_t reduction)",6, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::kl_div_backward_cpu( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",19, 111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::binary_cross_entropy_with_logits( const Tensor & input , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",17, 152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::binary_cross_entropy_with_logits_backward( const Tensor & grad , const Tensor & input , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",20, 181, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Onehot.cpp,"at::native::one_hot( const Tensor & self , int64_t num_classes)",28, 110, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::TensorIterator::parallel_reduce( const loop2d_t & loop)",11, 100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::use_two_pass_reduction( TensorIterator & iter)",3, 59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::two_pass_reduction( TensorIterator & iter , const loop2d_t & loop)",32, 92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::find_split_dim( TensorIterator & iter)",17, 75, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::round_columns( TensorIterator & iter , int dim , int multiple , int64_t begin , int64_t end)",8, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::parallel_dim_reduction( TensorIterator & iter , const loop2d_t & loop)",22, 87, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::TensorIterator::foreach_reduced_elt( const loop_subiter_t & loop , bool parallelize)",50, 123, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_linear_int8_weight( const Tensor & input , const Tensor & weight , const Tensor & packed , const Tensor & col_offsets , Scalar weight_scale , Scalar weight_zero_point , const Tensor & bias)",126, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::calc_col_offsets_transpose( int K , int N , const int8_t * Bint8 , int32_t B_zero_point , int32_t * col_offsets)",14, 45, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_linear_quantize_weight( const Tensor & weight)",49, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_is_cpu_supported()",3, 39, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_pack_quantized_matrix( const Tensor & weight , int64_t K , int64_t N)",39, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_linear_int8_weight( const Tensor & , const Tensor & , const Tensor & , const Tensor & , Scalar , Scalar , const Tensor &)",14, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_linear_quantize_weight( const Tensor &)",8, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_pack_quantized_matrix( const Tensor & , int64_t , int64_t)",10, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/QuantizedLinear.cpp,"at::native::fbgemm_is_cpu_supported()",3, 33, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::pairwise_distance( const Tensor & x1 , const Tensor & x2 , double p , double eps , bool keepdim)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::pdist( const Tensor & self , const double p)",7, 97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::cdist( const Tensor & x1 , const Tensor & x2 , const double p)",28, 153, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::_pdist_forward( const Tensor & self , const double p)",19, 115, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::_pdist_backward( const Tensor & grad , const Tensor & self , const double p , const Tensor & pdist)",9, 116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::cosine_similarity( const Tensor & x1 , const Tensor & x2 , int64_t dim , double eps)",6, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/DispatchStub.cpp,"at::native::compute_cpu_capability()",27, 72, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/DispatchStub.cpp,"at::native::get_cpu_capability()",4, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::ensure_has_index( Device device)",7, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to_impl( const Tensor & self , const TensorOptions & options , bool non_blocking)",4, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , const TensorOptions & options , bool non_blocking , bool copy)",29, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , Device device , ScalarType dtype , bool non_blocking , bool copy)",7, 95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , ScalarType dtype , bool non_blocking , bool copy)",6, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , const Tensor & other , bool non_blocking , bool copy)",10, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::relu( const Tensor & self)",3, 36, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::relu_( Tensor & self)",3, 37, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::selu( const Tensor & self)",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::selu_( Tensor & self)",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::celu( const Tensor & self , Scalar alpha)",4, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::celu_( Tensor & self , Scalar alpha)",4, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::rrelu( const Tensor & self , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::rrelu_( Tensor & self , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_out( optional<Tensor> opt_result , const Tensor & self , Scalar threshold , Scalar value , const Tensor & other)",11, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold( const Tensor & self , Scalar threshold , Scalar value)",3, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_( Tensor & self , Scalar threshold , Scalar value)",4, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_out( Tensor & result , const Tensor & self , Scalar threshold , Scalar value)",4, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_backward( const Tensor & grad , const Tensor & self , Scalar threshold)",3, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_kernel_share_weights( Tensor & result , const Tensor & input , const Tensor & weight)",19, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_kernel_multi_weights( Tensor & result , const Tensor & input , const Tensor & weight , int64_t input_dim0_size , int64_t channel_size , int64_t input_stride0 , int64_t input_stride1)",31, 78, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu( const Tensor & self , const Tensor & weight_)",47, 102, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_backward_kernel_share_weights( const Tensor & input , const Tensor & weight , const Tensor & grad_out , Tensor & input_grad , Tensor & weight_grad)",29, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_backward_kernel_multi_weights( const Tensor & input , const Tensor & weight , const Tensor & grad_out , Tensor & input_grad , Tensor & weight_grad_collector , int64_t input_dim0_size , int64_t channel_size , int64_t input_stride0 , int64_t input_stride1)",37, 85, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_backward_cpu( const Tensor & grad_out_ , const Tensor & self , const Tensor & weight_)",62, 116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::hardshrink_cpu( const Tensor & self , Scalar lambd)",15, 103, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::hardshrink_backward_cpu( const Tensor & grad , const Tensor & self , Scalar lambd)",17, 103, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"sample_poisson( double lambda , THGenerator * generator)",49, 78, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli( const Tensor & self , Generator * gen)",3, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli( const Tensor & self , double p , Generator * gen)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_out( Tensor & result , const Tensor & self , Generator * gen)",6, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_tensor_cpu_( Tensor & self , const Tensor & p_ , Generator * gen)",24, 104, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_scalar_cpu_( Tensor & self , double p , Generator * gen)",18, 96, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_standard_gamma_grad_cpu( const Tensor & self , const Tensor & output)",11, 84, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_s_poisson_cpu( const Tensor & lambda , Generator * gen)",13, 97, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_s_gamma_cpu( const Tensor & alpha , Generator * gen)",25, 147, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_reshape_from_tensor( const Tensor & self , const Tensor & shape_tensor)",9, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_shape_as_tensor( const Tensor & self)",4, 85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::broadcast_tensors( TensorList tensors)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_cat_no_zero_dim( TensorList tensors)",7, 86, 13, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat_out( Tensor & result , TensorList tensors , int64_t dim)",5, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sizes_match_except( IntArrayRef s1 , IntArrayRef s2 , int64_t dim_except)",11, 117, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_cat_sparse_dims( Tensor const & t , int64_t pos , IntArrayRef sizes , int64_t wrapped , int64_t sparse_dim , int64_t dense_dim)",15, 105, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat_sparse( TensorList tensors , int64_t dim)",89, 137, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat( TensorList tensors , int64_t dim)",9, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::chunk( const Tensor & self , int64_t chunks , int64_t dim)",20, 101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diagflat( const Tensor & self , int64_t offset)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diagonal( const Tensor & self , int64_t offset , int64_t dim1_ , int64_t dim2_)",45, 116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diag_embed( const Tensor & self , int64_t offset , int64_t dim1_ , int64_t dim2_)",15, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::expand( const Tensor & self , IntArrayRef size , bool implicit)",19, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::expand_as( const Tensor & self , const Tensor & other)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sum_to_size( const Tensor & self , IntArrayRef size)",6, 81, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided( const Tensor & self , IntArrayRef size , IntArrayRef stride , optional<int64_t> storage_offset_)",10, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided_( Tensor & self , IntArrayRef size , IntArrayRef stride , optional<int64_t> storage_offset_)",5, 109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow_copy_sparse( const Tensor & self , int64_t dim , int64_t start , int64_t length)",33, 111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow_copy_dense( const Tensor & self , int64_t dim , int64_t start , int64_t length)",3, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow( const Tensor & self , int64_t dim , int64_t start , int64_t length)",10, 100, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::permute( const Tensor & self , IntArrayRef dims)",19, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::repeat( const Tensor & self , IntArrayRef repeats)",29, 106, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::reshape( const Tensor & self , IntArrayRef proposed_shape)",10, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::reshape_as( const Tensor & self , const Tensor & other)",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::select( const Tensor & self , int64_t dim , int64_t index)",21, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::slice( const Tensor & self , int64_t dim , int64_t start , int64_t end , int64_t step)",32, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::split( const Tensor & self , int64_t split_size , int64_t dim)",23, 107, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::split_with_sizes( const Tensor & self , IntArrayRef split_sizes , int64_t dim)",21, 100, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::get_stack_inputs( TensorList tensors , int64_t dim)",7, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::stack( TensorList tensors , int64_t dim)",6, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::stack_out( Tensor & result , TensorList tensors , int64_t dim)",6, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sparse_transpose_( Tensor & self , int64_t dim0 , int64_t dim1)",31, 110, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::transpose_( Tensor & self , int64_t dim0 , int64_t dim1)",18, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::transpose( const Tensor & self , int64_t dim0 , int64_t dim1)",19, 69, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_t( const Tensor & self , const char * fn)",12, 91, 13, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::t( const Tensor & self)",4, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::t_( Tensor & self)",4, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferSqueezeGeometry( const Tensor & tensor)",13, 46, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferSqueezeGeometry( const Tensor & tensor , int64_t dim)",12, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferUnsqueezeGeometry( const Tensor & tensor , int64_t dim)",9, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze( const Tensor & self)",4, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze( const Tensor & self , int64_t dim)",10, 58, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze_( Tensor & self)",4, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze_( Tensor & self , int64_t dim)",10, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_unsafe_view( const Tensor & self , IntArrayRef size)",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze_sparse( Tensor const & self , int64_t dim)",19, 116, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze( const Tensor & self , int64_t dim)",10, 60, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze_( Tensor & self , int64_t dim)",6, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::flatten( const Tensor & self , int64_t start_dim , int64_t end_dim)",26, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::view_as( const Tensor & self , const Tensor & other)",3, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::numel( const Tensor & self)",3, 46, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unbind( const Tensor & self , int64_t dim)",9, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::meshgrid( TensorList tensors)",28, 118, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::clip_coordinates( scalar_t in , int64_t clip_limit)",3, 100, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::clip_coordinates_set_grad( scalar_t in , int64_t clip_limit , scalar_t * grad_in)",16, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::reflect_coordinates( scalar_t in , int64_t clip_limit)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::reflect_coordinates_set_grad( scalar_t in , int64_t clip_limit , scalar_t * grad_in)",25, 87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::within_bounds_2d( int64_t h , int64_t w , int64_t H , int64_t W)",3, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::within_bounds_3d( int64_t d , int64_t h , int64_t w , int64_t D , int64_t H , int64_t W)",3, 106, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::safe_add_2d( scalar_t * data , int64_t h , int64_t w , int64_t sH , int64_t sW , int64_t H , int64_t W , scalar_t delta)",7, 79, 33, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::safe_add_3d( scalar_t * data , int64_t d , int64_t h , int64_t w , int64_t sD , int64_t sH , int64_t sW , int64_t D , int64_t H , int64_t W , scalar_t delta)",8, 82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_cpu_impl( const Tensor & input , const Tensor & grid , GridSamplerInterpolation interpolation_mode , GridSamplerPadding padding_mode)",165, 112, 18, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_backward_cpu_impl( const Tensor & grad_output , const Tensor & input , const Tensor & grid , GridSamplerInterpolation interpolation_mode , GridSamplerPadding padding_mode)",231, 126, 16, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_2d_cpu( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",4, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_cpu( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",8, 85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_2d_backward_cpu( const Tensor & grad_output , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",4, 112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_backward_cpu( const Tensor & grad_output , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",9, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",54, 105, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::operator < <( std :: ostream & out , const ConvParams & params)",14, 73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_strided() const",7, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_dilated() const",7, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_padded() const",7, 45, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_output_padding_neg() const",7, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_output_padding_big() const",7, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_padding_neg() const",7, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::view1d_as_2d()",8, 54, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_cudnn( const at :: Tensor & input) const",16, 101, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_miopen( const at :: Tensor & input) const",10, 112, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_mkldnn( const at :: Tensor & input) const",10, 71, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_nnpack( const at :: Tensor & input) const",16, 91, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_depthwise( const at :: Tensor & input , const at :: Tensor & weight) const",9, 102, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::check_input_shape_forward( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , int64_t groups , bool transposed)",39, 101, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::view4d( const at :: Tensor & tensor)",6, 72, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::view3d( const at :: Tensor & tensor)",6, 72, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::subtensor( at :: Tensor & tensor , int dim , int groups , int g)",7, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv1d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , int64_t groups)",6, 85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv2d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , int64_t groups)",6, 85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv3d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , int64_t groups)",6, 85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose1d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , int64_t groups , IntArrayRef dilation)",6, 113, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose2d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , int64_t groups , IntArrayRef dilation)",6, 113, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose3d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef output_padding , int64_t groups , IntArrayRef dilation)",6, 113, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::convolution( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool transposed , IntArrayRef output_padding , int64_t groups)",9, 99, 26, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::convolution_expand_param_if_needed( IntArrayRef list_param , const char * param_name , int64_t expected_dim)",14, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution( const Tensor & input_r , const Tensor & weight_r , const Tensor & bias_r , IntArrayRef stride_ , IntArrayRef padding_ , IntArrayRef dilation_ , bool transposed_ , IntArrayRef output_padding_ , int64_t groups_ , bool benchmark , bool deterministic , bool cudnn_enabled)",122, 137, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution_nogroup( const Tensor & input , const Tensor & weight , const Tensor & bias , IntArrayRef stride , IntArrayRef padding , IntArrayRef dilation , bool transposed , IntArrayRef output_padding)",65, 67, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::subvariable( const Tensor & var , int dim , int groups , int g)",5, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution_double_backward( const Tensor & ggI , const Tensor & ggW_r , const Tensor & ggb , const Tensor & gO_r , const Tensor & weight_r , const Tensor & input , IntArrayRef stride_ , IntArrayRef padding_ , IntArrayRef dilation_ , bool transposed_ , IntArrayRef output_padding_ , int64_t groups_ , bool benchmark , bool deterministic , bool cudnn_enabled , std :: array<bool,3> output_mask)",220, 306, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/RangeFactories.cpp,"at::native::linspace_cpu_out( Tensor & result , Scalar start , Scalar end , int64_t steps)",32, 93, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/RangeFactories.cpp,"at::native::logspace_cpu_out( Tensor & result , Scalar start , Scalar end , int64_t steps)",33, 93, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/RangeFactories.cpp,"at::native::range_cpu_out( Tensor & result , Scalar start , Scalar end , Scalar step)",33, 90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RangeFactories.cpp,"at::native::arange_cpu_out( Tensor & result , Scalar start , Scalar end , Scalar step)",38, 96, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_lu_det_P_diag_U_info( const Tensor & self)",15, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::det( const Tensor & self)",15, 86, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::logdet( const Tensor & self)",20, 89, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::slogdet( const Tensor & self)",17, 90, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::pinverse( const Tensor & self , double rcond)",14, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_matrix_rank_helper( const Tensor & self , bool symmetric)",12, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_rank( const Tensor & self , double tol , bool symmetric)",8, 87, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_rank( const Tensor & self , bool symmetric)",9, 88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::check_1d( const Tensor & t , const char * arg , const char * fn)",3, 91, 1, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::ger( const Tensor & self , const Tensor & vec2)",5, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::ger_out( Tensor & result , const Tensor & self , const Tensor & vec2)",5, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mm( const Tensor & self , const Tensor & mat2)",6, 76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mm_out( Tensor & result , const Tensor & self , const Tensor & mat2)",6, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mv( const Tensor & self , const Tensor & vec)",4, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mv_out( Tensor & result , const Tensor & self , const Tensor & vec)",4, 72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv( const Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",4, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv_( Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",4, 96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv_out( Tensor & result , const Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",4, 121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr( const Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",5, 101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr_( Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",5, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr_out( Tensor & result , const Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",5, 122, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_cpu_kernel( const Tensor & result , const Tensor & self , const Tensor & mat2 , Scalar beta_ , Scalar alpha_)",40, 124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_out_or_baddbmm_( Tensor & self_or_result , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha , bool is_bmm_out)",67, 156, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_cpu( const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",4, 112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_out_cpu( Tensor & result , const Tensor & self_ , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",7, 134, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm__cpu( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 108, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_cpu( const Tensor & self , const Tensor & mat2)",4, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_out_cpu( Tensor & result , const Tensor & batch1 , const Tensor & batch2)",5, 82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::dot( const Tensor & self , const Tensor & tensor)",5, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::dot_out( Tensor & result , const Tensor & self , const Tensor & tensor)",5, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul( c10 :: optional<Tensor> out_opt , const Tensor & tensor1 , const Tensor & tensor2)",86, 115, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul( const Tensor & tensor1 , const Tensor & tensor2)",3, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul_out( Tensor & result , const Tensor & tensor1 , const Tensor & tensor2)",4, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_power( const Tensor & a , int64_t n)",37, 80, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm( const Tensor & self)",3, 44, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm( const Tensor & self , IntArrayRef dim , bool keepdim)",11, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm_out( Tensor & result , const Tensor & self , IntArrayRef dim , bool keepdim)",15, 76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::nuclear_norm( const Tensor & self , bool keepdim)",8, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::nuclear_norm_out( Tensor & result , const Tensor & self , bool keepdim)",8, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_chain_matmul_general( TensorList matrices , std :: vector<std::vector<int64_t>> & order , int64_t i , int64_t j)",6, 135, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_chain_matmul_three_matrices( TensorList matrices)",18, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::chain_matmul( TensorList matrices)",56, 96, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv( int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",3, 101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf( int m , int n , scalar_t * a , int lda , int * ipiv , int * info)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri( int n , scalar_t * a , int lda , int * ipiv , scalar_t * work , int lwork , int * info)",3, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholeskySolve( char uplo , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",3, 110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky( char uplo , int n , scalar_t * a , int lda , int * info)",3, 73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv<double>( int n , int nrhs , double * a , int lda , int * ipiv , double * b , int ldb , int * info)",3, 116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv<float>( int n , int nrhs , float * a , int lda , int * ipiv , float * b , int ldb , int * info)",3, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri<double>( int n , double * a , int lda , int * ipiv , double * work , int lwork , int * info)",3, 112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri<float>( int n , float * a , int lda , int * ipiv , float * work , int lwork , int * info)",3, 109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf<double>( int m , int n , double * a , int lda , int * ipiv , int * info)",3, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf<float>( int m , int n , float * a , int lda , int * ipiv , int * info)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholeskySolve<double>( char uplo , int n , int nrhs , double * a , int lda , double * b , int ldb , int * info)",3, 125, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholeskySolve<float>( char uplo , int n , int nrhs , float * a , int lda , float * b , int ldb , int * info)",3, 122, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky<double>( char uplo , int n , double * a , int lda , int * info)",3, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky<float>( char uplo , int n , float * a , int lda , int * info)",3, 88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_gesv( Tensor & b , Tensor & A , std :: vector<int64_t> & infos)",32, 98, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_gesv_helper_cpu( const Tensor & self , const Tensor & A)",14, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::gesv( const Tensor & self , const Tensor & A)",9, 96, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::gesv_out( Tensor & solution , Tensor & lu , const Tensor & self , const Tensor & A)",10, 106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_inverse( Tensor & self , std :: vector<int64_t> & infos)",40, 106, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_inverse_helper_cpu( const Tensor & self)",9, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::inverse( const Tensor & self)",10, 51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::inverse_out( Tensor & result , const Tensor & self)",7, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_cholesky_solve( Tensor & b , Tensor & A , bool upper , std :: vector<int64_t> & infos)",31, 98, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_cholesky_solve_helper_cpu( const Tensor & self , const Tensor & A , bool upper)",14, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky_solve( const Tensor & self , const Tensor & A , bool upper)",9, 96, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky_solve_out( Tensor & result , const Tensor & self , const Tensor & A , bool upper)",7, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_cholesky( Tensor & self , bool upper , std :: vector<int64_t> & infos)",27, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_cholesky_helper_cpu( const Tensor & self , bool upper)",13, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky( const Tensor & self , bool upper)",13, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky_out( Tensor & result , const Tensor & self , bool upper)",7, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_btrifact( Tensor & self , Tensor & pivots , Tensor & infos)",22, 93, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_btrifact_helper_cpu( const Tensor & self , bool pivot)",23, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::btrifact( const Tensor & self , bool pivot)",6, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::btrifact_out( Tensor & A_LU , Tensor & pivots , const Tensor & self , bool pivot)",12, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::btrifact_with_info( const Tensor & self , bool pivot)",7, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::btrifact_with_info_out( Tensor & A_LU , Tensor & pivots , Tensor & info , const Tensor & self , bool pivot)",13, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_triu_tril_single( scalar_t * result , scalar_t * self , bool inplace , int64_t k , int64_t n , int64_t m , int64_t res_row_stride , int64_t res_col_stride , int64_t self_row_stride , int64_t self_col_stride)",35, 109, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_triu_tril( Tensor & result , const Tensor & self , bool inplace , int64_t k)",31, 87, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::tril( const Tensor & self , int64_t k)",5, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::tril_cpu_( Tensor & self , int64_t k)",13, 66, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::tril_cpu_out( Tensor & result , const Tensor & self , int64_t k)",13, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::triu( const Tensor & self , int64_t k)",5, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::triu_cpu_( Tensor & self , int64_t k)",13, 65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::triu_cpu_out( Tensor & result , const Tensor & self , int64_t k)",13, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( scalar_t val)",3, 28, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( float val)",3, 26, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( double val)",3, 26, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::Reduction::apply( Tensor & res , Tensor & res_indices , const Tensor & self , c10 :: optional<int64_t> dim , bool greater)",63, 75, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::max_kernel_impl( Tensor & max , Tensor & max_indices , const Tensor & self , c10 :: optional<int64_t> dim)",9, 76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::min_kernel_impl( Tensor & min , Tensor & min_indices , const Tensor & self , c10 :: optional<int64_t> dim)",9, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_log_softmax_lastdim( scalar_t * input_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",62, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_softmax_lastdim( scalar_t * input_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",38, 74, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_host_softmax_backward_lastdim( scalar_t * grad_input_data_base , scalar_t * grad_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",50, 76, 16, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::vec_host_softmax_lastdim::apply( Tensor & output , const Tensor & input)",15, 68, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::vec_host_softmax_backward_lastdim::apply( Tensor & grad_input , const Tensor & grad , const Tensor & output)",15, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::softmax_lastdim_kernel_impl( Tensor & result , const Tensor & self)",5, 86, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::log_softmax_lastdim_kernel_impl( Tensor & result , const Tensor & self)",8, 71, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::softmax_backward_lastdim_kernel_impl( Tensor & grad_input , const Tensor & grad , const Tensor & output)",10, 72, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::log_softmax_backward_lastdim_kernel_impl( Tensor & grad_input , const Tensor & grad , const Tensor & output)",10, 76, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/CopyKernel.cpp,"at::native::copy_kernel_impl( Tensor & dst , const Tensor & src)",16, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::sum_kernel_impl( TensorIterator & iter)",8, 70, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::mean_kernel_impl( TensorIterator & iter)",10, 75, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::std_var_kernel_impl( TensorIterator & iter , bool unbiased , bool take_sqrt)",9, 87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::prod_kernel_impl( TensorIterator & iter)",9, 69, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::norm_kernel_tensor_iterator_impl( TensorIterator & iter , Scalar p)",55, 86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::and_kernel_impl( TensorIterator & iter)",23, 79, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::or_kernel_impl( TensorIterator & iter)",13, 61, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::min_values_kernel_impl( TensorIterator & iter)",8, 77, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::max_values_kernel_impl( TensorIterator & iter)",8, 77, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::Indexer::Indexer( int64_t num_indexers , char ** indexers , const int64_t * indexer_strides , IntArrayRef original_sizes , IntArrayRef original_strides)",10, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::Indexer::get( int64_t idx)",15, 101, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::is_constant_index( int ntensor , const int64_t * strides)",9, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::cpu_index_kernel( TensorIterator & iter , IntArrayRef index_size , IntArrayRef index_stride , const func_t & f , bool serial_execution = false)",32, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::index_kernel( TensorIterator & iter , IntArrayRef index_size , IntArrayRef index_stride)",7, 106, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::index_put_kernel( TensorIterator & iter , IntArrayRef index_size , IntArrayRef index_stride , bool accumulate)",16, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::_sigmoid( float * x , float * y , int64_t size)",24, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::_sigmoid( double * x , double * y , int64_t size)",19, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::sigmoid_kernel( Tensor & result , const Tensor & self)",34, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::bernoulli_mkl_kernel( Tensor & output , const double p , Generator * gen)",5, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::bernoulli_mkl_kernel( Tensor & self , const double p , Generator * gen)",49, 80, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/Activation.cpp,"at::native::threshold_kernel( TensorIterator & iter , Scalar threshold_scalar , Scalar value_scalar)",15, 99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::sign( Vec val)",4, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::map( const Vec & diff , const Vec & p)",1, 112, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::finish( const scalar_t agg , const scalar_t p)",1, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::map( const Vec & diff , const Vec & p)",1, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::finish( const scalar_t agg , const scalar_t p)",1, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 139, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::lttdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 219, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::map( const Vec & diff , const Vec & p)",1, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::finish( const scalar_t agg , const scalar_t p)",1, 99, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 168, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::map( const Vec & diff , const Vec & p)",1, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::finish( const scalar_t agg , const scalar_t p)",1, 107, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 213, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::map( const Vec & diff , const Vec & p)",1, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::red( const Vec & agg , const Vec & up)",1, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::finish( const scalar_t agg , const scalar_t p)",1, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 215, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_parallel( Tensor & result , const Tensor & self , const scalar_t p)",39, 136, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply( Tensor & result , const Tensor & self , const scalar_t p)",13, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_cdist_parallel( Tensor & result , const Tensor & t1 , const Tensor & t2 , const scalar_t p)",31, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply_cdist( Tensor & result , const Tensor & x1 , const Tensor & x2 , const scalar_t p)",13, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::backward_down_column( const scalar_t * self_i , scalar_t * res_i , const scalar_t * grad_k , const scalar_t * dist_k , const Vec & pvec , int64_t n , int64_t m , int64_t gs , int64_t count = Vec :: size())",22, 217, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_backward_parallel( Tensor & result , const Tensor & grad , const Tensor & self , const scalar_t p , const Tensor & dist)",28, 164, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply_backward( Tensor & result , const Tensor & grad , const Tensor & self , const double p , const Tensor & dist)",15, 123, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::pdist_forward_kernel_impl( Tensor & result , const Tensor & self , const double p)",5, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::pdist_backward_kernel_impl( Tensor & result , const Tensor & grad , const Tensor & self , const double p , const Tensor & dist)",5, 133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::cdist_kernel_impl( Tensor & result , const Tensor & x1 , const Tensor & x2 , const double p)",5, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocationBase::ComputeLocationBase( int64_t size)",2, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocationBase::unnormalize( const Vec & in) const",3, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Zeros>::apply( const Vec & in) const",3, 42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Zeros>::apply_get_grad( const Vec & in) const",3, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::ComputeLocation( int64_t size)",3, 50, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::apply( const Vec & in) const",3, 68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::apply_get_grad( const Vec & in) const",13, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::ComputeLocation( int64_t size)",5, 66, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::apply( const Vec & in) const",14, 77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::apply_get_grad( const Vec & in) const",20, 84, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::mask_scatter_add( const scalar_t * src , scalar_t * base_addr , const int_same_size_t<scalar_t> * offsets , const int_same_size_t<scalar_t> * mask , int64_t len)",12, 71, 17, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::ApplyGridSample( const TensorAccessor<scalar_t,4> & input)",9, 60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::compute_interp_params( const Vec & x , const Vec & y) const",48, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::forward( TensorAccessor<scalar_t,3> & out_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",47, 103, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::backward( TensorAccessor<scalar_t,3> & gInp_slice , TensorAccessor<scalar_t,3> & gGrid_slice , const TensorAccessor<scalar_t,3> & gOut_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",95, 103, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::ApplyGridSample( const TensorAccessor<scalar_t,4> & input)",9, 60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::forward( TensorAccessor<scalar_t,3> & out_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",33, 96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::backward( TensorAccessor<scalar_t,3> & gInp_slice , TensorAccessor<scalar_t,3> & gGrid_slice , const TensorAccessor<scalar_t,3> & gOut_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",38, 91, 32, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sample_2d_grid_slice_iterator( const TensorAccessor<scalar_t,3> & grid_slice , const ApplyFn & apply_fn)",114, 92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sampler_2d_cpu_kernel_impl( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",54, 115, 38, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sampler_2d_backward_cpu_kernel_impl( const Tensor & grad_output_ , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",63, 116, 38, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::add_kernel( TensorIterator & iter , Scalar alpha_scalar)",11, 73, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::sub_kernel( TensorIterator & iter , Scalar alpha_scalar)",3, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::mul_kernel( TensorIterator & iter)",9, 65, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::div_kernel( TensorIterator & iter)",21, 88, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp__cuda( Tensor & self , optional<Scalar> min , optional<Scalar> max)",3, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_out_cuda( Tensor & result , const Tensor & self , optional<Scalar> min , optional<Scalar> max)",16, 65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_max__cuda( Tensor & self , Scalar max)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_max_out_cuda( Tensor & result , const Tensor & self , Scalar max)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_min__cuda( Tensor & self , Scalar min)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_min_out_cuda( Tensor & result , const Tensor & self , Scalar min)",3, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( IntArrayRef input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_bias( const at :: Tensor & grad_output)",4, 87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 95, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_input( const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_weight( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward_input( IntArrayRef input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward_weight( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_output_size( IntArrayRef input_size , IntArrayRef weight_size , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",17, 82, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_input_size( IntArrayRef output_size , IntArrayRef weight_size , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",17, 110, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_weight_size( IntArrayRef input_size , IntArrayRef output_size , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",15, 110, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::narrowGroup( const Tensor & t , int dim , int group_idx , int64_t groups)",4, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::check_args( CheckedFrom c , IntArrayRef args , size_t expected_size , const char * arg_name)",18, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::convolution_shape_check( CheckedFrom c , const TensorGeometryArg & input , const TensorGeometryArg & weight , const TensorGeometryArg & output , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",18, 102, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::setConvolutionParams( ConvolutionParams * params , miopenHandle_t handle , const at :: Tensor & input , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool deterministic)",29, 67, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ConvolutionArgs::ConvolutionArgs( const Tensor & input , const Tensor & output , const Tensor & weight)",2, 132, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ParamsHash::operator ( )( const ConvolutionParams & params) const",9, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ParamsEqual::operator ( )( const ConvolutionParams & a , const ConvolutionParams & b) const",5, 82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::BenchmarkCache::find( const ConvolutionParams & params , T * results)",9, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::BenchmarkCache::insert( const ConvolutionParams & params , const T & results)",4, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::Workspace::Workspace( size_t size)",3, 63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::Workspace::~Workspace()",5, 56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvFwdAlgorithm_t)",13, 65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvBwdDataAlgorithm_t)",13, 69, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvBwdWeightsAlgorithm_t)",13, 72, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getBestAlgorithm( perf_t * perfResults , bool deterministic , int n_algo)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::cache()",1, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::wsscache()",1, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",19, 63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::cache()",1, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::wsscache()",1, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",19, 63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::cache()",1, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::wsscache()",1, 75, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",19, 64, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::findAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",27, 80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::chooseAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",24, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_add_bias_( CheckedFrom c , const TensorArg & output , const TensorArg & bias)",18, 97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",29, 136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",27, 88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",17, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_depthwise_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",29, 136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",25, 88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",17, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_input( const Tensor & grad_output_t , const Tensor & weight_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",12, 87, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 165, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",31, 142, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( CheckedFrom c , IntArrayRef input_size , const TensorArg & grad_output , const TensorArg & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",24, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_forward( CheckedFrom c , const TensorArg & grad_output , const TensorArg & weight , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",11, 111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( IntArrayRef input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",13, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_depthwise_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",31, 142, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward_input( CheckedFrom c , IntArrayRef input_size , const TensorArg & grad_output , const TensorArg & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",22, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward_input( IntArrayRef input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",13, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 155, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 165, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",16, 100, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",29, 136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( CheckedFrom c , IntArrayRef weight_size , const TensorArg & grad_output , const TensorArg & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",23, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_depthwise_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",29, 136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward_weight( CheckedFrom c , IntArrayRef weight_size , const TensorArg & grad_output , const TensorArg & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",23, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( IntArrayRef weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_weight( IntArrayRef weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_depthwise_convolution_backward_weight( IntArrayRef weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_bias( const Tensor & grad_output_t)",23, 101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double exponential_average_factor , double epsilon)",6, 79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm_backward( const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var , double epsilon)",7, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::expandScale( const Tensor & t , int64_t dim)",7, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , const Tensor & running_mean_t , const Tensor & running_var_t , bool training , double exponential_average_factor , double epsilon)",85, 119, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm_backward( const Tensor & input_t , const Tensor & grad_output_t , const Tensor & weight_t , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean_t , const Tensor & save_var_t , double epsilon)",67, 116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_mkl( const Tensor & input , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntArrayRef checked_signal_sizes , bool normalized , bool onesided , IntArrayRef output_sizes)",7, 64, 16, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_fill_with_conjugate_symmetry_slice( Tensor & output , int64_t signal_ndim , int64_t size_last_dim , int64_t start_last_dim_idx , int64_t i , int64_t num)",74, 89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_fill_with_conjugate_symmetry_( Tensor & input , int64_t signal_ndim , int64_t size_last_dim , int64_t last_dim_start_slice)",32, 97, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_mkl( const Tensor & self , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntArrayRef checked_signal_sizes , bool normalized , bool onesided , IntArrayRef output_sizes)",127, 103, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::_baddbmm_mkl_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::gemm_batched( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const float alpha , const float ** A , const float ** B , const float beta , float ** C)",10, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::gemm_batched( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const double alpha , const double ** A , const double ** B , const double beta , double ** C)",10, 94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::baddbmm_mkl_template( const Tensor & res , const Tensor & mat1 , const Tensor & mat2 , Scalar beta_ , Scalar alpha_)",25, 130, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::_baddbmm_mkl_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",8, 109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_dim_sparse( const SparseTensor & self)",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_dim_sparse( const SparseTensor & self)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::is_coalesced_sparse( const SparseTensor & self)",3, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_nnz_sparse( const SparseTensor & self)",3, 48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_indices_sparse( const SparseTensor & self)",3, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_values_sparse( const SparseTensor & self)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_coalesced_sparse_( SparseTensor & self , bool coalesced)",4, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::indices_sparse( const Tensor & self)",5, 90, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::values_sparse( const Tensor & self)",5, 89, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_sparse( const TensorOptions & options)",12, 95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_with_dims_sparse( int64_t sparse_dim , int64_t dense_dim , ArrayRef<int64_t> size , const TensorOptions & options)",5, 129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_with_dims_and_tensor_sparse( int64_t sparse_dim , int64_t dense_dim , ArrayRef<int64_t> size , const LongTensor & indices , const Tensor & values , const TensorOptions & options)",12, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::empty_sparse( IntArrayRef size , const TensorOptions & options)",3, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( ArrayRef<int64_t> size , const TensorOptions & options)",3, 94, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::expand_values_if_needed( const Tensor & values)",9, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( const Tensor & indices , const Tensor & values_ , const TensorOptions & options)",47, 127, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( const Tensor & indices , const Tensor & values_ , ArrayRef<int64_t> size , const TensorOptions & options)",44, 129, 15, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_sparse_coo_tensor_unsafe( const Tensor & indices , const Tensor & values_ , ArrayRef<int64_t> size , const TensorOptions & options)",12, 135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::clone_sparse( const SparseTensor & self)",5, 112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_resize_( SparseTensor & self , ArrayRef<int64_t> size , int64_t sparse_dim , int64_t dense_dim)",4, 114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_resize_and_clear_( SparseTensor & self , ArrayRef<int64_t> size , int64_t sparse_dim , int64_t dense_dim)",4, 124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_is_same_size_as_sparse( const SparseTensor & self , const SparseTensor & src)",3, 125, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::resize_as_sparse_( SparseTensor & self , const SparseTensor & src)",6, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_to_sparse( const Tensor & self)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_to_sparse( const Tensor & self , int64_t sparse_dim)",37, 102, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_to_dense( const SparseTensor & self)",4, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::copy_sparse_( SparseTensor & self , const SparseTensor & src , bool non_blocking)",6, 93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::coalesce_sparse_cpu( const SparseTensor & self)",72, 149, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_out_cpu_kernel( Tensor & r_values , const Tensor & t , const int64_t r_nnz , const int64_t sparse_dim , const LongTensor & mask_indices)",21, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_out_cpu( SparseTensor & r , const Tensor & t , const SparseTensor & mask)",56, 108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_cpu( const Tensor & t , SparseTensorRef mask)",5, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_to_csr( const int64_t * indices , int64_t dim , int64_t nnz)",19, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::zero_sparse_( SparseTensor & self)",5, 55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::wrapped_scalar_tensor( Scalar s)",5, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_zerodim( SparseTensor & r , const SparseTensor & t , const Tensor & value)",19, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_scalar( SparseTensor & r , const SparseTensor & t , Scalar value)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::log1p_out_sparse( SparseTensor & r , const SparseTensor & t)",15, 90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::log1p_sparse_( SparseTensor & t)",4, 94, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::pow_out_sparse_scalar( SparseTensor & r , const SparseTensor & t_ , Scalar value)",17, 128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::pow_sparse_scalar( const SparseTensor & t , Scalar value)",5, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::div_out_sparse_zerodim( SparseTensor & r , const SparseTensor & t , const Tensor & value)",19, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::div_out_sparse_scalar( SparseTensor & r , const SparseTensor & t , Scalar value)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::norm_sparse( const SparseTensor & self , Scalar value)",5, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_out_sparse_cpu( SparseTensor & r , const SparseTensor & t , const SparseTensor & src , Scalar value)",98, 208, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_dense_sparse_worker_cpu( Tensor & r , Scalar value , const SparseTensor & sparse , const Tensor & indices , const Tensor & values)",18, 133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_out_dense_sparse_cpu( Tensor & r , const Tensor & dense , SparseTensorRef sparse__ , Scalar value)",44, 136, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_cpu( SparseTensor & r , const Tensor & t_ , const Tensor & src_)",96, 140, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_out_sparse_dense_worker( int64_t nnz , int64_t dim_i , int64_t dim_j , int64_t dim_k , Tensor & r , Scalar beta , const Tensor & t , Scalar alpha , const Tensor & indices , const Tensor & values , const Tensor & dense)",44, 218, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_out_sparse_dense_cpu( Tensor & r , const Tensor & t , const SparseTensor & sparse_ , const Tensor & dense , Scalar beta , Scalar alpha)",51, 120, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_sparse_dense_cpu( const Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",11, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_sparse_dense_cpu_( Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",9, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_addmm( const Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",11, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_mm( const SparseTensor & sparse , const Tensor & dense)",7, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::hspmm_out_sparse_cpu( SparseTensor & r , const SparseTensor & sparse_ , const Tensor & dense)",68, 136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::hspmm_sparse_cpu( const SparseTensor & sparse , const Tensor & dense)",5, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sspaddmm_out_cpu( SparseTensor & r , const SparseTensor & t , const SparseTensor & sparse_ , const Tensor & dense , Scalar beta , Scalar alpha)",109, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sspaddmm_out_only_sparse( Tensor & result , const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",4, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::smm( const Tensor & self , const Tensor & mat2)",5, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::sspaddmm( const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",6, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input)",3, 48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , ScalarType dtype)",5, 66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , IntArrayRef dims_to_sum , ScalarType dtype)",3, 91, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , IntArrayRef dims_to_sum)",71, 160, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum_backward_cpu( const Tensor & grad_ , const SparseTensor & input_ , IntArrayRef dims_to_sum)",105, 210, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,"at::native::sparse_mask_out_cuda( SparseTensor & r , const Tensor & t , const SparseTensor & mask)",44, 108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,"at::native::sparse_mask_cuda( const Tensor & t , SparseTensorRef mask)",5, 71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",5, 85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_input( IntArrayRef input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool bias_defined)",5, 104, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_weights( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool bias_defined)",5, 104, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , std :: array<bool,3> output_mask)",5, 117, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::conv_output_size( IntArrayRef input_size , IntArrayRef weight_size , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",111, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_input( IntArrayRef input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool bias_defined)",111, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_weights( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool bias_defined)",132, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , std :: array<bool,3> output_mask)",18, 115, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp,"at::native::_cudnn_ctc_loss( const Tensor & log_probs , const Tensor & targets , IntArrayRef input_lengths , IntArrayRef target_lengths , int64_t BLANK , bool deterministic , bool zero_infinity)",3, 203, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp,"at::native::_cudnn_ctc_loss( const Tensor & log_probs_t , const Tensor & targets_t , IntArrayRef input_lengths_ , IntArrayRef target_lengths_ , int64_t BLANK , bool deterministic , bool zero_infinity)",53, 209, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double exponential_average_factor , double epsilon)",6, 79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm_backward( const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var , double epsilon)",7, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::expandScale( const Tensor & t , int64_t dim)",7, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , const Tensor & running_mean_t , const Tensor & running_var_t , bool training , double exponential_average_factor , double epsilon)",92, 119, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm_backward( const Tensor & input_t , const Tensor & grad_output_t , const Tensor & weight_t , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean_t , const Tensor & save_var_t , double epsilon)",72, 116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_forward( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",5, 89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_backward( const Tensor & grad_theta , int64_t N , int64_t C , int64_t H , int64_t W)",5, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::setSamplerDescriptor( SpatialTransformerDescriptor & desc , cudnnDataType_t dataType , int N , int C , int H , int W)",7, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_forward( const Tensor & theta_t , int64_t N , int64_t C , int64_t H , int64_t W)",22, 83, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_backward( const Tensor & grad_grid_t , int64_t N , int64_t C , int64_t H , int64_t W)",22, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_flatten_weight( TensorList weight_arr , int64_t weight_stride0 , int64_t input_size , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , bool fn_bidirectional)",9, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn( const Tensor & input_r , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf_r , const Tensor & hx , const Tensor & cx , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntArrayRef fn_batch_sizes , const Tensor & fn_dropout_state)",11, 70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward( const Tensor & input , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output , const Tensor & grad_output_r , const Tensor & grad_hy_r , const Tensor & grad_cy_r , int64_t mode , int64_t hidden_size , int64_t num_layers , bool batch_first , double dropout , bool train , bool bidirectional , IntArrayRef batch_sizes , const Tensor & dropout_state , const Tensor & reserve , std :: array<bool,4> output_mask)",12, 130, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_init_dropout_state( double dropout , bool train , int64_t dropout_seed , const TensorOptions & options)",3, 115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::DropoutDescriptorParams()",1, 33, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::set( bool train_ , double dropout_ , Tensor dropout_state_)",5, 68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::descriptor( cudnnHandle_t handle) const",10, 63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::num_directions() const",3, 37, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_mode( int64_t fn_mode)",22, 60, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_bidirectional( bool fn_bidirectional)",3, 85, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_algo( cudnnRNNAlgo_t algo)",3, 40, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set( int64_t mode , int64_t hidden_size , int64_t num_layers , bool bidirectional , cudnnDataType_t datatype , cudnnDataType_t input_datatype)",8, 148, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::descriptor( cudnnHandle_t handle , DropoutDescriptor && dropout_desc) const",5, 143, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::descriptor( cudnnHandle_t handle) const",5, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::rnn_descriptor_sequence( const Tensor & tensor , IntArrayRef batch_sizes)",14, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::rnn_descriptor( const Tensor & tensor , int64_t N)",7, 82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::is_input_packed() const",3, 38, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::set( IntArrayRef input_sizes , IntArrayRef batch_sizes_ , bool batch_first)",23, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::descriptors( Tensor x) const",8, 64, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::RNNDescriptors( const RNNParams & fn , cudnnHandle_t handle , Tensor x , Tensor y , Tensor hx , Tensor cx)",11, 106, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_descs( const std :: vector<TensorDescriptor> & descs)",8, 97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_x_descs()",3, 57, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_y_descs()",3, 57, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_num_weights( cudnnHandle_t handle , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , cudnnDataType_t datatype)",8, 107, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_num_linear_layers( cudnnRNNMode_t mode)",14, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_parameters( cudnnHandle_t handle , const RNNDescriptorParams & rnn , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , const FilterDescriptor & w_desc , const Tensor & weight_buf)",85, 115, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_expected_data_ptrs( const Tensor & weight_buf , cudnnHandle_t handle , const RNNDescriptorParams & rnn , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , cudnnDataType_t datatype)",37, 99, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_viewOrCopyParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to , bool copy)",21, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_copyParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to)",3, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_viewParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to)",3, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_input_size( const TensorDescriptorListParams & tensors)",7, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_hidden_size( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",3, 113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_output_size( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",7, 113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_algo( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors , const Tensor input)",24, 136, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::promote_rnn_math_type( cudnnDataType_t dtype)",10, 113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_flatten_weight( TensorList weight_arr , int64_t weight_stride0 , int64_t input_size , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , bool fn_bidirectional)",53, 112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn( const Tensor & input_r , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf_r , const Tensor & hx , const Tensor & cx , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntArrayRef fn_batch_sizes , const Tensor & fn_dropout_state)",142, 125, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward_input( const Tensor & input_r , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output_r , const Tensor & grad_output_r , const Tensor & grad_hy , const Tensor & grad_cy , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntArrayRef fn_batch_sizes , const Tensor & fn_dropout_state , const Tensor & fn_reserve , std :: array<bool,3> output_mask)",120, 115, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward_weight( const Tensor & input_r , TensorList weight_arr , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output_r , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntArrayRef fn_batch_sizes , const Tensor & fn_dropout_state , const Tensor & fn_reserve)",108, 129, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward( const Tensor & input , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output , const Tensor & grad_output_r , const Tensor & grad_hy_r , const Tensor & grad_cy_r , int64_t mode , int64_t hidden_size , int64_t num_layers , bool batch_first , double dropout , bool train , bool bidirectional , IntArrayRef batch_sizes , const Tensor & dropout_state , const Tensor & reserve , std :: array<bool,4> output_mask)",24, 294, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_init_dropout_state( double dropout , bool train , int64_t dropout_seed , const TensorOptions & options)",7, 115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::unpack_hidden( const Tensor & hidden)",3, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::unpack_hidden( const std :: tuple<Tensor,Tensor> & hidden)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden( const Tensor & hx , const Tensor & cx)",4, 102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden<Tensor>( const Tensor & hx , const Tensor & cx)",4, 65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden<std::tuple<Tensor,Tensor>>( const Tensor & hx , const Tensor & cx)",3, 105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutState::lock()",8, 82, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutState::unlock()",6, 23, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_dropout_state( double dropout_p , bool train , TensorOptions options)",21, 104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::try_get_weight_buf( const Tensor & input , TensorList parameters , bool has_biases , cudnnRNNMode_t mode , int64_t hidden_size , int64_t num_layers , bool bidirectional)",44, 100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_impl( const Tensor & input , const Tensor & _batch_sizes , const hidden_type & hidden , TensorList params , bool has_biases , cudnnRNNMode_t mode , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",28, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_impl( const Tensor & input , const hidden_type & hidden , TensorList params , bool has_biases , cudnnRNNMode_t mode , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",25, 96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::lstm_cudnn( Tensor & output , Tensor & hy , Tensor & cy , const Tensor & input , TensorList hx , TensorList params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",10, 96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::lstm_packed_cudnn( Tensor & output , Tensor & hy , Tensor & cy , const Tensor & data , const Tensor & batch_sizes , TensorList hx , TensorList params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",10, 84, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( IntArrayRef input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 86, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_bias( const at :: Tensor & grad_output)",4, 85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 95, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_input( const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_weight( IntArrayRef weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",6, 97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_output_size( IntArrayRef input_size , IntArrayRef weight_size , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",17, 82, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_input_size( IntArrayRef output_size , IntArrayRef weight_size , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",17, 110, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_weight_size( IntArrayRef input_size , IntArrayRef output_size , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",15, 110, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::narrowGroup( const Tensor & t , int dim , int group_idx , int64_t groups)",4, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::check_args( CheckedFrom c , IntArrayRef args , size_t expected_size , const char * arg_name)",18, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::convolution_shape_check( CheckedFrom c , const TensorGeometryArg & input , const TensorGeometryArg & weight , const TensorGeometryArg & output , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups)",20, 102, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::setConvolutionParams( ConvolutionParams * params , const at :: Tensor & input , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool deterministic)",27, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::ConvolutionArgs::ConvolutionArgs( const Tensor & input , const Tensor & output , const Tensor & weight)",2, 132, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::BenchmarkCache::find( const ConvolutionParams & params , T * results)",9, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::BenchmarkCache::insert( const ConvolutionParams & params , const T & results)",4, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::Workspace::Workspace( size_t size)",3, 63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::Workspace::~Workspace()",5, 56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionFwdAlgo_t algo , size_t * sz)",14, 52, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdDataAlgo_t algo , size_t * sz)",13, 57, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdFilterAlgo_t algo , size_t * sz)",13, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getMaxWorkspaceSize( const ConvolutionArgs & args , const algo_t * algo , int n_algo)",23, 90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getBestAlgorithm( perf_t * perfResults , const ConvolutionArgs & args , int n_algo)",43, 145, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgoPerf_t>::cache()",1, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgoPerf_t>::findAlgorithm( const ConvolutionArgs & args , bool benchmark)",43, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgoPerf_t>::getWorkspaceSize( const ConvolutionArgs & args , algo_t algo , size_t * workspaceSize)",13, 60, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgoPerf_t>::cache()",1, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgoPerf_t>::findAlgorithm( const ConvolutionArgs & args , bool benchmark)",41, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgoPerf_t>::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdDataAlgo_t algo , size_t * workspaceSize)",13, 65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgoPerf_t>::cache()",1, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgoPerf_t>::findAlgorithm( const ConvolutionArgs & args , bool benchmark)",42, 82, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgoPerf_t>::getWorkspaceSize( const ConvolutionArgs & args , algo_t algo , size_t * workspaceSize)",11, 96, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::findAlgorithm( const ConvolutionArgs & args , bool benchmark , perf_t * algoPerf)",53, 91, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::chooseAlgorithm( const ConvolutionArgs & args , bool benchmark , perf_t * algoPerf)",26, 73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_add_bias_( CheckedFrom c , const TensorArg & output , const TensorArg & bias)",19, 96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",38, 128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",27, 88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",17, 84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_input( const Tensor & grad_output_t , const Tensor & weight_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",12, 87, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 164, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",35, 134, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( CheckedFrom c , IntArrayRef input_size , const TensorArg & grad_output , const TensorArg & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",24, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_forward( CheckedFrom c , const TensorArg & grad_output , const TensorArg & weight , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",11, 111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( IntArrayRef input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",13, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 154, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntArrayRef padding , IntArrayRef output_padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",16, 100, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",33, 128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( CheckedFrom c , IntArrayRef weight_size , const TensorArg & grad_output , const TensorArg & input , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",23, 98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( IntArrayRef weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_weight( IntArrayRef weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntArrayRef padding , IntArrayRef stride , IntArrayRef dilation , int64_t groups , bool benchmark , bool deterministic)",15, 83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_bias( const Tensor & grad_output_t)",26, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_forward( const Tensor & input_t , const Tensor & grid_t)",4, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_backward( const Tensor & input_t , const Tensor & grid_t , const Tensor & grad_output_t)",5, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::setSamplerDescriptor( SpatialTransformerDescriptor & desc , cudnnDataType_t dataType , const at :: Tensor & tensor)",8, 114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::checkGridSize( CheckedFrom c , TensorArg grid , TensorArg input)",13, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_forward( const Tensor & input_t , const Tensor & grid_t)",34, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_backward( const Tensor & input_t , const Tensor & grid_t , const Tensor & grad_output_t)",42, 87, 12, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Arithmetic)",14, 33, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Comparisions)",12, 31, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Cast)",10, 38, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Construction)",9, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"to_string( const Half & h)",5, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Half2String)",4, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , HalfNumericLimits)",12, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , CommonMath)",47, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"fill_tensor( int64_t scalar , Tensor & t_)",6, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"test( Type & type , IntArrayRef shape , int64_t a = 0 , int64_t b = 1)",82, 81, 8, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Contiguous2D)",4, 38, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Small2D)",4, 32, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _2D)",4, 32, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _3D)",4, 33, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Medium3D)",4, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _10D)",4, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestResize( Type & type)",7, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestOnesAndDot( Type & type)",12, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSort( Type & type)",9, 75, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestRandperm( Type & type)",9, 62, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"SendContext()",4, 75, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAdd( Type & type)",8, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestLoadsOfAdds( Type & type)",16, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestLoadOfAddsWithCopy( Type & type)",16, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIsContiguous( Type & type)",6, 36, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestPermute( Type & type)",6, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestMm( Type & type)",6, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSqueeze( Type & type)",9, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestCopy( Type & type)",6, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestCopyBroadcasting( Type & type)",8, 40, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAbsValue( Type & type)",4, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAddingAValueWithScalar( Type & type)",4, 58, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSelect( Type & type)",7, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestZeroDim( Type & type)",16, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestTensorFromTH()",6, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestToCFloat()",9, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestToString()",7, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingByScalar()",21, 71, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingByZerodimTensor()",14, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingMixedDevice( Type & type)",6, 43, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestDispatch()",6, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestNegativeDim( Type & type)",6, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"test( Type & type)",28, 36, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TEST( BasicTest , BasicTestCPU)",5, 32, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TEST( BasicTest , BasicTestCUDA)",7, 33, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/undefined_tensor_test.cpp,"TEST( TestUndefined , UndefinedTest)",42, 95, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_cudnn_test.cpp,"TEST( CUDNNTest , CUDNNTestCUDA)",4, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/xla_tensor_test.cpp,"XLAFree( void * ptr)",3, 26, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/xla_tensor_test.cpp,"XLAMalloc( ptrdiff_t size)",3, 34, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/xla_tensor_test.cpp,"XLAAllocator::allocate( size_t size) const",4, 54, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/xla_tensor_test.cpp,"XLAAllocator::raw_deleter() const",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/xla_tensor_test.cpp,"TEST( XlaTensorTest , TestNoStorage)",10, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , SimpleLegacy)",15, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , Simple)",14, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , ExternalData)",18, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , Op)",11, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , PartiallyInitialized)",23, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , MutualResizes)",33, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , Op)",37, 93, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , SharedStorageRead)",28, 93, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , SharedStorageWrite)",13, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , MutualResizes)",29, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , Strided)",9, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , InplaceStrided)",17, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , NonRegularTensor)",6, 65, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , NonPOD)",6, 75, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( Caffe2ToPytorch , Nullptr)",6, 37, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tensor_interop_test.cpp,"TEST( PytorchToCaffe2 , Nullptr)",6, 39, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"require_equal_size_dim( const Tensor & lhs , const Tensor & rhs)",4, 68, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"should_expand( const IntArrayRef & from_size , const IntArrayRef & to_size)",15, 79, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"test( Type & T)",222, 83, 8, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"TEST( TestScalarTensor , TestScalarTensorCPU)",4, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"TEST( TestScalarTensor , TestScalarTensorCUDA)",7, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/tbb_init_test.cpp,"test( int given_num_threads)",12, 57, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tbb_init_test.cpp,"main()",18, 28, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/extension_backend_test.cpp,"empty_override( IntArrayRef size , const TensorOptions & options)",9, 115, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/extension_backend_test.cpp,"empty_like_override( const Tensor & self , const TensorOptions & options)",4, 81, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/extension_backend_test.cpp,"add_override( const Tensor & a , const Tensor & b , Scalar c)",4, 69, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/extension_backend_test.cpp,"TEST( BackendExtensionTest , TestRegisterOp)",36, 87, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"Foo::apply( Tensor a , Tensor b)",7, 67, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"Foo<Half>::apply( Tensor a , Tensor b)",1, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"test_overflow()",19, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"TEST( TestScalar , TestScalar)",72, 72, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"requireEqualTensorList( TensorList t1 , TensorList t2)",6, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestSplit( Type & T , Tensor & t)",10, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestChunk( Type & T , Tensor & t)",11, 51, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestStack( Type & T , Tensor & t)",21, 74, 8, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestSize( Type & T , Tensor & t)",17, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestMatmul( Type & T , Tensor & t , Type & AccT)",72, 81, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestStandardGammaGrad( Type & T , Tensor & t)",18, 67, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestWhere( Type & T , Tensor & t)",18, 63, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"test( Type & T , Type & AccT)",10, 33, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TEST( TestNative , NativeTestCPU)",5, 35, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TEST( TestNative , NativeTestGPU)",7, 39, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , TestParallel)",14, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , NestedParallel)",10, 84, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , Exceptions)",15, 67, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/verify_api_visibility.cpp,"main()",1, 22, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestSimpleCase( Type & T)",5, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestExpressionSpecification( Type & T)",10, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestEmptyTensor( Type & T)",4, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestScalarVs1Dim1Size( Type & T)",7, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TEST( TestWrapdim , TestWrapdim)",9, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakPointerGetsInvalidated)",6, 52, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakPointerLock)",11, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakUpdatesRefcountsTest)",29, 50, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/atest.cpp,"trace()",13, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/atest.cpp,"TEST( atest , atest)",78, 81, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"makeRandomNumber()",4, 34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"testCudaRNGMultithread()",9, 45, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"TEST( Cuda_RNGTest , MultithreadRNGTest)",4, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/dlconvertor_test.cpp,"TEST( TestDlconvertor , TestDlconvertor)",10, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CopyAndMoveTest)",37, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , GetAndSetTest)",18, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"thread_fun( at :: optional<at::cuda::CUDAStream> & cur_thread_stream)",6, 73, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , MultithreadGetAndSetTest)",17, 74, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CUDAGuardTest)",60, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , StreamPoolTest)",18, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , MultiGPUTest)",16, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CUDAEventSyncTest)",18, 59, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CrossDeviceTest)",24, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , Contiguous2D)",9, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , Contiguous3D)",9, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , PartialCollapse3D)",10, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , StridedCollapse2D)",10, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , PartialStridedCollapse4D)",12, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , CollapsesZerosAndOnes)",12, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , CollapseToPointTensor)",10, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , ExcludingInContiguous4D)",14, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , RovingExclusion)",14, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , InvalidExclusion)",7, 76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"cuda_get( T * addr)",5, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"cuda_set( T * addr , T value)",3, 72, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"TEST( CUDACaffe2ToPytorch , SimpleLegacy)",19, 57, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"TEST( CUDACaffe2ToPytorch , Simple)",19, 72, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"TEST( CUDACaffe2ToPytorch , Op)",14, 72, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"TEST( CUDAPytorchToCaffe2 , Op)",41, 93, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"TEST( CUDAPytorchToCaffe2 , SharedStorageWrite)",14, 80, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_tensor_interop_test.cpp,"TEST( CUDAPytorchToCaffe2 , MutualResizes)",30, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestEmptyTensor( Type & T)",4, 39, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2Basic( Type & T)",7, 75, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2WithScalar( Type & T)",7, 77, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2OldFallback( Type & T)",5, 36, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2MismatchedSizes( Type & T)",5, 40, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3Basic( Type & T)",9, 60, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3WithScalar( Type & T)",10, 74, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3OldFallback( Type & T)",6, 37, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3MismatchedSizes( Type & T)",6, 40, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2Basic( Type & T)",5, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2WithScalar( Type & T)",6, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2ExpandError( Type & T)",5, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3Basic( Type & T)",8, 67, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3WithScalar( Type & T)",11, 73, 22, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3ExpandError( Type & T)",6, 38, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimBasic( Type & T)",6, 66, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimWithScalar( Type & T)",8, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimWithMismatchedSizes( Type & T)",6, 51, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TEST( BroadcastTest , Broadcast)",28, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/test_install/main.cpp,"main()",3, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/cpu/FlushDenormal.cpp,"at::cpu::set_flush_denormal( bool on)",20, 84, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::CUDAGenerator( Context * context_)",4, 49, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::~CUDAGenerator()",3, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::copy( const Generator & from)",3, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::free()",4, 46, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::seed()",3, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::initialSeed()",3, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::manualSeed( uint64_t seed)",4, 58, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::manualSeedAll( uint64_t seed)",4, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::unsafeGetTH()",3, 64, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::initCUDAContextVectors()",5, 40, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::initDeviceProperty( DeviceIndex device_index)",5, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::warp_size()",3, 49, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentDeviceProperties()",4, 47, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getDeviceProperties( int64_t device)",7, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCUDADeviceAllocator()",3, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentCUDASparseHandle()",3, 77, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentCUDABlasHandle()",3, 75, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::allocator() const",3, 48, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::getDeviceFromPtr( void * data) const",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::generator() const",3, 78, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/PinnedMemoryAllocator.cpp,"at::cuda::getPinnedMemoryAllocator()",4, 47, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::initCUDA() const",10, 77, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::initCUDAGenerator( Context * context) const",4, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasCUDA() const",8, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasMAGMA() const",7, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasCuDNN() const",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::current_device() const",8, 44, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::getPinnedMemoryAllocator() const",3, 57, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::registerCUDATypes( Context * context) const",3, 60, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::compiledWithCuDNN() const",3, 44, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::compiledWithMIOpen() const",3, 45, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::supportsDilatedConvolutionWithCuDNN() const",10, 73, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::versionCuDNN() const",7, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::batchnormMinEpsilonCuDNN() const",8, 81, 6, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTGetPlanCacheMaxSize() const",7, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTSetPlanCacheMaxSize( int64_t max_size) const",7, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTGetPlanCacheSize() const",7, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTClearPlanCache() const",7, 53, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::getNumGPUs() const",11, 80, 8, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::getDataType( const at :: Tensor & t)",9, 85, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::set( const at :: Tensor & t , size_t pad)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::set( miopenDataType_t datatype , IntArrayRef t_sizes , IntArrayRef t_strides , size_t pad)",20, 112, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::miopenTypeToString( miopenDataType_t dtype)",12, 70, 6, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::operator < <( std :: ostream & out , const TensorDescriptor & d)",22, 74, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::print()",1, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::FilterDescriptor::set( const at :: Tensor & t , int64_t pad)",25, 95, 4, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Types.cpp,"at::native::getMiopenDataType( const at :: Tensor & tensor)",10, 63, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Types.cpp,"at::native::miopen_version()",3, 87, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::Handle::Handle()",3, 41, 4, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::Handle::~Handle()",5, 29, 6, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::getMiopenHandle()",8, 44, 2, 0
repos/cpp/pytorch/aten/src/ATen/detail/ComplexHooksInterface.cpp,"at::detail::getComplexHooks()",15, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/detail/CUDAHooksInterface.cpp,"at::detail::getCUDAHooks()",19, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/detail/HIPHooksInterface.cpp,"at::detail::getHIPHooks()",12, 72, 4, 0
repos/cpp/pytorch/aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.cpp,"c10::hip::HIPCachingAllocatorMasqueradingAsCUDA::get()",4, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.cpp,"c10::hip::HIPCachingAllocatorMasqueradingAsCUDA::recordStreamMasqueradingAsCUDA( void * ptr , HIPStreamMasqueradingAsCUDA stream)",3, 85, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/RegisterCPU.cpp,"at::register_cpu_types( Context * context)",4, 89, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",1, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::scalarType() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::typeMeta() const",3, 50, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::backend() const",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::toString() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::ID() const",3, 29, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::elementSizeInBytes() const",3, 45, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/RegisterCUDA.cpp,"at::register_cuda_types( Context * context)",3, 46, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",1, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::scalarType() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::typeMeta() const",3, 52, 4, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::backend() const",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::toString() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::ID() const",3, 29, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::elementSizeInBytes() const",3, 45, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::Dispatch::get_fn_table()",4, 75, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",1, 88, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::allocator() const",3, 56, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::getDeviceFromPtr( void * data) const",3, 54, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::generator() const",3, 56, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::backend() const",3, 35, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtension.cpp,"at::elementSizeInBytes() const",3, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/LegacyTHDispatcherDerived.cpp,"at::LegacyTHDispatcher( $ { Backend } TensorId())",1, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtensionDerived.cpp,"at::Type()",1, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtensionDerived.cpp,"at::scalarType() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtensionDerived.cpp,"at::typeMeta() const",3, 52, 4, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtensionDerived.cpp,"at::toString() const",3, 41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeExtensionDerived.cpp,"at::ID() const",3, 29, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::copy_( Tensor & self , const Tensor & src , bool non_blocking) const",9, 90, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::copy( const Tensor & src , bool non_blocking , optional<Device> to_device) const",11, 100, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::backward( Tensor & self , c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",7, 54, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::set_data( Tensor & self , Tensor new_data) const",3, 67, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::toBackend( Backend b) const",3, 65, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::toScalarType( ScalarType s) const",3, 62, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::defaultStrides( IntArrayRef sizes)",9, 64, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::computeStorageSize( IntArrayRef sizes , IntArrayRef strides)",12, 76, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorFromBlob( void * data , IntArrayRef sizes , const std :: function<void(void*)> & deleter) const",3, 119, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorFromBlob( void * data , IntArrayRef sizes , IntArrayRef strides , const std :: function<void(void*)> & deleter) const",4, 140, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorWithAllocator( IntArrayRef sizes , Allocator * allocator) const",3, 89, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorWithAllocator( IntArrayRef sizes , IntArrayRef strides , Allocator * allocator) const",4, 110, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",9, 116, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::storageWithAllocator( int64_t size , Allocator * allocator) const",4, 86, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::unsafeTensorFromTH( void * th_pointer , bool retain) const",7, 121, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::unsafeStorageFromTH( void * th_pointer , bool retain) const",6, 99, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::getDataType( const at :: Tensor & t)",11, 93, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::set( const at :: Tensor & t , size_t pad)",3, 62, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::set( cudnnDataType_t datatype , IntArrayRef t_sizes , IntArrayRef t_strides , size_t pad)",20, 111, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::cudnnTypeToString( cudnnDataType_t dtype)",26, 70, 6, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::operator < <( std :: ostream & out , const TensorDescriptor & d)",22, 87, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::print()",1, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::FilterDescriptor::set( const at :: Tensor & t , int64_t pad)",25, 93, 4, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Types.cpp,"at::native::getCudnnDataType( const at :: Tensor & tensor)",12, 61, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Types.cpp,"at::native::cudnn_version()",3, 26, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::Handle( bool create = false)",5, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::Handle( Handle && rhs)",1, 69, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::operator =( Handle rhs)",1, 81, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::~Handle()",15, 70, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::PoolWindow::PoolWindow()",1, 17, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::PoolWindow::~PoolWindow()",1, 30, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::PoolWindow::reserve( int device)",25, 95, 6, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::PoolWindow::release()",19, 117, 4, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::getCudnnHandle()",7, 41, 2, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_new( caffe2 :: TypeMeta data_type)",8, 61, 2, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_free( THStorage * storage)",6, 44, 2, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_size( const THStorage * self)",4, 48, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_retain( THStorage * storage)",6, 46, 4, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_resize( THStorage * storage , ptrdiff_t size)",26, 77, 6, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_free( THTensor * self)",5, 41, 2, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_setStorage( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntArrayRef size_ , at :: IntArrayRef stride_)",15, 138, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_setStorageNd( THTensor * self , THStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",27, 148, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_resize( THTensor * self , at :: IntArrayRef size , at :: IntArrayRef stride)",11, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_resizeNd( THTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",10, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_compute_stride( at :: IntArrayRef oldshape , at :: IntArrayRef oldstride , at :: IntArrayRef newshape)",61, 104, 8, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_stealAndSetStoragePtr( THTensor * tensor , THStorage * storage)",6, 85, 2, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THLogAdd( double log_a , double log_b)",21, 96, 4, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THLogSub( double log_a , double log_b)",19, 94, 4, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THExpMinusApprox( const double x)",31, 53, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isOpened( THFile * self)",5, 45, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_name( THFile * self)",5, 42, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"fread__( void * ptr , size_t size , size_t nitems , FILE * stream)",7, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_mode( const char * mode , int * isReadable , int * isWritable)",28, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_synchronize( THFile * self)",6, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_seek( THFile * self , ssize_t position)",22, 83, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_seekEnd( THFile * self)",19, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_position( THFile * self)",19, 85, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_close( THFile * self)",7, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_reverseMemory( void * dst , const void * src , ssize_t blockSize , ssize_t numBlocks)",21, 103, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isLittleEndianCPU( void)",10, 39, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isBigEndianCPU( void)",4, 43, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_nativeEndianEncoding( THFile * self)",6, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_littleEndianEncoding( THFile * self)",6, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_bigEndianEncoding( THFile * self)",6, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_longSize( THFile * self , int size)",7, 85, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_noBuffer( THFile * self)",8, 73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_free( THFile * self)",8, 44, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_readLong( THFile * self , int64_t * data , ssize_t n)",61, 96, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_writeLong( THFile * self , int64_t * data , ssize_t n)",72, 97, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_readString( THFile * self , const char * format , char ** str_)",88, 138, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_writeString( THFile * self , const char * str , ssize_t size)",18, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_new( const char * name , const char * mode , int isQuiet)",82, 106, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_mode( const char * mode , int * isReadable , int * isWritable)",19, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_free( THFile * self)",8, 44, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_new( const char * name , const char * mode , int isQuiet)",72, 215, 6, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_isOpened( THFile * self)",5, 47, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_strnextspace( int8_t * str_ , int8_t * c_)",23, 67, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_grow( THMemoryFile * self , ssize_t size)",21, 91, 39, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_mode( const char * mode , int * isReadable , int * isWritable)",28, 81, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_longSize( THFile * self , int size)",6, 85, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_storage( THFile * self)",9, 74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_synchronize( THFile * self)",5, 74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_seek( THFile * self , ssize_t position)",16, 74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_seekEnd( THFile * self)",7, 74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_position( THFile * self)",6, 74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_close( THFile * self)",7, 74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_free( THFile * self)",9, 46, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_readLong( THFile * self , int64_t * data , ssize_t n)",79, 200, 6, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_writeLong( THFile * self , int64_t * data , ssize_t n)",89, 159, 8, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_cloneString( const int8_t * str , ssize_t size)",6, 73, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_readString( THFile * self , const char * format , char ** str_)",63, 138, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_writeString( THFile * self , const char * str , ssize_t size)",18, 85, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_newWithStorage( THCharStorage * storage , const char * mode)",67, 124, 4, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_new( const char * mode)",4, 50, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"getTHDefaultAllocator()",3, 41, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",276, 150, 6, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( const char * filename , int flags , size_t size)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"WaitForReleaseHandle( PVOID lpParam , BOOLEAN TimerOrWaitFired)",14, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::close()",37, 113, 6, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( const char * filename , int flags , size_t size)",3, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",3, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::close()",1, 33, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocatorArgCheck::THRefcountedMapAllocatorArgCheck( int flags)",14, 90, 4, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( const char * filename , int flags , size_t size)",6, 97, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",6, 113, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::initializeAlloc()",21, 135, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::close()",33, 111, 4, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::incref()",5, 60, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::decref()",5, 60, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocatorArgCheck::THRefcountedMapAllocatorArgCheck( int flags)",1, 81, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( const char * filename , int flags , size_t size)",3, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",3, 115, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::initializeAlloc()",1, 52, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::~THRefcountedMapAllocator()",1, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"deleteTHMapAllocator( void * ptr)",3, 46, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"deleteTHRefcountedMapAllocator( void * ptr)",3, 56, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 71, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 91, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::makeDataPtr( const char * filename , int flags , size_t size , size_t * actual_size_out)",5, 113, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::makeDataPtr( WithFd , const char * filename , int fd , int flags , size_t size , size_t * actual_size_out)",5, 129, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::makeDataPtr( const char * filename , int flags , size_t size , size_t * actual_size_out)",5, 123, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::makeDataPtr( WithFd , const char * filename , int fd , int flags , size_t size , size_t * actual_size_out)",5, 139, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::data() const",3, 81, 2, 0
repos/cpp/pytorch/aten/src/TH/THSize.cpp,"THSize_isSameSizeAs( const int64_t * sizeA , int64_t dimsA , const int64_t * sizeB , int64_t dimsB)",11, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/THSize.cpp,"THSize_nElement( int64_t dims , int64_t * size)",12, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_newUnseeded()",10, 67, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_new()",6, 49, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_copy( THGenerator * self , THGenerator * from)",5, 68, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_free( THGenerator * self)",5, 41, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGeneratorState_isValid( THGeneratorState * _gen_state)",8, 80, 4, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGeneratorState_copy( THGeneratorState * self , THGeneratorState * from)",5, 88, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"readURandomLong()",14, 68, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_seed( THGenerator * _generator)",10, 48, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_manualSeed( THGenerator * _generator , uint64_t the_seed_)",23, 137, 4, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_initialSeed( THGenerator * _generator)",4, 55, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_nextState( THGenerator * _generator)",16, 61, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_random( THGenerator * _generator)",16, 71, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_random64( THGenerator * _generator)",6, 52, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"uniform_double( THGenerator * _generator)",5, 61, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"uniform_float( THGenerator * _generator)",5, 59, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_standard_uniform( THGenerator * _generator)",4, 58, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_uniform( THGenerator * _generator , double a , double b)",4, 69, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_uniformFloat( THGenerator * _generator , float a , float b)",4, 71, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_normal( THGenerator * _generator , double mean , double stdv)",20, 99, 4, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_exponential( THGenerator * _generator , double lambda)",4, 68, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_cauchy( THGenerator * _generator , double median , double sigma)",4, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_logNormal( THGenerator * _generator , double mean , double stdv)",5, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_geometric( THGenerator * _generator , double p)",5, 65, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_bernoulli( THGenerator * _generator , double p)",5, 60, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_bernoulliFloat( THGenerator * _generator , float p)",5, 62, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_readStringRaw( THFile * self , const char * format , char ** str_)",4, 75, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_writeStringRaw( THFile * self , const char * str , size_t size)",4, 73, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_synchronize( THFile * self)",4, 38, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_seek( THFile * self , size_t position)",4, 48, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_seekEnd( THFile * self)",4, 34, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_position( THFile * self)",4, 39, 2, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_close( THFile * self)",4, 32, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_free( THFile * self)",4, 31, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_isOpened( THFile * self)",4, 39, 2, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_binary( THFile * self)",4, 33, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_ascii( THFile * self)",4, 32, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_autoSpacing( THFile * self)",4, 38, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_noAutoSpacing( THFile * self)",4, 40, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_quiet( THFile * self)",4, 32, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_pedantic( THFile * self)",4, 35, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_clearError( THFile * self)",4, 37, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"defaultErrorHandlerFunction( const char * msg , void * data)",5, 69, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THError( const char * file , const int line , const char * fmt , ...)",21, 70, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THAssertionFailed( const char * file , const int line , const char * exp , const char * fmt , ...)",8, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetErrorHandler( THErrorHandlerFunction new_handler , void * data)",5, 71, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetDefaultErrorHandler( THErrorHandlerFunction new_handler , void * data)",8, 78, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"defaultArgErrorHandlerFunction( int argNumber , const char * msg , void * data)",8, 87, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THArgCheck( const char * file , int line , int condition , int argNumber , const char * fmt , ...)",23, 97, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetArgErrorHandler( THArgErrorHandlerFunction new_handler , void * data)",5, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetDefaultArgErrorHandler( THArgErrorHandlerFunction new_handler , void * data)",8, 84, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetGCHandler( void(*torchGCFunction_)(void*data) , void * data)",5, 72, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THAlloc( ptrdiff_t size)",7, 67, 4, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THRealloc( void * ptr , ptrdiff_t size)",26, 104, 4, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THFree( void * ptr)",4, 23, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog10( const double x)",4, 31, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog1p( const double x)",9, 75, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog2( const double x)",4, 30, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THExpm1( const double x)",4, 31, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetNumThreads( int num_threads)",17, 60, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THGetNumThreads( void)",8, 32, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THGetNumCores( void)",8, 30, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THInferNumThreads( void)",10, 80, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THSizeDesc( const int64_t * size , const int64_t ndim)",23, 66, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorFill.cpp,"THTensor_( fill)( THTensor * r_ , scalar_t value)",17, 84, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorFill.cpp,"THTensor_( zero)( THTensor * r_)",4, 35, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( storage)( const THTensor * self)",4, 52, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( storageOffset)( const THTensor * self)",4, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimension)( const THTensor * self)",4, 48, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimensionLegacyNoScalars)( const THTensor * self)",4, 63, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimensionLegacyAll)( const THTensor * self)",4, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( size)( const THTensor * self , int dim)",6, 94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( stride)( const THTensor * self , int dim)",6, 94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( data)( const THTensor * self)",3, 50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( new)( void)",8, 71, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithTensor)( THTensor * tensor)",15, 81, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage)( THStorage * storage , ptrdiff_t storageOffset , at :: IntArrayRef sizes , at :: IntArrayRef strides)",14, 131, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage1d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0)",5, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage2d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1)",6, 96, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage3d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2)",7, 112, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage4d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2 , int64_t size3 , int64_t stride3)",10, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize)( at :: IntArrayRef size , at :: IntArrayRef stride)",4, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize1d)( int64_t size0)",4, 50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize2d)( int64_t size0 , int64_t size1)",4, 65, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize3d)( int64_t size0 , int64_t size1 , int64_t size2)",4, 80, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize4d)( int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",4, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newClone)( THTensor * self)",9, 50, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newContiguous)( THTensor * self)",10, 51, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newSelect)( THTensor * tensor , int dimension_ , int64_t sliceIndex_)",6, 86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newNarrow)( THTensor * tensor , int dimension_ , int64_t firstIndex_ , int64_t size_)",6, 101, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newTranspose)( THTensor * tensor , int dimension1_ , int dimension2_)",6, 86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newUnfold)( THTensor * tensor , int dimension_ , int64_t size_ , int64_t step_)",6, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newView)( THTensor * tensor , at :: IntArrayRef size)",15, 116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize)( THTensor * self , at :: IntArrayRef size , at :: IntArrayRef stride)",4, 85, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resizeAs)( THTensor * self , THTensor * src)",5, 75, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize0d)( THTensor * tensor)",4, 47, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize1d)( THTensor * tensor , int64_t size0)",5, 58, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize2d)( THTensor * tensor , int64_t size0 , int64_t size1)",5, 73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize3d)( THTensor * tensor , int64_t size0 , int64_t size1 , int64_t size2)",5, 88, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize4d)( THTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",5, 101, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize5d)( THTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3 , int64_t size4)",5, 116, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set)( THTensor * self , THTensor * src)",10, 57, 28, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntArrayRef size_ , at :: IntArrayRef stride_)",4, 138, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage1d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_)",6, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage2d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_)",8, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage3d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_)",9, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage4d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_ , int64_t size3_ , int64_t stride3_)",12, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( narrow)( THTensor * self , THTensor * src , int dimension , int64_t firstIndex , int64_t size)",18, 103, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( select)( THTensor * self , THTensor * src , int dimension , int64_t sliceIndex)",29, 91, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( transpose)( THTensor * self , THTensor * src , int dimension1 , int dimension2)",22, 111, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( unfold)( THTensor * self , THTensor * src , int dimension , int64_t size , int64_t step)",35, 108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( squeeze)( THTensor * self , THTensor * src)",25, 55, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( squeeze1d)( THTensor * self , THTensor * src , int dimension)",29, 89, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( unsqueeze1d)( THTensor * self , THTensor * src , int dimension)",35, 90, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isTransposed)( const THTensor * self)",23, 52, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isContiguous)( const THTensor * self)",4, 50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isSameSizeAs)( const THTensor * self , const THTensor * src)",12, 71, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isSetTo)( const THTensor * self , const THTensor * src)",18, 79, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nElement)( const THTensor * self)",13, 52, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( retain)( THTensor * self)",4, 41, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( free)( THTensor * self)",4, 37, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( freeCopyTo)( THTensor * self , THTensor * dst)",10, 58, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorageNd)( THTensor * self , THStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",4, 150, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resizeNd)( THTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",4, 101, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set0d)( THTensor * tensor , scalar_t value)",5, 85, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get0d)( const THTensor * tensor)",5, 85, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set1d)( THTensor * tensor , int64_t x0 , scalar_t value)",6, 129, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get1d)( const THTensor * tensor , int64_t x0)",6, 129, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set2d)( THTensor * tensor , int64_t x0 , int64_t x1 , scalar_t value)",6, 126, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get2d)( const THTensor * tensor , int64_t x0 , int64_t x1)",6, 126, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set3d)( THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , scalar_t value)",6, 149, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get3d)( const THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2)",6, 149, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set4d)( THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3 , scalar_t value)",6, 187, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get4d)( const THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3)",6, 187, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( cat)( THTensor * r_ , THTensor * ta , THTensor * tb , int dimension)",7, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( check_shape_except_dim)( THTensor * first , THTensor * second , int dimension)",18, 97, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( catArray)( THTensor * result , THTensor ** inputs , int numInputs , int dimension)",111, 111, 10, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( desc)( const THTensor * tensor)",21, 69, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( sizeDesc)( const THTensor * tensor)",4, 80, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedFill)( THTensor * tensor , THByteTensor * mask , scalar_t value)",27, 81, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedCopy)( THTensor * tensor , THByteTensor * mask , THTensor * src)",34, 87, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedSelect)( THTensor * tensor , THTensor * src , THByteTensor * mask)",23, 82, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( nonzero)( THLongTensor * subscript , THTensor * tensor)",61, 87, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexSelect)( THTensor * tensor , THTensor * src , int dim , THLongTensor * index)",75, 115, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexCopy)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",39, 89, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( dataOffset)( THTensor * tensor , ptrdiff_t linearIndex)",11, 82, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( checkLinearIndex)( int64_t linearIndex , int64_t numel)",3, 123, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( wrapLinearIndex)( int64_t linearIndex , int64_t numel)",3, 87, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( take)( THTensor * r_ , THTensor * src , THLongTensor * index)",42, 80, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( put)( THTensor * tensor , THLongTensor * index , THTensor * src , int accumulate)",25, 99, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexAdd)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",40, 121, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexFill)( THTensor * tensor , int dim , THLongTensor * index , scalar_t val)",29, 117, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( gather)( THTensor * tensor , THTensor * src , int dim , THLongTensor * index)",26, 108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatter)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",31, 114, 13, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatterAdd)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",31, 114, 13, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatterFill)( THTensor * tensor , int dim , THLongTensor * index , scalar_t val)",27, 113, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( dot)( THTensor * tensor , THTensor * src)",14, 113, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( minall)( THTensor * tensor)",17, 93, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maxall)( THTensor * tensor)",17, 93, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sumall)( THTensor * tensor)",19, 119, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( prodall)( THTensor * tensor)",19, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( add)( THTensor * r_ , THTensor * t , scalar_t value)",26, 143, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sub)( THTensor * r_ , THTensor * t , scalar_t value)",4, 63, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( add_scaled)( THTensor * r_ , THTensor * t , scalar_t value , scalar_t alpha)",4, 86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sub_scaled)( THTensor * r_ , THTensor * t , scalar_t value , scalar_t alpha)",4, 86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( mul)( THTensor * r_ , THTensor * t , scalar_t value)",26, 143, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( div)( THTensor * r_ , THTensor * t , scalar_t value)",26, 143, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( lshift)( THTensor * r_ , THTensor * t , scalar_t value)",51, 164, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( rshift)( THTensor * r_ , THTensor * t , scalar_t value)",51, 164, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( fmod)( THTensor * r_ , THTensor * t , scalar_t value)",43, 153, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"modulo_wrap( scalar_t a , scalar_t b)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( remainder)( THTensor * r_ , THTensor * t , scalar_t value)",50, 193, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( bitand)( THTensor * r_ , THTensor * t , scalar_t value)",38, 148, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"modulo_wrap( scalar_t a , scalar_t b)",3, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( bitor)( THTensor * r_ , THTensor * t , scalar_t value)",38, 148, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( bitxor)( THTensor * r_ , THTensor * t , scalar_t value)",38, 148, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( clamp)( THTensor * r_ , THTensor * t , scalar_t min_value , scalar_t max_value)",31, 212, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cadd)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src)",35, 188, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( csub)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src)",4, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cmul)( THTensor * r_ , THTensor * t , THTensor * src)",31, 180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( pow)( THTensor * r_ , THTensor * t , scalar_t value)",42, 102, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( powOne)( scalar_t x , scalar_t y)",19, 62, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cpow)( THTensor * r_ , THTensor * t , THTensor * src)",37, 198, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cdiv)( THTensor * r_ , THTensor * t , THTensor * src)",31, 180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( clshift)( THTensor * r_ , THTensor * t , THTensor * src)",65, 193, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( crshift)( THTensor * r_ , THTensor * t , THTensor * src)",65, 193, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cfmod)( THTensor * r_ , THTensor * t , THTensor * src)",50, 184, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cremainder)( THTensor * r_ , THTensor * t , THTensor * src)",57, 233, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitand)( THTensor * r_ , THTensor * t , THTensor * src)",45, 180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitor)( THTensor * r_ , THTensor * t , THTensor * src)",45, 180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitxor)( THTensor * r_ , THTensor * t , THTensor * src)",45, 180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( tpow)( THTensor * r_ , scalar_t value , THTensor * t)",30, 166, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addcmul)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src1 , THTensor * src2)",37, 199, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addcdiv)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src1 , THTensor * src2)",37, 199, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addmv)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * mat , THTensor * vec)",72, 110, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( match)( THTensor * r_ , THTensor * m1 , THTensor * m2 , scalar_t gain)",41, 94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addmm)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * m1 , THTensor * m2)",142, 129, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addr)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * vec1 , THTensor * vec2)",66, 111, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addbmm)( THTensor * result , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * batch1 , THTensor * batch2)",42, 121, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorageCopy.cpp,"THStorage_( rawCopy)( THStorage * storage , scalar_t * src)",7, 60, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorageCopy.cpp,"THStorage_( copy)( THStorage * storage , THStorage * src)",5, 68, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( fill_DEFAULT)( scalar_t * x , const scalar_t c , const ptrdiff_t n)",14, 81, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( copy_DEFAULT)( scalar_t * x , const scalar_t * y , const ptrdiff_t n)",14, 82, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cadd_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const scalar_t c , const ptrdiff_t n)",15, 117, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( adds_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",15, 98, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cmul_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",15, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( muls_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",15, 98, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cdiv_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",15, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( divs_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",15, 98, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( interleaved_normal_fill_16)( scalar_t * data , const scalar_t mean , const scalar_t stddev)",15, 73, 50, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( normal_fill_DEFAULT)( scalar_t * data , int64_t size , THGenerator * generator , const scalar_t mean , const scalar_t stddev)",33, 67, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( trtrs)( char uplo , char trans , char diag , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",13, 128, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gels)( char trans , int m , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , scalar_t * work , int lwork , int * info)",12, 139, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( syev)( char jobz , char uplo , int n , scalar_t * a , int lda , scalar_t * w , scalar_t * work , int lwork , int * info)",12, 123, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( geev)( char jobvl , char jobvr , int n , scalar_t * a , int lda , scalar_t * wr , scalar_t * wi , scalar_t * vl , int ldvl , scalar_t * vr , int ldvr , scalar_t * work , int lwork , int * info)",12, 188, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gesdd)( char jobz , int m , int n , scalar_t * a , int lda , scalar_t * s , scalar_t * u , int ldu , scalar_t * vt , int ldvt , scalar_t * work , int lwork , int * iwork , int * info)",12, 178, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getrf)( int m , int n , scalar_t * a , int lda , int * ipiv , int * info)",12, 80, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getrs)( char trans , int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",12, 117, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getri)( int n , scalar_t * a , int lda , int * ipiv , scalar_t * work , int lwork , int * info)",12, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potri)( char uplo , int n , scalar_t * a , int lda , int * info)",12, 73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( pstrf)( char uplo , int n , scalar_t * a , int lda , int * piv , int * rank , scalar_t tol , scalar_t * work , int * info)",12, 124, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( geqrf)( int m , int n , scalar_t * a , int lda , scalar_t * tau , scalar_t * work , int lwork , int * info)",12, 111, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( orgqr)( int m , int n , int k , scalar_t * a , int lda , scalar_t * tau , scalar_t * work , int lwork , int * info)",12, 118, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( ormqr)( char side , char trans , int m , int n , int k , scalar_t * a , int lda , scalar_t * tau , scalar_t * c , int ldc , scalar_t * work , int lwork , int * info)",12, 163, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( baddbmm)( THTensor * result , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * batch1 , THTensor * batch2)",46, 145, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( numel)( THTensor * t)",4, 40, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( preserveReduceDimSemantics)( THTensor * r_ , int in_dims , int reduce_dimension , int keepdim)",8, 68, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( max)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",82, 110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( min)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",82, 110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sum)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",76, 110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( prod)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",76, 110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cumsum)( THTensor * r_ , THTensor * t , int dimension)",16, 116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cumprod)( THTensor * r_ , THTensor * t , int dimension)",16, 116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sign)( THTensor * r_ , THTensor * t)",15, 48, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( trace)( THTensor * t)",20, 78, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cross)( THTensor * r_ , THTensor * a , THTensor * b , int dimension)",46, 127, 23, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmax)( THTensor * r , THTensor * t , THTensor * src)",5, 74, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmin)( THTensor * r , THTensor * t , THTensor * src)",5, 74, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmaxValue)( THTensor * r , THTensor * t , scalar_t value)",5, 96, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cminValue)( THTensor * r , THTensor * t , scalar_t value)",5, 96, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( diag)( THTensor * r_ , THTensor * t , int k)",48, 142, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quicksortascend)( scalar_t * arr , int64_t * idx , int64_t elements , int64_t stride)",88, 102, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quicksortdescend)( scalar_t * arr , int64_t * idx , int64_t elements , int64_t stride)",88, 103, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sort)( THTensor * rt_ , THLongTensor * ri_ , THTensor * t , int dimension , int descendingOrder)",28, 111, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quickselectnoidx)( scalar_t * arr , int64_t k , int64_t elements , int64_t stride)",42, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quickselect)( scalar_t * arr , int64_t * idx , int64_t k , int64_t elements , int64_t stride)",42, 109, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( medianall)( THTensor * tensor)",24, 93, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( mode)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",65, 108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( kthvalue)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int64_t k , int dimension , int keepdim)",47, 120, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( median)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",11, 108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( topk)( THTensor * rt_ , THLongTensor * ri_ , THTensor * t , int64_t k , int dim , int dir , int sorted)",67, 109, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( triu)( THTensor * r_ , THTensor * t , int64_t k)",30, 80, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( equal)( THTensor * ta , THTensor * tb)",24, 68, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( atan2)( THTensor * r_ , THTensor * tx , THTensor * ty)",5, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( polygamma)( THTensor * r_ , int64_t n , THTensor * t)",7, 114, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( std)( THTensor * r_ , THTensor * t , int dimension , int biased , int keepdim)",40, 105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( var)( THTensor * r_ , THTensor * t , int dimension , int biased , int keepdim)",40, 105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( norm)( THTensor * r_ , THTensor * t , scalar_t value , int dimension , int keepdim)",46, 105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( normall)( THTensor * tensor , scalar_t value)",27, 92, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( renorm)( THTensor * res , THTensor * src , scalar_t value , int dimension , scalar_t maxnorm)",56, 113, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dist)( THTensor * tensor , THTensor * src , scalar_t value)",26, 87, 21, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( meanall)( THTensor * tensor)",4, 46, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( varall)( THTensor * tensor , int biased)",8, 64, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( stdall)( THTensor * tensor , int biased)",4, 79, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( histc)( THTensor * hist , THTensor * tensor , int64_t nbins , scalar_t minvalue , scalar_t maxvalue)",33, 109, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( bhistc)( THTensor * hist , THTensor * tensor , int64_t nbins , scalar_t minvalue , scalar_t maxvalue)",37, 156, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_alpha_small)( scalar_t x , scalar_t alpha , scalar_t beta)",12, 123, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_beta_small)( scalar_t x , scalar_t alpha , scalar_t beta)",15, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_alpha_mid)( double x , double alpha , double beta)",33, 99, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dirichlet_grad_one)( scalar_t x , scalar_t alpha , scalar_t total)",57, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dirichlet_grad)( THTensor * self , THTensor * x , THTensor * alpha , THTensor * total)",23, 106, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( data)( const THStorage * self)",4, 50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( size)( const THStorage * self)",4, 50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( elementSize)()",4, 33, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( new)( void)",4, 60, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize)( ptrdiff_t size)",9, 61, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithAllocator)( ptrdiff_t size , at :: Allocator * allocator)",10, 66, 40, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithMapping)( const char * filename , ptrdiff_t size , int flags)",18, 87, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize1)( scalar_t data0)",7, 52, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize2)( scalar_t data0 , scalar_t data1)",8, 68, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize3)( scalar_t data0 , scalar_t data1 , scalar_t data2)",9, 84, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize4)( scalar_t data0 , scalar_t data1 , scalar_t data2 , scalar_t data3)",10, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( retain)( THStorage * storage)",4, 44, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( free)( THStorage * storage)",4, 42, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithDataAndAllocator)( at :: DataPtr && data , ptrdiff_t size , at :: Allocator * allocator)",10, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( resize)( THStorage * storage , ptrdiff_t size)",4, 60, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( fill)( THStorage * storage , scalar_t value)",6, 58, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( set)( THStorage * self , ptrdiff_t idx , scalar_t value)",5, 71, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( get)( const THStorage * self , ptrdiff_t idx)",5, 71, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( swap)( THStorage * storage1 , THStorage * storage2)",4, 64, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",49, 82, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validConv2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",49, 82, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullConv2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",48, 84, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullXCorr2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",49, 84, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr2DRevptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",45, 80, 42, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",41, 89, 39, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validConv3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",41, 88, 38, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullConv3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",44, 87, 37, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullXCorr3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",39, 88, 38, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr3DRevptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",36, 92, 42, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2d)( scalar_t * output_data , scalar_t alpha , scalar_t * ptr_input , int64_t nInputRows , int64_t nInputCols , scalar_t * ptr_weight , int64_t nKernelRows , int64_t nKernelCols , int64_t srow , int64_t scol , const char * vf , const char * xc)",36, 87, 23, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3d)( scalar_t * output_data , scalar_t alpha , scalar_t * ptr_input , int64_t nInputDepth , int64_t nInputRows , int64_t nInputCols , scalar_t * ptr_weight , int64_t nKernelDepth , int64_t nKernelRows , int64_t nKernelCols , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",36, 109, 23, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( convsize)( int64_t x , int64_t k , int64_t s , const char * vf)",8, 84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2DRevger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol)",97, 130, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2DRevgerm)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol)",104, 131, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",125, 159, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmv)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",130, 158, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmm)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",143, 158, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",53, 159, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dcmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",71, 160, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmap)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , THTensor * map , int64_t srow , int64_t scol , const char * vf , const char * xc)",80, 174, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3DRevger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol)",79, 158, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",85, 112, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmv)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",87, 170, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",62, 170, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dcmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",79, 172, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmap)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , THTensor * map , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",89, 114, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( random)( THTensor * self , THGenerator * _generator)",24, 121, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( clampedRandom)( THTensor * self , THGenerator * _generator , int64_t min , int64_t max)",12, 144, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( cappedRandom)( THTensor * self , THGenerator * _generator , int64_t max)",4, 85, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( geometric)( THTensor * self , THGenerator * _generator , double p)",5, 94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( uniform)( THTensor * self , THGenerator * _generator , double a , double b)",11, 85, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal)( THTensor * self , THGenerator * _generator , double mean , double stddev)",10, 133, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_means)( THTensor * self , THGenerator * gen , THTensor * means , double stddev)",6, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_stddevs)( THTensor * self , THGenerator * gen , double mean , THTensor * stddevs)",7, 97, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_means_stddevs)( THTensor * self , THGenerator * gen , THTensor * means , THTensor * stddevs)",7, 107, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( exponential)( THTensor * self , THGenerator * _generator , double lambda)",5, 101, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( cauchy)( THTensor * self , THGenerator * _generator , double median , double sigma)",5, 103, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( logNormal)( THTensor * self , THGenerator * _generator , double mean , double stdv)",5, 103, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomialAliasSetup)( THTensor * probs , THLongTensor * J , THTensor * q)",87, 94, 24, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomialAliasDraw)( THLongTensor * self , THGenerator * _generator , THLongTensor * J , THTensor * q)",24, 112, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomial)( THLongTensor * self , THGenerator * _generator , THTensor * prob_dist , int n_sample , int with_replacement)",191, 138, 28, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( getRNGState)( THGenerator * _generator , THTensor * self)",11, 84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( setRNGState)( THGenerator * _generator , THTensor * self)",11, 84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( fill)( scalar_t * x , const scalar_t c , const ptrdiff_t n)",3, 73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cadd)( scalar_t * z , const scalar_t * x , const scalar_t * y , const scalar_t c , const ptrdiff_t n)",3, 111, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( adds)( scalar_t * r_ , const scalar_t * t , const scalar_t value , const ptrdiff_t n)",3, 104, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cmul)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",3, 93, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( muls)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cdiv)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",3, 93, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( divs)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",3, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( normal_fill)( scalar_t * data , const int64_t size , struct THGenerator * generator , const scalar_t mean , const scalar_t stddev)",7, 75, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( sigmoid)( scalar_t * y , const scalar_t * x , const ptrdiff_t n)",3, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( startup)",17, 60, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"sdot_( const int * n , const float * x , const int * incx , const float * y , const int * incy)",4, 107, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( swap)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",33, 84, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( scal)( int64_t n , scalar_t a , scalar_t * x , int64_t incx)",30, 83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( copy)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",29, 84, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( axpy)( int64_t n , scalar_t a , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",29, 96, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( dot)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",30, 87, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( gemv)( char trans , int64_t m , int64_t n , scalar_t alpha , scalar_t * a , int64_t lda , scalar_t * x , int64_t incx , scalar_t beta , scalar_t * y , int64_t incy)",58, 164, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( ger)( int64_t m , int64_t n , scalar_t alpha , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy , scalar_t * a , int64_t lda)",37, 136, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( gemm)( char transa , char transb , int64_t m , int64_t n , int64_t k , scalar_t alpha , scalar_t * a , int64_t lda , scalar_t * b , int64_t ldb , scalar_t beta , scalar_t * c , int64_t ldc)",139, 187, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( isTransposedContiguous)( THTensor * self)",4, 67, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( checkTransposed)( THTensor * self)",6, 55, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( newTransposedContiguous)( THTensor * self)",16, 68, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( checkLapackClone)( THTensor * result , THTensor * src , int nrows)",11, 116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( cloneColumnMajorNrows)( THTensor * self , THTensor * src , int nrows)",29, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( cloneColumnMajor)( THTensor * self , THTensor * src)",4, 76, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( trtrs)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a , const char * uplo , const char * trans , const char * diag)",49, 118, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gels)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a)",72, 116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( geev)( THTensor * re_ , THTensor * rv_ , THTensor * a_ , const char * jobvr)",79, 109, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( syev)( THTensor * re_ , THTensor * rv_ , THTensor * a , const char * jobz , const char * uplo)",46, 100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesdd)( THTensor * ru_ , THTensor * rs_ , THTensor * rv_ , THTensor * a , const char * some , const char * compute_uv)",6, 122, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesdd2)( THTensor * ru_ , THTensor * rs_ , THTensor * rv_ , THTensor * ra_ , THTensor * a , const char * some , const char * compute_uv)",126, 105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( getri)( THTensor * ra_ , THTensor * a)",43, 114, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( clearUpLoTriangle)( THTensor * a , const char * uplo)",32, 84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( copyUpLoTriangle)( THTensor * a , const char * uplo)",32, 84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potri)( THTensor * ra_ , THTensor * a , const char * uplo)",23, 86, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( pstrf)( THTensor * ra_ , THIntTensor * rpiv_ , THTensor * a , const char * uplo , scalar_t tol)",31, 104, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( qr)( THTensor * rq_ , THTensor * rr_ , THTensor * a)",19, 62, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( geqrf)( THTensor * ra_ , THTensor * rtau_ , THTensor * a)",40, 80, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( orgqr)( THTensor * ra_ , THTensor * a , THTensor * tau)",34, 84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( ormqr)( THTensor * ra_ , THTensor * a , THTensor * tau , THTensor * c , const char * side , const char * trans)",44, 115, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( btrisolve)( THTensor * rb_ , THTensor * b , THTensor * atf , THIntTensor * pivots)",106, 123, 2, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_fill_NEON( float * x , const float c , const ptrdiff_t n)",15, 82, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cmul_NEON( float * z , const float * x , const float * y , const ptrdiff_t n)",14, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_muls_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",14, 98, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cadd_NEON( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",14, 114, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_adds_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",14, 98, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cdiv_NEON( float * z , const float * x , const float * y , const ptrdiff_t n)",14, 99, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_divs_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",14, 98, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_fill_VSX( double * x , const double c , const ptrdiff_t n)",90, 82, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cadd_VSX( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",99, 122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_adds_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",81, 122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cmul_VSX( double * z , const double * x , const double * y , const ptrdiff_t n)",96, 122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_muls_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",81, 122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cdiv_VSX( double * z , const double * x , const double * y , const ptrdiff_t n)",96, 122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_divs_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",86, 122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_fill_VSX( float * x , const float c , const ptrdiff_t n)",90, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cadd_VSX( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",99, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_adds_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",79, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cmul_VSX( float * z , const float * y , const float * x , const ptrdiff_t n)",96, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_muls_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",79, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cdiv_VSX( float * z , const float * x , const float * y , const ptrdiff_t n)",96, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_divs_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",86, 121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_fill( double * x , const double c , const ptrdiff_t n)",5, 78, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_fill( float * x , const float c , const ptrdiff_t n)",5, 75, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cadd( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",5, 113, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cadd( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",5, 107, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_adds( double * y , const double * x , const double c , const ptrdiff_t n)",5, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_adds( float * y , const float * x , const float c , const ptrdiff_t n)",5, 91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cmul( double * z , const double * x , const double * y , const ptrdiff_t n)",5, 97, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cmul( float * z , const float * x , const float * y , const ptrdiff_t n)",5, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_muls( double * y , const double * x , const double c , const ptrdiff_t n)",5, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_muls( float * y , const float * x , const float c , const ptrdiff_t n)",5, 91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cdiv( double * z , const double * x , const double * y , const ptrdiff_t n)",5, 97, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cdiv( float * z , const float * x , const float * y , const ptrdiff_t n)",5, 92, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_divs( double * y , const double * x , const double c , const ptrdiff_t n)",5, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_divs( float * y , const float * x , const float c , const ptrdiff_t n)",5, 91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"randDouble()",4, 80, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"near( double a , double b)",15, 171, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_fill_VSX()",69, 96, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_fill_VSX()",69, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cadd_VSX()",70, 98, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cadd_VSX()",70, 97, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_adds_VSX()",65, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_adds_VSX()",66, 94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cmul_VSX()",69, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cmul_VSX()",69, 94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_muls_VSX()",69, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_muls_VSX()",68, 94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cdiv_VSX()",69, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cdiv_VSX()",69, 94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_divs_VSX()",69, 95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_divs_VSX()",69, 94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"main()",56, 63, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THDoubleVector_cadd_AVX2( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",18, 112, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_cadd_AVX2( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",18, 107, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"normal_fill_16_AVX2( float * data , const __m256 * two_pi , const __m256 * one , const __m256 * minus_two , const __m256 * mean , const __m256 * stddev)",22, 82, 2, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_normal_fill_AVX2( float * data , const int64_t size , THGenerator * generator , const float mean , const float stddev)",34, 82, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_sigmoid_AVX2( float * y , const float * x , const ptrdiff_t n)",21, 79, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_fill_AVX( double * x , const double c , const ptrdiff_t n)",15, 77, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cdiv_AVX( double * z , const double * x , const double * y , const ptrdiff_t n)",17, 133, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_divs_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",16, 132, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cmul_AVX( double * z , const double * x , const double * y , const ptrdiff_t n)",17, 95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_muls_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",16, 94, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cadd_AVX( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",15, 111, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_adds_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",16, 94, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_fill_AVX( float * x , const float c , const ptrdiff_t n)",15, 74, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cdiv_AVX( float * z , const float * x , const float * y , const ptrdiff_t n)",17, 129, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_divs_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",16, 128, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cmul_AVX( float * z , const float * x , const float * y , const ptrdiff_t n)",17, 91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_muls_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",16, 90, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cadd_AVX( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",15, 106, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_adds_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",16, 90, 0, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::memset_junk( void * data , size_t num)",16, 76, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::alloc_cpu( size_t nbytes)",38, 90, 6, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::free_cpu( void * data)",7, 28, 0, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIMemoryAllocationReporter::MemoryAllocationReporter()",1, 48, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIDefaultCPUAllocator::DefaultCPUAllocator()",1, 27, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIDefaultCPUAllocator::~DefaultCPUAllocator()",1, 37, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIDefaultCPUAllocator::allocate( size_t nbytes) const",8, 78, 6, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIDefaultCPUAllocator::ReportAndDelete( void * ptr)",7, 47, 4, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIDefaultCPUAllocator::raw_deleter() const",6, 50, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::C10_APIDefaultCPUAllocator::getMemoryAllocationReporter()",4, 67, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::NoDelete( void *)",1, 24, 0, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::GetCPUAllocator()",3, 40, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::SetCPUAllocator( at :: Allocator * alloc)",3, 45, 0, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::GetDefaultCPUAllocator()",3, 42, 0, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::MemoryAllocationReporter::New( void * ptr , size_t nbytes)",7, 78, 2, 0
repos/cpp/pytorch/c10/core/CPUAllocator.cpp,"c10::MemoryAllocationReporter::Delete( void * ptr)",9, 70, 2, 0
repos/cpp/pytorch/c10/core/Scalar.cpp,"c10::Scalar::operator -() const",9, 59, 4, 0
repos/cpp/pytorch/c10/core/TensorTypeId.cpp,"c10::operator < <( std :: ostream & str , c10 :: TensorTypeId rhs)",3, 69, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeId.cpp,"c10::toString( TensorTypeId id)",3, 44, 2, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::TensorTypeIds()",1, 60, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::singleton()",4, 44, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdCreator::TensorTypeIdCreator()",1, 60, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdCreator::create()",14, 79, 8, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::TensorTypeIdRegistry()",1, 81, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::registerId( c10 :: TensorTypeId id)",4, 62, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::deregisterId( c10 :: TensorTypeId id)",4, 64, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::createAndRegister()",5, 55, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::deregister( c10 :: TensorTypeId id)",3, 55, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistrar::TensorTypeIdRegistrar()",2, 61, 4, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistrar::~TensorTypeIdRegistrar()",3, 50, 0, 0
repos/cpp/pytorch/c10/core/Device.cpp,"c10::parse_type( const std :: string & device_string)",24, 134, 6, 0
repos/cpp/pytorch/c10/core/Device.cpp,"c10::Device::validate()",6, 69, 11, 0
repos/cpp/pytorch/c10/core/Device.cpp,"c10::Device::Device( const std :: string & device_string)",23, 71, 0, 0
repos/cpp/pytorch/c10/core/Device.cpp,"c10::operator < <( std :: ostream & stream , const Device & device)",7, 71, 0, 0
repos/cpp/pytorch/c10/core/DefaultDtype.cpp,"c10::set_default_dtype( caffe2 :: TypeMeta dtype)",3, 49, 0, 0
repos/cpp/pytorch/c10/core/DefaultDtype.cpp,"c10::get_default_dtype()",3, 46, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::grad()",7, 52, 4, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::grad() const",7, 52, 4, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( TensorTypeId type_id , const caffe2 :: TypeMeta & data_type , Allocator * allocator , bool is_variable)",8, 120, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( Storage && storage , TensorTypeId type_id , bool is_variable)",2, 82, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( Storage && storage , TensorTypeId type_id , const caffe2 :: TypeMeta & data_type , bool is_variable)",10, 117, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::sizes() const",3, 40, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::strides() const",3, 42, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::compute_contiguous() const",17, 46, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::release_resources()",5, 39, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::dim() const",3, 34, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::size( int64_t d) const",4, 44, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::stride( int64_t d) const",4, 46, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::maybe_zero_dim( bool condition_when_zero_dim)",7, 98, 2, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::has_storage() const",3, 39, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::storage() const",3, 45, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::deletePlacementDeleteContext( void * ptr)",3, 54, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::PlacementDeleteContext::makeDataPtr( at :: DataPtr && data_ptr , PlacementDtor placement_dtor , size_t size , at :: Device device)",11, 81, 10, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::AutogradMetaInterface::~AutogradMetaInterface()",1, 51, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::deleteInefficientStdFunctionContext( void * ptr)",3, 61, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::InefficientStdFunctionContext::makeDataPtr( void * ptr , const std :: function<void(void*)> & deleter , Device device)",9, 61, 10, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::SetAllocator( at :: DeviceType t , at :: Allocator * alloc)",3, 60, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::GetAllocator( const at :: DeviceType & t)",5, 58, 2, 0
repos/cpp/pytorch/c10/core/Stream.cpp,"c10::operator < <( std :: ostream & stream , const Stream & s)",4, 66, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::UndefinedTensorImpl()",3, 90, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::sizes() const",3, 50, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::size( int64_t d) const",3, 55, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::stride( int64_t d) const",3, 57, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::dim() const",3, 48, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::has_storage() const",3, 56, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::storage() const",3, 54, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::storage_offset() const",3, 62, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::strides() const",3, 52, 2, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::ThreadPool( std :: size_t pool_size , int numa_node_id)",11, 75, 4, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::~ThreadPool()",15, 56, 2, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::size() const",3, 34, 0, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::numAvailable() const",3, 42, 0, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::inThreadPool() const",8, 57, 4, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::run( const std :: function<void()> & func)",9, 73, 2, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::waitWorkComplete()",6, 45, 2, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::ThreadPool::main_loop( std :: size_t index)",51, 68, 6, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::setNumThreads( size_t v)",5, 85, 3, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::global_work_queue()",5, 79, 6, 0
repos/cpp/pytorch/c10/core/thread_pool.cpp,"c10::createC10ThreadPool( int device_id , int pool_size , bool create_new)",8, 57, 0, 0
repos/cpp/pytorch/c10/core/DeviceType.cpp,"c10::DeviceTypeName( DeviceType d , bool lower_case)",36, 79, 10, 0
repos/cpp/pytorch/c10/core/DeviceType.cpp,"c10::isValidDeviceType( DeviceType d)",17, 39, 0, 0
repos/cpp/pytorch/c10/core/DeviceType.cpp,"c10::operator < <( std :: ostream & stream , DeviceType type)",4, 66, 0, 0
repos/cpp/pytorch/c10/core/CopyBytes.cpp,"c10::_CopyBytesFunctionRegisterer::_CopyBytesFunctionRegisterer( DeviceType fromType , DeviceType toType , CopyBytesFunction func_sync , CopyBytesFunction func_async)",19, 79, 6, 0
repos/cpp/pytorch/c10/core/CopyBytes.cpp,"c10::CopyBytes( size_t nbytes , const void * src , Device src_device , void * dst , Device dst_device , bool async)",17, 78, 2, 0
repos/cpp/pytorch/c10/core/TensorOptions.cpp,"c10::operator < <( std :: ostream & stream , const TensorOptions & options)",9, 61, 2, 0
repos/cpp/pytorch/c10/core/impl/DeviceGuardImplInterface.cpp,"c10::impl::DeviceGuardImplRegistrar::DeviceGuardImplRegistrar( DeviceType type , const DeviceGuardImplInterface * impl)",3, 108, 0, 0
repos/cpp/pytorch/c10/test/core/TensorTypeId_test.cpp,"TEST( TensorTypeIdTest , Printing)",5, 35, 0, 0
repos/cpp/pytorch/c10/test/core/DeviceGuard_test.cpp,"TEST( DeviceGuard , ResetDeviceDifferentDeviceType)",12, 67, 2, 0
repos/cpp/pytorch/c10/test/core/DeviceGuard_test.cpp,"TEST( OptionalDeviceGuard , ResetDeviceDifferentDeviceType)",13, 77, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"dev( DeviceIndex index)",3, 40, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , Constructor)",33, 72, 6, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , ConstructorError)",4, 70, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , SetDevice)",15, 50, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , ResetDevice)",15, 50, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , SetIndex)",15, 50, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , Constructor)",31, 72, 6, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , NullaryConstructor)",20, 66, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , SetDevice)",14, 62, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , SetIndex)",14, 62, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"dev( DeviceIndex index)",3, 40, 2, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"stream( DeviceIndex index , StreamId sid)",3, 56, 0, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , Constructor)",17, 59, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamSameSameDevice)",16, 59, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamDifferentSameDevice)",18, 59, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamDifferentDevice)",20, 59, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , Constructor)",35, 65, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , ResetStreamSameDevice)",16, 65, 4, 0
repos/cpp/pytorch/c10/test/core/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , ResetStreamDifferentDevice)",18, 65, 4, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"half_legacy_impl::halfbits2float( unsigned short h)",30, 68, 2, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"half_legacy_impl::float2halfbits( float src)",51, 73, 2, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"TEST( HalfDoubleConversionTest , Half2Double)",17, 64, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::map_to_size::operator ( )( T) const",1, 96, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_sametype)",7, 85, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::map_make_shared::operator ( )( T)",3, 74, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_differenttypes)",5, 130, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::Class1::func()",1, 51, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::Class2::func()",1, 56, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::mapper_call_func::operator ( )( T)",1, 100, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_members)",7, 86, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::mapper_call_nonexistent_function::operator ( )( T)",1, 126, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_empty)",7, 86, 8, 0
repos/cpp/pytorch/c10/test/util/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceTrue)",4, 37, 0, 0
repos/cpp/pytorch/c10/test/util/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceFalse)",11, 57, 2, 0
repos/cpp/pytorch/c10/test/util/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceEquals)",16, 55, 4, 0
repos/cpp/pytorch/c10/test/util/logging_test.cpp,"c10_test::TEST( LoggingTest , EnforceShowcase)",24, 80, 6, 0
repos/cpp/pytorch/c10/test/util/logging_test.cpp,"c10_test::TEST( LoggingTest , Join)",8, 52, 2, 0
repos/cpp/pytorch/c10/test/util/logging_test.cpp,"c10_test::TEST( LoggingDeathTest , TestEnforceUsingFatal)",6, 62, 2, 0
repos/cpp/pytorch/c10/test/util/flags_test.cpp,"c10_test::TEST( FlagsTest , TestGflagsCorrectness)",11, 72, 2, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_equality_comparable::operator ==( const EqualityComparable & , const EqualityComparable &)",1, 101, 4, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"std::hash<test_is_hashable::Hashable>::operator ( )( const test_is_hashable :: Hashable &)",1, 76, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_function_type::Functor::operator ( )()",1, 29, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_function_type::func()",4, 21, 8, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeClass1Parameter::SomeClass1Parameter( int param_)",1, 53, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeClass2Parameters::SomeClass2Parameters( int param1_ , int param2_)",2, 49, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeBaseClass::SomeBaseClass( int v_)",1, 35, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeChildClass::SomeChildClass( int v)",1, 46, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::DestructableMock( bool * resourcesReleased , bool * wasDestructed)",2, 80, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::~DestructableMock()",3, 33, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::release_resources()",3, 38, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"ChildDestructableMock::ChildDestructableMock( bool * resourcesReleased , bool * wasDestructed)",2, 70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"NullType1::singleton()",3, 44, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"NullType2::singleton()",3, 44, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith0Parameters)",6, 72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith1Parameter)",5, 47, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith2Parameters)",6, 50, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , TypeIsAutoDeductible)",5, 58, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , CanAssignToBaseClassPtr)",4, 72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTargetTest , whenAllocatedOnStack_thenDoesntCrash)",3, 69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCallingGet_thenReturnsObject)",5, 73, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCallingConstGet_thenReturnsObject)",5, 78, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCallingGet_thenReturnsNullptr)",4, 76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenDereferencing_thenReturnsObject)",5, 76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenConstDereferencing_thenReturnsObject)",5, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenArrowDereferencing_thenReturnsObject)",5, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenConstArrowDereferencing_thenReturnsObject)",7, 67, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenPointsToSameObject)",7, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenOldInstanceInvalid)",6, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",8, 68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenStaysValid)",5, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToSelf_thenStaysInvalid)",7, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenNewInstanceIsValid)",9, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenPointsToSameObject)",9, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningFromInvalidPtr_thenNewInstanceIsInvalid)",9, 78, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",10, 74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenOldInstanceInvalid)",8, 74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",9, 75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",10, 75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",9, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenMoveAssigningToDifferentNullptr_thenHasNewNullptr)",12, 74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenPointsToSameObject)",7, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenOldInstanceValid)",6, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",8, 68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenStaysValid)",5, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToSelf_thenStaysInvalid)",7, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigning_thenNewInstanceIsValid)",9, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",8, 75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenOldInstanceInvalid)",8, 74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",9, 75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",10, 75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssigningInvalidPtrToBaseClass_thenNewInstanceIsInvalid)",9, 80, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenCopyAssigningToDifferentNullptr_thenHasNewNullptr)",12, 74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenPointsToSameObject)",6, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenOldInstanceInvalid)",5, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenNewInstanceValid)",5, 77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenPointsToSameObject)",9, 75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenOldInstanceInvalid)",7, 75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenNewInstanceValid)",7, 74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenMoveConstructingToDifferentNullptr_thenHasNewNullptr)",11, 77, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenPointsToSameObject)",7, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenOldInstanceValid)",5, 77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenNewInstanceValid)",5, 77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenPointsToSameObject)",9, 75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenOldInstanceInvalid)",7, 75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenNewInstanceInvalid)",7, 75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenCopyConstructingToDifferentNullptr_thenHasNewNullptr)",11, 77, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunction)",9, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethod)",9, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionFromInvalid)",9, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodFromInvalid)",9, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionWithInvalid)",9, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodWithInvalid)",9, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionInvalidWithInvalid)",7, 57, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodInvalidWithInvalid)",7, 55, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInContainer)",5, 57, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInSet)",5, 54, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInUnorderedSet)",5, 62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInMap)",11, 48, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInUnorderedMap)",11, 49, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_AfterCopyConstructor)",6, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_AfterCopyAssignment)",7, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_Nullptr)",6, 43, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality)",6, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality_NullptrLeft)",6, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality_NullptrRight)",6, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsDifferent)",7, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsDifferent_ValidAndInvalid)",7, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_AfterCopyConstructor)",7, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_AfterCopyAssignment)",8, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_BothNullptr)",7, 52, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , OneIsLess)",7, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsLess1)",5, 66, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsLess2)",5, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsNotLessThanNullptr)",5, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenIsInvalid)",6, 66, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenHoldsNullptr)",6, 69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenDestructed_thenDestructsObject)",11, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructed_thenDestructsObjectAfterSecondDestructed)",14, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructedToBaseClass_thenDestructsObjectAfterSecondDestructed)",14, 88, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsOldObject)",14, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsOldObject)",16, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithBaseClassCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenMoveAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsObjectAfterSecondDestructed)",16, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsObjectAfterSecondDestructed)",16, 88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 89, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 100, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 108, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",20, 94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",21, 105, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssigned_thenDestructsOldObject)",14, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClass_thenDestructsOldObject)",16, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithBaseClassCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCopyAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenDestructs)",10, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCallingReset_thenDestructsAfterCopyDestructed)",16, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCallingResetOnCopy_thenDestructsAfterOriginalDestructed)",16, 84, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithMoved_whenCallingReset_thenDestructsAfterMovedDestructed)",16, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithMoved_whenCallingResetOnMoved_thenDestructsImmediately)",13, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsMoveConstructingToConst)",4, 60, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsCopyConstructingToConst)",4, 60, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsMoveAssigningToConst)",5, 66, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsCopyAssigningToConst)",5, 72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNewPtr_thenHasUseCount1)",4, 62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNewPtr_thenIsUnique)",4, 62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenEmptyPtr_thenHasUseCount0)",4, 57, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenEmptyPtr_thenIsNotUnique)",4, 56, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenResetPtr_thenHasUseCount0)",5, 62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenResetPtr_thenIsNotUnique)",5, 62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenHasUseCount1)",5, 67, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenIsUnique)",5, 63, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenOldHasUseCount0)",5, 70, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenOldIsNotUnique)",5, 69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenHasUseCount1)",6, 64, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenIsUnique)",6, 63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenOldHasUseCount0)",6, 67, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenOldIsNotUnique)",6, 66, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenHasUseCount2)",5, 67, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenIsNotUnique)",5, 66, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenOldHasUseCount2)",5, 70, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenOldIsNotUnique)",5, 69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenDestructingCopy_thenHasUseCount1)",10, 68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenDestructingCopy_thenIsUnique)",10, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenReassigningCopy_thenHasUseCount1)",10, 68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenReassigningCopy_thenIsUnique)",10, 64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_thenHasUseCount2)",7, 64, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_thenIsNotUnique)",7, 63, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenDestructingCopy_thenHasUseCount1)",11, 65, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenDestructingCopy_thenIsUnique)",9, 80, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenReassigningCopy_thenHasUseCount1)",11, 65, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenReassigningCopy_thenIsUnique)",9, 80, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenDoesntCrash)",6, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",23, 80, 10, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenStackObject_whenReclaimed_thenCrashes)",7, 69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenNonOwningReclaimed_thenDoesntCrash)",9, 74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenNonOwningReclaimed_thenIsDestructedAtEnd)",22, 80, 10, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"IntrusiveAndWeak::IntrusiveAndWeak( intrusive_ptr<T> ptr_)",1, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_weak_intrusive( Args && ... args)",3, 78, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_weak_only( Args && ... args)",4, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_invalid_weak()",3, 72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCreatingAndDestructing_thenDoesntCrash)",5, 70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenLocking_thenReturnsCorrectObject)",5, 76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenPointsToSameObject)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenOldInstanceInvalid)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenNewInstanceIsValid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenStaysValid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenPointsToSameObject)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToSelf_thenStaysInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigning_thenNewInstanceIsValid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigning_thenPointsToSameObject)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToSelf_thenStaysInvalid)",8, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",8, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningFromInvalidPtr_thenNewInstanceIsInvalid)",9, 78, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningFromWeakOnlyPtr_thenNewInstanceIsInvalid)",9, 79, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",11, 80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenOldInstanceInvalid)",9, 80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",10, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",11, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",9, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",10, 77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",11, 77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",9, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenMoveAssigningToDifferentNullptr_thenHasNewNullptr)",10, 93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenPointsToSameObject)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenOldInstanceValid)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenStaysValid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigning_thenNewInstanceIsValid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToSelf_thenStaysInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigning_thenNewInstanceIsValid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigning_thenPointsToSameObject)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToSelf_thenStaysInvalid)",8, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",8, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",9, 81, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenOldInstanceInvalid)",9, 81, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",10, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",11, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigningInvalidPtrToBaseClass_thenNewInstanceIsInvalid)",9, 81, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",10, 77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",11, 77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigningWeakOnlyPtrToBaseClass_thenNewInstanceIsValid)",9, 80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenCopyAssigningToDifferentNullptr_thenHasNewNullptr)",10, 93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenPointsToSameObject)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenOldInstanceInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenNewInstanceValid)",5, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenPointsToSameObject)",10, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenOldInstanceInvalid)",8, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenNewInstanceValid)",8, 69, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenMoveConstructingToDifferentNullptr_thenHasNewNullptr)",9, 93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenPointsToSameObject)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenOldInstanceValid)",5, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenNewInstanceValid)",5, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenPointsToSameObject)",10, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenOldInstanceInvalid)",8, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenNewInstanceInvalid)",8, 71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenCopyConstructingToDifferentNullptr_thenHasNewNullptr)",9, 93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunction)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethod)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionFromInvalid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodFromInvalid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWithInvalid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWithInvalid)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionInvalidWithInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodInvalidWithInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionFromWeakOnlyPtr)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodFromWeakOnlyPtr)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWithWeakOnlyPtr)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWithWeakOnlyPtr)",9, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWeakOnlyPtrWithWeakOnlyPtr)",7, 69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWeakOnlyPtrWithWeakOnlyPtr)",7, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInContainer)",7, 60, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInSet)",7, 57, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInUnorderedSet)",7, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInMap)",13, 52, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInUnorderedMap)",13, 53, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyConstructor)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyAssignment)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyAssignment_WeakOnly)",6, 68, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_Invalid)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality)",6, 66, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_InvalidLeft)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_InvalidRight)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_WeakOnly)",6, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_ValidAndInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_ValidAndWeakOnly)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_WeakOnlyAndWeakOnly)",7, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyConstructor)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyConstructor_WeakOnly)",7, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyAssignment)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyAssignment_WeakOnly)",8, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_BothInvalid)",7, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , OneIsLess)",7, 74, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsLess1)",5, 76, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsLess2)",5, 77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsNotLessThanInvalid)",5, 72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingResetOnWeakPtr_thenIsInvalid)",6, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingResetOnStrongPtr_thenIsInvalid)",6, 81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsMoveConstructingToConst)",4, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsCopyConstructingToConst)",4, 68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsMoveAssigningToConst)",5, 80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsCopyAssigningToConst)",5, 80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNewPtr_thenHasUseCount1)",4, 70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNewPtr_thenIsNotExpired)",4, 70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_thenHasUseCount0)",4, 70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_thenIsExpired)",4, 70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_thenHasUseCount0)",4, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_thenIsExpired)",4, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingWeakReset_thenHasUseCount0)",5, 77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingWeakReset_thenIsExpired)",5, 74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingStrongReset_thenHasUseCount0)",5, 79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingStrongReset_thenIsExpired)",5, 76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenHasUseCount1)",5, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenIsNotExpired)",5, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenOldHasUseCount0)",5, 74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenOldIsExpired)",5, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenHasUseCount1)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenIsNotExpired)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenOldHasUseCount0)",6, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenOldIsExpired)",6, 71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenHasUseCount1)",5, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenIsNotExpired)",5, 71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenOldHasUseCount1)",5, 74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenOldIsNotExpired)",5, 74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenLastStrongPointerResets_thenReleasesResources)",15, 88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenDestructedButStillHasStrongPointers_thenDoesntReleaseResources)",15, 88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenDestructed_thenDestructsObject)",11, 85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructed_thenDestructsObjectAfterSecondDestructed)",14, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructedToBaseClass_thenDestructsObjectAfterSecondDestructed)",14, 88, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsOldObject)",14, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsOldObject)",16, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithBaseClassCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenMoveAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsObjectAfterSecondDestructed)",16, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsObjectAfterSecondDestructed)",16, 88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 89, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 100, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 108, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",20, 94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",21, 105, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigned_thenDestructsOldObject)",14, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClass_thenDestructsOldObject)",16, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithBaseClassCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCopyAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingReset_thenDestructs)",10, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCallingReset_thenDestructsAfterCopyDestructed)",16, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCallingResetOnCopy_thenDestructsAfterOriginalDestructed)",16, 84, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithMoved_whenCallingReset_thenDestructsAfterMovedDestructed)",16, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithMoved_whenCallingResetOnMoved_thenDestructsImmediately)",13, 83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenDoesntCrash)",6, 80, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenReleasedAndReclaimed_thenDoesntCrash)",8, 67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",32, 71, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",26, 80, 10, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenStackObject_whenReclaimed_thenCrashes)",7, 73, 0, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , TypeMetaStatic)",11, 79, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , Names)",14, 75, 6, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , TypeMeta)",37, 80, 6, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassAllowAssignment::ClassAllowAssignment()",1, 36, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassAllowAssignment::ClassAllowAssignment( const ClassAllowAssignment & src)",1, 70, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassNoAssignment::ClassNoAssignment()",1, 33, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , CtorDtorAndCopy)",28, 75, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , Float16IsNotUint16)",3, 65, 2, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , givenInt_whenWritingAndReading_thenChangesArePresent)",12, 76, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , givenVector_whenWritingAndReading_thenChangesArePresent)",11, 79, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , givenVector_whenWritingReturnsValue_thenValueIsReturned)",7, 79, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , readsCanBeConcurrent)",23, 75, 4, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , writesCanBeConcurrentWithReads_readThenWrite)",26, 75, 4, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , writesCanBeConcurrentWithReads_writeThenRead)",26, 75, 4, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , writesCannotBeConcurrentWithWrites)",26, 72, 12, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , whenReadThrowsException_thenThrowsThrough)",8, 65, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , whenWriteThrowsException_thenThrowsThrough)",8, 66, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , givenInt_whenWriteThrowsExceptionOnFirstCall_thenResetsToOldState)",22, 89, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , givenInt_whenWriteThrowsExceptionOnSecondCall_thenKeepsNewState)",28, 87, 0, 0
repos/cpp/pytorch/c10/test/util/LeftRight_test.cpp,"TEST( LeftRightTest , givenVector_whenWriteThrowsException_thenResetsToOldState)",22, 81, 0, 0
repos/cpp/pytorch/c10/test/util/tempfile_test.cpp,"TEST( TempFileTest , MatchesExpectedPattern)",4, 68, 2, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"MovableOnly::MovableOnly( int val_)",1, 78, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"MovableOnly::operator ==( const MovableOnly & lhs , const MovableOnly & rhs)",1, 104, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting()",1, 52, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting( const CopyCounting & rhs)",1, 105, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting( CopyCounting && rhs)",1, 100, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::operator =( const CopyCounting & rhs)",5, 55, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::operator =( CopyCounting && rhs)",5, 50, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex)",8, 106, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_singleInput)",4, 73, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_movableOnly)",6, 128, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_onlyCopiesIfNecessary)",13, 142, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_onlyMovesIfNecessary)",10, 140, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_keepsLValueReferencesIntact)",5, 87, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::map_to_double::operator ( )( T a) const",3, 65, 6, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap)",6, 113, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_emptyInput)",6, 84, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_emptyOutput)",6, 104, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_movableOnly_byRValue)",12, 149, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_movableOnly_byValue)",12, 149, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyCopiesIfNecessary)",18, 167, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyMovesIfNecessary_1)",15, 158, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyMovesIfNecessary_2)",16, 162, 8, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::Foo::Foo( int x)",3, 33, 4, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::Foo::~Foo()",1, 20, 2, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::Bar::Bar( int x)",3, 33, 2, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::AnotherBar::AnotherBar( int x)",3, 40, 2, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::TEST( RegistryTest , CanRunCreator)",6, 76, 2, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::TEST( RegistryTest , ReturnNullOnNonExistingCreator)",3, 68, 2, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::RegisterFooDefault()",4, 65, 6, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::RegisterFooDefaultAgain()",4, 65, 6, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::RegisterFooBarFallback()",4, 66, 6, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::RegisterFooBarPreferred()",4, 67, 6, 0
repos/cpp/pytorch/c10/test/util/registry_test.cpp,"c10_test::TEST( RegistryTest , RegistryPriorities)",19, 79, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::enforce_detail::EnforceFailMessage::EnforceFailMessage( std :: string && msg)",3, 75, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::GetFetchStackTrace()",4, 99, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::SetStackTraceFetcher( std :: function<string(void)> fetcher)",3, 65, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ThrowEnforceNotMet( const char * file , const int line , const char * condition , const std :: string & msg , const void * caller)",12, 79, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::Error::Error( SourceLocation source_location , const std :: string & msg)",7, 69, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::InitCaffeLogging( int * argc , char ** argv)",17, 73, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::UpdateLoggingLevelsFromFlags()",13, 77, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ShowLogInfoToStderr()",4, 70, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::InitCaffeLogging( int * argc , char ** argv)",19, 81, 17, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::UpdateLoggingLevelsFromFlags()",1, 39, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ShowLogInfoToStderr()",3, 33, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::MessageLogger::MessageLogger( const char * file , int line , int severity)",31, 81, 10, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::MessageLogger::~MessageLogger()",39, 80, 4, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::Error( const std :: string & new_msg , const std :: string & backtrace , const void * caller)",8, 68, 4, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::Error( const char * file , const uint32_t line , const char * condition , const std :: string & msg , const std :: string & backtrace , const void * caller)",19, 43, 14, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::msg() const",5, 70, 13, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::msg_without_backtrace() const",3, 81, 2, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::AppendMessage( const std :: string & new_msg)",9, 73, 2, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::warn( SourceLocation source_location , std :: string msg)",3, 70, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::set_warning_handler( handler_t handler)",3, 55, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::print_warning( const SourceLocation & source_location , const char * msg)",5, 71, 2, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::GetExceptionString( const std :: exception & e)",7, 68, 2, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::detail::_ThrowRuntimeTypeLogicError( const string & msg)",5, 77, 2, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::TypeMeta::_typeMetaDataInstance<detail::_Uninitialized>()",3, 108, 0, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::TypeIdentifier::createTypeId()",11, 149, 8, 0
repos/cpp/pytorch/c10/util/thread_name.cpp,"c10::setThreadName( std :: string name)",8, 54, 2, 0
repos/cpp/pytorch/c10/util/Half.cpp,"c10::operator < <( std :: ostream & out , const Half & value)",4, 65, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::IsNUMAEnabled()",3, 65, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMABind( int numa_node_id)",19, 41, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNUMANode( const void * ptr)",18, 45, 6, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNumNUMANodes()",7, 38, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMAMove( void * ptr , size_t size , int numa_node_id)",27, 72, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetCurrentNUMANode()",8, 45, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::IsNUMAEnabled()",3, 23, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMABind( int numa_node_id)",2, 34, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNUMANode( const void * ptr)",3, 35, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNumNUMANodes()",3, 24, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMAMove( void * ptr , size_t size , int numa_node_id)",2, 58, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetCurrentNUMANode()",3, 27, 0, 0
repos/cpp/pytorch/c10/util/UniqueVoidPtr.cpp,"c10::detail::deleteNothing( void *)",1, 29, 0, 0
repos/cpp/pytorch/c10/util/SmallVector.cpp,"c10::SmallVectorBase::grow_pod( void * FirstEl , size_t MinSizeInBytes , size_t TSize)",28, 79, 2, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::detail::StripBasename( const std :: string & full_path)",9, 58, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::operator < <( std :: ostream & out , const SourceLocation & loc)",4, 73, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::ReplaceAll( std :: string & s , const char * from , const char * to)",14, 70, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::GlobalInitStream()",4, 40, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::SetUsageMessage( const string & str)",4, 53, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::UsageMessage()",3, 40, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::ParseCommandLineFlags( int * pargc , char ** * pargv)",83, 80, 6, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::CommandLineFlagsHasBeenParsed()",3, 50, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<string>( const string & content , string * value)",6, 46, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<int>( const string & content , int * value)",10, 79, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<int64_t>( const string & content , int64_t * value)",18, 77, 4, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<double>( const string & content , double * value)",12, 80, 4, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<bool>( const string & content , bool * value)",22, 81, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::SetUsageMessage( const string & str)",7, 69, 4, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::UsageMessage()",3, 40, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::ParseCommandLineFlags( int * pargc , char ** * pargv)",6, 68, 2, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::CommandLineFlagsHasBeenParsed()",4, 79, 2, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::is_python_frame( const FrameInformation & frame)",4, 76, 2, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::parse_frame_information( const std :: string & frame_string)",66, 84, 0, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::get_backtrace( size_t frames_to_skip , size_t maximum_number_of_frames , bool skip_python_frames)",79, 81, 2, 0
repos/cpp/pytorch/c10/util/Type.cpp,"c10::demangle( const char * name)",26, 91, 2, 0
repos/cpp/pytorch/c10/util/Type.cpp,"c10::demangle( const char * name)",3, 41, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::LeakyStreamInternals::~LeakyStreamInternals()",9, 81, 4, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::operator < <( std :: ostream & stream , StreamIdType s)",17, 65, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::streamIdType( StreamId s)",3, 62, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::streamIdIndex( StreamId s)",3, 68, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::makeStreamId( StreamIdType st , size_t si)",4, 62, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::pointer_within( const T * ptr , const A & arr)",4, 60, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_getStreamId( const LeakyStreamInternals * ptr)",39, 79, 8, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initGlobalStreamState()",18, 78, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initDeviceStreamState( DeviceIndex device_index)",25, 74, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initCUDAStreamsOnce()",15, 80, 6, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::check_gpu( DeviceIndex device_index)",3, 59, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::get_idx( std :: atomic<uint32_t> & counter)",4, 58, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_internals( CUDAStream s)",30, 86, 10, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_fromInternals( const LeakyStreamInternals * ptr)",8, 71, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream::stream() const",5, 42, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getStreamFromPool( const bool isHighPriority , DeviceIndex device_index)",20, 80, 4, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getDefaultCUDAStream( DeviceIndex device_index)",8, 67, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getCurrentCUDAStream( DeviceIndex device_index)",8, 66, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::setCurrentCUDAStream( CUDAStream stream)",6, 47, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::operator < <( std :: ostream & stream , const CUDAStream & s)",3, 70, 0, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::DeviceStats::DeviceStats()",3, 52, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::DeviceStats::increaseAllocated( size_t delta)",4, 77, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::DeviceStats::decreaseAllocated( size_t delta)",3, 41, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::DeviceStats::increaseCached( size_t delta)",4, 68, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::DeviceStats::decreaseCached( size_t delta)",3, 38, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::Block::Block( int device , cudaStream_t stream , size_t size , BlockPool * pool , void * ptr)",3, 84, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::Block::Block( int device , cudaStream_t stream , size_t size)",3, 81, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::BlockComparator( const Block * a , const Block * b)",13, 60, 0, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::format_size( uint64_t size)",18, 48, 0, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::THCCachingAllocator()",3, 39, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::get_stats_for_device( int device)",7, 50, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::malloc( void ** devPtr , size_t size , cudaStream_t stream)",93, 84, 12, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::free( void * ptr)",23, 72, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::emptyCache()",6, 73, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::getBaseAllocation( void * ptr , size_t * outSize)",21, 54, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::cacheInfoAux( BlockPool & blocks , int dev_id , size_t * total , size_t * largest)",12, 83, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::cacheInfo( int dev_id , size_t * total , size_t * largest)",6, 61, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::recordStream( void * ptr , cuda :: CUDAStream stream)",14, 77, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::free_block( Block * block)",8, 61, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::try_merge_blocks( Block * dst , Block * src , BlockPool & pool)",21, 65, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::get_pool( size_t size)",7, 37, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::should_split( Block * block , size_t size)",10, 49, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::round_size( size_t size)",7, 75, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::get_allocation_size( size_t size)",9, 69, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::cuda_malloc_retry( int device , void ** devPtr , size_t size)",15, 78, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::free_cached_blocks( int device)",15, 50, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::free_blocks( BlockPool & blocks , BlockPool :: iterator it , BlockPool :: iterator end)",18, 87, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::find_allocated_block( void * ptr)",7, 43, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::insert_events( Block * block)",20, 80, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::THCCachingAllocator::process_events()",28, 78, 4, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::CudaCachingDeleter( void * ptr)",3, 44, 0, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::CudaCachingAllocator::allocate( size_t size) const",9, 78, 6, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::CudaCachingAllocator::raw_deleter() const",3, 46, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::get( void)",4, 28, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::emptyCache( void)",3, 34, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::cacheInfo( int dev_id , size_t * cachedAndFree , size_t * largestBlock)",3, 74, 0, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::getBaseAllocation( void * ptr , size_t * size)",4, 57, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::recordStream( void * ptr , cuda :: CUDAStream stream)",4, 54, 0, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::getFreeMutex()",4, 45, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::assertValidDevice( int device)",5, 80, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::currentMemoryAllocated( int device)",5, 74, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::maxMemoryAllocated( int device)",4, 78, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::resetMaxMemoryAllocated( int device)",5, 71, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::currentMemoryCached( int device)",5, 71, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::maxMemoryCached( int device)",4, 75, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::resetMaxMemoryCached( int device)",5, 71, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::getIpcDevPtr( std :: string handle)",32, 91, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::raw_alloc( size_t nbytes)",10, 76, 2, 0
repos/cpp/pytorch/c10/cuda/CUDACachingAllocator.cpp,"c10::cuda::CUDACachingAllocator::raw_delete( void * ptr)",3, 31, 2, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::has_cuda_gpu()",6, 46, 2, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::c10_cuda_test()",7, 39, 4, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::c10_cuda_private_test()",3, 30, 0, 0
repos/cpp/pytorch/c10/cuda/test/impl/CUDATest.cpp,"TEST( CUDATest , SmokeTest)",3, 28, 0, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::RocksDBCursor( rocksdb :: DB * db)",4, 57, 6, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::~RocksDBCursor()",1, 22, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::Seek( const string & key)",1, 62, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::SupportsSeek()",1, 48, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::SeekToFirst()",1, 56, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::Next()",1, 42, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::key()",1, 60, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::value()",1, 64, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBCursor::Valid()",1, 51, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBTransaction::RocksDBTransaction( rocksdb :: DB * db)",4, 59, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBTransaction::~RocksDBTransaction()",1, 38, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBTransaction::Put( const string & key , const string & value)",3, 62, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDBTransaction::Commit()",6, 80, 4, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDB::RocksDB( const string & source , Mode mode)",21, 73, 4, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDB::Close()",1, 41, 2, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDB::NewCursor()",3, 50, 4, 0
repos/cpp/pytorch/modules/rocksdb/rocksdb.cc,"caffe2::db::RocksDB::NewTransaction()",3, 55, 4, 0
repos/cpp/pytorch/modules/module_test/module_test_dynamic.cc,"caffe2::Caffe2ModuleTestDynamicDummyOp::Run( int)",3, 54, 2, 0
repos/cpp/pytorch/modules/module_test/module_test_dynamic.cc,"caffe2::Caffe2ModuleTestDynamicDummyOp::type()",3, 26, 2, 0
repos/cpp/pytorch/modules/detectron/softmax_focal_loss_op.cc,"caffe2::GetSoftmaxFocalLossGradient::GetGradientDefs()",7, 55, 8, 0
repos/cpp/pytorch/modules/detectron/smooth_l1_loss_op.cc,"caffe2::GetSmoothL1LossGradient::GetGradientDefs()",7, 55, 8, 0
repos/cpp/pytorch/modules/detectron/spatial_narrow_as_op.cc,"caffe2::SpatialNarrowAsGradient::GetGradientDefs()",6, 51, 2, 0
repos/cpp/pytorch/modules/detectron/group_spatial_softmax_op.cc,"caffe2::GetGroupSpatialSoftmaxGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/detectron/sigmoid_focal_loss_op.cc,"caffe2::GetSigmoidFocalLossGradient::GetGradientDefs()",8, 76, 8, 0
repos/cpp/pytorch/modules/detectron/select_smooth_l1_loss_op.cc,"caffe2::GetSelectSmoothL1LossGradient::GetGradientDefs()",7, 55, 8, 0
repos/cpp/pytorch/modules/detectron/ps_roi_pool_op.cc,"caffe2::GetPSRoIPoolGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/detectron/batch_permutation_op.cc,"caffe2::BatchPermutationOp<float,CPUContext>::RunOnDevice()",39, 88, 4, 0
repos/cpp/pytorch/modules/detectron/batch_permutation_op.cc,"caffe2::GetBatchPermutationGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/detectron/roi_pool_f_op.cc,"caffe2::GetRoIPoolFGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/detectron/upsample_nearest_op.cc,"caffe2::GetUpsampleNearestGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/detectron/sample_as_op.cc,"caffe2::GetSampleAsGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/detectron/sigmoid_cross_entropy_loss_op.cc,"caffe2::GetSigmoidCrossEntropyLossGradient::GetGradientDefs()",7, 51, 2, 0
repos/cpp/pytorch/modules/observers/net_observer_reporter_print.cc,"caffe2::NetObserverReporterPrint::report( NetBase * net , std :: map<std::string,PerformanceInformation> & info)",59, 81, 29, 0
repos/cpp/pytorch/modules/observers/net_observer_reporter_print.cc,"caffe2::get_tensor_shapes( PerformanceInformation p)",19, 65, 0, 0
repos/cpp/pytorch/modules/observers/net_observer_reporter_print.cc,"caffe2::get_op_args( PerformanceInformation p)",27, 59, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::registerGlobalPerfNetObserverCreator( int * , char ** *)",20, 79, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfNetObserver::PerfNetObserver( NetBase * subject_)",2, 52, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfNetObserver::~PerfNetObserver()",1, 39, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfNetObserver::Start()",40, 79, 2, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfNetObserver::Stop()",43, 77, 6, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfNetObserver::getObserverName( const OperatorBase * op , int idx) const",13, 81, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfOperatorObserver::PerfOperatorObserver( OperatorBase * op , PerfNetObserver * netObserver)",8, 77, 2, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfOperatorObserver::~PerfOperatorObserver()",1, 49, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfOperatorObserver::Start()",8, 78, 5, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfOperatorObserver::Stop()",6, 75, 2, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfOperatorObserver::getMilliseconds() const",3, 55, 0, 0
repos/cpp/pytorch/modules/observers/perf_observer.cc,"caffe2::PerfOperatorObserver::getTensorShapes() const",3, 73, 0, 0
