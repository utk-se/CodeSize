File, Function, Total Width, Leading Space(s), Leading Tab(s)
repos/cpp/pytorch/torch/abi-check.cpp,"main()",39, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGetRank()",51, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGetNumProcesses()",59, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllReduceMultiGPU( THDTensorDescriptor * data , size_t len , THDReduceOp operation , THDGroup group)",53, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllReduce( THDTensorDescriptor & desc , THDReduceOp operation , THDGroup group)",50, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDReduceMultiGPU( THDTensorDescriptor * desc , size_t len , THDReduceOp operation , int dst_rank , THDGroup group)",75, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDReduce( THDTensorDescriptor & desc , THDReduceOp operation , int dst_rank , THDGroup group)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBroadcastMultiGPU( THDTensorDescriptor * desc , size_t len , int src_rank , THDGroup group)",67, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBroadcast( THDTensorDescriptor & desc , int src_rank , THDGroup group)",77, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDIsend( THDTensorDescriptor & desc , int dst_rank)",64, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDIrecv( THDTensorDescriptor & desc , int src_rank)",64, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDSend( THDTensorDescriptor & desc , int dst_rank)",56, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRecvAnySource( THDTensorDescriptor & desc)",50, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRecv( THDTensorDescriptor & desc , int src_rank)",56, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllGatherMultiGPU( THDTensorDescriptor * output , size_t outputLen , THDTensorDescriptor * input , size_t inputLen , THDGroup group)",65, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllGather( THDTensorDescriptor * output , size_t len , THDTensorDescriptor & input , THDGroup group)",58, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGatherSend( THDTensorDescriptor & input , int dst_rank , THDGroup group)",79, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGatherRecv( THDTensorDescriptor * output , size_t len , THDTensorDescriptor & input , THDGroup group)",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDScatterSend( THDTensorDescriptor * input , size_t len , THDTensorDescriptor & output , THDGroup group)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDScatterRecv( THDTensorDescriptor & output , int src_rank , THDGroup group)",81, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBarrier( THDGroup group)",34, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDNewGroup( const int * ranks , size_t len)",53, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRequest_isCompleted( THDRequest * request)",51, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRequest_wait( THDRequest * request)",44, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDProcessGroupInit( THDChannelType channel_type , std :: string init_method = "env://" , int world_size = - 1 , std :: string group_name = "" , int rank = - 1)",75, 2, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDProcessGroupDestroy()",32, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDClearGroupCache( THDGroup group)",42, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"master()",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"worker( int id)",70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"main()",54, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"test( std :: shared_ptr<thd::DataChannel> data_channel)",74, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"run_all_tests( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",73, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"init_gloo_master( int workers)",67, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"init_gloo_worker( unsigned int id , int workers)",70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"main( void)",75, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_accept_timeout.cpp,"master()",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_accept_timeout.cpp,"main()",37, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/tensor_smoke.cpp,"main()",66, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_tensor( std :: shared_ptr<thd::DataChannel> data_channel)",77, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_tensor_any_source( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",76, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_scalar( std :: shared_ptr<thd::DataChannel> data_channel)",77, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_broadcast( std :: shared_ptr<thd::DataChannel> data_channel)",74, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"_test_reduce_helper( std :: shared_ptr<thd::DataChannel> data_channel , THDReduceOp op_type , int64_t init_value , int64_t expected_value)",69, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_reduce( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",80, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"_test_allReduce_helper( std :: shared_ptr<thd::DataChannel> data_channel , THDReduceOp op_type , int64_t init_value , int64_t expected_value)",80, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allReduce( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",79, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_scatter( std :: shared_ptr<thd::DataChannel> data_channel)",68, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_gather( std :: shared_ptr<thd::DataChannel> data_channel)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allGather( std :: shared_ptr<thd::DataChannel> data_channel)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_barrier( std :: shared_ptr<thd::DataChannel> data_channel)",76, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_isend( std :: shared_ptr<thd::DataChannel> data_channel)",70, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_irecv( std :: shared_ptr<thd::DataChannel> data_channel)",70, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_interlaces( std :: shared_ptr<thd::DataChannel> data_channel)",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_broadcast_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",65, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_reduce_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",72, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allReduce_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",76, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_scatter_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",78, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_gather_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",77, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allGather_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",68, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_barrier_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",78, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_invalid_rank( std :: shared_ptr<thd::DataChannel> data_channel)",77, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_empty_group( std :: shared_ptr<thd::DataChannel> data_channel)",72, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_process_not_in_group( std :: shared_ptr<thd::DataChannel> data_channel)",81, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_tensors_do_not_match_group_size( std :: shared_ptr<thd::DataChannel> data_channel)",70, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_tensors_are_not_the_same( std :: shared_ptr<thd::DataChannel> data_channel)",81, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"run_all_tests( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",59, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_tcp_master( int workers)",67, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_tcp_worker( unsigned int id , int workers)",70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_gloo_master( int workers)",67, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_gloo_worker( unsigned int id , int workers)",70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_mpi_process()",78, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"main( int argc , char const * argv [ ])",77, 8, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"master()",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"worker( int id)",70, 6, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"main()",54, 4, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_mpi_smoke.cpp,"main( int argc , char ** argv)",73, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannelRequest.cpp,"THDRequest_free( void * request)",46, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDSetCudaStatePtr( THCState ** state)",44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDRegisterCudaStream( cudaStream_t stream)",50, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDGetStreamId( cudaStream_t stream)",64, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::setSocketNoDelay( int socket)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::getSocketPort( int fd)",74, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::splitAddress( const std :: string & addr)",80, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::sockaddrToString( struct sockaddr * addr)",80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::listen( port_type port)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::connect( const std :: string & address , port_type port , bool wait , int timeout)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::accept( int listen_socket , int timeout)",76, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::newChannel( THDChannelType type , std :: string init_method , int world_size , std :: string group_name , int rank)",77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::Group()",31, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::Group( std :: vector<rank_type> ranks , rank_type max_rank)",78, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::~Group()",32, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::size() const",53, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::mustGetGroupRank( rank_type global_rank) const",77, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::getGroupRank( rank_type global_rank) const",70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::mustGetGlobalRank( rank_type group_rank) const",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::getGlobalRank( rank_type group_rank) const",67, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::sendPeerName( int socket)",74, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::getInterfaceAddresses()",78, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::discoverWorkers( int listen_socket , rank_type world_size)",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::discoverMaster( std :: vector<std::string> addresses , port_type port)",64, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::getRank( const std :: vector<int> & ranks , int assigned_rank , size_t order)",80, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::mustGetEnv( const char * env)",65, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::loadWorkerEnv()",61, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::maybeLoadEnv( const char * env_name , int value , std :: string parameter_name)",58, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::initEnv( std :: string argument , int world_size_r , std :: string group_name , int rank)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethod.cpp,"thd::getInitConfig( std :: string argument , int world_size , std :: string group_name , int rank)",71, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::lockLoop( int fd , struct flock & oflock)",62, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::lockFile( int fd)",41, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::unlockFile( int fd)",41, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::waitForGroup( std :: string file_path , std :: string group_name , std :: fstream & file)",76, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::waitForData( int fd , std :: fstream & file , rank_type world_size)",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::parseFile( std :: fstream & file , rank_type world_size , std :: string group_name)",78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::initFile( std :: string argument , int world_size_r , std :: string group_name , int assigned_rank)",72, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::getRandomString()",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::MulticastMessage( std :: string group_name , port_type port , int rank)",69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::MulticastMessage( std :: string msg)",57, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::pack()",66, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::isMulticastAddress( struct sockaddr * address)",63, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::bindMulticastSocket( struct sockaddr * address , struct sockaddr_storage * sock_addr , int timeout_sec = 1 , int ttl = 1)",80, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::getMessages( struct sockaddr * addr , rank_type world_size , std :: string group_name , std :: string packed_msg)",76, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCPMaster( std :: string address , std :: string str_port , rank_type world_size , int assigned_rank)",71, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCPMulticast( std :: string group_name , rank_type world_size , int assigned_rank , struct sockaddr * addr)",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCP( std :: string argument , int world_size_r , std :: string group_name , int rank)",78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::_getNcclDataType( at :: ScalarType type)",72, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::getDevicesList( const std :: string & deviceSeq)",64, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::DataChannelNccl( InitMethod :: Config config , int timeout)",79, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcastUniqueNcclId( ncclUniqueId * ncclId)",81, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::~DataChannelNccl()",80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_destroySockets()",62, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::destroy()",71, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_destroyNcclResources( THDGroup groupId)",80, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::clearGroupCache( THDGroup groupId)",58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::init()",81, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::getRank()",39, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::getNumProcesses()",47, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_getNcclResourcePair( std :: vector<at::Tensor> & input , THDGroup groupId)",81, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_tensorCheckHelper( const std :: vector<at::Tensor> & input , const std :: vector<at::Tensor> & output , size_t outputOverInput)",81, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",64, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup groupId)",44, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",74, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup groupId)",50, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",64, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",48, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",64, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcast( at :: Tensor & data , rank_type srcRank , THDGroup groupId)",44, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::barrier( THDGroup groupId)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::newGroup( const std :: vector<rank_type> & ranks)",74, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_checkGroupIdValid( THDGroup groupId)",66, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dstRank , THDGroup groupId)",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type srcRank , THDGroup groupId)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::send( Scalar & data , rank_type dstRank)",69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::send( at :: Tensor & data , rank_type dstRank)",69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( Scalar & data , rank_type srcRank)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( at :: Tensor & data)",55, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( at :: Tensor & data , rank_type srcRank)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::isend( at :: Tensor & data , rank_type dstRank)",70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::ireceive( at :: Tensor & data , rank_type srcRank)",73, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::log2ceil( uint32_t value)",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::pow2( T value)",32, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::RequestTCP( QueueWorker :: Request && request)",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::~RequestTCP()",45, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::isCompleted()",49, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::wait()",42, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::DataChannelTCP( InitMethod :: Config config)",58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::DataChannelTCP( InitMethod :: Config config , int timeout)",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::~DataChannelTCP()",59, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::destroy()",34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::initWorker()",80, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::initMaster()",78, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::init()",76, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::getRank()",38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::getNumProcesses()",46, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",79, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",78, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",78, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::send( Scalar & data , rank_type dst_rank)",65, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::send( at :: Tensor & data , rank_type dst_rank)",66, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( Scalar & data , rank_type src_rank)",68, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( at :: Tensor & data)",79, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( at :: Tensor & data , rank_type src_rank)",69, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::isend( at :: Tensor & data , rank_type dst_rank)",64, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::ireceive( at :: Tensor & data , rank_type src_rank)",67, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::barrier( THDGroup group_id)",81, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::newGroup( const std :: vector<rank_type> & ranks)",73, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_send( const Scalar & data , rank_type dst_rank)",80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_send( const at :: Tensor & data , rank_type dst_rank)",80, 3, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_receive( Scalar & data , rank_type src_rank)",81, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_receive( const at :: Tensor & data , rank_type src_rank)",81, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_reduce( at :: Tensor & result , at :: Tensor & data , THDReduceOp operation) const",60, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::clearGroupCache( THDGroup group_id)",58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::StoreDeamon( int listen_socket)",67, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::~StoreDeamon()",37, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::join()",34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::deamon()",81, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::query( rank_type rank)",74, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::checkAndUpdate( std :: vector<std::string> & keys) const",80, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::Store( const std :: string & addr , port_type port , int listen_socket)",73, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::~Store()",59, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::set( const std :: string & key , const std :: vector<char> & data)",73, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::get( const std :: string & key)",55, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::wait( const std :: vector<std::string> & keys)",65, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::RequestMPI()",44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::~RequestMPI()",44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::isCompleted()",79, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::wait()",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::save_buffer( std :: shared_ptr<T> ptr)",71, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::save_tensor_buffer( at :: Tensor & t)",69, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::new_request()",57, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::DataChannelMPI()",67, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::~DataChannelMPI()",57, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::destroy()",34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::init()",105, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::getRank()",38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::getNumProcesses()",46, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::_newLikeFlat( std :: vector<at::Tensor> & tensors) const",79, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",77, 10, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",78, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",71, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",75, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::send( Scalar & data , rank_type dst_rank)",62, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::send( at :: Tensor & data , rank_type dst_rank)",66, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( Scalar & data , rank_type src_rank)",65, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( at :: Tensor & data)",67, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( at :: Tensor & data , rank_type src_rank)",69, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::barrier( THDGroup group_id)",50, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::isend( at :: Tensor & data , rank_type dst_rank)",64, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::ireceive( at :: Tensor & data , rank_type src_rank)",67, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::newGroup( const std :: vector<rank_type> & ranks)",81, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",56, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::clearGroupCache( THDGroup group_id)",58, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::RequestGloo( QueueWorker :: Request && request)",74, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::~RequestGloo()",48, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::isCompleted()",51, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::wait()",44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::Group::Group( const std :: string & addr , port_type port , std :: vector<rank_type> ranks , rank_type max_rank , int store_socket)",54, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::DataChannelGloo( InitMethod :: Config config)",79, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::~DataChannelGloo()",38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::destroy()",35, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::init()",75, 22, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::getRank()",39, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::getNumProcesses()",47, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGatherT( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",77, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",78, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduceT( at :: Tensor & t , THDReduceOp operation , THDGroup group_id)",69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",71, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",71, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcastT( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",75, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",70, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::send( Scalar & data , rank_type dst_rank)",69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::send( at :: Tensor & data , rank_type dst_rank)",69, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( Scalar & data , rank_type src_rank)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( at :: Tensor & data)",67, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( at :: Tensor & data , rank_type src_rank)",72, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::isend( at :: Tensor & data , rank_type dst_rank)",70, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::ireceive( at :: Tensor & data , rank_type src_rank)",73, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",57, 6, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::clearGroupCache( THDGroup group_id)",59, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::barrier( THDGroup group_id)",66, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::newGroup( const std :: vector<rank_type> & ranks)",74, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::~Work()",31, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::isCompleted()",44, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::isSuccess() const",45, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::exception() const",59, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::sourceRank() const",59, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::synchronize()",42, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::wait()",45, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::finish( std :: exception_ptr exception)",64, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::ProcessGroup( int rank , int size)",77, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::~ProcessGroup()",33, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::setSocketNoDelay( int socket)",80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::getSocketPort( int fd)",74, 6, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::sockaddrToString( struct :: sockaddr * addr)",80, 4, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::listen( PortType port)",80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::connect( const std :: string & address , PortType port , bool wait , const std :: chrono :: milliseconds & timeout)",81, 2, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::accept( int listenSocket , const std :: chrono :: milliseconds & timeout)",78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::cudaAwareMpiCheck()",43, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSingleTensorHelper( const at :: Tensor & tensor)",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSingleTensor( const std :: vector<at::Tensor> & tensors)",69, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSameSizeAndType( const at :: Tensor & tensor , const std :: vector<at::Tensor> & tensors)",78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::AsyncWork( at :: Tensor tensor , MPI_Request request)",78, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()",76, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::isCompleted()",59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::isSuccess() const",75, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::sourceRank() const",53, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::wait()",59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::populateException()",81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::mpiExit()",59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::initMPIOnce()",73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::createProcessGroupMPI( std :: vector<int> ranks)",76, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::ProcessGroupMPI( int rank , int size , MPI_Comm pgComm)",72, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::~ProcessGroupMPI()",38, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::destroy()",59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::abort()",36, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::runLoop()",48, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::enqueue( std :: unique_ptr<WorkEntry> entry)",62, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::broadcast( std :: vector<at::Tensor> & tensors , const BroadcastOptions & opts)",65, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::allreduce( std :: vector<at::Tensor> & tensors , const AllreduceOptions & opts)",65, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::reduce( std :: vector<at::Tensor> & tensors , const ReduceOptions & opts)",80, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::allgather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const AllgatherOptions & opts)",76, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::gather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const GatherOptions & opts)",81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::scatter( std :: vector<at::Tensor> & outputTensors , std :: vector<std::vector<at::Tensor>> & inputTensors , const ScatterOptions & opts)",82, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::send( std :: vector<at::Tensor> & tensors , int dstRank , int tag)",61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::recv( std :: vector<at::Tensor> & tensors , int srcRank , int tag)",61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::recvAnysource( std :: vector<at::Tensor> & tensors , int tag)",68, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::barrier( const BarrierOptions & opts)",65, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::getGroupRank()",63, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Store.cpp,"c10d::Store::~Store()",19, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Store.cpp,"c10d::Store::setTimeout( const std :: chrono :: seconds & timeoutSec)",65, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::GlooStore( const std :: shared_ptr<::c10d::Store> & store)",76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::set( const std :: string & key , const std :: vector<char> & value)",78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::get( const std :: string & key)",59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::wait( const std :: vector<std::string> & keys)",61, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",59, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::toFunction( const ReduceOp & r)",50, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setInputs( O & opts , std :: vector<at::Tensor> & tensors)",67, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setInput( O & opts , at :: Tensor & tensor)",60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setOutputs( O & opts , std :: vector<at::Tensor> & tensors)",68, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setOutput( O & opts , at :: Tensor & tensor)",61, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::pinnedLike( at :: Tensor & tensor)",80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::initializeStreamsEvents( std :: vector<at::Tensor> & tensors , std :: vector<at::cuda::CUDAStream> & streams , std :: vector<at::cuda::CUDAEvent> & events)",72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::initializeStreamsEvents( std :: vector<std::vector<at::Tensor>> & tensors , std :: vector<at::cuda::CUDAStream> & streams , std :: vector<at::cuda::CUDAEvent> & events)",83, 12, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::SendWork::SendWork( at :: Tensor & tensor , std :: unique_ptr<::gloo::transport::UnboundBuffer> buffer)",62, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::SendWork::wait()",45, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::RecvWork( at :: Tensor & tensor , std :: unique_ptr<::gloo::transport::UnboundBuffer> buffer)",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::sourceRank() const",53, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::wait()",45, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::Options::Options()",53, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::ProcessGroupGloo( const std :: shared_ptr<Store> & store , int rank , int size , Options options)",80, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::~ProcessGroupGloo()",49, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::nextTag()",39, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::runLoop( int workerIndex)",50, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::enqueue( std :: shared_ptr<AsyncWork> work)",66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::AsyncBroadcastWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , uint32_t tag)",53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::broadcast( at :: Tensor & tensor)",61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::run()",50, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::AsyncBroadcastCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , uint32_t tag)",73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::run()",65, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::synchronize()",57, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::broadcast( std :: vector<at::Tensor> & inputs , const BroadcastOptions & opts)",72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::AsyncAllreduceWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , ReduceOp reduceOp , uint32_t tag)",74, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::allreduce( std :: vector<at::Tensor> & tensors)",63, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::run()",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::getFunction( gloo :: AllreduceOptions :: Func & fn , const ReduceOp op)",74, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::getFunction( const at :: ScalarType & dtype , const ReduceOp op)",52, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::AsyncAllreduceCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , ReduceOp reduceOp , uint32_t tag)",67, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::run()",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::synchronize()",57, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::allreduce( std :: vector<at::Tensor> & inputs , const AllreduceOptions & opts)",72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::AsyncReduceWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , ReduceOp reduceOp , uint32_t tag)",53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::reduce( std :: vector<at::Tensor> & tensors)",65, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::run()",24, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::getFunction( gloo :: ReduceOptions :: Func & fn , const ReduceOp op)",71, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::getFunction( const at :: ScalarType & dtype , const ReduceOp op)",52, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::AsyncReduceCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , ReduceOp reduceOp , uint32_t tag)",80, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::run()",58, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::synchronize()",57, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::reduce( std :: vector<at::Tensor> & inputs , const ReduceOptions & opts)",69, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::AsyncAllgatherWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , uint32_t tag)",72, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::allgather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs)",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::run()",32, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::AsyncAllgatherCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , uint32_t tag)",73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::run()",72, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::synchronize()",63, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::allgather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , const AllgatherOptions & opts)",72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::AsyncGatherWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , int root , uint32_t tag)",53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::gather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs)",73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::run()",29, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::AsyncGatherCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , int root , uint32_t tag)",73, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::run()",72, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::synchronize()",81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::gather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , const GatherOptions & opts)",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::AsyncScatterWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , int root , uint32_t tag)",53, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::scatter( std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs)",66, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::run()",30, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::AsyncScatterCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , int root , uint32_t tag)",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::run()",64, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::synchronize()",78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::scatter( std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , const ScatterOptions & opts)",70, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::checkSingleTensor( std :: vector<at::Tensor> & tensors)",78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::checkTag( int32_t tag)",50, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::send( std :: vector<at::Tensor> & tensors , int dstRank , int tag)",70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::recv( std :: vector<at::Tensor> & tensors , int srcRank , int tag)",70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::recvAnysource( std :: vector<at::Tensor> & tensors , int tag)",70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBarrierWork::AsyncBarrierWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::weak_ptr<AsyncWork>> priorWork , uint32_t tag)",71, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBarrierWork::run()",40, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::barrier( const BarrierOptions & opts)",77, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::getGroupRank()",78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::PrefixStore( const std :: string & prefix , Store & store)",66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::joinKey( const std :: string & key)",59, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::joinKeys( const std :: vector<std::string> & keys)",48, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::set( const std :: string & key , const std :: vector<uint8_t> & value)",41, 4, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::get( const std :: string & key)",64, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::add( const std :: string & key , int64_t value)",66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::check( const std :: vector<std::string> & keys)",64, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::wait( const std :: vector<std::string> & keys)",63, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",48, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::syscall( F fn)",51, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::Lock( int fd , int operation)",51, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::~Lock()",14, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::Lock( Lock && other)",32, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::unlock()",22, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::flock( int operation)",59, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::File( const std :: string & path , int flags , std :: chrono :: milliseconds timeout)",78, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::~File()",18, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::lockShared()",31, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::lockExclusive()",31, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::seek( off_t offset , int whence)",62, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::tell()",59, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::size()",35, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const void * buf , size_t count)",62, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( void * buf , size_t count)",61, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const std :: string & str)",69, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const std :: vector<uint8_t> & data)",70, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( std :: string & str)",40, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( std :: vector<uint8_t> & data)",42, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::refresh( File & file , off_t pos , std :: unordered_map<std::string,std::vector<uint8_t>> & cache)",68, 4, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::FileStore( const std :: string & path , int numWorkers)",72, 8, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::~FileStore()",79, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::set( const std :: string & key , const std :: vector<uint8_t> & value)",81, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::get( const std :: string & key)",77, 6, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::addHelper( const std :: string & key , int64_t i)",66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::add( const std :: string & key , int64_t i)",60, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::check( const std :: vector<std::string> & keys)",62, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::wait( const std :: vector<std::string> & keys)",61, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getNcclDataType( at :: ScalarType type)",78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getKeyFromDevices( const std :: vector<at::Device> & devices)",72, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getDeviceList( const std :: vector<at::Tensor> & tensors)",80, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::syncStreams( const std :: vector<at::Device> & devices , std :: vector<at::cuda::CUDAEvent> & ncclEvents , std :: vector<at::cuda::CUDAStream> & ncclStreams)",74, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::WorkNCCL( const std :: vector<at::Device> & devices)",77, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL()",43, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::isCompleted()",49, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::isSuccess() const",53, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::exception() const",70, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecution()",61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::synchronize()",78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::wait()",42, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::ProcessGroupNCCL( const std :: shared_ptr<Store> & store , int rank , int size , const std :: string & groupName)",78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::~ProcessGroupNCCL()",54, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::broadcastUniqueNCCLID( ncclUniqueId * ncclID)",74, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::getNCCLComm( const std :: string & devicesKey , const std :: vector<at::Device> & devices)",81, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::tensorCheckHelper( const std :: vector<at::Tensor> & input , const std :: vector<at::Tensor> & output , int outputOverInput)",81, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::allreduce( std :: vector<at::Tensor> & tensors , const AllreduceOptions & opts)",69, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::broadcast( std :: vector<at::Tensor> & tensors , const BroadcastOptions & opts)",69, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::reduce( std :: vector<at::Tensor> & tensors , const ReduceOptions & opts)",69, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::allgather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const AllgatherOptions & opts)",75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::barrier( const BarrierOptions & opts)",78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::gather( std :: vector<std::vector<at::Tensor>> & , std :: vector<at::Tensor> & , const GatherOptions &)",72, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::scatter( std :: vector<at::Tensor> & , std :: vector<std::vector<at::Tensor>> & , const ScatterOptions &)",73, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::send( std :: vector<at::Tensor> & , int , int)",70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::recv( std :: vector<at::Tensor> & , int , int)",70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::recvAnysource( std :: vector<at::Tensor> & , int)",70, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::getGroupRank()",79, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::TCPStoreDaemon( int storeListenSocket)",75, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::~TCPStoreDaemon()",37, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::join()",30, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::run()",80, 8, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::stop()",39, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::query( int socket)",55, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::wakeupWaitingClients( const std :: string & key)",68, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::setHandler( int socket)",58, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::addHandler( int socket)",77, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::getHandler( int socket) const",52, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::checkHandler( int socket) const",81, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::waitHandler( int socket)",51, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::checkKeys( const std :: vector<std::string> & keys) const",78, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::TCPStore( const std :: string & masterAddr , PortType masterPort , bool isServer)",78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::~TCPStore()",61, 4, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::set( const std :: string & key , const std :: vector<uint8_t> & data)",79, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::get( const std :: string & key)",63, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::add( const std :: string & key , int64_t value)",63, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::check( const std :: vector<std::string> & keys)",76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::wait( const std :: vector<std::string> & keys)",60, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",77, 32, 0
repos/cpp/pytorch/torch/lib/c10d/example/allreduce.cpp,"main( int argc , char ** argv)",80, 8, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"initialize( const std :: string & path , int N , Args && ... args)",78, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::AsyncTest( const std :: string & path)",54, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::AsyncTest( AsyncTest && other)",36, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::getProcessGroup()",48, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::start( int rank , int size)",75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::AsyncInputIsOutputTest( const std :: string & path , int numTensors)",77, 14, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::wait( std :: shared_ptr<ProcessGroup::Work> & work)",57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::getTensors()",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncAllreduceTest::AsyncAllreduceTest( const std :: string & path , int numTensors)",62, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncAllreduceTest::run()",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncBroadcastTest::AsyncBroadcastTest( const std :: string & path , int numTensors)",62, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncBroadcastTest::run( int rootRank , int rootTensor)",80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"runAsyncAllreduceTest( const std :: string & path , size_t numProcesses , size_t numTensors)",79, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"runAsyncBroadcastTest( const std :: string & path , size_t numProcesses , size_t numTensors)",81, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"main( int argc , char ** argv)",47, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::NCCLTestBase( const std :: string & path)",57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::NCCLTestBase( NCCLTestBase && other)",39, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::getProcessGroup()",48, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::initialize( int rank , int size)",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::NCCLTest( const std :: string & path , int worldSize)",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::wait( std :: shared_ptr<ProcessGroup::Work> & work)",57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::getTensors()",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::getOutputTensors()",70, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::numDevices() const",27, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllreduceNCCLTest::AllreduceNCCLTest( const std :: string & path , int worldSize)",60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllreduceNCCLTest::run()",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"BroadcastNCCLTest::BroadcastNCCLTest( const std :: string & path , int worldSize)",60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"BroadcastNCCLTest::run( int rootRank , int rootTensor)",80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"ReduceNCCLTest::ReduceNCCLTest( const std :: string & path , int worldSize)",57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"ReduceNCCLTest::run( int rootRank , int rootTensor)",80, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllgatherNCCLTest::AllgatherNCCLTest( const std :: string & path , int worldSize)",60, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllgatherNCCLTest::run()",67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testAllreduce( const std :: string & path , int rank , int size)",66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testBroadcast( const std :: string & path , int rank , int size)",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testReduce( const std :: string & path , int rank , int size)",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testAllgather( const std :: string & path , int rank , int size)",66, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"main( int argc , char ** argv)",72, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"tmppath()",72, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"testHelper( const std :: string prefix = "")",71, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"main( int argc , char ** argv)",46, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/TCPStoreTest.cpp,"testHelper( const std :: string & prefix = "")",81, 12, 0
repos/cpp/pytorch/torch/lib/c10d/test/TCPStoreTest.cpp,"main( int argc , char ** argv)",46, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::SignalTest( const std :: string & path)",55, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::~SignalTest()",27, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::arm( int pid , int signal)",34, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::run( int rank , int size)",75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testSignal( const std :: string & path , int signal)",56, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::initialize( const std :: string & path , int num)",74, 10, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::CollectiveTest( const std :: string & path)",59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::CollectiveTest( CollectiveTest && other)",43, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::getProcessGroup()",48, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::start( int rank , int size)",75, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"copyTensors( const std :: vector<std::vector<at::Tensor>> & inputs)",63, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testAllreduce( const std :: string & path , const at :: Backend b)",71, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testBroadcast( const std :: string & path , const at :: Backend b)",76, 8, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testBarrier( const std :: string & path)",71, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"main( int argc , char ** argv)",67, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"waitWork( std :: shared_ptr<c10d::ProcessGroupMPI> pg , std :: vector<std::shared_ptr<c10d::ProcessGroup::Work>> works)",69, 6, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testAllreduce( int iter = 1000)",79, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testBroadcast( int iter = 10000)",79, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testReduce( int iter = 10000)",76, 4, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testAllgather( int iter = 10000)",76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testGather( int iter = 10000)",76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testScatter( int iter = 1)",75, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testSendRecv( bool recvAnysource , int iter = 10000)",73, 8, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"main( int argc , char ** argv)",72, 4, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"libshm_init( const char * manager_exec_path)",50, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"deleteTHManagedMapAllocator( void * ptr)",53, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"THManagedMapAllocator::makeDataPtr( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",126, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"THManagedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",85, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"get_alloc_info( const char * filename)",73, 4, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"start_manager()",71, 4, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"get_manager_socket( const std :: string & manager_handle)",71, 4, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"libshm_init( const char * manager_exec_path)",60, 2, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocatorInit::THManagedMapAllocatorInit( const char * manager_handle , const char * filename)",103, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::THManagedMapAllocator( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",122, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::close()",62, 2, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"deleteTHManagedMapAllocator( void * ptr)",53, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::makeDataPtr( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",126, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",85, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"ClientSession::ClientSession( ManagerSocket s)",66, 2, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"register_fd( int fd)",27, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"unregister_fd( int fd)",66, 8, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"print_init_message( const char * message)",47, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"object_exists( const char * name)",40, 2, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"free_used_object( const std :: string & name)",65, 4, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"main( int argc , char * argv [ ])",86, 10, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_New( void * data , void(*destructor)(void*))",68, 2, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_check( PyObject * obj)",53, 2, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_get( PyObject * obj)",38, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",89, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_dealloc( THPWrapper * self)",49, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_init( PyObject * module)",48, 2, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_New( at :: Layout layout , const std :: string & name)",68, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_repr( THPLayout * self)",42, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_init( PyObject * module)",79, 2, 0
repos/cpp/pytorch/torch/csrc/stub.cpp,"init_C()",24, 0, 0
repos/cpp/pytorch/torch/csrc/stub.cpp,"PyInit__C()",27, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_New()",85, 4, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_NewWithGenerator( at :: Generator & cdata)",63, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_dealloc( THPGenerator * self)",53, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",91, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_getState( THPGenerator * self)",90, 2, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_setState( THPGenerator * self , PyObject * _new_state)",94, 4, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_manualSeed( THPGenerator * self , PyObject * seed)",78, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_seed( THPGenerator * self)",56, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_initialSeed( THPGenerator * self)",63, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_init( PyObject * module)",74, 2, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_New( at :: ScalarType scalar_type , const std :: string & name)",77, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_is_floating_point( THPDtype * self)",53, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_reduce( THPDtype * self)",75, 2, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_repr( THPDtype * self)",42, 2, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_init( PyObject * module)",77, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_getCallable( PyObject * arg , PyObject ** result)",61, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackSize( PyObject * arg)",75, 4, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_tryUnpackLongs( PyObject * arg , THLongStoragePtr & result)",83, 6, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackLongs( PyObject * arg)",93, 8, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_tryUnpackLongVarArgs( PyObject * args , int ignore_first , THLongStoragePtr & result)",97, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_checkIntTuple( PyObject * arg)",59, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackIntTuple( PyObject * arg)",68, 4, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_setError( const char * format , ...)",58, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_addPyMethodDefs( std :: vector<PyMethodDef> & vector , PyMethodDef * methods)",86, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"classOrTypename( PyObject * obj)",52, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_dispatchStateless( PyObject * tensor , const char * name , PyObject * args , PyObject * kwargs)",86, 2, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_invalidArguments( PyObject * given_args , PyObject * given_kwargs , const char * function_name , size_t num_options , ...)",77, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THPGenerator>::free()",40, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"setBackCompatBroadcastWarn( bool warn)",45, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"getBackCompatBroadcastWarn()",36, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"setBackCompatKeepdimWarn( bool warn)",43, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"getBackCompatKeepdimWarn()",34, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"maybeThrowBackCompatKeepdimWarn( char * func)",112, 8, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THTensor>::free()",38, 4, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THPStorage>::free()",38, 0, 0
repos/cpp/pytorch/torch/csrc/Storage.cpp,"THPPointer<THStorage>::free()",37, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialRead<int>( int fildes , void * buf , size_t nbytes)",67, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialRead<PyObject*>( PyObject * fildes , void * buf , size_t nbytes)",79, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialWrite<int>( int fildes , void * buf , size_t nbytes)",68, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialWrite<PyObject*>( PyObject * fildes , void * buf , size_t nbytes)",80, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"isUnsupportedOperation()",78, 2, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonReadBuffered( PyObject * fildes , void * buf , size_t raw_nbytes)",100, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonIO( PyObject * fildes , void * buf , size_t nbytes , bool is_read)",100, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonReadInto( PyObject * fildes , void * buf , size_t nbytes)",85, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonWrite( PyObject * fildes , void * buf , size_t nbytes)",82, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doRead( io fildes , void * raw_buf , size_t nbytes)",96, 4, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doWrite( io fildes , void * raw_buf , size_t nbytes)",83, 4, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"THPException_init( PyObject * module)",99, 2, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::replaceAll( std :: string & str , const std :: string & old_str , const std :: string & new_str)",64, 2, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::processErrorMsg( std :: string str)",74, 4, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::formatMessage( const char * format , va_list fmt_args)",73, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::IndexError::IndexError( const char * format , ...)",50, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::TypeError::TypeError( const char * format , ...)",48, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::ValueError::ValueError( const char * format , ...)",50, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_initNames( PyObject * self , PyObject * arg)",73, 4, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_initExtension( PyObject * _unused , PyObject * shm_manager_path)",99, 4, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfCsrcASAN( PyObject * module , PyObject * arg)",81, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfCsrcUBSAN( PyObject * module , PyObject * arg)",82, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfATenASAN( PyObject * module , PyObject * arg)",90, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getNumThreads( PyObject * module)",60, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setNumThreads( PyObject * module , PyObject * arg)",78, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDefaultTensorType( PyObject * _unused , PyObject * type)",77, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDefaultDtype( PyObject * _unused , PyObject * dtype)",73, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_safeCall( PyObject * _unused , PyObject * args , PyObject * kwargs)",82, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_addDocStr( PyObject * _unused , PyObject * args)",80, 8, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_inferSize( PyObject * _unused , PyObject * args)",79, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBackcompatBroadcastWarn( PyObject * module , PyObject * arg)",89, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getBackcompatBroadcastWarn( PyObject * module)",72, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBackcompatKeepdimWarn( PyObject * module , PyObject * arg)",87, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getBackcompatKeepdimWarn( PyObject * module)",70, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_hasDistributed( PyObject * _unused)",54, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"DLPack_Capsule_Destructor( PyObject * data)",91, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_toDLPack( PyObject * _unused , PyObject * data)",75, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_fromDLPack( PyObject * _unused , PyObject * data)",91, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setUserEnabledCuDNN( PyObject * _unused , PyObject * arg)",74, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_userEnabledCuDNN( PyObject * _unused)",62, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDeterministicCuDNN( PyObject * _unused , PyObject * arg)",80, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_deterministicCuDNN( PyObject * _unused)",64, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBenchmarkCuDNN( PyObject * _unused , PyObject * arg)",76, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_benchmarkCuDNN( PyObject * _unused)",60, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setFlushDenormal( PyObject * _unused , PyObject * arg)",73, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getDefaultDtype( PyObject * _unused , PyObject * arg)",72, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_isDefaultTypeCuda( PyObject * _unused , PyObject * arg)",74, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THCUDNN_cudnn_version( PyObject * self , PyObject * args)",72, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THCUDNN_methods()",33, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"warning_handler( const c10 :: SourceLocation & source_location , const char * msg)",56, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"initModule()",106, 2, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"pytorch_duplicate_guard()",68, 4, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"call_duplicate_guard::call_duplicate_guard()",56, 2, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes16( void * ptr)",64, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes32( void * ptr)",73, 3, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes64( void * ptr)",64, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt16LE( const uint8_t * data)",61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt16BE( const uint8_t * data)",61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt32LE( const uint8_t * data)",61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt32BE( const uint8_t * data)",61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt64LE( const uint8_t * data)",61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt64BE( const uint8_t * data)",61, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_nativeByteOrder()",61, 2, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt16Buffer( int16_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",94, 4, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt32Buffer( int32_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",94, 4, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt64Buffer( int64_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",94, 4, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeHalfBuffer( THHalf * dst , const uint8_t * src , THPByteOrder order , size_t len)",91, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeFloatBuffer( float * dst , const uint8_t * src , THPByteOrder order , size_t len)",91, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeDoubleBuffer( double * dst , const uint8_t * src , THPByteOrder order , size_t len)",93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt16Buffer( uint8_t * dst , const int16_t * src , THPByteOrder order , size_t len)",93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt32Buffer( uint8_t * dst , const int32_t * src , THPByteOrder order , size_t len)",93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt64Buffer( uint8_t * dst , const int64_t * src , THPByteOrder order , size_t len)",93, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeFloatBuffer( uint8_t * dst , const float * src , THPByteOrder order , size_t len)",91, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeDoubleBuffer( uint8_t * dst , const double * src , THPByteOrder order , size_t len)",93, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"setSignalHandler( int signal , void(*handler)(int,siginfo_t*,void*) , struct sigaction * old_sa_ptr)",120, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"handler_SIGTERM( int sig , siginfo_t * info , void * ctx)",80, 2, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerSignalHandlers( PyObject * module , PyObject * arg)",86, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_errorIfAnyWorkerFails( PyObject * module)",101, 6, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_updateWorkerPIDs( PyObject * module , PyObject * args)",98, 4, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_removeWorkerPIDs( PyObject * module , PyObject * loader_id)",94, 4, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerSignalHandlers( PyObject * module , PyObject * _ignored)",91, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_updateWorkerPIDs( PyObject * module , PyObject * _ignored)",84, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_removeWorkerPIDs( PyObject * module , PyObject * _ignored)",84, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_errorIfAnyWorkerFails( PyObject * module , PyObject * _ignored)",89, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_New( const at :: Device & device)",57, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_repr( THPDevice * self)",58, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_str( THPDevice * self)",49, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",88, 6, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_type( THPDevice * self)",49, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_index( THPDevice * self)",53, 4, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_hash( THPDevice * self)",114, 2, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_rc( PyObject * a , PyObject * b , int op)",60, 6, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_reduce( THPDevice * self)",89, 4, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_init( PyObject * module)",79, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::get_backend( bool is_cuda , bool is_sparse)",56, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::get_type( const std :: string & name , bool is_cuda , bool is_sparse)",76, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getPyTypeObject( const at :: Storage & storage)",59, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerStoragePyTypeObject( PyTypeObject * pytype , const std :: string & name , bool is_cuda , bool is_sparse)",110, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerDtypeObject( THPDtype * dtype , at :: ScalarType scalarType)",71, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerLayoutObject( THPLayout * layout , at :: Backend backend)",68, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getVariableType( at :: ScalarType scalarType , const THPLayout & layout , const at :: Device & device)",121, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getDtype( at :: ScalarType scalarType)",61, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getLayout( at :: Backend backend)",60, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getDeviceType( const at :: Type & type)",74, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::createPyObject( const at :: Storage & storage)",117, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::isStorage( PyObject * obj)",56, 2, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::createStorage( PyObject * obj)",72, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_New( const at :: ScalarType & type)",60, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_New( const at :: ScalarType & type)",60, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_str( THPFInfo * self)",49, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_str( THPIInfo * self)",49, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",96, 10, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",88, 8, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_compare( THPDTypeInfo * a , THPDTypeInfo * b , int op)",75, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_bits( THPDTypeInfo * self , void *)",64, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_eps( THPFInfo * self , void *)",68, 16, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_max( THPFInfo * self , void *)",81, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_max( THPFInfo * self , void *)",69, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_tiny( THPFInfo * self , void *)",81, 2, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_init( PyObject * module)",76, 2, 0
repos/cpp/pytorch/torch/csrc/nvrtc.cpp,"PyInit__nvrtc( void)",123, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_New( const torch :: autograd :: Variable & var)",88, 4, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_NewFromSizes( int dim , const int64_t * sizes)",69, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"isTracedZeroDimVar( PyObject * item)",67, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",88, 26, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_repr( THPSize * self)",70, 4, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"wrap_tuple_fn( Args ... args)",89, 4, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_numel( THPSize * self)",67, 2, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_init( PyObject * module)",73, 2, 0
repos/cpp/pytorch/torch/csrc/multiprocessing/init.cpp,"torch::multiprocessing::multiprocessing_init( PyObject * _unused)",71, 2, 0
repos/cpp/pytorch/torch/csrc/multiprocessing/init.cpp,"torch::multiprocessing::python_functions()",34, 0, 0
repos/cpp/pytorch/torch/csrc/onnx/init.cpp,"torch::onnx::initONNXBindings( PyObject * module)",81, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::throw_nccl_error( ncclResult_t status)",72, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::NcclCommList( const std :: vector<int> & devices)",78, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::~NcclCommList()",75, 5, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::ref() const",56, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_get_communicators( TensorList inputs)",74, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_get_data_type( const Type & type)",59, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_check_inputs( TensorList inputs , TensorList outputs , int input_multiplier , int output_multiplier)",98, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::is_available( TensorList tensors)",45, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::version()",60, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::get_max_count()",25, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::broadcast( TensorList tensors , const stream_list & streams , const comm_list & user_comms)",76, 40, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::reduce( const std :: vector<at::Tensor> & inputs , std :: vector<at::Tensor> & outputs , int32_t root , int32_t op , const stream_list & streams , const comm_list & user_comms)",79, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::reduce( std :: vector<at::Tensor> & inputs , int32_t root , int32_t op , const stream_list & streams , const comm_list & user_comms)",69, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_version( PyObject * self , PyObject * args)",68, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_unique_id( PyObject * self , PyObject * args)",70, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_nccl_comm( PyObject * capsule)",68, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"destroy_nccl_comm( PyObject * capsule)",73, 3, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_streams( PyObject * obj , size_t size)",101, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_comms( PyObject * obj , size_t size)",77, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_init_rank( PyObject * self , PyObject * args)",75, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_reduce( PyObject * self , PyObject * args)",102, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_all_reduce( PyObject * self , PyObject * args)",81, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_broadcast( PyObject * self , PyObject * args)",78, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_all_gather( PyObject * self , PyObject * args)",81, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_reduce_scatter( PyObject * self , PyObject * args)",81, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"extract_tensors( PyObject * obj)",77, 10, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"torch::cuda::unique_type_checker::show( const at :: Type & t)",33, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"torch::cuda::broadcast( const Tensor & tensor , IntList devices)",102, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/utils.cpp,"THPUtils_PySequence_to_CUDAStreamList( PyObject * obj)",108, 6, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",89, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_init( PyObject * module)",78, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/python_comm.cpp,"torch::cuda::python::initCommMethods( PyObject * module)",85, 12, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setDevice( int device)",38, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setDevice_wrap( PyObject * self , PyObject * arg)",77, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDevice_wrap( PyObject * self)",53, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDeviceCount_wrap( PyObject * self)",58, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCurrentStream_wrap( PyObject * self)",79, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setStream_wrap( PyObject * self , PyObject * obj)",70, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_isDriverSufficient( PyObject * self)",57, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDriverVersion( PyObject * self)",65, 20, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCompiledVersion( PyObject * self)",57, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getRNGState( PyObject * _unused)",83, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setRNGState( PyObject * _unused , PyObject * obj)",99, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_manualSeed( PyObject * _unused , PyObject * seed)",76, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_manualSeedAll( PyObject * _unused , PyObject * seed)",76, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_seed( PyObject * _unused)",53, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_seedAll( PyObject * _unused)",56, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_initialSeed( PyObject * _unused)",60, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaHostAllocator( PyObject * _unused)",65, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaSynchronize( PyObject * _unused)",57, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaSleep( PyObject * _unused , PyObject * cycles)",86, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaLockMutex( PyObject * module)",79, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaUnlockMutex( PyObject * module)",56, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_emptyCache( PyObject * _unused)",52, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_memoryAllocated( PyObject * _unused , PyObject * arg)",84, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_maxMemoryAllocated( PyObject * _unused , PyObject * arg)",88, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_memoryCached( PyObject * _unused , PyObject * arg)",81, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_maxMemoryCached( PyObject * _unused , PyObject * arg)",85, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"bindCudaDeviceProperties( PyObject * module)",101, 13, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_initExtension( PyObject * self)",77, 4, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_useNccl()",52, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCurrentBlasHandle_wrap( PyObject * self)",64, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_methods()",36, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"torch::cuda::initModule( PyObject * module)",36, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_initProcessGroup( PyObject * _unused , PyObject * args)",156, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_destroyProcessGroup( PyObject * _unused)",62, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_registerStream( PyObject * _unused , PyObject * _stream)",75, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_getRank( PyObject * _unused)",48, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_getNumProcesses( PyObject * _unused)",56, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_makeDescriptor( PyObject * obj)",63, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_unpackRequest( PyObject * obj)",56, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_getReduceOp( PyObject * obj)",61, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_getGroup( PyObject * obj)",78, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_clearGroupCache( PyObject * _unused , PyObject * args)",84, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_isend( PyObject * _unused , PyObject * args)",90, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_irecv( PyObject * _unused , PyObject * args)",91, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_send( PyObject * _unused , PyObject * args)",89, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_recvAnySource( PyObject * _unused , PyObject * _tensor)",79, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_recv( PyObject * _unused , PyObject * args)",90, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allReduceMultiGPU( PyObject * _unused , PyObject * args)",78, 28, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_reduceMultiGPU( PyObject * _unused , PyObject * args)",77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_broadcastMultiGPU( PyObject * _unused , PyObject * args)",78, 28, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allGatherMultiGPU( PyObject * _unused , PyObject * args)",81, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allReduce( PyObject * _unused , PyObject * args)",106, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_reduce( PyObject * _unused , PyObject * args)",86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_broadcast( PyObject * _unused , PyObject * args)",86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allGather( PyObject * _unused , PyObject * args)",77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_gatherSend( PyObject * _unused , PyObject * args)",86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_gatherRecv( PyObject * _unused , PyObject * args)",77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_scatterSend( PyObject * _unused , PyObject * args)",77, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_scatterRecv( PyObject * _unused , PyObject * args)",86, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_barrier( PyObject * _unused , PyObject * _group)",66, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_newGroup( PyObject * _unused , PyObject * args)",80, 2, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_requestIsCompleted( PyObject * _unused , PyObject * _req)",88, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_requestWait( PyObject * _unused , PyObject * _req)",81, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_initExtension( PyObject * _unused , PyObject * args)",119, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_methods()",36, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/init.cpp,"torch::distributed::c10d::c10d_init( PyObject * _unused)",81, 14, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/init.cpp,"torch::distributed::c10d::python_functions()",34, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::copyBroadcastTensorsToReplicas( const std :: vector<std::vector<at::Tensor>> & broadcastTensors , std :: vector<std::vector<at::Tensor>> & replicaData)",80, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::bucketTensors( std :: vector<at::Tensor> & tensors , int64_t bucketSize , bool fineGrained)",68, 6, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::distBroadcastCoalesced( ProcessGroup & processGroup , std :: vector<at::Tensor> & tensors , int64_t bufferSize , bool fineGrained)",81, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::syncParams( ProcessGroup & processGroup , std :: vector<std::vector<at::Tensor>> & parameterData , std :: vector<std::vector<at::Tensor>> & bufferData , const std :: vector<int64_t> & devices , int64_t broadcastBucketSize , bool broadcastBuffers)",80, 4, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::queueReduction( ProcessGroup & processGroup , std :: vector<std::vector<at::Tensor>> & gradsBatch , const std :: vector<int64_t> & devices)",76, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::syncReduction( std :: shared_ptr<ProcessGroup::Work> & reductionWork , std :: vector<at::Tensor> & gradsBatch , at :: Tensor & gradsBatchCoalesced)",78, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::device_count()",50, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::is_available()",81, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::cudnn_is_available()",66, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/jit.cpp,"torch::jit::compile( const std :: string & source)",83, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::check_is_little_endian()",58, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::flip_endianness( uint32_t value)",68, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_int32( std :: ifstream & stream)",73, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::expect_int32( std :: ifstream & stream , uint32_t expected)",79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::join_paths( std :: string head , const std :: string & tail)",68, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_images( const std :: string & root , bool train)",76, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_targets( const std :: string & root , bool train)",78, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::MNIST( const std :: string & root , Mode mode)",60, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::get( size_t index)",44, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::size() const",39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::is_train() const",40, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::images() const",38, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::targets() const",39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::BatchSize( size_t size)",51, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::size() const",42, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::operator size_t() const",46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::StreamSampler( size_t epoch_size)",77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::reset( optional<size_t> new_size)",55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::next( size_t batch_size)",63, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::save( serialize :: OutputArchive & archive) const",76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::load( serialize :: InputArchive & archive)",61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::SequentialSampler( size_t size)",67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::reset( optional<size_t> new_size)",59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::next( size_t batch_size)",76, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::save( serialize :: OutputArchive & archive) const",72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::load( serialize :: InputArchive & archive)",65, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::index() const",51, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::RandomSampler( int64_t size , Dtype index_dtype)",62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::reset( optional<size_t> new_size)",78, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::next( size_t batch_size)",79, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::save( serialize :: OutputArchive & archive) const",68, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::load( serialize :: InputArchive & archive)",61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::index() const",47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::AdamOptions::AdamOptions( double learning_rate)",47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::step()",81, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::save( serialize :: OutputArchive & archive) const",59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::load( serialize :: InputArchive & archive)",52, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/serialize.cpp,"torch::optim::detail::serialize( serialize :: OutputArchive & archive , const std :: string & key , const std :: vector<int64_t> & steps)",66, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/serialize.cpp,"torch::optim::detail::serialize( serialize :: InputArchive & archive , const std :: string & key , std :: vector<int64_t> & steps)",43, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::OptimizerBase( std :: vector<Tensor> parameters)",61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::add_parameters( const std :: vector<Tensor> & parameters)",79, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::zero_grad()",40, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::parameters() const",72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::parameters()",60, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::size() const",46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::buffer_at( std :: vector<Tensor> & buffers , size_t index)",79, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::save( serialize :: OutputArchive & archive) const",69, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::load( serialize :: InputArchive & archive)",62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::operator < <( serialize :: OutputArchive & archive , const OptimizerBase & optimizer)",39, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::operator > >( serialize :: InputArchive & archive , OptimizerBase & optimizer)",38, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSpropOptions::RMSpropOptions( double learning_rate)",53, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::step()",78, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::save( serialize :: OutputArchive & archive) const",62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::load( serialize :: InputArchive & archive)",55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGSOptions::LBFGSOptions( double learning_rate)",49, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::gather_flat_grad()",48, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::add_grad( const torch :: Tensor & step_size , const Tensor & update)",77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::step( LossClosure closure)",76, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::save( serialize :: OutputArchive & archive) const",60, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::load( serialize :: InputArchive & archive)",53, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::AdagradOptions::AdagradOptions( double learning_rate)",53, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::step()",72, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::save( serialize :: OutputArchive & archive) const",62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::load( serialize :: InputArchive & archive)",55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGDOptions::SGDOptions( double learning_rate)",80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::step()",75, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::save( serialize :: OutputArchive & archive) const",68, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::load( serialize :: InputArchive & archive)",68, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/python/init.cpp,"torch::python::bind_ordered_dict( py :: module module , const char * dict_name)",74, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/python/init.cpp,"torch::python::init_bindings( PyObject * module)",76, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::InputArchive()",58, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::read( const std :: string & key , Tensor & tensor , bool is_buffer)",77, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::read( const std :: string & key , InputArchive & archive)",73, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::load_from( const std :: string & filename , c10 :: optional<torch::Device> device)",62, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::load_from( std :: istream & stream , c10 :: optional<torch::Device> device)",62, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::OutputArchive()",58, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::write( const std :: string & key , const Tensor & tensor , bool is_buffer)",55, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::write( const std :: string & key , OutputArchive & nested_archive)",57, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::save_to( const std :: string & filename)",59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::save_to( std :: ostream & stream)",52, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::Fan::Fan( Tensor & tensor)",91, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::constant_( Tensor tensor , Scalar value)",48, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::dirac_( Tensor tensor)",68, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::eye_( Tensor matrix)",81, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::normal_( Tensor tensor , double mean , double std)",57, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::ones_( Tensor tensor)",30, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::orthogonal_( Tensor tensor , double gain)",75, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::sparse_( Tensor tensor , double sparsity , double std)",81, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::uniform_( Tensor tensor , double low , double high)",58, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::xavier_normal_( Tensor tensor , double gain)",63, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::xavier_uniform_( Tensor tensor , double gain)",63, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::zeros_( Tensor tensor)",31, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::join_name( const std :: string & name_prefix , const std :: string & name)",81, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::extend( std :: vector<Tensor> & vector , const OrderedDict<std::string,Tensor> & dict)",52, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::Module()",78, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::Module( std :: string name)",46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::name() const",81, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::clone( const optional<Device> & device) const",78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ModuleApplyFunction & function)",79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ConstModuleApplyFunction & function) const",79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const NamedModuleApplyFunction & function , const std :: string & name_prefix)",76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ConstNamedModuleApplyFunction & function , const std :: string & name_prefix) const",76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ModulePointerApplyFunction & function) const",79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const NamedModulePointerApplyFunction & function , const std :: string & name_prefix) const",57, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::parameters( bool recurse) const",80, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_parameters( bool recurse) const",80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::buffers( bool recurse) const",79, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_buffers( bool recurse) const",77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::modules( bool include_self) const",80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_modules( const std :: string & name_prefix , bool include_self) const",77, 12, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::children() const",64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_children() const",75, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::train( bool on)",34, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::eval()",23, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Device device , torch :: Dtype dtype , bool non_blocking)",79, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Dtype dtype , bool non_blocking)",57, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Device device , bool non_blocking)",59, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::is_training() const",44, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::zero_grad()",40, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::save( serialize :: OutputArchive & archive) const",69, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::load( serialize :: InputArchive & archive)",68, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::register_parameter( std :: string name , Tensor tensor , bool requires_grad)",65, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::register_buffer( std :: string name , Tensor tensor)",67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::clone_( Module & other , const optional<Device> & device)",70, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply_to_submodules( const NamedModulePointerApplyFunction & function , const std :: string & name_prefix) const",66, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::shared_from_this_checked() const",75, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator < <( serialize :: OutputArchive & archive , const std :: shared_ptr<nn::Module> & module)",64, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator > >( serialize :: InputArchive & archive , const std :: shared_ptr<nn::Module> & module)",66, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormOptions::BatchNormOptions( int64_t features)",78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::BatchNormImpl( BatchNormOptions options)",76, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::reset()",79, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::forward( const Tensor & input)",62, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::pure_forward( const Tensor & input , const Tensor & mean , const Tensor & variance)",76, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingOptions::EmbeddingOptions( int64_t count , int64_t dimension)",69, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::EmbeddingImpl( EmbeddingOptions options)",55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::reset()",69, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::forward( const Tensor & input)",54, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::ConvImpl( ConvOptions<D> options)",55, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::reset()",77, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv1dImpl::forward( const Tensor & input)",50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv2dImpl::forward( const Tensor & input)",50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv3dImpl::forward( const Tensor & input)",50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::FunctionalImpl( Function function)",50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::reset()",32, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::forward( Tensor input)",47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::operator ( )( Tensor input)",50, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearOptions::LinearOptions( int64_t in , int64_t out)",78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::LinearImpl( LinearOptions options)",67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::reset()",79, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::forward( const Tensor & input)",52, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNOptionsBase::RNNOptionsBase( int64_t input_size , int64_t hidden_size)",72, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::RNNImplBase( const RNNOptionsBase & options_ , optional<CuDNNMode> cudnn_mode , int64_t number_of_gates)",43, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::reset()",75, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Device device , torch :: Dtype dtype , bool non_blocking)",47, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Dtype dtype , bool non_blocking)",71, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Device device , bool non_blocking)",73, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::flatten_parameters()",65, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::generic_forward( std :: function<RNNFunctionSignature> function , const Tensor & input , Tensor state)",79, 8, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::flat_weights() const",77, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::any_parameters_alias() const",80, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::RNNOptions( int64_t input_size , int64_t hidden_size)",64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::tanh()",42, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::relu()",42, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::RNNImpl( const RNNOptions & options)",76, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::forward( const Tensor & input , Tensor state)",64, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::LSTMImpl::LSTMImpl( const LSTMOptions & options)",47, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::LSTMImpl::forward( const Tensor & input , Tensor state)",81, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::GRUImpl::GRUImpl( const GRUOptions & options)",44, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::GRUImpl::forward( const Tensor & input , Tensor state)",81, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::detail::DropoutImplBase<Derived>::DropoutImplBase( DropoutOptions options_)",77, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::detail::DropoutImplBase<Derived>::reset()",42, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutOptions::DropoutOptions( double rate)",61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutImpl::forward( const Tensor & input)",68, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::FeatureDropoutImpl::forward( const Tensor & input)",76, 2, 0
repos/cpp/pytorch/torch/csrc/utils/object_ptr.cpp,"THPPointer<PyObject>::free()",36, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::TupleParser( PyObject * args , int num_args)",78, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( bool & x , const std :: string & param_name)",74, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( int & x , const std :: string & param_name)",73, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( double & x , const std :: string & param_name)",76, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( std :: vector<int> & x , const std :: string & param_name)",86, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( std :: string & x , const std :: string & param_name)",81, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::next_arg()",46, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::invalid_type( const std :: string & expected , const std :: string & param_name)",115, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_list.cpp,"torch::utils::recursive_to_list( char * data , IntList sizes , IntList strides , int64_t dim , ScalarType scalarType , int64_t elementSize)",95, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_list.cpp,"torch::utils::tensor_to_list( const Tensor & tensor)",63, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::backend_to_string( const at :: Type & type)",65, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::type_to_string( const at :: Type & type)",83, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::type_from_string( const std :: string & str)",103, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::all_declared_types()",118, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_layouts.cpp,"torch::utils::initializeLayouts()",90, 2, 0
repos/cpp/pytorch/torch/csrc/utils/cuda_lazy_init.cpp,"torch::utils::cuda_lazy_init()",82, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::py_typename( PyObject * object)",44, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SimpleType::SimpleType( std :: string & name)",48, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SimpleType::is_matching( PyObject * object)",48, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::MultiType::MultiType( std :: initializer_list<std::string> accepted_types)",64, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::MultiType::is_matching( PyObject * object)",74, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::NullableType::NullableType( std :: unique_ptr<Type> type)",70, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::NullableType::is_matching( PyObject * object)",59, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::TupleType::TupleType( std :: vector<std::unique_ptr<Type>> types)",55, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::TupleType::is_matching( PyObject * object)",63, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SequenceType::SequenceType( std :: unique_ptr<Type> type)",44, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SequenceType::is_matching( PyObject * object)",61, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Argument::Argument( std :: string name , std :: unique_ptr<Type> type)",58, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( std :: vector<Argument> arguments , bool is_variadic , bool has_out)",86, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( bool is_variadic , bool has_out)",66, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( Option && other)",75, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_splitString( const std :: string & s , const std :: string & delim)",88, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_buildType( std :: string type_name , bool is_nullable)",79, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_parseOption( const std :: string & _option_str , const std :: unordered_map<std::string,PyObject*> & kwargs)",78, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_argcountMatch( const Option & option , const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",62, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_formattedArgDesc( const Option & option , const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",83, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_argDesc( const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",68, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_tryMatchKwargs( const Option & option , const std :: unordered_map<std::string,PyObject*> & kwargs)",73, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::format_invalid_args( PyObject * given_args , PyObject * given_kwargs , const std :: string & function_name , const std :: vector<std::string> & options)",96, 10, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::maybe_initialize_cuda( const Type & type)",47, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::maybe_initialize_cuda( const Device device)",50, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_zeros( const Type & type , optional<Device> device , IntList sizes)",82, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_ones( const Type & type , optional<Device> device , IntList sizes)",81, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_full( const Type & type , Scalar fill_value , optional<Device> device , IntList sizes)",100, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_sizes( const Type & type , optional<Device> device , IntList sizes)",82, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_storage( const Type & type , Storage storage)",61, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_tensor( const Type & type , const Tensor & other)",87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::compute_sizes( PyObject * seq)",100, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::infer_scalar_type( PyObject * obj)",98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::recursive_store( char * data , IntList sizes , IntList strides , int64_t dim , ScalarType scalarType , int elementSize , PyObject * obj)",87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::internal_new_from_data( const Type & type , c10 :: optional<Device> device_opt , PyObject * data , bool copy_variables , bool copy_numpy , bool type_inference)",131, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_from_data_copy( const Type & type , c10 :: optional<Device> device , PyObject * data)",83, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_new_from_sequence( const Type & type , c10 :: optional<Device> device , PyObject * data)",88, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::check_legacy_ctor_device( const Type & type , c10 :: optional<Device> device)",80, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_sparse_tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_sparse_tensor_new( const Type & type , PyObject * args , PyObject * kwargs)",101, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::typeWithDefault( PythonArgs & r , int64_t dtype_idx , int64_t device_idx , const Type & type)",102, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_tensor_new( const Type & type , PyObject * args , PyObject * kwargs)",98, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_new_from_data( const Type & type , c10 :: optional<Device> device , PyObject * data)",85, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::sparse_coo_tensor_ctor( const Type & default_type , PyObject * args , PyObject * kwargs)",149, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",110, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::as_tensor( const Type & type , PyObject * args , PyObject * kwargs)",107, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_tensor( const Type & type , PyObject * args , PyObject * kwargs)",115, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_empty( const Type & type , PyObject * args , PyObject * kwargs)",106, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_full( const Type & type , PyObject * args , PyObject * kwargs)",122, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_ones( const Type & type , PyObject * args , PyObject * kwargs)",105, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_zeros( const Type & type , PyObject * args , PyObject * kwargs)",106, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::StridedData::StridedData( const Tensor & tensor)",57, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::StridedData::step( int dim)",55, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::recursive_apply( IntList sizes , ScalarType scalarType , int64_t dim , PyObject * fn , std :: array<StridedData,N> strided_data)",85, 28, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::apply_( Tensor & self , PyObject * fn)",67, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::map_( Tensor & self , const Tensor & other_ , PyObject * fn)",74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::map2_( Tensor & self , const Tensor & x_ , const Tensor & y_ , PyObject * fn)",125, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_dtypes.cpp,"torch::utils::getDtypeNames( at :: ScalarType scalarType)",86, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_dtypes.cpp,"torch::utils::initializeDtypes()",95, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_to_numpy( const at :: Tensor & tensor)",74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_from_numpy( PyObject * obj)",74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::is_numpy_scalar( PyObject * obj)",74, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::to_numpy_shape( IntList x)",58, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::to_aten_shape( int ndim , npy_intp * values)",72, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_to_numpy( const at :: Tensor & tensor)",81, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_from_numpy( PyObject * obj)",82, 8, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::aten_to_dtype( const at :: Type & type)",80, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::numpy_dtype_to_aten( int dtype)",85, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::is_numpy_scalar( PyObject * obj)",42, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::should_allow_numbers_as_tensors( const std :: string & name)",71, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::FunctionParameter( const std :: string & fmt , bool keyword_only)",87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::check( PyObject * obj)",102, 8, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::type_name() const",65, 4, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::parse_as_integer( const std :: string & s)",78, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::parse_intlist_args( const std :: string & s , int64_t size)",102, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::set_default_str( const std :: string & str)",86, 6, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::FunctionSignature( const std :: string & fmt)",73, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::toString() const",50, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::extra_args( const FunctionSignature & signature , ssize_t nargs)",87, 4, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::missing_args( const FunctionSignature & signature , int idx)",72, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::find_param( FunctionSignature & signature , PyObject * name)",74, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::extra_kwargs( FunctionSignature & signature , PyObject * kwargs , ssize_t num_pos_args)",97, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::parse( PyObject * args , PyObject * kwargs , PyObject * dst [ ] , bool raise_exception)",83, 2, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::PythonArgParser( std :: vector<std::string> fmts , bool traceable)",80, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::raw_parse( PyObject * args , PyObject * kwargs , PyObject * parsed_args [ ])",99, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::print_error( PyObject * args , PyObject * kwargs , PyObject * parsed_args [ ])",97, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::take_tensors( TensorList tensors , size_t size_limit , bool fine_grained)",76, 6, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::reorder_tensors_like( std :: vector<Tensor> & tensors , TensorList order)",76, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::get_indices( const at :: Tensor & t)",46, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::get_values( const at :: Tensor & t)",45, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::flatten_sparse_tensors( at :: TensorList tensors)",83, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::unflatten_sparse_tensors( const at :: Tensor & flat_indices , const at :: Tensor & flat_values , at :: TensorList tensors)",85, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::PyTensorType::aten_type()",139, 6, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::unavailable_type( const PyTensorType & type)",104, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",87, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_instancecheck( PyTensorType * self , PyObject * arg)",78, 4, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_dtype( PyTensorType * self)",52, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_layout( PyTensorType * self)",53, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_is_cuda( PyTensorType * self)",47, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_is_sparse( PyTensorType * self)",53, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_initialize_metaclass( PyTypeObject & metaclass)",65, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_initialize_tensor_type( PyTypeObject & type , const char * name , PyObject * tp_dict)",97, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_module( Backend backend)",63, 4, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_name( Backend backend , ScalarType scalarType)",72, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_storage_obj( const Type & type)",88, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_type( PyTensorType & type_obj , Backend backend , ScalarType scalarType)",91, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_name( PyTensorType & type_obj , const std :: string & name)",72, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_tensor_dict()",77, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::initialize_aten_types( std :: vector<PyTensorType> & tensor_types)",77, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::initialize_python_bindings()",94, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_bind_tensor_types( const std :: vector<PyTensorType> & tensor_types)",101, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::PyTensorType_Check( PyObject * obj)",67, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_tensor_type( THPDtype * dtype , THPLayout * layout , bool is_cuda)",89, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_set_default_tensor_type( PyObject * obj)",49, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_set_default_dtype( PyObject * obj)",94, 28, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_default_tensor_type( const at :: Type & type)",91, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_default_tensor_type()",38, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::getDevice( const at :: Tensor & tensor)",66, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::noop( const Node * n)",41, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::checkImplicitTensorToNum( at :: Tensor t , bool toInt)",102, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::normalizeIndex(int64_t idx,int64_t list_size)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::getItem(TList&list,int64_t idx)",76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listAppend(const Node*node)",41, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSelect(const Node*node)",41, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listLen(const Node*node)",47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listEq(const Node*node)",64, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listNe(const Node*node)",52, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::tensor_list_equal(Shared<TensorList>a,Shared<TensorList>b)",79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listEq<Shared<TensorList>>(const Node*node)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listNe<Shared<TensorList>>(const Node*node)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listAdd(const Node*node)",73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSlice(const Node*node)",64, 8, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSetItem(const Node*node)",42, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::_check_size_factor(size_t dim,const IValue&size,const IValue&scale_factor)",86, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::_output_size(const at::Tensor&input,size_t dim,const IValue&size,const IValue&scale_factors)",122, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::interpolate(const at::Tensor&input,const IValue&size,const IValue&scale_factors,const std::string&mode,c10::optional<bool>align_corners)",105, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::interpolate_op(const Node*n)",102, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::convert_scale_factor_to_double(const IValue&int_ivalue)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_nearest_op(const Node*n)",93, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_op(const Node*n)",108, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_bilinear_op(const Node*n)",86, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::leaky_relu(const at::Tensor&tensor,double scalar)",65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::cat(const std::vector<at::Tensor>&tensors)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::getNodeStackTraceString( const Node * n)",53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::validateBlock( Block * b , onnx_torch :: OperatorExportTypes operator_export_type)",166, 12, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::validateGraph( const std :: shared_ptr<Graph> & graph , onnx_torch :: OperatorExportTypes operator_export_type)",112, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::get_model_proto()",39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeIntermediateValueInfo( onnx :: GraphProto * graph_proto , const Value * n)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ATenTypeToOnnxType( at :: ScalarType at_type)",72, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncoderBase( onnx_torch :: OperatorExportTypes operator_export_type , bool strip_doc)",95, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeValueInfo( onnx :: GraphProto * graph_proto , onnx :: ValueInfoProto * v , const Value * n)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeGraph( onnx :: GraphProto * graph_proto , const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers)",58, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeBlock( onnx :: GraphProto * graph_proto , const Block * block , const std :: vector<at::Tensor> & initializers)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::AddAttribute( onnx :: NodeProto * node_proto , const jit :: Node * node , const jit :: Symbol name)",109, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::get_raw_data_export_map()",47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::GraphEncoder( const std :: shared_ptr<Graph> & graph , int64_t onnx_opset_version , onnx_torch :: OperatorExportTypes operator_export_type , const std :: vector<at::Tensor> & initializers , bool defer_weight_export , bool strip_doc)",70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::EncodeTensor( onnx :: TensorProto * tensor_proto , const at :: Tensor & tensor , const c10 :: optional<std::string> external_ref)",123, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::ScriptModuleSerializer( const std :: string & filename)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::ScriptModuleSerializer( std :: ostream * ofs)",66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::serialize( const script :: Module & module)",76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertModel( const script :: Module & module , torch :: ModelDef * model_def)",86, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::addTensor( const at :: Tensor & tensor)",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertAndWriteTensor( size_t tensor_id , const at :: Tensor & tensor , torch :: TensorDef * tensor_proto , std :: unordered_map<const void*,std::string> & storageMap)",90, 10, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::writeTensorTable( torch :: ModelDef * model_def)",76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertModule( const script :: Module & module , const std :: string & prefix , const std :: string & name , torch :: ModuleDef * module_def)",82, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertParameter( const script :: NamedParameter & param , torch :: ParameterDef * param_def)",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::idt( size_t indent)",63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::nlidt( size_t indent)",42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TensorProto & tensor , std :: ostream & stream)",74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TensorShapeProto & shape , std :: ostream & stream)",71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TypeProto_Tensor & tensor_type , std :: ostream & stream)",77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TypeProto & type , std :: ostream & stream)",63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: ValueInfoProto & value_info , std :: ostream & stream)",74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: AttributeProto & attr , std :: ostream & stream , size_t indent)",92, 6, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: NodeProto & node , std :: ostream & stream , size_t indent)",78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: GraphProto & graph , std :: ostream & stream , size_t indent)",80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: OperatorSetIdProto & operator_set_id , std :: ostream & stream)",83, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: ModelProto & model , std :: ostream & stream , size_t indent)",84, 9, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::prettyPrint( const onnx :: ModelProto & model)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::pretty_print_onnx( const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers , int64_t onnx_opset_version , bool defer_weight_export , :: torch :: onnx :: OperatorExportTypes operator_export_type , bool google_printer)",95, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::export_onnx( const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers , int64_t onnx_opset_version , bool defer_weight_export , :: torch :: onnx :: OperatorExportTypes operator_export_type)",96, 4, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ExportModule( const script :: Module & module , std :: ostream & out)",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ExportModule( const script :: Module & module , const std :: string & filename)",79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::loadPythonClasses()",76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::runJITCPPTests()",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::initJITBindings( PyObject * module)",119, 3, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printValueRef( std :: ostream & out , const Value * n)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const std :: vector<T> & nodes)",77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printValueRefs( std :: ostream & out , const at :: ArrayRef<T> & nodes)",82, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const at :: ArrayRef<const Value*> & nodes)",89, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const at :: ArrayRef<Value*> & nodes)",83, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::const_value_list_with_types::const_value_list_with_types( ArrayRef<const Value*> values , bool use_newlines = false)",88, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const_value_list_with_types l)",78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printAttributes( std :: ostream & out , const Node * n , bool ignore_subgraph = false)",87, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::indent( std :: ostream & out , size_t level)",65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printNode( std :: ostream & out , size_t level , const Node * n , std :: vector<const Node*> * groups)",113, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const Node & n)",63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const Graph & g)",97, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::prettyPrint( std :: ostream & out)",55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::dumpPretty()",47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::checkSameDevice( const Node * node)",77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::lint() const",106, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::lint() const",83, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::dump() const",30, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::LintGraph( std :: shared_ptr<Graph> & graph)",48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::Block( Graph * graph_ , Node * node_)",60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::reIndexTopology()",58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::cloneFrom( Block * src , std :: function<Value*(Value*)> value_map)",78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::destroy()",69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::copy()",79, 8, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::mustBeNone() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::uniqueNameBase() const",87, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::setUniqueName( const std :: string & name)",89, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::copyMetadata( Value * from)",43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::replaceFirstUseWith( Value * newValue)",56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::replaceAllUsesWith( Value * newValue)",51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::findArgument( const FunctionSchema & the_schema , Symbol name)",100, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::get( Symbol name) const",53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::namedInput( Symbol name) const",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::matches( const char * signature_literal , at :: ArrayRef<Symbol> const_inputs) const",93, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::dump() const",30, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::findSchema() const",44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::maybeSchema() const",50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isNondeterministic() const",142, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::assignTopoPosition()",63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::Node( Graph * graph_ , NodeKind kind_)",45, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::eraseOutput( size_t i)",48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addBlock()",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::eraseBlock( size_t i)",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::destroy()",39, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::cloneFrom( Node * s)",45, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceAllUsesWith( Node * n)",55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertInput( size_t i , Value * value)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addInput( Value * value)",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceInput( size_t i , Value * newValue)",56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceInputWith( Value * from , Value * to)",56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addOutput()",56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertOutput( size_t i)",61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isBeforeOrAfter( const Node * n , MoveSide moveSide) const",80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isBefore( const Node * n) const",47, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isAfter( const Node * n) const",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertBefore( Node * n)",37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertAfter( Node * n)",50, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveAfterTopologicallyValid( Node * n , const AliasDb & aliasDb)",74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::couldMoveAfterTopologically( Node * n , const AliasDb & aliasDb)",74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveBeforeTopologicallyValid( Node * n , const AliasDb & aliasDb)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::couldMoveBeforeTopologically( Node * n , const AliasDb & aliasDb)",75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::WorkingSet( Node * mover , const AliasDb & aliasDb)",59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::add( Node * n)",56, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::eraseMover()",65, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::nodes()",36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::dependsOn( Node * n) const",63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::hasDataDependency( Node * n) const",42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::hasMutabilityDependency( Node * n) const",79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::producesFor( Node * n) const",81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::consumesFrom( Node * n) const",71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::getUsersSameBlock( Node * n) const",63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::getWritersSameBlock( Node * n) const",65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::findSameBlock( Node * target , Node * n)",75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::tryMove( Node * movePoint , MoveSide moveSide , const AliasDb & aliasDb , bool dryRun)",94, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::move( Node * movePoint , MoveSide moveSide)",54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveAfter( Node * n)",33, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveBefore( Node * n)",34, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeInput( size_t i)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeAllInputs()",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::findUseForInput( size_t i)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::dropInput( size_t i)",36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeFromList()",33, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::fakeRange()",94, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::insert( Symbol opname , at :: ArrayRef<NamedValue> args , at :: ArrayRef<NamedValue> kwargs , const c10 :: optional<SourceRange> & range)",47, 4, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::create( NodeKind kind , size_t num_outputs)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::create( NodeKind kind , ArrayRef<Value*> inputs , size_t num_outputs)",82, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createUndefined()",34, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNone( TypePtr typ)",62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNoneGenerator()",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createFusionGroup()",66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTuple( at :: ArrayRef<Value*> values)",65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleUnpack( Value * v)",52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleIndex( Value * tup , int64_t index)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleSlice( Value * tup , int64_t beg , int64_t end)",71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createList( const TypePtr & elem_type , at :: ArrayRef<Value*> values)",81, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createListUnpack( Value * v , size_t size)",57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNumToTensor( Value * value)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createImplicitTensorToNum( const TypePtr & type , Value * value)",76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createClone( Node * n , const std :: function<Value*(Value*)> & value_map , bool copy_blocks)",103, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::insertConstant( IValue val , c10 :: optional<SourceRange> loc , c10 :: optional<ScopePtr> scope)",87, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::toString() const",38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::~Graph()",37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeNode( Node * n)",37, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeValue( Value * v)",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeBlock( Block * b)",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::defaultAllocPythonOp( Graph * g)",97, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::allocPythonOp( Graph * g)",36, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::setAllocPythonOp( PythonOp *(*v)(Graph*g))",50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::SchemaParser( const std :: string & str)",39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDeclaration()",91, 8, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDeclarations()",52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseIdent()",54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseBaseType()",69, 8, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseAliasAnnotation()",78, 14, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseType()",73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseArgument( size_t idx , bool is_return , bool kwarg_only)",78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseSingleConstant( TypeKind kind)",114, 8, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::convertToList( TypeKind kind , const SourceRange & range , std :: vector<IValue> vs)",90, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseConstantList( TypeKind kind)",58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseTensorDefault( const SourceRange & range)",56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDefaultValue( const TypePtr & arg_type , c10 :: optional<int32_t> arg_N)",89, 8, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseList( int begin , int sep , int end , const std :: function<void()> & callback)",87, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::canonicalSchemaString( const FunctionSchema & schema)",66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::registerPendingOperators()",66, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::registerOperator( Operator && op)",70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::lookupByLiteral( const char * name)",96, 6, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::getOperators( Symbol name)",76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getRegistry()",34, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::registerOperator( Operator && op)",82, 10, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getAllOperatorsFor( Symbol name)",80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::sig( const char * signature)",52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::parseSchema( const std :: string & schema)",65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::Operator::matches( const Node * node) const",86, 4, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::findOperatorFor( const Node * node)",62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getOperatorFor( const Node * node)",61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorSet::OperatorSet( std :: initializer_list<const char*> sig_literals)",77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorSet::find( const Node * n) const",52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::createTripCountConjunctiveCondition( Graph * g , Value * cur_trip_count , Value * max_trip_count , Value * cond)",78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::desugarTripCounts( Block * b)",81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::flattenIO( Graph & graph)",67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::dropUnused( Block * b)",68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::findLastUses( Graph & g)",87, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::PreprocessGraph::PreprocessGraph( Graph & g)",66, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::ContainerTensor()",97, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::sizes() const",60, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::strides() const",62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::dim() const",58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::storage() const",62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::relativeJump( int from_inst , int to_inst)",47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::CodeImpl( const std :: shared_ptr<Graph> & graph_)",49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJumpFalse( int from_inst , int to_inst)",54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJumpTrue( int from_inst , int to_inst)",54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJump( int from_inst , int to_inst)",54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertNodesFromBlock( Block * block)",121, 10, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertInstruction( Node * n)",112, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertInstruction( Symbol sym , std :: shared_ptr<SourceLocation> debug_location , ArrayRef<Value*> inputs , ArrayRef<uint8_t> move_flags , ArrayRef<Value*> outputs)",75, 27, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::moveFlags( Node * n)",42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::moveFlags( Block * b)",42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertAssign( std :: shared_ptr<SourceLocation> debug_location , ArrayRef<Value*> inputs , ArrayRef<uint8_t> move_flags , ArrayRef<Value*> outputs)",153, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::get( const ListHandle<int> & list , int i) const",56, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::get( const ListHandle<bool> & list , int i) const",57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listBegin( ListHandle<int> & list)",43, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listInsert( ListHandle<int> & list , int value)",97, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listBegin( ListHandle<bool> & list)",44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listInsert( ListHandle<bool> & list , int value)",98, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::aliasRegistersTo( ArrayRef<Value*> new_allocations , ArrayRef<Value*> existing_allocations)",99, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::getOrAllocateRegister( Value * n , bool required = false)",64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::grad_executors()",71, 8, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::dumpInstruction( std :: ostream & out , size_t pc) const",63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::dump( std :: ostream & out) const",54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::InterpreterStateImpl( const Code & code)",42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::intrusive_from_this()",68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::runImpl( Stack & stack)",107, 10, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::getOrCreateFuture()",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::runAsync( Stack & stack)",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::run( Stack & stack)",57, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::get( const ListHandle<int> & list , int i)",49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::get( const ListHandle<bool> & list , int i)",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::loadTensorsFromRegisters( const UseList & uses , Stack & stack)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::operator < <( std :: ostream & out , const Code & code)",67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::Code::Code( const std :: shared_ptr<Graph> & graph)",48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::Code::grad_executors()",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::InterpreterState( const Code & code)",62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::run( Stack & stack)",63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::runAsync( Stack & stack)",75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::getFuture()",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::InterpreterState( c10 :: intrusive_ptr<c10::intrusive_ptr_target> pImpl_)",89, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_interpreter.cpp,"torch::jit::createPythonOperation( const Node * op_)",126, 8, 0
repos/cpp/pytorch/torch/csrc/jit/hooks_for_testing.cpp,"torch::jit::didFinishEmitModule( std :: shared_ptr<script::Module> module)",77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/hooks_for_testing.cpp,"torch::jit::setEmitModuleHook( std :: function<void(std::shared_ptr<script::Module>module)> cb)",99, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::wrapDim( int64_t & dim , const std :: vector<int64_t> & sizes)",66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::isDifferentiable( Node * n)",188, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::isDifferentiable( Graph & g)",69, 21, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::gradientForNode( Node * node , ArrayRef<Value*> grad_values)",216, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::linearGradientForNode( Node * node , ArrayRef<Value*> grad_values)",93, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::ReverseDetails::ReverseDetails( value_map && grad_map , Block * reverse_block)",62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::createAutogradAdd( Value * a , Value * b)",80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::addReverseInline( Gradient & grad_desc)",101, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::getReverseCaptures( Gradient & grad_desc)",72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::liftConstants( Gradient & grad_desc , ReverseDetails & rev_info)",75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::deduplicateSizeCaptures( Gradient & grad_desc , ReverseDetails & rev_info)",104, 23, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::eliminateDeadCode( ReverseDetails & rev_info)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::Optimize( Gradient & grad_desc , ReverseDetails & rev_info)",88, 2, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::lambdaLiftReverse( Gradient & grad_desc , ReverseDetails & rev_info)",97, 6, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::differentiate( std :: shared_ptr<Graph> & graph)",81, 14, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::getPythonName( const PyObject * obj_)",68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::printPyObject( std :: ostream & out , const THPObjectPtr & obj)",75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::ConcretePythonOp( Graph * graph)",33, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::name() const",44, 3, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::cloneFrom( Node * other_)",51, 3, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::allocNewInstance( Graph * g)",47, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::autogradFunction() const",69, 3, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::writeScalars( std :: ostream & out) const",55, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::pythonAllocPythonOp( Graph * g)",42, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::initPythonIRBindings( PyObject * module_)",112, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::ExecutionPlan( std :: shared_ptr<Graph> graph)",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::run( Stack & stack) const",46, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::operator bool() const",37, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::getDebugState()",39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::DifferentiableGraphBackward( GraphExecutor executor , size_t capture_size)",75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::apply( variable_list && inputs)",91, 10, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::capture( const IValue & val , bool is_output)",70, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::DifferentiableGraphOp( Gradient grad)",55, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::operator ( )( Stack & stack) const",98, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::detachVariables( Stack & stack) const",86, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::captureInputs( DifferentiableGraphBackward & grad_fn , at :: ArrayRef<IValue> inputs) const",97, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::captureOutputs( DifferentiableGraphBackward & grad_fn , at :: ArrayRef<IValue> outputs) const",99, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::packGradient( Gradient gradient , Node * dnode)",97, 7, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::getGradient( const Node * n)",89, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::detail::getGradExecutor( Operation & op)",59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::prepareGraph( std :: shared_ptr<Graph> & graph)",78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::countFlatInputs( const TypePtr & ptr)",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::countFlatInputs( const std :: shared_ptr<Graph> & graph)",71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::hasMutableOperators( Block * block)",58, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::GraphExecutorImpl( std :: shared_ptr<Graph> graph , bool optimize)",77, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::run( Stack & stack)",107, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::graphFor( const Stack & stack) const",82, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getDebugState()",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::debugDisableAutodiffSubgraphInlining()",63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getOrCompileFallback()",53, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getOrCompile( const Stack & stack)",99, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::compileSpec( const ArgumentSpec & spec)",91, 6, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runOptimization( std :: shared_ptr<Graph> & graph , const ArgumentSpec & spec)",82, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runNondiffOptimization( std :: shared_ptr<Graph> & graph)",63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::needsGradient( const std :: shared_ptr<const Graph> & graph)",73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::mayIntroduceGradient( const Block * b)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runTraced( Stack & stack)",94, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::GraphExecutor( std :: shared_ptr<Graph> graph , bool optimize)",74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::run( Stack & inputs)",42, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::graph() const",54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::graphFor( const Stack & inputs) const",76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::getDebugState()",52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::debugDisableAutodiffSubgraphInlining()",61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::runRequiredPasses( const std :: shared_ptr<Graph> & g)",69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( const std :: string & filename)",80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( std :: istream * is)",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::deserialize( ModuleLookup module_lookup , c10 :: optional<at::Device> device)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::loadTensorTable( torch :: ModelDef * model_def)",77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::loadTensor( const torch :: TensorDef & tensor_proto , std :: unordered_map<std::string,at::Storage> & storageMap)",94, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::convertModule( const torch :: ModuleDef & module_def)",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( ModuleLookup module_lookup , std :: istream & in , c10 :: optional<at::Device> device)",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( ModuleLookup module_lookup , const std :: string & filename , c10 :: optional<at::Device> device)",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( std :: istream & in , c10 :: optional<at::Device> device)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( const std :: string & filename , c10 :: optional<at::Device> device)",66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::cast_handle_sequence( std :: vector<py::handle> objs)",64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::flatten_rec( PyObject * obj , ParsedArgs & args)",92, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::flatten( py :: handle obj)",61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::cast_sequence( std :: vector<py::object> objs)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::unflatten_rec( ArrayRef<Variable> :: iterator & var_it , ArrayRef<Variable> :: iterator & var_it_end , std :: string :: const_iterator & desc_it)",75, 6, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::unflatten( ArrayRef<Variable> vars , const IODescriptor & desc)",73, 0, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::push( Symbol name)",66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::getRoot()",44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::getDepth()",44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::namesFromRoot( const std :: string & separator) const",73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::detail::genericAddInput( Node * n , T value)",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::detail::badArgType( const T & v)",64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , int64_t value)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , bool value)",108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , double value)",108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: Scalar & value)",108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const c10 :: optional<at::Scalar> & value)",86, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const std :: string & value)",108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: Tensor & value)",108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: SparseTensorRef & value)",108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Generator * value)",79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Device value)",63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Layout value)",63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: ScalarType value)",67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: TensorList value)",98, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: TensorOptions & options)",93, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: IntList value)",82, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const ArrayRef<double> & value)",76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addOutput( Node * node , const at :: Tensor & output)",61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addOutput( Node * node , const std :: vector<at::Tensor> & outputs)",100, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getTracingState()",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setTracingState( std :: shared_ptr<TracingState> state)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::TracingState::TracingState()",29, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getSizeOf( const autograd :: Variable & var , int64_t dim)",88, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ArgumentStash::stashIntListElem( const std :: string & arg_name , size_t size , size_t idx , const Variable & var)",114, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ArgumentStash::stashValue( const std :: string & arg_name , size_t idx , const Variable & var , const TypePtr & type)",116, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::defaultRecordSourceLocation( Node * n)",45, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::recordSourceLocation( Node * n)",43, 2, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setRecordSourceLocation( void(*v)(Node*))",49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::defaultWarn( const std :: string & str)",43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::_do_warn( const char * _reason , const char * _kind)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setWarn( warn_fn_type fn)",32, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::tensorEqual( const at :: Tensor & lhs , const at :: Tensor & rhs)",65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::tensorListEqual( const std :: vector<at::Tensor> & lhs , const std :: vector<at::Tensor> & rhs)",95, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::attributesEqualCSE( const Node * lhs , const Node * rhs)",74, 8, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::HashNode::operator ( )( const Node * k) const",88, 18, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::EqualNode::operator ( )( const Node * lhs , const Node * rhs) const",91, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::getPythonInterpreterStackTrace()",78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::createGraphByTracing( const py :: function & func , Stack trace_inputs , const py :: function & var_name_lookup_fn , bool force_outplace , const c10 :: optional<size_t> & num_real_inputs)",101, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::preRecordPythonTrace( THPObjectPtr pyobj , const std :: string & arg_types , at :: ArrayRef<Variable> inputs , pyobj_list scalar_args)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::pythonRecordSourceLocation( Node * n)",86, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::pythonWarn( const std :: string & reason)",75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::initPythonTracerBindings( PyObject * module)",96, 2, 0
repos/cpp/pytorch/torch/csrc/jit/constants.cpp,"torch::jit::insertConstant( Graph & g , const IValue & val , c10 :: optional<SourceRange> loc , c10 :: optional<ScopePtr> scope)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/constants.cpp,"torch::jit::toIValue(const Value*v)",70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::ModuleAccessorValue( std :: shared_ptr<script::Module> module)",62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",118, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::OpsValue( size_t version)",27, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",118, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::ConstantValue( IValue value)",32, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::kind() const",59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::asValue( const SourceRange & loc , script :: Method & m)",73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::ConstantTableValue( ArrayRef<at::Tensor> constants)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",118, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::parseVersionNumber( script :: Lexer & L)",101, 4, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::import_methods( const std :: shared_ptr<script::Module> & mod , const std :: string & src , const std :: vector<at::Tensor> & constant_table)",137, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( at :: Tensor data , at :: Tensor mask , at :: Tensor dims)",83, 6, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( const at :: Tensor & data , int64_t batch_size)",70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( const std :: vector<at::Tensor> & datalist , at :: Tensor dims)",85, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::examples()",68, 8, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::initBatchTensorBindings( PyObject * module)",60, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::removeTupleNodes( Node * n , bool must_remove_tuples)",77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::VisitNode( Node * n , Node * insert_point)",110, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerAllTuples( Block * block)",82, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::EnsureNoTuples( ArrayRef<Value*> values)",58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::EnsureNoTuples( Block * block)",43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerAllTuples( std :: shared_ptr<Graph> & graph)",53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerSimpleTuples( Block * block)",54, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerSimpleTuples( std :: shared_ptr<Graph> & graph)",56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::ToONNX( std :: shared_ptr<Graph> & graph , :: torch :: onnx :: OperatorExportTypes operator_export_type)",120, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::BlockToONNX( Block * old_block , Block * new_block , :: torch :: onnx :: OperatorExportTypes operator_export_type , std :: unordered_map<Value*,Value*> env)",152, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimizeImpl( Block * block , bool addmm_fusion_enabled)",141, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimize( Block * block , bool addmm_fusion_enabled)",72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimize( const std :: shared_ptr<Graph> & graph , bool addmm_fusion_enabled)",88, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_grad_of.cpp,"torch::jit::LowerGradOf( Graph & g)",73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::isSimpleMap( Node * node)",77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::broadcastSizes( at :: ArrayRef<Value*> sizes)",86, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::GraphFuser( Block * block , std :: shared_ptr<Graph> graph)",57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tensorInputs( Node * node)",57, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusable( Node * node)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableCatNode( Node * node)",103, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableAsExitNode( Node * node)",61, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableOnlyAsExitNode( Node * node)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::calculatesSize( Node * node)",62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::allUsersAreThisConsumerOrCalcSizes( Node * consumer , Value * producer)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mustRemainAsFusionGroupOutput( Value * producer)",70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::getSubgraph( Node * n)",48, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mergeFusionGroups( Node * consumer_group , Node * producer_group)",90, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mergeNodeIntoGroup( Node * group , Node * n)",137, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::createSingletonFusionGroup( Node * n)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::insertAt( Node ** insertion_point , Node * n)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tryFuse( Node * consumer , Value * producer , const AliasDb & aliasDb)",105, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::canFuseChunk( Node * consumer , Value * producer)",70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::findFusedChunk( Node * group , Value * input)",80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseChunkByReusingExistingFusedChunk( Node * group , Node * chunk , Node * existingFusedChunk)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseChunk( Node * consumer , Value * producer)",82, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::sortReverseTopological( ArrayRef<Value*> inputs)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::scanNodeForChunks( Node * consumer)",65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::insertExplicitBroadcast( Node * node)",93, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::promoteChunkToBroadcastingChunk( Node * chunk)",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tryToMoveChunk( Node * consumer , Value * producer)",105, 34, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::scanNode( Node * consumer , const AliasDb & aliasDb)",80, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::replaceIntermediateBroadcastingChunks()",94, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::usedOnlyInSize( Value * v)",106, 23, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::buildShapeExpressions( Node * fusion_group)",108, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::removeOutputsUsedOnlyInSize( Node * fusion_group)",78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::run()",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::PeepholeOptimizeShapeExpressions( Block * block)",87, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::FuseGraph( std :: shared_ptr<Graph> & graph)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::isValidArgumentForRunning( Value * v)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::isValidReturnForRunning( Value * v)",55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::ShapePropagator( std :: shared_ptr<Graph> graph)",57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnBlock( Block * block , bool insert_expands = true)",80, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::setUnshapedType( Node * node)",43, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::wrapDim( int64_t dim , at :: IntList sizes)",52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::jitDeviceIndexToDevice( int device)",68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::representativeValue( Value * v)",78, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::gatherTensorTypes( Node * node)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::mergeTypes( ArrayRef<Value*> lhs , ArrayRef<Value*> rhs , ArrayRef<Value*> outputs)",74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::broadcastBinary( Node * node , std :: vector<CompleteTensorTypePtr> & types , size_t idx1 , size_t idx2)",78, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::dependsOnMutation( Node * node)",81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::canPropagateShapeByRunningIt( Node * node)",59, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnNodeByRunningIt( Node * node)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateCatShape( Node * cat_node)",81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnNode( Node * node , bool insert_expands = true)",85, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::determineListSize( Value * list)",64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateTensorShapeOnNode( Node * node , bool insert_expands)",191, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateCompleteShapeOnNode( Node * node , bool insert_expands , std :: vector<CompleteTensorTypePtr> tensor_types)",83, 12, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::PropagateInputShapes( const std :: shared_ptr<Graph> & graph)",65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( at :: ArrayRef<Value*> vals)",56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( Block * b)",41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( const std :: shared_ptr<Graph> & graph)",66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::ChunkOutput::ChunkOutput( Value * v , size_t o)",35, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::getChunkOutputs( Node * chunk)",112, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::CanonicalizeOps( Block * block)",114, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::CanonicalizeOps( const std :: shared_ptr<Graph> & graph)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::canRunWithAutograd( Node * node)",44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::scanNode( Node * node , size_t threshold)",73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::InlineAutodiffSubgraphs( Block * block , size_t threshold)",72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::InlineAutodiffSubgraphs( std :: shared_ptr<Graph> & graph , size_t threshold)",80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::runNode( Node * n)",60, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::propagateNode( Node * n)",69, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::inlineIf( Block * body , Node * n)",82, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::isTrueConstant( Value * val)",60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::inlineIf( Node * n)",36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::removeExtraNodeOutputs( Node * n)",72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( Node * n , const AliasDb & aliasDb , bool recurse)",74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( Block * block , const AliasDb & aliasDb , bool recurse)",79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( std :: shared_ptr<Graph> & graph)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::isInplaceOp( const Node * node)",55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::RemoveInplaceOps( Block * block)",74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::RemoveInplaceOps( const std :: shared_ptr<Graph> & graph)",61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/erase_number_types.cpp,"torch::jit::EraseNumberTypesOnBlock( Block * block)",80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/erase_number_types.cpp,"torch::jit::EraseNumberTypes( const std :: shared_ptr<Graph> & graph)",61, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize.cpp,"torch::jit::Canonicalize( const std :: shared_ptr<Graph> & graph , bool keep_unique_names)",92, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_pooling.cpp,"torch::jit::ConstantPooling( Block * block , std :: unordered_set<Node*,HashNode,EqualNode> & constants)",97, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_pooling.cpp,"torch::jit::ConstantPooling( const std :: shared_ptr<Graph> & graph)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isTrueConstant( Value * val)",60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isForLoop( Node * node)",70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::limitedBlockSize( Block * body , int64_t limit)",55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isSmallBlock( Block * body)",67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::inlineBody( Node * loop)",87, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::repeatBody( Block * body , int64_t times)",88, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::replaceLoopCounter( Node * loop)",82, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::unroll( Node * loop)",138, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::UnrollLoops( Block * block)",92, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::UnrollLoops( std :: shared_ptr<Graph> & graph)",50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::getRequiresGrad( Value * value)",38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( Value * value , bool req_value)",55, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( at :: ArrayRef<Value*> outputs , const std :: vector<bool> & values)",86, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( Node * node , const std :: vector<bool> & values)",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::bitwiseOr( std :: vector<bool> a , const std :: vector<bool> & b)",79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGradSimpleNode( Node * node)",89, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( Node * node)",92, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( Block * block)",44, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( std :: shared_ptr<Graph> & graph)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::have_same_shape( at :: TensorList inputs)",61, 21, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::shape_is_fast_for_reduce( const at :: Tensor & lhs , const at :: Tensor & rhs)",78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::mm( Node * mm)",34, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::transpose( Node * t , TreeToken & inp_token)",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::add( Node * add , TreeToken & l , TreeToken & r)",89, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::operator bool()",29, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::removeTransposesAndGatherMatmuls()",98, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMMTreeReduce( Block * block)",99, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::shape_is_fast_for_side( const at :: Tensor & other_side_input)",66, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::gatherIndependentMMUses( Value * value , const AliasDb & alias_db)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMMSide( Block * block , const AliasDb & alias_db)",81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::hasMutableOperators( Block * block)",57, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMM( std :: shared_ptr<Graph> & graph)",87, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inplace_check.cpp,"torch::jit::CheckInplace( Block * block)",79, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inplace_check.cpp,"torch::jit::CheckInplace( std :: shared_ptr<Graph> & graph)",51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isPrint( char s)",31, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::printQuotedString( std :: ostream & stmt , const std :: string & str)",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isValidIdentifierChar( char c , size_t pos)",75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isValidIdentifier( const std :: string & name)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::QualifiedName( QualifiedNamePtr prefix , std :: string name)",59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::create( QualifiedNamePtr prefix , std :: string name)",83, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::create( std :: string name)",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::str() const",28, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::emit( std :: ostream & out) const",39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::createTensorToParameterNameMap( const script :: Module & module , const QualifiedNamePtr & prefix , std :: unordered_map<at::Tensor*,QualifiedNamePtr> & result)",75, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::isConstantLike( Node * n)",33, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::canInline( Value * v)",105, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanValue( Node * block_point , Value * v)",116, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::previousNonConstant( Node * n)",39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanNode( Node * n)",74, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanBlock( Block * b)",44, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::getOrAddTensorConstant( at :: Tensor t)",78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::buildConstantList( Node * n , std :: vector<Node*> & constants)",87, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::buildConstantList( Block * b , std :: vector<Node*> & constants)",68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genNameImpl( const std :: string & candidate , std :: unordered_set<std::string> & used)",97, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genName( const std :: string & candidate)",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genMethodName( const std :: string & candidate)",60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::makeValidIdentifier( const std :: string & candidate)",73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genUniqueNameFor( Value * v)",74, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::useOf( Value * v) const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValue( Value * v , const std :: string & s)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValue( Value * v , Value * w)",41, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValuesToTheirUniqueNames( at :: ArrayRef<Value*> values)",69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::indent()",41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::WithIndented()",33, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::zipWith( at :: ArrayRef<T0> list_a , at :: ArrayRef<T1> list_b , F action) const",64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printValueList( std :: ostream & stmt , at :: ArrayRef<Value*> list , const char * begin = "" , const char * end = "")",117, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printAssignment( at :: ArrayRef<Value*> lhs , at :: ArrayRef<Value*> rhs)",34, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printIf( IfView stmt)",63, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::shouldEmitAsForLoop( LoopView stmt)",116, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printLoop( LoopView stmt)",87, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printNode( Node * node , bool print_const)",82, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMaybeAnnotatedConstantList( std :: ostream & stmt , const char * the_type , size_t list_size , const IValue & the_list)",56, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printConstant( std :: ostream & stmt , const IValue & v)",85, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printRHS( std :: ostream & stmt , Node * node)",95, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printBlock( Block * root , bool block_has_other_statements)",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printDefaultValue( std :: ostream & stmt , const IValue & value)",91, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printFunctionDefinition( Graph & graph , const std :: string & name , const std :: vector<c10::optional<IValue>> & defaults = { } , const std :: vector<std::string> & param_names = { })",92, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::PythonPrintPass( std :: ostream & out_ , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",91, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::resultType( const Graph & graph)",77, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printFunction( Graph & graph , const std :: string & name , const std :: vector<c10::optional<IValue>> & defaults = { } , const std :: vector<std::string> & param_names = { })",65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMethod( script :: Method & method)",101, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMethod( script :: Method & method , const std :: unordered_map<at::Tensor*,QualifiedNamePtr> & parameter_names)",83, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printModule( script :: Module & module)",93, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const Graph & graph , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",132, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const script :: Method & method , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",142, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const script :: Module & module , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",142, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::printerHasSpecialCaseFor( Symbol sym)",73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_expands.cpp,"torch::jit::RemoveExpands( Block * block)",80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_expands.cpp,"torch::jit::RemoveExpands( const std :: shared_ptr<Graph> & graph)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::getBatchOperator( const std :: string & name , int64_t num_inputs)",139, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitAten( Node * n , Block * block , Block * res_block)",146, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitConstant( Node * n , Block * block , Block * res_block)",70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitNumToTensor( Node * n , Block * block , Block * res_block)",132, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitTensorToNum( Node * n , Block * block , Block * res_block)",73, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitListConstruct( Node * n , Block * block , Block * res_block)",94, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitIf( Node * n , Block * block , Block * res_block)",119, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitLoop( Node * n , Block * block , Block * res_block)",164, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::toBatch( Block * block , Block * res_block)",141, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::to_batch_graph( std :: shared_ptr<Graph> & graph)",70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::initRegisterBatchOpsBindings( PyObject * module)",87, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/specialize_undef.cpp,"torch::jit::specializeUndef( Graph & g)",89, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::shouldAnnotate( const TypePtr & type)",70, 7, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::shouldAnnotate( const Value * v)",38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::AliasDb( std :: shared_ptr<Graph> graph)",76, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::buildWildcardIndex( const Block * b)",51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWildcard( const Node * n) const",49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWildcardImpl( const Node * n) const",53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesTo( Node * n , const Value * v) const",73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWrites( Node * n) const",43, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesToInputAlias( Node * n) const",75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWriters( const Node * n) const",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getAliases( const Value * v) const",77, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWrites( Node * n) const",69, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::dump() const",69, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( const std :: shared_ptr<Graph> & graph)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( Block * block)",38, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( Node * node)",79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeIf( Node * node)",71, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeLoop( Node * node)",76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeSubgraph( Node * node)",78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeCreator( Node * node)",43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeExtractor( Node * node)",51, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeChunk( Node * node)",48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeBroadcastingChunk( Node * node)",80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getFreshAlias( bool isGraphInput)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::addAlias( const Value * value , AliasInfo alias)",62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::addAlias( const Value * value , Symbol alias)",59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::addAlias( const Value * value , const Value * from)",64, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::mapAliases( at :: ArrayRef<Value*> to , at :: ArrayRef<Value*> from)",79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::giveFreshAlias( const Value * value)",80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/common_subexpression_elimination.cpp,"torch::jit::EliminateCommonSubexpression( Block * block , const AliasDb & aliasDb , std :: function<Node*(Node*)> parent_lookup_fn)",78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/common_subexpression_elimination.cpp,"torch::jit::EliminateCommonSubexpression( std :: shared_ptr<Graph> & graph)",67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::SubgraphSlicer( Block * block , std :: shared_ptr<Graph> graph , size_t minSubgraphSize)",45, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::run( std :: vector<Node*> & diffGraphs)",81, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::inlineIfTooSmall( Node * n)",77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::sortReverseTopological( ArrayRef<Value*> inputs)",70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::shouldConsiderForMerge( Node * node)",53, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::scanNode( Node * consumer , const AliasDb & aliasDb)",78, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::tryMerge( Node * consumer , Node * producer , const AliasDb & aliasDb)",67, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::CreateAutodiffSubgraphs( const std :: shared_ptr<Graph> & graph , size_t threshold)",68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::DeadCodeEliminator( std :: shared_ptr<Graph> graph)",60, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::run( Block * block , bool recurse)",81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::setDeleteCallback( std :: function<void(const std::unordered_set<const Value*>&)> deleteCallback)",67, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::setLastWildcard()",58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::markReturnNode( Node * node)",81, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::mark( Block * block)",81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::markIfLive( Node * node)",59, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::mark( Node * node)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::sweep( Block * block , bool recurse)",74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::hasUntrackedMutation( Node * node)",79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::hasSideEffects( Node * node)",79, 26, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::removeDeadBlockOutputs( Node * node)",68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::removeDeadLoopOutputs( Node * node)",78, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( const std :: shared_ptr<Graph> & graph)",67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( Block * block , bool recurse)",53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( Block * block , std :: function<void(const std::unordered_set<const Value*>&)> cb)",71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isRNN( const Node * node)",62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isNopTranspose( const std :: vector<int64_t> & perm)",67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::composeTransposes( const std :: vector<int64_t> & t1 , const std :: vector<int64_t> & t2)",74, 39, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::getBroadcastPositions( Node * node)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fusibleExpandTo( at :: IntList from , at :: IntList to)",74, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseBroadcast( Block * b)",77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseConsecutiveTransposes( Block * b)",99, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::eliminateNopTranspose( Block * b)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseTransposeIntoGemm( Block * b)",104, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::pushPackingPastRnn( Block * b)",93, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::removeNopPacking( Block * graph)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::hackFixupPadPackedShapes( Block * graph)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultRNNState( Graph * graph , Node * n , int input_index)",151, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultRnnHiddenState( Block * b)",69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultLstmCellState( Block * b)",69, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isSafeToSpeculate( Node * n)",41, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::speculateOps( Block * block)",80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::replaceInputWithList( Node * node , size_t i , ArrayRef<Value*> to)",78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::eraseListConstruct( Block * block)",92, 10, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::PeepholeOptimizeONNX( std :: shared_ptr<Graph> & graph)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,"torch::jit::FixupONNXLoops( Block * block)",50, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,"torch::jit::FixupONNXLoops( std :: shared_ptr<Graph> & graph)",53, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,"torch::jit::PrepareDivisionForONNXOnBlock( Block * block)",95, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,"torch::jit::PrepareDivisionForONNX( const std :: shared_ptr<Graph> & graph)",67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepCopy( const IValue & self)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepCopy( const Stack & stack)",37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepEquals( const IValue & lhs , const IValue & rhs)",71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::AliasAndIValue::AliasAndIValue( c10 :: optional<at::AliasInfo> aliasInfo , IValue iValue)",70, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkInputPreconditions( const Stack & inputs)",52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkAliases( const std :: vector<AliasAndIValue> & inputs , const std :: vector<AliasAndIValue> & outputs)",80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkWrites( const std :: vector<AliasAndIValue> & inputs , const std :: vector<IValue> & deepCopiedInputs)",61, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::findNodeForOp( const Graph & g , const std :: string & unqualifiedOpName)",76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::toIValueProp( const Value * v)",78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkAliasAnnotation( const std :: shared_ptr<Graph> & graph , std :: vector<IValue> pythonInputs , const std :: string & unqualifiedOpName)",80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::isSubgraphNodeKind( Symbol s)",67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::isSubgraphNodeKind( Node * n)",40, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::mergeSubgraph( Node * mergeTo , Node * mergeFrom)",61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::getSubgraph( Node * n)",46, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::unmergeSubgraph( Node * subgraphNode)",72, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::mergeNodeIntoSubgraph( Node * toMerge , Node * subgraphNode)",79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::createSingletonSubgraph( Node * n , Symbol subgraphKind)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::typeString( py :: handle h)",49, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::toSimple( Value * v)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::PythonValue( py :: object self)",31, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::getSchema( const size_t n_args , const size_t n_binders)",101, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs_ , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",173, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::kind() const",63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::getattr( const SourceRange & loc , const std :: string & name)",72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonModuleValue::PythonModuleValue( py :: object mod)",78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonModuleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",75, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::ConstantPythonTupleValue( py :: object tup)",85, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",62, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::asValue( const SourceRange & loc , Method & m)",55, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::ModuleValue( std :: shared_ptr<Module> module)",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",177, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",177, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",93, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::BooleanDispatchValue( py :: dict dispatched_fn)",52, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::removeIndex( at :: ArrayRef<NamedValue> arr , size_t index)",42, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",79, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::toSugaredValue( py :: object obj , Method & m , SourceRange loc , bool is_constant , bool is_submodule)",88, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::unpackVariableTensorList( std :: vector<at::Tensor> outputs)",71, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::gatherParametersAndBuffers( std :: vector<at::Tensor*> & values , const Module & m)",94, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::pythonResolver( const ResolutionCallback & rcb)",75, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::getSchemaWithNameAndDefaults( const SourceRange & range , const FunctionSchema & schema , const at :: optional<std::string> & new_name , const FunctionDefaults & default_args)",89, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::initJitScriptBindings( PyObject * module)",132, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::SharedParserData::isUnary( int kind , int * prec)",54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::SharedParserData::isBinary( int kind , int * prec)",55, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::stringToKind( const std :: string & str)",62, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::kindToString( int kind)",73, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::sharedParserData()",70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::getAllBuiltinFunctionsFor( Symbol name)",82, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::loadSource( const std :: string & source)",83, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::loadBuiltinFunctions()",58, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::getAllBuiltinFunctionsFor( Symbol name)",79, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::mergeTypesFromTypeComment( const Decl & decl , const Decl & type_annotation_decl , bool is_method)",120, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::ParserImpl( const std :: string & str)",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseIdent()",71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::createApply( const Expr & expr)",66, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::followsTuple( int kind)",39, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExpOrExpTuple()",61, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseBaseExp()",76, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAssignmentOp()",46, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseTrinary( TreeRef true_branch , const SourceRange & range , int binary_prec)",81, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExp()",42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExp( int precedence)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseList( int begin , int sep , int end , T(ParserImpl::*parse)())",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseConst()",45, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseConcatenatedStringLiterals()",64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAttributeValue()",31, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseOperatorArguments( TreeList & inputs , TreeList & attributes)",83, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSubscriptExp()",106, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSubscript( const TreeRef & value)",85, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseParam()",85, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseBareTypeAnnotation()",114, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseTypeComment()",87, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAssign( const Expr & lhs)",73, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseStmt()",87, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseOptionalIdentList()",64, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseIf( bool expect_if = true)",89, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseWhile()",59, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseFor()",82, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseStatements( bool expect_indent = true)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseDecl()",94, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseFunction( bool is_method)",80, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::lexer()",19, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::c( int kind , const SourceRange & range , TreeList && trees)",68, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::makeList( const SourceRange & range , TreeList && trees)",65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::Parser( const std :: string & src)",39, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseFunction( bool is_method)",48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::lexer()",25, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseTypeComment()",36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::SourceRangeFactory::SourceRangeFactory( std :: string source)",69, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::SourceRangeFactory::create( int line , int start_col , int end_col)",84, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::wrap_list( const SourceRange & fallback_pos , std :: vector<T> && vec)",75, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::wrap_maybe( const SourceRange & fallback_pos , T * val)",86, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::initTreeViewBindings( PyObject * module)",130, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::placeholderCreator( Method &)",36, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::try_emit_call_to( Graph & graph , const SourceRange & loc , Method & callee , c10 :: optional<NamedValue> self , ArrayRef<NamedValue> args , ArrayRef<NamedValue> kwargs , std :: stringstream & failure_messages , Method * caller , bool conv_tensors_to_nums)",116, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Method::emit_call_to( const SourceRange & loc , Method & callee , ArrayRef<NamedValue> args , ArrayRef<NamedValue> kwargs)",140, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Method::ensure_defined()",41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: Device device , at :: ScalarType dtype , bool non_blocking)",78, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: ScalarType dtype , bool non_blocking)",59, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: Device device , bool non_blocking)",57, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::save( std :: ostream & out)",39, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::save( const std :: string & filename)",49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to_impl( const c10 :: optional<at::Device> & device , const c10 :: optional<at::ScalarType> & dtype , bool non_blocking)",69, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::NoneValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::GetAttrValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::PrintValue::kind() const",38, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::PrintValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",103, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::CastValue::CastValue( TypePtr type , c10 :: Symbol method)",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::CastValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::asSimple( const SugaredValuePtr & value)",66, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::meaningfulName( const std :: string & name)",54, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::Environment( Method & method , Resolver resolver , Block * b , std :: shared_ptr<Environment> next = nullptr)",105, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setVariableTypeError( const std :: string & name , const std :: string & msg)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findVariableTypeError( const std :: string & name)",78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInThisFrame( const std :: string & name)",61, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInParentFrame( const std :: string & name)",63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInAnyFrame( const std :: string & name)",68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getValueInThisFrame( const SourceRange & loc , const std :: string & name)",80, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::createCapturedInput( Value * orig , const std :: string & name)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::createCapturedInputIfNeeded( const SourceRange & loc , const std :: string & ident)",98, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::block()",19, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getBlockOwningKind()",45, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setVar( const SourceRange & loc , const std :: string & name , Value * value)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setSugaredVar( const SourceRange & loc , const std :: string & name , SugaredValuePtr value)",111, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getSugaredVar( const Ident & ident , bool required = true)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getVar( const Ident & ident)",65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getSugaredVar( const std :: string & ident , const SourceRange & range , bool required = true)",106, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getVar( const std :: string & ident , const SourceRange & range)",70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::deleteExtraInputs( const SourceRange & loc)",71, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::definedVariables()",48, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::packOutputs( Graph & g , at :: ArrayRef<Value*> values)",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::createTupleUnpack( Value * v)",77, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::unwrapOptional( TypePtr opt_type)",64, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isIntOrFloatUsedAsList( const Value * value , const Argument & arg)",72, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::convertibleToList( const TypePtr & type , const TypePtr & list_type_)",80, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryConvertToType( const SourceRange & loc , Graph & graph , const TypePtr & concrete_type , Value * value , bool allow_conversions)",100, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryMatchArgument( const Argument & arg , Graph & graph , const SourceRange & loc , const NamedValue & named_value , const std :: function<std::ostream&()> & err , bool allow_conversions , TypeEnv & type_env)",118, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::findInputWithName( const std :: string & name , at :: ArrayRef<NamedValue> kwargs)",46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryCreateList( const TypePtr & elem_type , Graph & graph , const SourceRange & loc , at :: ArrayRef<NamedValue> varargs , const std :: function<std::ostream&()> & err , bool convert_tensor_to_num , TypeEnv & type_env)",97, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::materializeConstant( T val , Graph & graph , const SourceRange & r , std :: unordered_map<T,Value*> & map)",64, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryMatchSchema( const FunctionSchema & schema , const SourceRange & loc , Graph & graph , c10 :: optional<NamedValue> self , at :: ArrayRef<NamedValue> args , at :: ArrayRef<NamedValue> kwargs , std :: ostream & failure_messages , bool allow_conversions)",94, 16, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::prefixLine( const std :: string & str , const std :: string & prefix)",83, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::emitBuiltinNode( const MatchedSchema & matched_schema , const SourceRange & loc , Graph & graph , Symbol name)",74, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::emitBuiltinCall( const SourceRange & loc , Graph & graph , Symbol name , const c10 :: optional<NamedValue> & self , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , bool required)",86, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::ensureInt( const SourceRange & range , Value * v)",62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::BuiltinFunction::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",65, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isSupportedListElementType( const TypePtr & type)",62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::to_ir( Def def_ , Resolver resolver_ , SugaredValuePtr self_ , Method & method)",93, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::pushFrame( Block * b)",95, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::popFrame()",49, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::evaluateDefaults( const SourceRange & r , const std :: vector<Expr> & default_types , const std :: vector<Expr> & default_exprs)",143, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::parseArgsFromDecl( const Decl & decl)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::parseReturnsFromDecl( const Decl & decl)",109, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::extractSchemaFromDef( const Def & def)",68, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitFormalArguments( const SugaredValuePtr & self , const FunctionSchema & schema)",105, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitReturn( c10 :: optional<Return> return_stmt_ , const FunctionSchema & schema)",103, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStatements( const List<Stmt> & statements)",65, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStatements( List<Stmt> :: const_iterator begin , List<Stmt> :: const_iterator end)",90, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSingleIfBranch( Block * b , const List<Stmt> & branch)",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::create( Symbol kind , const SourceRange & loc , size_t n_outputs)",73, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTernaryIf( const TernaryIf & expr)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitShortCircuitIf( const SourceRange & loc , const TreeRef & first_expr , const TreeRef & second_expr , bool is_or)",76, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIfExpr( const SourceRange & range , Value * cond_value , std :: function<Value*()> true_expr , std :: function<Value*()> false_expr)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitCond( const Expr & cond)",72, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIfElseBlocks( Value * cond_value , const If & stmt)",108, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIf( const If & stmt)",105, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitLoopCommon( SourceRange range , c10 :: optional<Expr> max_trip_count , c10 :: optional<Expr> cond , const List<Stmt> & body , c10 :: optional<Ident> itr_ident)",105, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitForRange( const SourceRange & range , const Ident & target , const List<Expr> & args , const List<Stmt> & body)",117, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitFor( const For & stmt)",103, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitWhile( const While & stmt)",63, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitRaise( const SourceRange & loc)",66, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAssert( const Assert & stmt)",68, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::calcNumStarredUnpack( const List<Expr> & lhs , const SourceRange & r)",82, 30, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getAugOp( const AugAssign & stmt , bool isTensor)",70, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignment( const AugAssign & stmt)",58, 12, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToSelectVar( const AugAssign & stmt)",121, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToVar( const AugAssign & stmt)",77, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToSubscript( const AugAssign & stmt)",80, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscriptAssign( const SourceRange & stmtRange , const Subscript & lhs , const Expr & rhs)",65, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscriptAssign( const SourceRange & stmtRange , const Subscript & lhs , const NamedValue & rhs)",81, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleAssign( const TupleLiteral & tl , const Expr & rhs)",108, 10, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAssignment( const Assign & stmt)",99, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNodeKind( int kind , int ninputs)",74, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNamedValues( const TreeList & trees , bool maybe_unpack)",93, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNamedValues( const List<Expr> & trees , bool maybe_unpack)",64, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getValues( const TreeList & trees , bool maybe_unpack)",66, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getValues( const List<Expr> & trees , bool maybe_unpack)",59, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAttributes( const List<Attribute> & attributes)",83, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitApplyExpr( Apply & apply , size_t n_binders)",88, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitExpr( const Expr & tree , TypePtr type_hint = nullptr)",90, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::reverseComparision( NodeKind kind)",86, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSugaredExpr( const Expr & tree , size_t n_binders , TypePtr type_hint = nullptr)",113, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitNegate( const TreeRef & tree)",72, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitForkExpr( SourceRange loc , const std :: shared_ptr<SugaredValue> & forked , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes)",96, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSimpleExpr( const TreeRef & tree , const TypePtr & type_hint = nullptr)",87, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitConst( const Const & c)",88, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStringLiteral( const StringLiteral & c)",56, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSelect( const SourceRange & loc , Value * input , int64_t dim , Value * index)",68, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSlice( const SourceRange & loc , Value * input , c10 :: optional<int64_t> dim , const SliceExpr & slice)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIndex( const SourceRange & loc , Value * input , at :: ArrayRef<Value*> indices)",95, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIntAndSliceIndexing( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",86, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitMultidimSlicing( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",71, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitBasicSlice( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",74, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getTupleIndexVal( const SourceRange & loc , const TupleTypePtr & tuple_type , Value * idx_val , bool allow_out_of_bounds)",77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleIndex( const SourceRange & loc , Value * tuple_val , Value * idx_val)",94, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleSlice( const SourceRange & loc , const NamedValue & tuple_val , const NamedValue & beg_val , const at :: optional<NamedValue> & end_val)",105, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscript( const Subscript & subscript)",53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscript( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",67, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitBasicGather( const SourceRange & loc , Value * gatherable , const List<Expr> & subscript_exprs)",88, 17, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::builtin_cast_methods()",84, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::SimpleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",112, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::inlineCallTo( Graph & g , Graph & callee , ArrayRef<Value*> inputs)",85, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::defineMethodsInModule( const std :: shared_ptr<Module> & m , const std :: vector<Def> & definitions , const std :: vector<Resolver> & resolvers , const SugaredValuePtr & self)",169, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::ident_to_type_lut()",73, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::subscript_to_type_fns()",127, 8, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isTorch( const Expr & expr)",70, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::parseBaseTypeName( const Expr & expr)",65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::parseTypeFromExpr( const Expr & expr)",100, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::handleBroadcastList( const Expr & expr)",107, 4, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::defineMethodsInModule( std :: shared_ptr<Module> m , const std :: string & source , const Resolver & resolver , const SugaredValuePtr & self)",138, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::SimpleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint)",103, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::getKernelCache()",43, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::debugNumCachedKernelSpecs()",51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::normalizeGraphForCache( const std :: shared_ptr<Graph> & graph)",85, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::store( std :: shared_ptr<Graph> graph)",67, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::nolock_retrieve( KernelCacheImpl & cache , const int64_t key)",54, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::retrieve( const int64_t key)",57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::lookupGraph( std :: shared_ptr<Graph> graph)",70, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::registerFusion( const Node * fusion_group)",70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::runFusion( const int64_t key , Stack & stack)",70, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::canFuseOnCPU()",38, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::canFuseOnGPU()",28, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::overrideCanFuseOnCPU( bool value)",40, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::debugLaunchGraph( Graph & graph , at :: ArrayRef<at::Tensor> inputs)",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::nCompiledKernels()",44, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::getMapSize( const KernelSpec & spec , at :: TensorList args , at :: IntList arg_subset)",82, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::canRunKernel( const KernelSpec & spec , at :: TensorList args)",82, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::expandArgs( const KernelSpec & spec , std :: vector<at::Tensor> & args , std :: vector<int64_t> & map_size)",55, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::computeNumel( const at :: ArrayRef<int64_t> & sizes)",67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::computeMapSize( const at :: Tensor & tensor , const PartitionDesc & chunkDesc)",76, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::compressContiguous( const at :: IntList & sizes , const at :: IntList & strides , const std :: vector<bool> & cont , uint32_t * c_sizes , uint32_t * c_strides)",65, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::launchFusion( const FusedKernel & fusion , const at :: Device device , const at :: ArrayRef<at::Tensor> & inputs , std :: vector<at::Tensor> & outputs)",122, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::runFusion( const int64_t key , Stack & stack)",83, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::valueName( const Value * n)",47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const int64_t v)",50, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const bool v)",47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const double v)",49, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarTypeName( const at :: ScalarType type)",63, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::calcScalarTypeName( const at :: ScalarType type)",67, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::variableType( const std :: shared_ptr<c10::Type> & t)",85, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::typeCastedValueName( const std :: shared_ptr<c10::Type> & t , const at :: ScalarType outtype , const std :: string & vn)",131, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::encodeRHS( const Node * n)",108, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::usedInFusedChunk( const Value * input)",52, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::emitIndexingFor( std :: ostream & out , const std :: string & tensor , const int ndim , const bool last_is_cont)",78, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::generateKernel( const std :: string & name , const Graph & graph , const std :: vector<TensorDesc> & input_desc , const std :: vector<TensorDesc> & output_desc , const bool use_cuda)",119, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::nCompiledKernels()",60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::debugFuser()",60, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::usedInFusedChunk( const Value * input)",58, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::setInputChunkDescriptors( KernelSpec & spec)",84, 6, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::getInputDependencies( const Value * output)",72, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::setInputBroadcastGroups( KernelSpec & spec)",96, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::upfrontCompilation( KernelSpec & spec)",51, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::registerFusion( const Node * fusion_group)",79, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::compileKernel( const KernelSpec & spec , const ArgSpec & arg_spec , const std :: vector<int64_t> & map_size , const at :: Device device)",133, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/fallback.cpp,"torch::jit::fuser::runFallback( int64_t key , Stack & stack)",77, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::checkCUDAVersion( const cudaDeviceProp & prop)",90, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::getMajorMinor( const cudaDeviceProp * const prop , int & major , int & minor)",86, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::FusedKernelCUDA( int16_t device , std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",109, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::ceilDiv( const int a , const int b)",47, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::launch_raw( const uint32_t numel , std :: vector<void*> & arguments) const",88, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::programExists( const std :: string & program)",56, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::CompilerConfig::CompilerConfig()",41, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::getConfig()",37, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::runCompiler( const std :: string & cpp_file , const std :: string & so_file)",101, 4, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::disas( const std :: string & so_file)",48, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::FusedKernelCPU::FusedKernelCPU( std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",86, 4, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( New)( THWStorage * ptr)",56, 2, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( dealloc)( THPStorage * self)",51, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( newWithAllocator)( int64_t size , at :: Allocator * allocator)",89, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( pynew)( PyTypeObject * type , PyObject * args , PyObject * kwargs)",91, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( length)( THPStorage * self)",56, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( get)( THPStorage * self , PyObject * index)",102, 18, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( set)( THPStorage * self , PyObject * index , PyObject * value)",90, 6, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( initCopyMethods)()",115, 2, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( init)( PyObject * module)",78, 2, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( postInit)( PyObject * module)",97, 2, 0
repos/cpp/pytorch/torch/csrc/generic/serialization.cpp,"THPStorage_( writeFileRaw)( THWStorage * self , io fd)",121, 2, 0
repos/cpp/pytorch/torch/csrc/generic/serialization.cpp,"THPStorage_( readFileRaw)( io file , THWStorage * _storage)",124, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedDecref)( THPStorage * self)",88, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedIncref)( THPStorage * self)",88, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( __newHandle)()",51, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newFilenameStorage)( ptrdiff_t size)",126, 6, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( pyNewFilenameStorage)( PyObject * _unused , PyObject * args)",87, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareFilename)( THPStorage * self)",82, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedFilename)( PyObject * _unused , PyObject * args)",111, 12, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newFdStorage)( ptrdiff_t size)",100, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( pyNewFdStorage)( PyObject * _unused , PyObject * args)",81, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareFd)( THPStorage * self)",76, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedFd)( PyObject * _unused , PyObject * args)",104, 12, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareCuda)( THPStorage * self)",114, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedCuda)( PyObject * _unused , PyObject * args)",89, 8, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( weakRef)( THPStorage * self , PyObject * args)",75, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newWithWeakPtr)( PyObject * _unused , PyObject * arg)",74, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( freeWeakRef)( PyObject * _unused , PyObject * arg)",70, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( expired)( PyObject * _unused , PyObject * arg)",86, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedFd)( THPStorage * self)",70, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( isShared)( THPStorage * self)",69, 6, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( size)( THPStorage * self)",72, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( dataPtr)( THPStorage * self)",75, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( copy_)( PyObject * self , PyObject * args , PyObject * kwargs)",87, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( isPinned)( THPStorage * self)",99, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( elementSize)( THPStorage * self)",74, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( new)( THPStorage * self)",69, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( resize_)( THPStorage * self , PyObject * number_arg)",79, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fill_)( THPStorage * self , PyObject * number_arg)",83, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fromBuffer)( PyObject * _unused , PyObject * args , PyObject * keywds)",97, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fromFile)( PyObject * _unused , PyObject * args , PyObject * keywds)",93, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( writeFile)( THPStorage * self , PyObject * args)",79, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( newWithFile)( PyObject * _unused , PyObject * file)",71, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( setFromFile)( THPStorage * self , PyObject * args)",82, 4, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( getDevice)( THPStorage * self)",77, 2, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( _setCdata)( THPStorage * self , PyObject * new_cdata)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::PyFunctionPreHook( PyObject * dict , int value_idx)",68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::~PyFunctionPreHook()",42, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::operator ( )( const variable_list & values)",81, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::PyFunctionPostHook( PyObject * dict)",70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::~PyFunctionPostHook()",44, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::operator ( )( const variable_list & _outputs , const variable_list & _inputs)",70, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"wrap_variables( const variable_list & c_variables)",66, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"unwrap_variables( PyObject * py_variables)",68, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"check_result( PyObject * prev , PyObject * result , PyObject * hook)",87, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"check_single_result( PyObject * _original , PyObject * _result , PyObject * hook)",90, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"hook_name( PyObject * hook)",63, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/saved_variable.cpp,"torch::autograd::SavedVariable::SavedVariable( const Variable & variable , bool is_output)",75, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/saved_variable.cpp,"torch::autograd::SavedVariable::unpack( std :: shared_ptr<Function> saved_for) const",79, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::VariableInfo::VariableInfo( const Variable & var)",48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::VariableInfo::zeros( at :: OptionalDeviceGuard & device_guard) const",76, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::legacy_apply( const variable_list & inputs)",82, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::apply( variable_list && inputs)",99, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::is_traceable()",90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::release_variables()",47, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::name() const",67, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::get_shared_ptr()",65, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_traverse( THPFunction * self , visitproc visit , void * arg)",79, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_clear( THPFunction * self)",80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_dealloc( THPFunction * self)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_mark_dirty( THPFunction * self)",75, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_wrap_outputs( THPFunction * self , PyObject * inputs_tuple , PyObject * raw_output , PyObject * outputs , bool is_executable)",112, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_save_variables( THPFunction * self)",78, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_parse_non_differentiable( THPFunction * self)",78, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"unpack_input( PyObject * args)",99, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_assert_not_tracing( const char * name , const variable_list & input_vars)",85, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trace_pre_record( PyObject * op_obj , PyObject * input_objects , const variable_list & input_vars)",72, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trace_post_record( Node * node , PyObject * op_obj , const variable_list & input_vars , PyObject * output_objects , bool is_inplace , bool unpack_output)",81, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"process_outputs( PyObject * op_obj , THPFunction * grad_fn , const UnpackedInput & unpacked , PyObject * inputs , THPObjectPtr && raw_output , bool is_executable , Node * node)",97, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_do_forward( THPFunction * self , PyObject * _inputs)",90, 51, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_apply( PyObject * cls , PyObject * inputs)",90, 51, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_prepare_grads( THPFunction * self , THPObjectPtr & raw_grads , bool is_grad_output)",92, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trim_grad_input( THPFunction * self , THPObjectPtr & grad_input)",74, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_do_backward( THPFunction * self , PyObject * args)",84, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction__register_hook_dict( THPFunction * self , PyObject * _var)",87, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_register_hook( THPFunction * self , PyObject * hook)",71, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"unpack_saved_variables( THPFunction * self , const std :: function<PyObject*(const Variable&)> & unpack_fn)",65, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_saved_tensors( THPFunction * self , void * _unused)",70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_saved_variables( THPFunction * self , void * _unused)",72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_next_functions( THPFunction * self , void * _unused)",76, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_metadata( THPFunction * self , void * _unused)",83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getObject( PyObject * obj , void * _unused)",52, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"setObject( PyObject * obj , PyObject * value , void * _unused)",63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getMember( PyObject * obj , void * _unused)",52, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getImplMember( PyObject * obj , void * _unused)",56, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getRequiresGrad( PyObject * obj , void * _unused)",58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_initModule( PyObject * module)",77, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"Decref::operator ( )( PyFunction * p) const",41, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_asFunction( THPFunction * self)",70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::getEventList()",62, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::mark( std :: string name , bool include_cuda)",69, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::c_str( const char * str)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::c_str( std :: string & str)",60, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::pushRangeImpl( T name , const char * msg = "" , int64_t sequence_nr = - 1)",74, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::pushRange( std :: string name)",35, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::popRange()",73, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( Function * fn)",86, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( std :: string name)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( const char * name)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( const char * name , int64_t current_sequence_nr)",78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::onEachDevice( std :: function<void(int)> op)",56, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::enableProfiler( ProfilerState new_state)",111, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::disableProfiler()",78, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/grad_mode.cpp,"torch::autograd::GradMode::is_enabled()",30, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/grad_mode.cpp,"torch::autograd::GradMode::set_enabled( bool enabled)",43, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"THPAutograd_initExtension( PyObject * _unused)",81, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::set_grad_enabled( PyObject * _unused , PyObject * arg)",79, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::is_grad_enabled( PyObject * _unused , PyObject * arg)",70, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::set_anomaly_mode_enabled( PyObject * _unused , PyObject * arg)",79, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::is_anomaly_mode_enabled( PyObject * _unused , PyObject * arg)",78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::python_functions()",34, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::peek_at_next_sequence_nr()",48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::get_next_sequence_nr()",45, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::name() const",46, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::metadata()",78, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::gatherFunctions( Function * func , std :: vector<std::shared_ptr<Function>> & stack)",53, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::deleteFunction( Function * function)",60, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::add( size_t pos , Variable var)",58, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::device() const",42, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::variables( InputBuffer && g)",72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_call( PyObject * self , PyObject * args , PyObject * kwargs)",82, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_traverse( PyObject * self , visitproc visit , void * arg)",72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_clear( PyObject * self)",55, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_dealloc( PyObject * self)",48, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_next_functions( THPCppFunction * self , PyObject * hook)",78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_metadata( THPCppFunction * self , void * _unused)",84, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_requires_grad( THPCppFunction * self)",63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_register_hook_dict( PyObject * self , PyObject * _var)",85, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_register_hook( PyObject * self , PyObject * hook)",71, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_name( PyObject * self)",48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::_initFunctionPyTypeObject( PyTypeObject & type , const char * name , PyGetSetDef * function_properties , PyMethodDef * function_methods)",83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::DefaultFunctionType::DefaultFunctionType()",70, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::functionToPyObject( const std :: shared_ptr<Function> & cdata)",69, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::registerCppFunction( const std :: type_info & type , PyTypeObject * pytype)",79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::registerFunctionHook( Function & fn , PyObject * hook)",90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::Impl( at :: Tensor data , bool requires_grad , Edge gradient_edge)",95, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::numel() const",40, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::sizes() const",40, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::strides() const",42, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::is_contiguous() const",45, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::dim() const",38, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::size( int64_t d) const",48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::stride( int64_t d) const",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::resize_dim( int64_t ndim)",54, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_size( int64_t dim , int64_t new_size)",63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_stride( int64_t dim , int64_t new_stride)",67, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_storage_offset( int64_t storage_offset)",66, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::slow_data() const",51, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::storage() const",53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::storage_offset() const",49, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::get_device_slow() const",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::get_grad_accumulator()",87, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::detach_()",67, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::backward( c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_data( Tensor new_data)",77, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::release_resources()",43, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::DifferentiableViewImpl( Variable base , at :: Tensor data , Edge gradient_edge)",109, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::get_grad_fn()",77, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::rebase_history( Edge gradient_edge)",78, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::release_resources()",61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::rebase_history( Edge gradient_edge)",73, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::FunctionTask::FunctionTask( GraphTask * base , std :: shared_ptr<Function> fn , InputBuffer inputs)",82, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::CompareFunctionTaskTime::operator ( )( FunctionTask const & t1 , FunctionTask const & t2)",70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::ExecInfo::Capture::Capture( int input_idx , int output_idx)",95, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::ExecInfo::should_execute() const",34, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::can_checkpoint()",30, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::GraphTask( bool keep_graph , bool grad_mode)",45, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ReadyQueue::push( FunctionTask item)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ReadyQueue::pop()",76, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_init( int device)",57, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_main( GraphTask * graph_task)",90, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_on_exception( FunctionTask & task , std :: exception & e)",82, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_pre_hooks( Function & fn , variable_list inputs)",74, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_post_hooks( Function & fn , variable_list outputs , const variable_list & inputs)",105, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::is_compatible_type( const at :: Type & expected , const at :: Type & actual)",83, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::validate_outputs( const edge_list & edges , variable_list & grads , const F & format_error)",100, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_function( FunctionTask & task)",102, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::evaluate_function( FunctionTask & task)",97, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::compute_dependencies( Function * root , GraphTask & task)",77, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::ClearCallbacks( std :: vector<std::function<void()>> & callbacks , std :: mutex & callbacks_lock)",64, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::~ClearCallbacks()",33, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::clear()",54, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::execute( const edge_list & roots , const variable_list & inputs , bool keep_graph , bool create_graph , const edge_list & outputs)",91, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::get_base_engine()",35, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::set_default_engine_stub( EngineStub stub)",48, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::get_default_engine()",39, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::queue_callback( std :: function<void()> callback)",62, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::is_checkpoint_valid()",37, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::ready_queue( int device)",54, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::start_threads()",72, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::init_to_execute( Function & graph_root , const edge_list & outputs)",94, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp,"torch::autograd::PyAnomalyMetadata::store_stack()",73, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp,"torch::autograd::PyAnomalyMetadata::print_stack()",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_legacy_variable.cpp,"torch::autograd::THPVariable_pynew( PyTypeObject * type , PyObject * args , PyObject * kwds)",100, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_legacy_variable.cpp,"torch::autograd::init_legacy_variable( PyObject * module)",68, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_length( PyObject * self)",61, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::count_specified_dimensions( PyObject * index)",142, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::invalid_index( PyObject * obj)",76, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySlice( const Variable & self , int64_t dim , PyObject * slice , bool ensure_view = false)",105, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySelect( const Variable & self , int64_t dim , int64_t index)",86, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::sequenceToVariable( const at :: Type & type , PyObject * seq)",74, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::valueToTensor( const at :: Type & type , PyObject * value)",90, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::boolToIndexingTensor( const Variable & self , bool value)",95, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySlicing( const Variable & self , PyObject * index , variable_list & outIndices)",114, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::typeConvertIndices( const Variable & self , const variable_list & indices)",100, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::dispatch_index( const Variable & self , const variable_list & indices)",85, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::dispatch_index_put_( Variable & self , const variable_list & indices , const Variable & value)",107, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::treatSequenceAsTuple( PyObject * index)",99, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::wrapTuple( PyObject * index)",84, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_getitem( PyObject * self , PyObject * index)",72, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::slicePrefix1sSize( IntList sizes)",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::copy_to( Variable dst , const Variable & src)",80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_setitem( PyObject * self , PyObject * index , PyObject * py_value)",103, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::VariableType( Context * context , TypeExtendedInterface * baseType)",83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::scalarType() const",46, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::typeMeta() const",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::backend() const",40, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allocator() const",45, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::getDeviceFromPtr( void * data) const",59, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",117, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unsafeStorageFromTH( void * th_pointer , bool retain) const",82, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::storageWithAllocator( int64_t size , Allocator * allocator) const",87, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unsafeTensorFromTH( void * th_pointer , bool retain) const",99, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::generator() const",61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toString() const",46, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::elementSizeInBytes() const",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toBackend( Backend b) const",63, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toScalarType( ScalarType s) const",66, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::ID() const",35, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::register_variable_type_for( TypeExtendedInterface * baseType)",67, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableTypeRegistry::VariableTypeRegistry()",108, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::VariableHooks( at :: VariableHooksArgs)",42, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::registerVariableTypeFor( at :: LegacyTypeDispatch * context , at :: Backend backend , at :: ScalarType scalar_type) const",134, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::getVariableTypeFromBaseType( const at :: Type & baseType) const",87, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::isVariableType( const at :: Type & type)",58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::getVariableTypeFromBaseType( const at :: Type & baseType)",97, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::allTypesForBackends( at :: ArrayRef<at::Backend> backends)",106, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allCPUTypes()",68, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allCUDATypes()",70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::checked_cast_variable( const Tensor & t , const char * name , int pos)",128, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::checked_cast_variable( Tensor & t , const char * name , int pos)",128, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( const Tensor & t , const char * name , int pos)",84, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( Tensor & t , const char * name , int pos)",72, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( SparseTensorRef t , const char * name , int pos)",86, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack_opt( const Tensor & t , const char * name , int pos)",80, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( at :: TensorList tl , const char * name , int pos)",113, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::backward( Tensor & self , c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",70, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::set_data( Tensor & self , Tensor new_data) const",68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::s_copy_( Tensor & self , const Tensor & src , bool non_blocking) const",145, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::_s_copy_from( const Tensor & self , const Tensor & dst , bool non_blocking) const",102, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::resize_( Tensor & self , IntList size) const",68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::resize_as_( Tensor & self , const Tensor & the_template) const",86, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::detach( const Tensor & self) const",118, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::detach_( Tensor & self) const",64, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"get_python_engine()",37, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::thread_init( int device)",74, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::thread_on_exception( FunctionTask & task , std :: exception & e)",80, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::make_anomaly_metadata()",73, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::execute( const edge_list & roots , const variable_list & inputs , bool keep_graph , bool create_graph , const edge_list & outputs)",78, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"_maybe_reinitialize_engine_after_fork()",79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_run_backward( THPEngine * self , PyObject * args , PyObject * kwargs)",96, 10, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_queue_callback( PyObject * self , PyObject * _callback)",101, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_is_checkpoint_valid( PyObject * self)",58, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",78, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"child_atfork()",31, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_initModule( PyObject * module)",79, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_NewWithVar( PyTypeObject * type , Variable var)",80, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_Wrap( Variable var)",83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_traverse( THPVariable * self , visitproc visit , void * arg)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_clear( THPVariable * self)",66, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_dealloc( THPVariable * self)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",89, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_make_subclass( PyObject * _ignored , PyObject * args , PyObject * kwargs)",99, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_cdata( THPVariable * self)",63, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_version( THPVariable * self)",53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_grad_fn( THPVariable * self)",53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_grad_fn( THPVariable * self , PyObject * obj)",78, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_leaf( THPVariable * self)",56, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_data( THPVariable * self)",69, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_data( THPVariable * self , PyObject * data)",100, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_grad( THPVariable * self)",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_grad( THPVariable * self , PyObject * py_grad)",100, 7, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_volatile( THPVariable * self)",80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_volatile( THPVariable * self , PyObject * obj)",63, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_output_nr( THPVariable * self)",69, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_requires_grad( THPVariable * self)",59, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_requires_grad( THPVariable * self , PyObject * obj)",90, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_name( THPVariable * self)",58, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_backwards_hooks( THPVariable * self)",61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_backwards_hooks( THPVariable * self , PyObject * obj)",71, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_base( THPVariable * self)",50, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_shape( THPVariable * self)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_cuda( THPVariable * self)",56, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_sparse( THPVariable * self)",58, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_dtype( THPVariable * self)",83, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_layout( THPVariable * self)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_device( THPVariable * self)",64, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_initModule( PyObject * module)",77, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/basic_ops.cpp,"torch::autograd::Error::apply( variable_list && inputs)",61, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/basic_ops.cpp,"torch::autograd::DelayedError::apply( variable_list && inputs)",80, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp,"torch::autograd::AccumulateGrad::AccumulateGrad( Variable variable_)",51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp,"torch::autograd::AccumulateGrad::apply( variable_list && grads)",90, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"DelayedErrorCtor::operator ( )( PyObject * args)",46, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"NoCtor::operator ( )( PyObject * args)",50, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"addClass( PyObject * module , PyTypeObject & type , const char * name , PyGetSetDef * function_properties = nullptr , PyMethodDef * function_methods = nullptr)",91, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"getTupleAttr( PyObject * obj , void * _unused)",58, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"getValueAttr( PyObject * obj , void * _unused)",53, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"accumulateGradVar( PyObject * _self , void * _unused)",67, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"THPAutograd_initFunctions()",111, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Scatter::Scatter( std :: vector<at::Device> devices , const c10 :: optional<std::vector<int64_t>> & chunk_sizes , int64_t dim , const c10 :: optional<std::vector<c10::optional<at::cuda::CUDAStream>>> & streams , bool unsqueeze_scalars)",84, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Scatter::apply( variable_list && inputs)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Gather::Gather( const at :: Device & destination_device , int64_t dim)",66, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Gather::apply( variable_list && inputs)",73, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/utils.cpp,"torch::autograd::wrap_outputs( const variable_list & inputs , tensor_list && outputs , const function_constructor & ctr)",82, 8, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/utils.cpp,"torch::autograd::check_input_variables( const char * name , const variable_list & inputs , int args , int required_args)",105, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopyBackwards::apply( variable_list && grads)",68, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::CopySlices( const Variable & base_var , at :: TensorGeometry view_ , std :: shared_ptr<Function> fn_)",73, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::apply( variable_list && inputs)",81, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::release_variables()",39, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"init_MakeDataPyExt()",67, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"resizeJPEG( PyObject * self , PyObject * args)",108, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::DecoderThread( PyObject * py_list_src , int start_img , int end_img , int target_size , bool crop_to_square)",146, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::~DecoderThread()",33, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::run()",50, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::getTargetList()",43, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::makeJPEG( int idx)",104, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"sqrt( int _X)",48, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"log( int _X)",46, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_init( MTYPE * data , int64 numRows , int64 numCols , bool transpose , bool ownsData)",95, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix()",36, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( int64 numRows , int64 numCols)",80, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( int64 numRows , int64 numCols , bool transpose)",80, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( const Matrix & like)",68, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( MTYPE * data , int64 numRows , int64 numCols)",60, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( MTYPE * data , int64 numRows , int64 numCols , bool transpose)",76, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( const PyArrayObject * src)",88, 20, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::~Matrix()",49, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_updateDims( int64 numRows , int64 numCols)",57, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_checkBounds( int64 startRow , int64 endRow , int64 startCol , int64 endCol) const",94, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::slice( int64 startRow , int64 endRow , int64 startCol , int64 endCol) const",124, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::slice( int64 startRow , int64 endRow , int64 startCol , int64 endCol , Matrix & target) const",103, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceRows( int64 startRow , int64 endRow) const",64, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceRows( int64 startRow , int64 endRow , Matrix & target) const",77, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceCols( int64 startCol , int64 endCol) const",64, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceCols( int64 startCol , int64 endCol , Matrix & target) const",77, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtractFromScalar( MTYPE scalar)",48, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtractFromScalar( MTYPE scalar , Matrix & target) const",70, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThanScalar( MTYPE scalar)",46, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThanScalar( MTYPE scalar)",47, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equalsScalar( MTYPE scalar)",42, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThanScalar( MTYPE scalar , Matrix & target) const",68, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThanScalar( MTYPE scalar , Matrix & target) const",69, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equalsScalar( MTYPE scalar , Matrix & target) const",64, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m)",36, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , Matrix & target)",52, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scale)",49, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m)",41, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m , Matrix & target)",57, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m , MTYPE scale)",54, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m , MTYPE scale , Matrix & target)",70, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scaleM , Matrix & target)",66, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scaleThis , MTYPE scaleM)",67, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scaleThis , MTYPE scaleM , Matrix & target)",87, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addScalar( MTYPE scalar)",39, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addScalar( MTYPE scalar , Matrix & target) const",61, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWithScalar( MTYPE scalar)",43, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWithScalar( MTYPE scalar , Matrix & target) const",65, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWithScalar( MTYPE scalar)",43, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWithScalar( MTYPE scalar , Matrix & target) const",65, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThan( Matrix & a)",37, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThan( Matrix & a , Matrix & target) const",59, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThan( Matrix & a)",38, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThan( Matrix & a , Matrix & target) const",60, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equals( Matrix & a)",33, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equals( Matrix & a , Matrix & target) const",55, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::notEquals( Matrix & a)",36, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::notEquals( Matrix & a , Matrix & target) const",58, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWith( Matrix & a)",34, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWith( Matrix & a , Matrix & target) const",56, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWith( Matrix & a)",34, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWith( Matrix & a , Matrix & target) const",56, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec , MTYPE scale , Matrix & target)",160, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec , MTYPE scale)",57, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec)",44, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec , Matrix & target)",60, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMultByVector( const Matrix & vec)",54, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMultByVector( const Matrix & vec , Matrix & target)",160, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b , MTYPE scale)",55, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b)",42, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b , Matrix & target) const",64, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b , MTYPE scaleAB , Matrix & target) const",79, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addProduct( const Matrix & a , const Matrix & b , MTYPE scaleAB , MTYPE scaleThis)",110, 12, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addProduct( const Matrix & a , const Matrix & b)",60, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::transpose() const",81, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::transpose( bool hard) const",55, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::tile( int64 timesY , int64 timesX) const",83, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::tile( int64 timesY , int64 timesX , Matrix & target) const",70, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_tileTo2( Matrix & target) const",69, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::resize( int64 newNumRows , int64 newNumCols)",71, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::resize( const Matrix & like)",50, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::scale( MTYPE alpha)",34, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::scale( MTYPE alpha , Matrix & target)",58, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::copy( Matrix & dest , int64 srcStartRow , int64 srcEndRow , int64 srcStartCol , int64 srcEndCol , int64 destStartRow , int64 destStartCol) const",152, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::copy() const",39, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::copy( Matrix & target) const",82, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_copyAllTo( Matrix & target) const",80, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::min() const",41, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::min( int64 axis) const",98, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::min( int64 axis , Matrix & target) const",53, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::max() const",42, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::max( int64 axis) const",98, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::max( int64 axis , Matrix & target) const",53, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sum() const",33, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::norm() const",29, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::norm2() const",39, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sum( int64 axis) const",98, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sum( int64 axis , Matrix & target) const",53, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregate( int64 axis , Matrix & target , MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",113, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregateRow( int64 row , MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",100, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregateCol( int64 col , MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",100, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregate( MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",86, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::printShape( const char * name) const",65, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::print() const",43, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::print( int64 rows , int64 cols) const",51, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::print( int64 startRow , int64 rows , int64 startCol , int64 cols) const",85, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::apply( Matrix :: FUNCTION f)",41, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::apply( Matrix :: FUNCTION f , Matrix & target)",64, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMult( const Matrix & a , Matrix & target) const",66, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivide( const Matrix & a , Matrix & target) const",68, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMult( const Matrix & a)",44, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivide( const Matrix & a)",46, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::randomizeUniform()",34, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::randomizeNormal()",64, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::randomizeNormal( MTYPE , MTYPE)",64, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivideByVector( const Matrix & vec)",56, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivideByVector( const Matrix & vec , Matrix & target)",113, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_divideByVector( const Matrix & vec , Matrix & target)",66, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::reshape( int64 numRows , int64 numCols)",53, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::reshaped( int64 numRows , int64 numCols)",60, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop( MTYPE(*func)(MTYPE) , Matrix & target)",68, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop( MTYPE(*func)(MTYPE))",48, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop2( const Matrix & a , MTYPE(*func)(MTYPE,MTYPE) , Matrix & target) const",94, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop2( const Matrix & a , MTYPE(*func)(MTYPE,MTYPE,MTYPE) , MTYPE scalar , Matrix & target) const",115, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop2( const Matrix & a , MTYPE(*func)(MTYPE,MTYPE,MTYPE,MTYPE) , MTYPE scalar1 , MTYPE scalar2 , Matrix & target) const",138, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoopScalar( const MTYPE scalar , MTYPE(*func)(MTYPE,MTYPE) , Matrix & target) const",102, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::hasNan() const",47, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::hasInf() const",47, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::DecoderThread( PyObject * pyList , Matrix & target , int start_img , int end_img , int img_size , int inner_size , bool test , bool multiview)",148, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::~DecoderThread()",33, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::run()",142, 16, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::decodeJpeg( int idx , int & width , int & height)",97, 8, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::randUniform()",60, 4, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::randUniform( double min , double max)",67, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::crop( int64 i , int64 src_width , int64 src_height , bool flip)",82, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::crop( int64 i , int64 src_width , int64 src_height , bool flip , int64 crop_start_x , int64 crop_start_y)",122, 0, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op.cpp,"caffe2::detail::workspaceInfo<CPUContext>( const CPUContext &)",75, 2, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op.cpp,"caffe2::GetCTCGradient::GetGradientDefs()",66, 8, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op_gpu.cpp,"caffe2::detail::workspaceInfo<CUDAContext>( const CUDAContext & context)",72, 0, 0
repos/cpp/pytorch/c10/DeviceType.cpp,"c10::DeviceTypeName( DeviceType d , bool lower_case)",79, 10, 0
repos/cpp/pytorch/c10/DeviceType.cpp,"c10::isValidDeviceType( DeviceType d)",39, 0, 0
repos/cpp/pytorch/c10/DeviceType.cpp,"c10::operator < <( std :: ostream & stream , DeviceType type)",66, 0, 0
repos/cpp/pytorch/c10/Stream.cpp,"c10::operator < <( std :: ostream & stream , const Stream & s)",66, 0, 0
repos/cpp/pytorch/c10/Half.cpp,"c10::operator < <( std :: ostream & out , const Half & value)",65, 0, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::parse_type( const std :: string & device_string)",130, 6, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::Device::validate()",69, 11, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::Device::Device( const std :: string & device_string)",71, 0, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::operator < <( std :: ostream & stream , const Device & device)",71, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::UndefinedTensorImpl()",90, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::sizes() const",50, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::size( int64_t d) const",55, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::stride( int64_t d) const",57, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::dim() const",48, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::storage() const",54, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::storage_offset() const",62, 2, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::strides() const",52, 2, 0
repos/cpp/pytorch/c10/core/DefaultDtype.cpp,"c10::set_default_dtype( caffe2 :: TypeMeta dtype)",49, 0, 0
repos/cpp/pytorch/c10/core/DefaultDtype.cpp,"c10::get_default_dtype()",46, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::grad()",50, 2, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::grad() const",50, 2, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( TensorTypeId type_id , const caffe2 :: TypeMeta & data_type , Allocator * allocator , bool is_variable)",120, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( Storage && storage , TensorTypeId type_id , bool is_variable)",82, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( Storage && storage , TensorTypeId type_id , const caffe2 :: TypeMeta & data_type , bool is_variable)",117, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::sizes() const",36, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::strides() const",38, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::compute_contiguous() const",46, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::release_resources()",39, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::dim() const",34, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::size( int64_t d) const",44, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::stride( int64_t d) const",46, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::maybe_zero_dim( bool condition_when_zero_dim)",98, 2, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::storage() const",45, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::deletePlacementDeleteContext( void * ptr)",54, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::PlacementDeleteContext::makeDataPtr( at :: DataPtr && data_ptr , PlacementDtor placement_dtor , size_t size , at :: Device device)",81, 10, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::TensorTypeIds()",60, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::singleton()",44, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdCreator::TensorTypeIdCreator()",60, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdCreator::create()",79, 8, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::TensorTypeIdRegistry()",81, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::registerId( c10 :: TensorTypeId id)",62, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::deregisterId( c10 :: TensorTypeId id)",64, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::createAndRegister()",55, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::deregister( c10 :: TensorTypeId id)",55, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistrar::TensorTypeIdRegistrar()",61, 4, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistrar::~TensorTypeIdRegistrar()",50, 0, 0
repos/cpp/pytorch/c10/core/CopyBytes.cpp,"c10::_CopyBytesFunctionRegisterer::_CopyBytesFunctionRegisterer( DeviceType fromType , DeviceType toType , CopyBytesFunction func_sync , CopyBytesFunction func_async)",79, 6, 0
repos/cpp/pytorch/c10/core/CopyBytes.cpp,"c10::CopyBytes( size_t nbytes , const void * src , Device src_device , void * dst , Device dst_device , bool async)",78, 2, 0
repos/cpp/pytorch/c10/core/Scalar.cpp,"c10::Scalar::operator -() const",59, 4, 0
repos/cpp/pytorch/c10/core/TensorTypeId.cpp,"c10::operator < <( std :: ostream & str , c10 :: TensorTypeId rhs)",69, 0, 0
repos/cpp/pytorch/c10/core/TensorOptions.cpp,"c10::operator < <( std :: ostream & stream , const TensorOptions & options)",61, 2, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::deleteInefficientStdFunctionContext( void * ptr)",61, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::InefficientStdFunctionContext::makeDataPtr( void * ptr , const std :: function<void(void*)> & deleter , Device device)",61, 10, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"caffe2::SetAllocator( at :: DeviceType t , at :: Allocator * alloc)",60, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"caffe2::GetAllocator( const at :: DeviceType & t)",58, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStreamInternals::~CUDAStreamInternals()",43, 4, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::operator < <( std :: ostream & stream , StreamIdType s)",65, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::streamIdType( StreamId s)",62, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::streamIdIndex( StreamId s)",68, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::makeStreamId( StreamIdType st , size_t si)",89, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::pointer_within( const T * ptr , const A & arr)",113, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_getStreamId( const CUDAStreamInternals * ptr)",95, 4, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initGlobalStreamState()",60, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initDeviceStreamState( DeviceIndex device_index)",74, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initCUDAStreamsOnce()",93, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::check_gpu( DeviceIndex device_index)",59, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::get_idx( std :: atomic<uint32_t> & counter)",58, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_internals( CUDAStream s)",112, 26, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_fromInternals( const CUDAStreamInternals * ptr)",77, 20, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream::stream() const",42, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getStreamFromPool( const bool isHighPriority , DeviceIndex device_index)",83, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getDefaultCUDAStream( DeviceIndex device_index)",67, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getCurrentCUDAStream( DeviceIndex device_index)",66, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::setCurrentCUDAStream( CUDAStream stream)",47, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::operator < <( std :: ostream & stream , const CUDAStream & s)",70, 0, 0
repos/cpp/pytorch/c10/cuda/test/impl/CUDATest.cpp,"TEST( CUDATest , SmokeTest)",28, 0, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::c10_cuda_test()",22, 0, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::c10_cuda_private_test()",30, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::GlobalInitStream()",40, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::SetUsageMessage( const string & str)",53, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::UsageMessage()",40, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::ParseCommandLineFlags( int * pargc , char ** * pargv)",80, 6, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::CommandLineFlagsHasBeenParsed()",50, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<string>( const string & content , string * value)",46, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<int>( const string & content , int * value)",79, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<int64_t>( const string & content , int64_t * value)",77, 4, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<double>( const string & content , double * value)",80, 4, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<bool>( const string & content , bool * value)",81, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::Error( const std :: string & new_msg , const std :: string & backtrace , const void * caller)",68, 4, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::Error( const char * file , const int line , const char * condition , const std :: string & msg , const std :: string & backtrace , const void * caller)",43, 14, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::msg() const",70, 13, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::msg_without_backtrace() const",81, 2, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::AppendMessage( const std :: string & new_msg)",73, 2, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::warn( SourceLocation source_location , std :: string msg)",70, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::set_warning_handler( handler_t handler)",55, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::print_warning( const SourceLocation & source_location , const char * msg)",71, 2, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::GetExceptionString( const std :: exception & e)",68, 2, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::detail::StripBasename( const std :: string & full_path)",58, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::operator < <( std :: ostream & out , const SourceLocation & loc)",73, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::ReplaceAll( std :: string & s , const char * from , const char * to)",70, 0, 0
repos/cpp/pytorch/c10/util/Type.cpp,"c10::demangle( const char * name)",91, 2, 0
repos/cpp/pytorch/c10/util/Type.cpp,"c10::demangle( const char * name)",41, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::enforce_detail::EnforceFailMessage::EnforceFailMessage( std :: string && msg)",75, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::GetFetchStackTrace()",99, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::SetStackTraceFetcher( std :: function<string(void)> fetcher)",65, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ThrowEnforceNotMet( const char * file , const int line , const char * condition , const std :: string & msg , const void * caller)",79, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::Error::Error( SourceLocation source_location , const std :: string & msg)",69, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::InitCaffeLogging( int * argc , char ** argv)",73, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::UpdateLoggingLevelsFromFlags()",77, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ShowLogInfoToStderr()",70, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::InitCaffeLogging( int * argc , char ** argv)",81, 17, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::UpdateLoggingLevelsFromFlags()",39, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ShowLogInfoToStderr()",33, 2, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::MessageLogger::MessageLogger( const char * file , int line , int severity)",81, 10, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::MessageLogger::~MessageLogger()",80, 4, 0
repos/cpp/pytorch/c10/util/UniqueVoidPtr.cpp,"c10::detail::deleteNothing( void *)",29, 0, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::detail::_ThrowRuntimeTypeLogicError( const string & msg)",77, 2, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::TypeMeta::_typeMetaDataInstance<detail::_Uninitialized>()",108, 0, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::TypeIdentifier::createTypeId()",149, 8, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::is_python_frame( const FrameInformation & frame)",76, 2, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::parse_frame_information( const std :: string & frame_string)",84, 0, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::get_backtrace( size_t frames_to_skip , size_t maximum_number_of_frames , bool skip_python_frames)",81, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::IsNUMAEnabled()",65, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMABind( int numa_node_id)",41, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNUMANode( const void * ptr)",45, 6, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNumNUMANodes()",38, 4, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMAMove( void * ptr , size_t size , int numa_node_id)",72, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetCurrentNUMANode()",45, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::IsNUMAEnabled()",23, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMABind( int numa_node_id)",38, 4, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNUMANode( const void * ptr)",36, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNumNUMANodes()",36, 2, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMAMove( void * ptr , size_t size , int numa_node_id)",58, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetCurrentNUMANode()",36, 2, 0
repos/cpp/pytorch/c10/util/SmallVector.cpp,"c10::SmallVectorBase::grow_pod( void * FirstEl , size_t MinSizeInBytes , size_t TSize)",79, 2, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::SetUsageMessage( const string & str)",69, 4, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::UsageMessage()",40, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::ParseCommandLineFlags( int * pargc , char ** * pargv)",68, 2, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::CommandLineFlagsHasBeenParsed()",79, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::Foo::Foo( int x)",33, 4, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::Foo::~Foo()",20, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::Bar::Bar( int x)",33, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::AnotherBar::AnotherBar( int x)",40, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::TEST( RegistryTest , CanRunCreator)",76, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::TEST( RegistryTest , ReturnNullOnNonExistingCreator)",68, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooDefault()",65, 6, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooDefaultAgain()",65, 6, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooBarFallback()",66, 6, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooBarPreferred()",67, 6, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::TEST( RegistryTest , RegistryPriorities)",79, 2, 0
repos/cpp/pytorch/c10/test/flags_test.cpp,"c10_test::TEST( FlagsTest , TestGflagsCorrectness)",72, 2, 0
repos/cpp/pytorch/c10/test/DeviceGuard_test.cpp,"TEST( DeviceGuard , ResetDeviceDifferentDeviceType)",67, 2, 0
repos/cpp/pytorch/c10/test/DeviceGuard_test.cpp,"TEST( OptionalDeviceGuard , ResetDeviceDifferentDeviceType)",77, 2, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceTrue)",37, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceFalse)",57, 2, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceEquals)",55, 4, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , EnforceShowcase)",80, 6, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , Join)",52, 2, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingDeathTest , TestEnforceUsingFatal)",62, 2, 0
repos/cpp/pytorch/c10/test/core/TensorTypeId_test.cpp,"TEST( TensorTypeIdTest , Printing)",35, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeClass1Parameter::SomeClass1Parameter( int param_)",53, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeClass2Parameters::SomeClass2Parameters( int param1_ , int param2_)",49, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeBaseClass::SomeBaseClass( int v_)",35, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeChildClass::SomeChildClass( int v)",46, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::DestructableMock( bool * resourcesReleased , bool * wasDestructed)",80, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::~DestructableMock()",28, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::release_resources()",38, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"ChildDestructableMock::ChildDestructableMock( bool * resourcesReleased , bool * wasDestructed)",70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"NullType1::singleton()",44, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"NullType2::singleton()",44, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith0Parameters)",72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith1Parameter)",47, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith2Parameters)",50, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , TypeIsAutoDeductible)",58, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , CanAssignToBaseClassPtr)",72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTargetTest , whenAllocatedOnStack_thenDoesntCrash)",69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCallingGet_thenReturnsObject)",73, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCallingConstGet_thenReturnsObject)",78, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCallingGet_thenReturnsNullptr)",76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenDereferencing_thenReturnsObject)",76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenConstDereferencing_thenReturnsObject)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenArrowDereferencing_thenReturnsObject)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenConstArrowDereferencing_thenReturnsObject)",67, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenPointsToSameObject)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenOldInstanceInvalid)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenStaysValid)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToSelf_thenStaysInvalid)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenNewInstanceIsValid)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenPointsToSameObject)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningFromInvalidPtr_thenNewInstanceIsInvalid)",78, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenOldInstanceInvalid)",74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenMoveAssigningToDifferentNullptr_thenHasNewNullptr)",74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenPointsToSameObject)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenOldInstanceValid)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenStaysValid)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToSelf_thenStaysInvalid)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigning_thenNewInstanceIsValid)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenOldInstanceInvalid)",74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssigningInvalidPtrToBaseClass_thenNewInstanceIsInvalid)",80, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenCopyAssigningToDifferentNullptr_thenHasNewNullptr)",74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenPointsToSameObject)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenOldInstanceInvalid)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenNewInstanceValid)",77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingFromInvalidPtr_thenNewInstanceInvalid)",74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenPointsToSameObject)",75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenOldInstanceInvalid)",75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenNewInstanceValid)",74, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenMoveConstructingToDifferentNullptr_thenHasNewNullptr)",77, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenPointsToSameObject)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenOldInstanceValid)",77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenNewInstanceValid)",77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingFromInvalidPtr_thenNewInstanceInvalid)",74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenPointsToSameObject)",75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenOldInstanceInvalid)",75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenNewInstanceInvalid)",75, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenCopyConstructingToDifferentNullptr_thenHasNewNullptr)",77, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunction)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethod)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionFromInvalid)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodFromInvalid)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionWithInvalid)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodWithInvalid)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionInvalidWithInvalid)",57, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodInvalidWithInvalid)",55, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInContainer)",57, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInSet)",54, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInUnorderedSet)",62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInMap)",48, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInUnorderedMap)",49, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_AfterCopyConstructor)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_AfterCopyAssignment)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_Nullptr)",43, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality_NullptrLeft)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality_NullptrRight)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsDifferent)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsDifferent_ValidAndInvalid)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_AfterCopyConstructor)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_AfterCopyAssignment)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_BothNullptr)",52, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , OneIsLess)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsLess1)",66, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsLess2)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsNotLessThanNullptr)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenIsInvalid)",66, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenHoldsNullptr)",69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenDestructed_thenDestructsObject)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructed_thenDestructsObjectAfterSecondDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructedToBaseClass_thenDestructsObjectAfterSecondDestructed)",88, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithBaseClassCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenMoveAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsObjectAfterSecondDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsObjectAfterSecondDestructed)",88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedAndDestructed_thenDestructsObjectAfterLastDestruction)",89, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",100, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",108, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedAndDestructed_thenDestructsObjectAfterLastDestruction)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",105, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssigned_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClass_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithBaseClassCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCopyAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenDestructs)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCallingReset_thenDestructsAfterCopyDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCallingResetOnCopy_thenDestructsAfterOriginalDestructed)",84, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithMoved_whenCallingReset_thenDestructsAfterMovedDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithMoved_whenCallingResetOnMoved_thenDestructsImmediately)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsMoveConstructingToConst)",60, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsCopyConstructingToConst)",60, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsMoveAssigningToConst)",66, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsCopyAssigningToConst)",72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNewPtr_thenHasUseCount1)",62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNewPtr_thenIsUnique)",62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenEmptyPtr_thenHasUseCount0)",57, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenEmptyPtr_thenIsNotUnique)",56, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenResetPtr_thenHasUseCount0)",62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenResetPtr_thenIsNotUnique)",62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenHasUseCount1)",67, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenIsUnique)",63, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenOldHasUseCount0)",70, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenOldIsNotUnique)",69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenHasUseCount1)",64, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenIsUnique)",63, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenOldHasUseCount0)",67, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenOldIsNotUnique)",66, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenHasUseCount2)",67, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenIsNotUnique)",66, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenOldHasUseCount2)",70, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenOldIsNotUnique)",69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenDestructingCopy_thenHasUseCount1)",68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenDestructingCopy_thenIsUnique)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenReassigningCopy_thenHasUseCount1)",68, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenReassigningCopy_thenIsUnique)",64, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_thenHasUseCount2)",64, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_thenIsNotUnique)",63, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenDestructingCopy_thenHasUseCount1)",65, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenDestructingCopy_thenIsUnique)",80, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenReassigningCopy_thenHasUseCount1)",65, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenReassigningCopy_thenIsUnique)",80, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenDoesntCrash)",79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",80, 10, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenStackObject_whenReclaimed_thenCrashes)",69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"IntrusiveAndWeak::IntrusiveAndWeak( intrusive_ptr<T> ptr_)",79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_weak_intrusive( Args && ... args)",78, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_weak_only( Args && ... args)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_invalid_weak()",72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCreatingAndDestructing_thenDoesntCrash)",70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenLocking_thenReturnsCorrectObject)",76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenOldInstanceInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenNewInstanceIsValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenStaysValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToSelf_thenStaysInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigning_thenNewInstanceIsValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigning_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToSelf_thenStaysInvalid)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningFromInvalidPtr_thenNewInstanceIsInvalid)",78, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningFromWeakOnlyPtr_thenNewInstanceIsInvalid)",79, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenOldInstanceInvalid)",80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenMoveAssigningToDifferentNullptr_thenHasNewNullptr)",93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenOldInstanceValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenStaysValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigning_thenNewInstanceIsValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToSelf_thenStaysInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigning_thenNewInstanceIsValid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigning_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToSelf_thenStaysInvalid)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",81, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenOldInstanceInvalid)",81, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigningInvalidPtrToBaseClass_thenNewInstanceIsInvalid)",81, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigningWeakOnlyPtrToBaseClass_thenNewInstanceIsValid)",80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenCopyAssigningToDifferentNullptr_thenHasNewNullptr)",93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenOldInstanceInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenNewInstanceValid)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingFromInvalidPtr_thenNewInstanceInvalid)",74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingFromWeakOnlyPtr_thenNewInstanceInvalid)",75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenPointsToSameObject)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenOldInstanceInvalid)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenNewInstanceValid)",69, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromWeakOnlyPtr_thenNewInstanceInvalid)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenMoveConstructingToDifferentNullptr_thenHasNewNullptr)",93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenPointsToSameObject)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenOldInstanceValid)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenNewInstanceValid)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingFromInvalidPtr_thenNewInstanceInvalid)",74, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingFromWeakOnlyPtr_thenNewInstanceInvalid)",75, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenPointsToSameObject)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenOldInstanceInvalid)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenNewInstanceInvalid)",71, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromWeakOnlyPtr_thenNewInstanceInvalid)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenCopyConstructingToDifferentNullptr_thenHasNewNullptr)",93, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunction)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethod)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionFromInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodFromInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWithInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWithInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionInvalidWithInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodInvalidWithInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionFromWeakOnlyPtr)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodFromWeakOnlyPtr)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWithWeakOnlyPtr)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWithWeakOnlyPtr)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWeakOnlyPtrWithWeakOnlyPtr)",69, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWeakOnlyPtrWithWeakOnlyPtr)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInContainer)",60, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInSet)",57, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInUnorderedSet)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInMap)",52, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInUnorderedMap)",53, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyConstructor)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyAssignment)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyAssignment_WeakOnly)",68, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_Invalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality)",66, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_InvalidLeft)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_InvalidRight)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_WeakOnly)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_ValidAndInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_ValidAndWeakOnly)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_WeakOnlyAndWeakOnly)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyConstructor)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyConstructor_WeakOnly)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyAssignment)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyAssignment_WeakOnly)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_BothInvalid)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , OneIsLess)",74, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsLess1)",76, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsLess2)",77, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsNotLessThanInvalid)",72, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingResetOnWeakPtr_thenIsInvalid)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingResetOnStrongPtr_thenIsInvalid)",81, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsMoveConstructingToConst)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsCopyConstructingToConst)",68, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsMoveAssigningToConst)",80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsCopyAssigningToConst)",80, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNewPtr_thenHasUseCount1)",70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNewPtr_thenIsNotExpired)",70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_thenHasUseCount0)",70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_thenIsExpired)",70, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_thenHasUseCount0)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_thenIsExpired)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingWeakReset_thenHasUseCount0)",77, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingWeakReset_thenIsExpired)",74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingStrongReset_thenHasUseCount0)",79, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingStrongReset_thenIsExpired)",76, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenHasUseCount1)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenIsNotExpired)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenOldHasUseCount0)",74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenOldIsExpired)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenHasUseCount1)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenIsNotExpired)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenOldHasUseCount0)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenOldIsExpired)",71, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenHasUseCount1)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenIsNotExpired)",71, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenOldHasUseCount1)",74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenOldIsNotExpired)",74, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenLastStrongPointerResets_thenReleasesResources)",88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenDestructedButStillHasStrongPointers_thenDoesntReleaseResources)",88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenDestructed_thenDestructsObject)",85, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructed_thenDestructsObjectAfterSecondDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructedToBaseClass_thenDestructsObjectAfterSecondDestructed)",88, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithBaseClassCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenMoveAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsObjectAfterSecondDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsObjectAfterSecondDestructed)",88, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedAndDestructed_thenDestructsObjectAfterLastDestruction)",89, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",100, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",108, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedAndDestructed_thenDestructsObjectAfterLastDestruction)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",97, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",105, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigned_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClass_thenDestructsOldObject)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",86, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithBaseClassCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",94, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCopyAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",96, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingReset_thenDestructs)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCallingReset_thenDestructsAfterCopyDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCallingResetOnCopy_thenDestructsAfterOriginalDestructed)",84, 4, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithMoved_whenCallingReset_thenDestructsAfterMovedDestructed)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithMoved_whenCallingResetOnMoved_thenDestructsImmediately)",83, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenDoesntCrash)",80, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenReleasedAndReclaimed_thenDoesntCrash)",67, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",71, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",80, 10, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenStackObject_whenReclaimed_thenCrashes)",73, 0, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"MovableOnly::MovableOnly( int val_)",78, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"MovableOnly::operator ==( const MovableOnly & lhs , const MovableOnly & rhs)",104, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting()",52, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting( const CopyCounting & rhs)",105, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting( CopyCounting && rhs)",100, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::operator =( const CopyCounting & rhs)",55, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::operator =( CopyCounting && rhs)",50, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex)",106, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_singleInput)",73, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_movableOnly)",128, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_onlyCopiesIfNecessary)",142, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_onlyMovesIfNecessary)",140, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_keepsLValueReferencesIntact)",87, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::map_to_double::operator ( )( T a) const",65, 6, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap)",113, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_emptyInput)",84, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_emptyOutput)",104, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_movableOnly_byRValue)",149, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_movableOnly_byValue)",149, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyCopiesIfNecessary)",167, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyMovesIfNecessary_1)",158, 8, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyMovesIfNecessary_2)",162, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_equality_comparable::operator ==( const EqualityComparable & , const EqualityComparable &)",101, 4, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"std::hash<test_is_hashable::Hashable>::operator ( )( const test_is_hashable :: Hashable &)",76, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_function_type::Functor::operator ( )()",29, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_function_type::func()",21, 8, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , TypeMetaStatic)",79, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , Names)",75, 6, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , TypeMeta)",80, 6, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassAllowAssignment::ClassAllowAssignment()",36, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassAllowAssignment::ClassAllowAssignment( const ClassAllowAssignment & src)",70, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassNoAssignment::ClassNoAssignment()",33, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , CtorDtorAndCopy)",75, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , Float16IsNotUint16)",65, 2, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::map_to_size::operator ( )( T) const",96, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_sametype)",85, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::map_make_shared::operator ( )( T)",74, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_differenttypes)",130, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::Class1::func()",51, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::Class2::func()",56, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::mapper_call_func::operator ( )( T)",100, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_members)",86, 8, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::mapper_call_nonexistent_function::operator ( )( T)",126, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_empty)",86, 8, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"half_legacy_impl::halfbits2float( unsigned short h)",68, 2, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"half_legacy_impl::float2halfbits( float src)",73, 2, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"TEST( HalfDoubleConversionTest , Half2Double)",64, 8, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"dev( DeviceIndex index)",40, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , Constructor)",72, 6, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , ConstructorError)",70, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , SetDevice)",50, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , ResetDevice)",50, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , SetIndex)",50, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , Constructor)",72, 6, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , NullaryConstructor)",66, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , SetDevice)",62, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , SetIndex)",62, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"dev( DeviceIndex index)",40, 2, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"stream( DeviceIndex index , StreamId sid)",56, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , Constructor)",59, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamSameSameDevice)",59, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamDifferentSameDevice)",59, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamDifferentDevice)",59, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , Constructor)",65, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , ResetStreamSameDevice)",65, 4, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , ResetStreamDifferentDevice)",65, 4, 0
repos/cpp/pytorch/c10/impl/DeviceGuardImplInterface.cpp,"c10::impl::DeviceGuardImplRegistrar::DeviceGuardImplRegistrar( DeviceType type , const DeviceGuardImplInterface * impl)",108, 0, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::deviceForInputs( Stack & stack , size_t N)",75, 0, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::as_bool_array( at :: ArrayRef<int64_t> vec)",63, 0, 0
repos/cpp/pytorch/tools/cwrap/plugins/templates/nn_tail.cpp,"torch::nn::short_name( PyObject * c_module)",67, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_apply_( PyObject * self , PyObject * arg)",68, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_size( PyObject * self , PyObject * args , PyObject * kwargs)",85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_stride( PyObject * self , PyObject * args , PyObject * kwargs)",87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_get_device( PyObject * self_ , PyObject * args)",74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage_offset( PyObject * self_ , PyObject * args)",78, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_dim( PyObject * self , PyObject * args)",66, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_contiguous( const Tensor & self)",57, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_contiguous( PyObject * self , PyObject * args)",89, 6, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_copy_( Tensor & self , const Tensor & other , bool non_blocking)",87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_copy_( PyObject * self , PyObject * args , PyObject * kwargs)",86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CDouble( const Tensor & self)",85, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CComplexDouble( const Tensor & self)",85, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CLong( const Tensor & self)",85, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_float_scalar( PyObject * self , PyObject * args)",97, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_integral_scalar( PyObject * self , PyObject * args)",99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_index_scalar( PyObject * self , PyObject * args)",97, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_invert( const Tensor & self)",53, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_invert( PyObject * self , PyObject * args)",80, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , Device device , bool non_blocking , bool copy)",109, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , ScalarType dtype , bool non_blocking , bool copy)",97, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , Device device , ScalarType dtype , bool non_blocking , bool copy)",112, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_cpu( PyObject * self , PyObject * args)",95, 3, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_cuda( PyObject * self , PyObject * args , PyObject * kwargs)",85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_to_type( PyObject * self , ScalarType scalarType)",79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_byte( PyObject * self , PyObject * args)",69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_char( PyObject * self , PyObject * args)",69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_double( PyObject * self , PyObject * args)",71, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_float( PyObject * self , PyObject * args)",70, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_half( PyObject * self , PyObject * args)",69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_int( PyObject * self , PyObject * args)",68, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_long( PyObject * self , PyObject * args)",69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_short( PyObject * self , PyObject * args)",70, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_element_size( PyObject * self , PyObject * args)",75, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_numpy( PyObject * self , PyObject * arg)",96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_record_stream( PyObject * self , PyObject * arg)",99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_requires_grad_( PyObject * self , PyObject * args , PyObject * kwargs)",95, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_is_contiguous( Tensor & self)",52, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_is_contiguous( PyObject * self_ , PyObject * args)",77, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_item( PyObject * self , PyObject * args)",98, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_map_( PyObject * self , PyObject * args , PyObject * kwargs)",85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_map2_( PyObject * self , PyObject * args , PyObject * kwargs)",86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new( PyObject * self , PyObject * args , PyObject * kwargs)",88, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_empty( PyObject * self , PyObject * args , PyObject * kwargs)",90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_full( PyObject * self , PyObject * args , PyObject * kwargs)",89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_ones( PyObject * self , PyObject * args , PyObject * kwargs)",89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",91, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_zeros( PyObject * self , PyObject * args , PyObject * kwargs)",90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage( PyObject * self , PyObject * arg)",69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage_type( PyObject * self , PyObject * arg)",74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_to( PyObject * self , PyObject * args , PyObject * kwargs)",91, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_tolist( PyObject * self , PyObject * args)",96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_type( PyObject * self , PyObject * args , PyObject * kwargs)",116, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_bool( PyObject * self , PyObject * args)",99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::check_out_type_matches( Tensor result , ScalarType scalarType , bool scalarType_is_none , const THPLayout & layout , bool layout_is_none , const Device & device , bool device_is_none)",95, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar end , Tensor result)",59, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar end , const TensorOptions & options)",74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar start , Scalar end , Scalar step , Tensor result)",86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",101, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::allIntegral( std :: initializer_list<std::reference_wrapper<Scalar>> l)",90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_arange( PyObject * self , PyObject * args , PyObject * kwargs)",173, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_range( Scalar start , Scalar end , Scalar step , Tensor result)",85, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_range( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",100, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_range( PyObject * self , PyObject * args , PyObject * kwargs)",172, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , Generator * generator , Tensor result)",99, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , Generator * generator , const TensorOptions & options)",115, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , Tensor result)",76, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , const TensorOptions & options)",92, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , Generator * generator , Tensor result)",112, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , Generator * generator , const TensorOptions & options)",128, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , Tensor result)",89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , const TensorOptions & options)",105, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_randint( PyObject * self_ , PyObject * args , PyObject * kwargs)",195, 4, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_as_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_from_numpy( PyObject * module , PyObject * arg)",84, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable__promote_types( PyObject * self , PyObject * args , PyObject * kwargs)",95, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_sparse_coo_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",98, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_get_device( PyObject * self_ , PyObject * args , PyObject * kwargs)",92, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::initTorchFunctions( PyObject * module)",96, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::IndexRangeGenerator::range( size_t range_size)",40, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::IndexRangeGenerator::size()",30, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::copy_range( variable_list & out , IndexRange range , const Tensor & t)",87, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::copy_range( variable_list & out , IndexRange range , at :: ArrayRef<Tensor> t)",98, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::not_implemented( const char * name)",76, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::maybe_multiply( const Tensor & t , const Scalar & s)",60, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_safe_size( IntList sizes , IntList dim)",49, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::norm_backward( const Tensor & grad , const Tensor & self , const Scalar & p_ , const Tensor & norm)",105, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::norm_backward( Tensor grad , const Tensor & self , const Scalar & p_ , Tensor norm , int64_t dim , bool keepdim)",116, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward( Tensor grad , const Tensor & self , const Scalar & exponent_)",82, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_self( Tensor grad , const Tensor & self , const Tensor & exponent)",110, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_exponent( Tensor grad , const Tensor & self , const Tensor & exponent)",90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_exponent( Tensor grad , const Scalar & base , const Tensor & exponent)",90, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mvlgamma_backward( Tensor grad , const Tensor & self , int64_t p)",72, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::permute_backwards( const Tensor & grad , IntList fwd_dims)",66, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_multiple( const Tensor & t , IntList dim , size_t n_dims)",74, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_backward( const Tensor & grad , IntList sizes , IntList dims , bool keepdim)",86, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::reverse_list( const IntList list)",65, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::reverse_dim( const Tensor & t , int64_t dim)",84, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_safe_zeros_backward( const Tensor & grad , const Tensor & inp , int64_t dim)",94, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_backward( const Tensor & grad , const Tensor & input , const Tensor & result)",90, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_backward( Tensor grad , const Tensor & input , Tensor result , int64_t dim , bool keepdim)",99, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_scan_exclusive( const Tensor & x , int64_t dim)",58, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumprod_backward( const Tensor & grad , const Tensor & input , int64_t dim)",93, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumprod_backward( const Tensor & grad , const Tensor & input , int64_t dim , ScalarType dtype)",98, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::gesv_backward_self( const Tensor & grad , const Tensor & self , const Tensor & A)",88, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::gesv_backward_A( const Tensor & grad , const Tensor & self , const Tensor & A , const Tensor & solution)",110, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumsum_backward( const Tensor & x , int64_t dim)",64, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumsum_backward( const Tensor & x , int64_t dim , ScalarType input_dtype)",79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::logsumexp_backward( Tensor grad , const Tensor & self , Tensor result , int64_t dim , bool keepdim)",104, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unbind_backward( const variable_list & grads , int64_t dim)",142, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_to( const Tensor & self , IntList sizes)",58, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_to( const Tensor & self , int64_t dim , IntList sizes)",84, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cat_tensors_backward( const Tensor & grad , const std :: vector<std::vector<int64_t>> & sizes , int64_t dim)",125, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::clamp_backward( const Tensor & grad , const Tensor & self , const optional<Scalar> & min , const optional<Scalar> & max)",125, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mm_mat1_backward( const Tensor & grad , const Tensor & mat2 , const Tensor & mat1 , const Scalar & alpha)",111, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mm_mat2_backward( const Tensor & grad , const Tensor & mat1 , IntList sizes , IntList strides , const Scalar & alpha)",122, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_sparse_addmm_sparse_backward( const Tensor & grad , const Tensor & sparse_ , const Tensor & dense , const Scalar & alpha)",124, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::renorm_backward( const Tensor & grad , const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",106, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_tensorlist( TensorList tl)",64, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::repeat_backward( Tensor grad , int64_t input_dims , IntList repeats)",75, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_fused_dropout_backward( Tensor grad , Tensor mask , double p1m)",71, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::select_equals_backward( Tensor grad , const Tensor & input , const Tensor & value)",89, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::index_select_backward( Tensor grad , int64_t dim , Tensor indices , IntList sizes , bool keepdim)",102, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::slice_backward( Tensor grad , IntList input_sizes , int64_t dim , int64_t start , int64_t end , int64_t step)",113, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::select_backward( Tensor grad , IntList input_sizes , int64_t dim , int64_t index)",87, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::trace_backward( const Tensor & grad , IntList sizes)",99, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unfold_backward( const Tensor & grad , IntList input_sizes , int64_t dim , int64_t size , int64_t step)",108, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::var_backward( const Tensor & grad , const Tensor & self , bool unbiased)",79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::var_backward( Tensor grad , const Tensor & self , IntList dim , bool unbiased , bool keepdim)",100, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::masked_scatter_backward( const Tensor & grad , const Tensor & mask , IntList sizes)",91, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cholesky_backward( Tensor grad , bool upper , Tensor L)",68, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::split_with_sizes_backward( const std :: vector<torch::autograd::Variable> & grads , IntList split_sizes , int64_t dim , IntList sizes , const Type & type)",102, 33, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::split_backward( const std :: vector<torch::autograd::Variable> & grads , int64_t split_size , int64_t dim , IntList sizes , const Type & type)",90, 22, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::max_pool_double_backward( const Tensor & grad , const Tensor & indices , int dim)",88, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::glu_double_backward( const Tensor & grad , const Tensor & grad_output , const Tensor & input , int64_t dim)",121, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::glu_double_backward_grad_output( const Tensor & grad , const Tensor & input , int64_t dim)",97, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::kl_div_double_backward_grad_output( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",129, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::kl_div_target_backward( Tensor grad_output , Tensor self , Tensor target , int64_t reduction)",112, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::binary_cross_entropy_with_logits_target_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",194, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log_sigmoid_double_backward( const Tensor & grad , const Tensor & input)",80, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::softmax_double_backward( const Tensor & grad , const Tensor & grad_output , int dim , const Tensor & output)",114, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log_softmax_double_backward( const Tensor & grad , const Tensor & grad_output , int dim , const Tensor & output)",118, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::l1_loss_double_backward_grad_output( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",130, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::smooth_l1_loss_double_backward( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",125, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::smooth_l1_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",165, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::diag_backward( const Tensor & grad , IntList input_sizes , int64_t diagonal)",83, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::diagonal_backward( const Tensor & grad , IntList input_sizes , int64_t offset , int64_t dim1 , int64_t dim2)",113, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mse_loss_double_backward( const Tensor & grad , const Tensor & input , int64_t reduction)",96, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mse_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",159, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::soft_margin_loss_double_backward( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",127, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::soft_margin_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",167, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::softplus_double_backward( const Tensor & grad , const Tensor & input , Scalar beta , Scalar threshold)",108, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_maybe_overlapping_memory( IntList sizes , IntList strides)",79, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_min_storage_size( IntList sizes , IntList strides , int64_t storage_offset)",98, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::as_strided_backward( Tensor grad , TensorGeometry input_geometry , IntList sizes , IntList strides , int64_t storage_offset)",129, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::atan2_backward( const Tensor & grad , const Tensor & self , const Tensor & other , std :: array<bool,2> output_mask)",138, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prelu_double_backward( const Tensor & grad_grad_input , const Tensor & grad_grad_weight , const Tensor & grad_out , const Tensor & input_ , const Tensor & weight_)",109, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::svd_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , bool some , bool compute_uv , const Tensor & raw_u , const Tensor & sigma , const Tensor & raw_v)",105, 11, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::symeig_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , bool eigenvectors , bool upper , const Tensor & lambda , const Tensor & v)",109, 13, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::det_backward( const Tensor & grad , const Tensor & self , const Tensor & det)",82, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::logdet_backward( const Tensor & grad , const Tensor & self , const Tensor & logdet)",88, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::slogdet_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , const Tensor & signdet , const Tensor & logabsdet)",87, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::trtrs_backward( const Tensor & grad_x , const Tensor & grad_m , const Tensor & b , const Tensor & a , const Tensor & x , const bool upper , const bool transpose , const bool unitriangular , std :: array<bool,2> output_mask)",77, 4, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::fft_backward( const Tensor & self , const Tensor & grad , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntList checked_signal_sizes , bool normalized , bool onesided , IntList output_sizes)",105, 8, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_exclude_dim1( const Tensor & to_sum , bool keepdim = true)",72, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_dim1( const Tensor & src , const Tensor & target)",68, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::expand_as_dim1( const Tensor & src , const Tensor & target)",68, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::batchnorm_double_backward( const Tensor & input , const Tensor & gamma , const Tensor & ggI , const Tensor & ggG , const Tensor & ggB , const Tensor & gO , const Tensor & running_mean , const Tensor & running_var , bool training , double eps , const Tensor & save_mean , const Tensor & save_invstd , std :: array<bool,3> output_mask)",102, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_trilinear_backward( const Tensor & grad_out , const Tensor & i1 , const Tensor & i2 , const Tensor & i3 , IntList expand1 , IntList expand2 , IntList expand3 , IntList sumdim , int64_t unroll_dim , std :: array<bool,3> grad_mask)",133, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log1p_backward( const Tensor & grad , const Tensor & self)",88, 6, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sparse_constructor_values_backward( const Tensor & sparse_grad_out , const Tensor & indices , IntList values_shape)",120, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::to_dense_backward( const Tensor & grad , const Tensor & input_)",69, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::constant_pad_nd_backward( const Tensor & grad , IntList pad)",105, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_functions.cpp,"torch::autograd::generated::addClass( PyTypeObject & type , const char * name , PyGetSetDef * function_properties = NULL , PyMethodDef * function_methods = NULL)",80, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_functions.cpp,"torch::autograd::generated::initialize_autogenerated_functions()",44, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_nn_functions.cpp,"torch::autograd::THPVariable__parse_to( PyObject * module , PyObject * args , PyObject * kwargs)",112, 2, 0
repos/cpp/pytorch/tools/autograd/templates/python_nn_functions.cpp,"torch::autograd::initNNFunctions( PyObject * module)",65, 2, 0
repos/cpp/pytorch/test/cpp/common/main.cpp,"add_negative_flag( const std :: string & flag)",57, 0, 0
repos/cpp/pytorch/test/cpp/common/main.cpp,"main( int argc , char * argv [ ])",77, 4, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( NoGradTest , SetsGradModeCorrectly)",58, 2, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"AutogradTest::AutogradTest()",54, 4, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanTakeDerivatives)",43, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanTakeDerivativesOfZeroDimTensors)",59, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanPassCustomGradientInputs)",52, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( NNInitTest , CanInitializeTensorThatRequiresGrad)",71, 2, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( TempFileTest , MatchesExpectedPattern)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , AllocatesTensorOnTheCorrectDevice_MultiCUDA)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDevice_MultiCUDA)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToTensorAndTensorAttributes_MultiCUDA)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDoesNotCopyWhenOptionsAreAllTheSame_CUDA)",93, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDeviceAndDtype_MultiCUDA)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromSharedPointer)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromConcreteType)",53, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromModuleHolder)",56, 4, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , PushBackAddsAnElement)",49, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , AccessWithAt)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , AccessWithPtr)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardOnEmptySequentialIsDisallowed)",78, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardChainsCorrectly)",70, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardWithTheWrongReturnTypeThrows)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , TheReturnTypeOfForwardDefaultsToTensor)",65, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ForwardReturnsTheLastValue)",70, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , SanityCheckForHoldingStandardModules)",63, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ExtendPushesModulesFromOtherSequential)",65, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , HasReferenceSemantics)",62, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , IsCloneable)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , RegistersElementsAsSubmodules)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CloneToDevice_CUDA)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DifferentiableScatter_MultiCUDA)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DifferentiableGather_MultiCUDA)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , Replicate_MultiCUDA)",81, 6, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApply_MultiCUDA)",66, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApplyWithDifferentOutputDevice_MultiCUDA)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApplyRethrowsException_MultiCUDA)",69, 2, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DataParallelPlacesTheOutputOnTheRequestedDevice_MultiCUDA)",80, 8, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DataParallelUsesAllAvailableCUDADevices_CUDA)",69, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::DummyDataset( size_t size = 100)",60, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::get( size_t index)",35, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DatasetCallsGetCorrectly)",57, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TransformCallsGetApplyCorrectly)",65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"InfiniteStreamDataset::get_batch( size_t batch_size)",59, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"InfiniteStreamDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , InfiniteStreamDataset)",66, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , NoSequencerIsIdentity)",66, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , OrderedSequencerIsSetUpWell)",61, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , OrderedSequencerReOrdersValues)",77, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , BatchLambdaAppliesFunctionToBatch)",79, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , LambdaAppliesFunctionToExample)",68, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CollateReducesBatch)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CollationReducesBatch)",61, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerReturnsIndicesInOrder)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerReturnsLessValuesForLastBatch)",70, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerResetsWell)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerResetsWithNewSizeWell)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanSaveAndLoadSequentialSampler)",50, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerReturnsIndicesInCorrectRange)",60, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerReturnsLessValuesForLastBatch)",61, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerResetsWell)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerResetsWithNewSizeWell)",53, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SavingAndLoadingRandomSamplerYieldsSameSequence)",66, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerReturnsTheBatchSizeAndThenRemainder)",67, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerResetsWell)",53, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerResetsWithNewSizeWell)",53, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorDatasetConstructsFromSingleTensor)",81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorDatasetConstructsFromInitializerListOfTensors)",81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StackTransformWorksForExample)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StackTransformWorksForTensorExample)",75, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"T::operator ( )( torch :: Tensor input)",59, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TensorStringDataset::get( size_t index)",79, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TensorStringDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorTransformWorksForAnyTargetType)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorLambdaWorksforAnyTargetType)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnCopyableDataset::get( size_t index)",57, 12, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnCopyableDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , MapDoesNotCopy)",76, 25, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePushAndPopFromSameThread)",48, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePopWithTimeoutThrowsUponTimeout)",66, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePushAndPopFromDifferentThreads)",74, 8, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueueClearEmptiesTheQueue)",62, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleCanPushAndPopJob)",54, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleCanPushAndPopResult)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttlePopResultReturnsNulloptWhenNoJobsInFlight)",71, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleDrainMeansPopResultReturnsNullopt)",63, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttlePopResultTimesOut)",72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::UncopyableDataset( const std :: string &)",56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::get( size_t index)",35, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SharedBatchDatasetReallyIsShared)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SharedBatchDatasetDoesNotIncurCopyWhenPassedDatasetObject)",76, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndex::TestIndex( size_t offset , std :: vector<size_t> index)",63, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndex::size() const",33, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::TestIndexDataset( size_t size)",56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::get_batch( TestIndex index)",57, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::TestIndexSampler( size_t size)",58, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::reset( torch :: optional<size_t> new_size = torch :: nullopt)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::next( size_t batch_size)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::save( torch :: serialize :: OutputArchive & archive) const",72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::load( torch :: serialize :: InputArchive & archive)",65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanUseCustomTypeAsIndexType)",63, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DataLoaderOptionsDefaultAsExpected)",59, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DataLoaderOptionsCoalesceOptionalValues)",64, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , MakeDataLoaderDefaultsAsExpected)",81, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnsizedDataset::get( size_t i)",45, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnsizedDataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , MakeDataLoaderThrowsWhenConstructingSamplerWithUnsizedDataset)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsCompareEqualToThemselves)",72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ValidIteratorsCompareUnequalToEachOther)",72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , SentinelIteratorsCompareEqualToEachOther)",72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsCompareEqualToSentinelWhenExhausted)",74, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsShareState)",74, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanDereferenceIteratorMultipleTimes)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanUseIteratorAlgorithms)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CallingBeginWhileOtherIteratorIsInFlightThrows)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IncrementingExhaustedValidIteratorThrows)",75, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DereferencingExhaustedValidIteratorThrows)",70, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IncrementingSentinelIteratorThrows)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DereferencingSentinelIteratorThrows)",78, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , YieldsCorrectBatchSize)",65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ReturnsLastBatchWhenSmallerThanBatchSizeWhenDropLastIsFalse)",67, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DoesNotReturnLastBatchWhenSmallerThanBatchSizeWhenDropLastIsTrue)",72, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , RespectsTimeout)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"Barrier::Barrier( size_t target)",56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"Barrier::wait()",62, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::Dataset( const Dataset & other)",43, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::get_batch( torch :: ArrayRef<size_t> indices)",79, 4, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::size() const",50, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , EnforcesOrderingAmongThreadsWhenConfigured)",67, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , Reset)",74, 6, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , TestExceptionsArePropagatedFromWorkers)",78, 8, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"f( T && m)",16, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"f( T && m)",54, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , AllOf)",61, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , AnyOf)",63, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , EnableIfModule)",81, 2, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , Apply)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , IsEmptyAfterDefaultConstruction)",57, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , InsertAddsElementsWhenTheyAreYetNotPresent)",68, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , GetReturnsValuesWhenTheyArePresent)",60, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , GetThrowsWhenPassedKeysThatAreNotPresent)",66, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanInitializeFromList)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , InsertThrowsWhenPassedElementsThatArePresent)",70, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FrontReturnsTheFirstItem)",50, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FrontThrowsWhenEmpty)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , BackReturnsTheLastItem)",48, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , BackThrowsWhenEmpty)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FindReturnsPointersToValuesWhenPresent)",64, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FindReturnsNullPointersWhenPasesdKeysThatAreNotPresent)",80, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorThrowsWhenPassedKeysThatAreNotPresent)",80, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorReturnsItemsPositionallyWhenPassedIntegers)",67, 4, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorsThrowswhenPassedKeysThatAreNotPresent)",81, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , UpdateInsertsAllItemsFromAnotherOrderedDict)",69, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , UpdateAlsoChecksForDuplicates)",69, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanIterateItems)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ClearMakesTheDictEmpty)",48, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanCopyConstruct)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanCopyAssign)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanMoveConstruct)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanMoveAssign)",48, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanInsertWithBraces)",45, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ErrorMessagesIncludeTheKeyDescription)",74, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , KeysReturnsAllKeys)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ValuesReturnsAllValues)",54, 2, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ItemsReturnsAllItems)",60, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"xor_model()",32, 6, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"save_and_load( torch :: Tensor input)",51, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Basic)",47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , BasicToFile)",49, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Resized)",47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Sliced)",47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , NonContiguous)",47, 2, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , XOR)",80, 6, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Optim)",81, 8, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , XOR_CUDA)",80, 6, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , CanSerializeModulesWithIntermediateModulesWithoutParametersOrBuffers)",76, 4, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::getState()",29, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::getReward()",23, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::isDone()",20, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::reset()",53, 4, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::CartPole()",15, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::step( int action)",79, 4, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"test_mnist( size_t batch_size , size_t number_of_epochs , bool with_cuda , M && model , F && forward_op , O && optimizer)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , CartPole)",80, 10, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , MNIST_CUDA)",74, 6, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , MNISTBatchNorm_CUDA)",74, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"CPUDevice()",31, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"CUDADevice( DeviceIndex index)",43, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCUDATypes_CUDA)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCUDATensors_MultiCUDA)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"test::AGIUnit2::AGIUnit2()",43, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanEnableAndDisableTrainingMode)",54, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ZeroGrad)",61, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ZeroGradWithUndefined)",75, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterModuleThrowsForEmptyOrDottedName)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterModuleThrowsForDuplicateModuleName)",65, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterParameterThrowsForEmptyOrDottedName)",71, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterParameterThrowsForDuplicateModuleName)",68, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterBufferThrowsForEmptyOrDottedName)",68, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterBufferThrowsForDuplicateModuleName)",81, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanGetName)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , AsCastsModulesCorrectly)",53, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , Conversion_MultiCUDA)",71, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CallingCloneOnModuleThatDoesNotOverrideCloneThrows)",74, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CallingCloneOnModuleThatDoesOverrideCloneDoesNotThrow)",76, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneCreatesDistinctParameters)",73, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ClonePreservesExternalReferences)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneCopiesTheValuesOfVariablesOfSubmodules)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneToDevicePreservesTheDeviceOfParameters_CUDA)",71, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloningToAParticularDevicePlacesAllParametersThere_MultiCUDA)",68, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"ParameterTestModule::ParameterTestModule()",58, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , HasCorrectNumberOfParameters)",51, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ContainsParametersWithTheCorrectName)",59, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"BufferTestModule::BufferTestModule()",55, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , HasCorrectNumberOfBuffers)",48, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ContainsBuffersWithTheCorrectName)",56, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"AImpl::AImpl()",23, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"AImpl::AImpl( int x)",26, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , DefaultConstructorOfModuleHolderCallsDefaultConstructorOfImpl)",69, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ValueConstructorOfModuleHolderCallsCorrectConstructorInImpl)",67, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NullptrConstructorLeavesTheModuleHolderInEmptyState)",74, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestModule::TestModule( int64_t size)",57, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestModule::forward( torch :: Tensor input)",47, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesReturnsExpectedSubmodulesForFlatModel)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesExcludesSelfWhenIncludeSelfSetToFalse)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesReturnsExpectedNamedSubmodulesForFlatModel)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesExcludesSelfWhenIncludeSelfSetToFalse)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ChildrenReturnsExpectedSubmodulesForFlatModel)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedChildrenReturnsExpectedNamedSubmodulesForFlatModel)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ParametersReturnsExpectedTensorsForFlatModel)",67, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedParametersReturnsExpectedTensorsForFlatModel)",72, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , BuffersReturnsExpectedTensorsForFlatModel)",64, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedBuffersReturnsExpectedTensorsForFlatModel)",69, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestContainer::TestContainer( int64_t number , std :: vector<TestContainer> modules = { })",73, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"get_test_container_item( std :: shared_ptr<torch::nn::Module> module)",77, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"make_deeply_nested_test_container()",73, 12, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"make_key_value_pairs_for_deeply_nested_container()",53, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesReturnsExpectedSubmodulesForDeepModel)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesReturnsExpectedNamedSubmodulesForDeepModel)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ChildrensReturnsExpectedSubmodulesForDeepModel)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedChildrensReturnsExpectedNamedSubmodulesForDeepModel)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModuleApplyIteratesCorreclty)",76, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ConstModuleApplyIteratesCorreclty)",76, 4, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModuleApplyIteratesCorreclty)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ConstNamedModuleApplyIteratesCorreclty)",70, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulePointerApplyIteratesCorreclty)",76, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulePointerApplyIteratesCorreclty)",78, 8, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ThrowsWhenAttemptingtoGetTopLevelModuleAsSharedPtr)",73, 0, 0
repos/cpp/pytorch/test/cpp/api/jit.cpp,"TEST( TorchScriptTest , CanCompileMultipleFunctions)",83, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TestModel::TestModel()",53, 8, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"NestedModel::NestedModel()",71, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv1d)",60, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv2dEven)",63, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv2dUneven)",63, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv3d)",66, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear)",58, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , SimpleContainer)",61, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , EmbeddingBasic)",73, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , EmbeddingList)",50, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Dropout)",62, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Parameters)",56, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalCallsSuppliedFunction)",77, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalWithTorchFunction)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalArgumentBinding)",75, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormStateful)",47, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormStateless)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormPureForward)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear_CUDA)",78, 6, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear2_CUDA)",58, 2, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromInitializerList)",62, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromVector)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromArray)",71, 2, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromSingleValue)",58, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , ThrowsWhenConstructedWithIncorrectNumberOfArgumentsInInitializerList)",76, 4, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , ThrowsWhenConstructedWithIncorrectNumberOfArgumentsInVector)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TestValue::TestValue( const int & x)",51, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TestValue::TestValue( int && x)",46, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , ForwardRvaluesCorrectly)",49, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , ForwardLvaluesCorrectly)",48, 0, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , CanConstructUniquePtrOfArray)",55, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , DefaultsToTheRightValues)",52, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ReturnsTheCorrectType)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , UtilityFunctionsReturnTheRightTensorOptions)",75, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCPUTypes)",75, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCPUTensors)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromVariables)",60, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( DeviceTest , ParsesCorrectlyFromString)",65, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"DefaultDtypeTest::DefaultDtypeTest()",56, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"DefaultDtypeTest::~DefaultDtypeTest()",56, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , CanSetAndGetDefaultDtype)",53, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , NewTensorOptionsHasCorrectDefault)",62, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , NewTensorsHaveCorrectDefaultDtype)",62, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , SimpleReturnType)",42, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , SimpleReturnTypeAndSingleArgument)",59, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , StringLiteralReturnTypeAndArgument)",70, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , StringReturnTypeWithConstArgument)",66, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , TensorReturnTypeAndStringArgumentsWithFunkyQualifications)",75, 6, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , WrongArgumentType)",51, 6, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , WrongNumberOfArguments)",67, 6, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"M::M( int value_)",68, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"M::forward( float x)",25, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , GetWithCorrectTypeSucceeds)",52, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , GetWithIncorrectTypeThrows)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithBaseClassSucceeds)",50, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithGoodDowncastSuccceeds)",54, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithBadDowncastThrows)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , DefaultStateIsEmpty)",46, 4, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , AllMethodsThrowForEmptyAnyModule)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , CanMoveAssignDifferentModules)",55, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , ConstructsFromModuleHolder)",74, 4, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , ConvertsVariableToTensorCorrectly)",72, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::TestValue::TestValue( T && value)",68, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::TestValue::operator ( )()",34, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::make_value( T && value)",46, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesIntWhenCorrectType)",60, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesConstIntWhenCorrectType)",65, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesStringLiteralWhenCorrectType)",70, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesStringWhenCorrectType)",63, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesPointersWhenCorrectType)",65, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesReferencesWhenCorrectType)",67, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TryGetReturnsNullptrForTheWrongType)",60, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , GetThrowsForTheWrongType)",49, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , MoveConstructionIsAllowed)",50, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , MoveAssignmentIsAllowed)",48, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForInt)",69, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForStringLiteral)",77, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForString)",77, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"exactly_equal( at :: Tensor left , T right)",47, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"almost_equal( at :: Tensor left , T right , T tolerance = 1e - 4)",66, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToDtype)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToTensorAndTensorAttributes)",67, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToOptionsWithRequiresGrad)",74, 8, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToDoesNotCopyWhenOptionsAreAllTheSame)",71, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValueForSingleValue)",55, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesForManyValues)",55, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesForManyValuesVariable)",63, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesWhenConstructedFromVector)",79, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , UsesOptionsThatAreSupplied)",62, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlob)",78, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlobUsesDeleter)",62, 8, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlobWithStrides)",77, 6, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , Item)",49, 4, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , Item_CUDA)",62, 4, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"test_RNN_xor( Func && model_maker , bool cuda = false)",73, 8, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"check_lstm_sizes( RNNOutput output)",77, 2, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , CheckOutputSizes)",64, 2, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , CheckOutputValuesMatchPyTorch)",69, 2, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndLSTM)",65, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndGRU)",81, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNRelu)",70, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNTanh)",70, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , Sizes_CUDA)",80, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndLSTM_CUDA)",71, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndGRU_CUDA)",69, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNRelu_CUDA)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNTanh_CUDA)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"test_optimizer_xor( Options options)",79, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"assign_parameter( const Parameters & parameters , const char * name , torch :: Tensor new_tensor)",41, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"check_exact_values( Options options , std :: vector<std::vector<torch::Tensor>> expected_parameters)",81, 10, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , BasicInterface)",71, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_SGD)",73, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_Adagrad)",63, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_RMSprop)",80, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_RMSpropWithMomentum)",62, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_Adam)",78, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_AdamWithAmsgrad)",59, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_Adam)",75, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdamWithWeightDecay)",61, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdamWithWeightDecayAndAMSGrad)",71, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_Adagrad)",60, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdagradWithWeightDecay)",64, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdagradWithWeightDecayAndLRDecay)",74, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSprop)",60, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecay)",64, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecayAndCentered)",75, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecayAndCenteredAndMomentum)",75, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGD)",72, 2, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecay)",60, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecayAndMomentum)",71, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecayAndNesterovMomentum)",79, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ZeroGrad)",56, 4, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ExternalVectorOfParameters)",76, 6, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , AddParameter_LBFGS)",76, 2, 0
repos/cpp/pytorch/test/cpp/jit/no-gtest.cpp,"torch::jit::runJITCPPTests()",42, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::CPUComplexFloatType()",39, 12, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::empty( IntList size , const TensorOptions & options) const",77, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::ComplexHooks::ComplexHooks( ComplexHooksArgs)",36, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::ComplexHooks::registerComplexTypes( Context * context) const",76, 8, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::scalarType() const",53, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::typeMeta() const",57, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::backend() const",47, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::toString() const",52, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::ID() const",41, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::elementSizeInBytes() const",57, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",45, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension.cpp,"tanh_add( Tensor x , Tensor y)",38, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",53, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::Net( int64_t in , int64_t out)",54, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::reset()",62, 4, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::forward( torch :: Tensor x)",43, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::set_bias( torch :: Tensor bias)",38, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::get_bias() const",35, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_parameter( const std :: string & name , torch :: Tensor tensor)",74, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_buffer( const std :: string & name , torch :: Tensor tensor)",71, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_submodule( const std :: string & name)",59, 4, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",58, 6, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension2.cpp,"exp_add( Tensor x , Tensor y)",37, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"cudnn_relu_check( const torch :: Tensor & inputs , const torch :: Tensor & outputs)",81, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"cudnn_relu( const torch :: Tensor & inputs , const torch :: Tensor & outputs)",81, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",75, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"sigmoid_add( torch :: Tensor x , torch :: Tensor y)",62, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::MatrixMultiplier( int A , int B)",80, 8, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::forward( torch :: Tensor weights)",49, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::get() const",30, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"function_taking_optional( c10 :: optional<torch::Tensor> tensor)",69, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",65, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cuda_extension.cpp,"sigmoid_add( torch :: Tensor x , torch :: Tensor y)",79, 6, 0
repos/cpp/pytorch/test/cpp_extensions/cuda_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",65, 2, 0
repos/cpp/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp,"dummy( int)",20, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"helpers::check_all_parameters( const torch :: jit :: script :: Module & module , Predicate predicate)",58, 2, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"get_operator_from_registry_and_execute()",57, 6, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"load_serialized_module_with_custom_op_and_execute( const std :: string & path_to_exported_script_module)",57, 4, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_argument_checking_for_serialized_modules( const std :: string & path_to_exported_script_module)",76, 12, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_move_to_device( const std :: string & path_to_exported_script_module)",78, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_move_to_dtype( const std :: string & path_to_exported_script_module)",77, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"main( int argc , const char * argv [ ])",81, 2, 0
repos/cpp/pytorch/test/custom_operator/op.cpp,"custom_op( torch :: Tensor tensor , double scalar , int64_t repeat)",41, 2, 0
repos/cpp/pytorch/test/custom_operator/op.cpp,"custom_op2( std :: string s1 , std :: string s2)",53, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THDefaultAllocator::allocate( size_t size) const",53, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THDefaultAllocator::raw_deleter() const",50, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"getTHDefaultAllocator()",41, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",150, 6, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( const char * filename , int flags , size_t size)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"WaitForReleaseHandle( PVOID lpParam , BOOLEAN TimerOrWaitFired)",83, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::close()",113, 6, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( const char * filename , int flags , size_t size)",79, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( WithFd , const char * filename , int fd , int flags)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::~THMapAllocator( THMapAllocator * ctx)",56, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocatorArgCheck::THRefcountedMapAllocatorArgCheck( int flags)",90, 4, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( const char * filename , int flags , size_t size)",97, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",113, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::initializeAlloc()",135, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::close()",111, 4, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::incref()",60, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::decref()",60, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocatorArgCheck::THRefcountedMapAllocatorArgCheck( int flags)",81, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( const char * filename , int flags , size_t size)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",115, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::initializeAlloc()",52, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::~THRefcountedMapAllocator()",57, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"deleteTHMapAllocator( void * ptr)",46, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"deleteTHRefcountedMapAllocator( void * ptr)",56, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",71, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",91, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::makeDataPtr( const char * filename , int flags , size_t size , size_t * actual_size_out)",113, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::makeDataPtr( WithFd , const char * filename , int fd , int flags , size_t size , size_t * actual_size_out)",129, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::makeDataPtr( const char * filename , int flags , size_t size , size_t * actual_size_out)",123, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::makeDataPtr( WithFd , const char * filename , int fd , int flags , size_t size , size_t * actual_size_out)",139, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::data() const",81, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isOpened( THFile * self)",45, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_name( THFile * self)",42, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"fread__( void * ptr , size_t size , size_t nitems , FILE * stream)",95, 4, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_mode( const char * mode , int * isReadable , int * isWritable)",79, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_synchronize( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_seek( THFile * self , ssize_t position)",83, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_seekEnd( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_position( THFile * self)",85, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_close( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_reverseMemory( void * dst , const void * src , ssize_t blockSize , ssize_t numBlocks)",103, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isLittleEndianCPU( void)",39, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isBigEndianCPU( void)",43, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_nativeEndianEncoding( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_littleEndianEncoding( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_bigEndianEncoding( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_longSize( THFile * self , int size)",85, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_noBuffer( THFile * self)",73, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_free( THFile * self)",44, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_readLong( THFile * self , int64_t * data , ssize_t n)",96, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_writeLong( THFile * self , int64_t * data , ssize_t n)",97, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_readString( THFile * self , const char * format , char ** str_)",138, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_writeString( THFile * self , const char * str , ssize_t size)",83, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_new( const char * name , const char * mode , int isQuiet)",106, 6, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_mode( const char * mode , int * isReadable , int * isWritable)",79, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_free( THFile * self)",44, 2, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_new( const char * name , const char * mode , int isQuiet)",215, 6, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_readStringRaw( THFile * self , const char * format , char ** str_)",75, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_writeStringRaw( THFile * self , const char * str , size_t size)",73, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_synchronize( THFile * self)",38, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_seek( THFile * self , size_t position)",48, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_seekEnd( THFile * self)",34, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_position( THFile * self)",39, 2, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_close( THFile * self)",32, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_free( THFile * self)",31, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_isOpened( THFile * self)",39, 2, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_binary( THFile * self)",33, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_ascii( THFile * self)",32, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_autoSpacing( THFile * self)",38, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_noAutoSpacing( THFile * self)",40, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_quiet( THFile * self)",32, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_pedantic( THFile * self)",35, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_clearError( THFile * self)",37, 0, 0
repos/cpp/pytorch/aten/src/TH/THSize.cpp,"THSize_isSameSizeAs( const int64_t * sizeA , int64_t dimsA , const int64_t * sizeB , int64_t dimsB)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/THSize.cpp,"THSize_nElement( int64_t dims , int64_t * size)",57, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"defaultErrorHandlerFunction( const char * msg , void * data)",69, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THError( const char * file , const int line , const char * fmt , ...)",70, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THAssertionFailed( const char * file , const int line , const char * exp , const char * fmt , ...)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetErrorHandler( THErrorHandlerFunction new_handler , void * data)",71, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetDefaultErrorHandler( THErrorHandlerFunction new_handler , void * data)",78, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"defaultArgErrorHandlerFunction( int argNumber , const char * msg , void * data)",87, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THArgCheck( const char * file , int line , int condition , int argNumber , const char * fmt , ...)",97, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetArgErrorHandler( THArgErrorHandlerFunction new_handler , void * data)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetDefaultArgErrorHandler( THArgErrorHandlerFunction new_handler , void * data)",84, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetGCHandler( void(*torchGCFunction_)(void*data) , void * data)",72, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THAllocInternal( ptrdiff_t size)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THAlloc( ptrdiff_t size)",102, 4, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THRealloc( void * ptr , ptrdiff_t size)",104, 4, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THFree( void * ptr)",23, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog10( const double x)",31, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog1p( const double x)",75, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog2( const double x)",30, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THExpm1( const double x)",31, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetNumThreads( int num_threads)",60, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THGetNumThreads( void)",32, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THGetNumCores( void)",30, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THInferNumThreads( void)",80, 2, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THSizeDesc( const int64_t * size , const int64_t ndim)",66, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_free( THTensor * self)",41, 2, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_setStorage( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",130, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_setStorageNd( THTensor * self , THStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",148, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_resize( THTensor * self , at :: IntList size , at :: IntList stride)",75, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_resizeNd( THTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_compute_stride( at :: IntList oldshape , at :: IntList oldstride , at :: IntList newshape)",104, 8, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_stealAndSetStoragePtr( THTensor * tensor , THStorage * storage)",83, 2, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_new( caffe2 :: TypeMeta data_type)",61, 2, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_free( THStorage * storage)",44, 2, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_size( const THStorage * self)",48, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_retain( THStorage * storage)",46, 4, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_resize( THStorage * storage , ptrdiff_t size)",77, 6, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_isOpened( THFile * self)",47, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_strnextspace( int8_t * str_ , int8_t * c_)",67, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_grow( THMemoryFile * self , ssize_t size)",91, 39, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_mode( const char * mode , int * isReadable , int * isWritable)",81, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_longSize( THFile * self , int size)",85, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_storage( THFile * self)",74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_synchronize( THFile * self)",74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_seek( THFile * self , ssize_t position)",74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_seekEnd( THFile * self)",74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_position( THFile * self)",74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_close( THFile * self)",74, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_free( THFile * self)",46, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_readLong( THFile * self , int64_t * data , ssize_t n)",200, 6, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_writeLong( THFile * self , int64_t * data , ssize_t n)",159, 8, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_cloneString( const int8_t * str , ssize_t size)",73, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_readString( THFile * self , const char * format , char ** str_)",138, 2, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_writeString( THFile * self , const char * str , ssize_t size)",85, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_newWithStorage( THCharStorage * storage , const char * mode)",124, 4, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_new( const char * mode)",50, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_newUnseeded()",67, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_new()",49, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_copy( THGenerator * self , THGenerator * from)",68, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_free( THGenerator * self)",41, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGeneratorState_isValid( THGeneratorState * _gen_state)",80, 4, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGeneratorState_copy( THGeneratorState * self , THGeneratorState * from)",88, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"readURandomLong()",68, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_seed( THGenerator * _generator)",48, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_manualSeed( THGenerator * _generator , uint64_t the_seed_)",137, 4, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_initialSeed( THGenerator * _generator)",55, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_nextState( THGenerator * _generator)",61, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_random( THGenerator * _generator)",71, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_random64( THGenerator * _generator)",52, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"uniform_double( THGenerator * _generator)",61, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"uniform_float( THGenerator * _generator)",59, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_standard_uniform( THGenerator * _generator)",58, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_uniform( THGenerator * _generator , double a , double b)",69, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_uniformFloat( THGenerator * _generator , float a , float b)",71, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_normal( THGenerator * _generator , double mean , double stdv)",99, 4, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_exponential( THGenerator * _generator , double lambda)",68, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_cauchy( THGenerator * _generator , double median , double sigma)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_logNormal( THGenerator * _generator , double mean , double stdv)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_geometric( THGenerator * _generator , double p)",65, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_bernoulli( THGenerator * _generator , double p)",60, 2, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_bernoulliFloat( THGenerator * _generator , float p)",62, 0, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THLogAdd( double log_a , double log_b)",96, 4, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THLogSub( double log_a , double log_b)",94, 4, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THExpMinusApprox( const double x)",53, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",82, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validConv2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",82, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullConv2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",84, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullXCorr2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",84, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr2DRevptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",80, 42, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",89, 39, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validConv3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",88, 38, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullConv3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",87, 37, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullXCorr3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",88, 38, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr3DRevptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",92, 42, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2d)( scalar_t * output_data , scalar_t alpha , scalar_t * ptr_input , int64_t nInputRows , int64_t nInputCols , scalar_t * ptr_weight , int64_t nKernelRows , int64_t nKernelCols , int64_t srow , int64_t scol , const char * vf , const char * xc)",87, 23, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3d)( scalar_t * output_data , scalar_t alpha , scalar_t * ptr_input , int64_t nInputDepth , int64_t nInputRows , int64_t nInputCols , scalar_t * ptr_weight , int64_t nKernelDepth , int64_t nKernelRows , int64_t nKernelCols , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",109, 23, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( convsize)( int64_t x , int64_t k , int64_t s , const char * vf)",84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2DRevger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol)",130, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2DRevgerm)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol)",131, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",159, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmv)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",158, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmm)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",158, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",159, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dcmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",160, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmap)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , THTensor * map , int64_t srow , int64_t scol , const char * vf , const char * xc)",174, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3DRevger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol)",158, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",112, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmv)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",170, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",170, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dcmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",172, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmap)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , THTensor * map , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",114, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"modulo_wrap( scalar_t a , scalar_t b)",57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( bitor)( THTensor * r_ , THTensor * t , scalar_t value)",148, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( bitxor)( THTensor * r_ , THTensor * t , scalar_t value)",148, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( clamp)( THTensor * r_ , THTensor * t , scalar_t min_value , scalar_t max_value)",212, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cadd)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src)",188, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( csub)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src)",79, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cmul)( THTensor * r_ , THTensor * t , THTensor * src)",180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( pow)( THTensor * r_ , THTensor * t , scalar_t value)",102, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cpow)( THTensor * r_ , THTensor * t , THTensor * src)",198, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cdiv)( THTensor * r_ , THTensor * t , THTensor * src)",180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( clshift)( THTensor * r_ , THTensor * t , THTensor * src)",193, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( crshift)( THTensor * r_ , THTensor * t , THTensor * src)",193, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cfmod)( THTensor * r_ , THTensor * t , THTensor * src)",184, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cremainder)( THTensor * r_ , THTensor * t , THTensor * src)",233, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitand)( THTensor * r_ , THTensor * t , THTensor * src)",180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitor)( THTensor * r_ , THTensor * t , THTensor * src)",180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitxor)( THTensor * r_ , THTensor * t , THTensor * src)",180, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( tpow)( THTensor * r_ , scalar_t value , THTensor * t)",166, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addcmul)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src1 , THTensor * src2)",199, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addcdiv)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src1 , THTensor * src2)",199, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addmv)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * mat , THTensor * vec)",110, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( match)( THTensor * r_ , THTensor * m1 , THTensor * m2 , scalar_t gain)",94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addmm)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * m1 , THTensor * m2)",129, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addr)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * vec1 , THTensor * vec2)",111, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addbmm)( THTensor * result , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * batch1 , THTensor * batch2)",121, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( isTransposedContiguous)( THTensor * self)",67, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( checkTransposed)( THTensor * self)",55, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( newTransposedContiguous)( THTensor * self)",68, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( checkLapackClone)( THTensor * result , THTensor * src , int nrows)",116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( cloneColumnMajorNrows)( THTensor * self , THTensor * src , int nrows)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( cloneColumnMajor)( THTensor * self , THTensor * src)",76, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesv)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a)",96, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( trtrs)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a , const char * uplo , const char * trans , const char * diag)",118, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gels)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a)",116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( geev)( THTensor * re_ , THTensor * rv_ , THTensor * a_ , const char * jobvr)",109, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( syev)( THTensor * re_ , THTensor * rv_ , THTensor * a , const char * jobz , const char * uplo)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesdd)( THTensor * ru_ , THTensor * rs_ , THTensor * rv_ , THTensor * a , const char * some , const char * compute_uv)",122, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesdd2)( THTensor * ru_ , THTensor * rs_ , THTensor * rv_ , THTensor * ra_ , THTensor * a , const char * some , const char * compute_uv)",105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( getri)( THTensor * ra_ , THTensor * a)",114, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( clearUpLoTriangle)( THTensor * a , const char * uplo)",84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( copyUpLoTriangle)( THTensor * a , const char * uplo)",84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potrf)( THTensor * ra_ , THTensor * a , const char * uplo)",106, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potrs)( THTensor * rb_ , THTensor * b , THTensor * a , const char * uplo)",118, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potri)( THTensor * ra_ , THTensor * a , const char * uplo)",86, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( pstrf)( THTensor * ra_ , THIntTensor * rpiv_ , THTensor * a , const char * uplo , scalar_t tol)",104, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( qr)( THTensor * rq_ , THTensor * rr_ , THTensor * a)",62, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( geqrf)( THTensor * ra_ , THTensor * rtau_ , THTensor * a)",80, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( orgqr)( THTensor * ra_ , THTensor * a , THTensor * tau)",84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( ormqr)( THTensor * ra_ , THTensor * a , THTensor * tau , THTensor * c , const char * side , const char * trans)",115, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( btrifact)( THTensor * ra_ , THIntTensor * rpivots_ , THIntTensor * rinfo_ , int pivot , THTensor * a)",108, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( btrisolve)( THTensor * rb_ , THTensor * b , THTensor * atf , THIntTensor * pivots)",123, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( random)( THTensor * self , THGenerator * _generator)",121, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( clampedRandom)( THTensor * self , THGenerator * _generator , int64_t min , int64_t max)",144, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( cappedRandom)( THTensor * self , THGenerator * _generator , int64_t max)",85, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( geometric)( THTensor * self , THGenerator * _generator , double p)",94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( uniform)( THTensor * self , THGenerator * _generator , double a , double b)",85, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal)( THTensor * self , THGenerator * _generator , double mean , double stddev)",133, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_means)( THTensor * self , THGenerator * gen , THTensor * means , double stddev)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_stddevs)( THTensor * self , THGenerator * gen , double mean , THTensor * stddevs)",97, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_means_stddevs)( THTensor * self , THGenerator * gen , THTensor * means , THTensor * stddevs)",107, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( exponential)( THTensor * self , THGenerator * _generator , double lambda)",101, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( cauchy)( THTensor * self , THGenerator * _generator , double median , double sigma)",103, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( logNormal)( THTensor * self , THGenerator * _generator , double mean , double stdv)",103, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomialAliasSetup)( THTensor * probs , THLongTensor * J , THTensor * q)",94, 24, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomialAliasDraw)( THLongTensor * self , THGenerator * _generator , THLongTensor * J , THTensor * q)",112, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomial)( THLongTensor * self , THGenerator * _generator , THTensor * prob_dist , int n_sample , int with_replacement)",138, 28, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( getRNGState)( THGenerator * _generator , THTensor * self)",84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( setRNGState)( THGenerator * _generator , THTensor * self)",84, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( baddbmm)( THTensor * result , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * batch1 , THTensor * batch2)",145, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( numel)( THTensor * t)",40, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( preserveReduceDimSemantics)( THTensor * r_ , int in_dims , int reduce_dimension , int keepdim)",68, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( max)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( min)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sum)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( prod)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cumsum)( THTensor * r_ , THTensor * t , int dimension)",116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cumprod)( THTensor * r_ , THTensor * t , int dimension)",116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sign)( THTensor * r_ , THTensor * t)",48, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( trace)( THTensor * t)",78, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cross)( THTensor * r_ , THTensor * a , THTensor * b , int dimension)",127, 23, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmax)( THTensor * r , THTensor * t , THTensor * src)",74, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmin)( THTensor * r , THTensor * t , THTensor * src)",74, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmaxValue)( THTensor * r , THTensor * t , scalar_t value)",96, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cminValue)( THTensor * r , THTensor * t , scalar_t value)",96, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( zerosLike)( THTensor * r_ , THTensor * input)",57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( onesLike)( THTensor * r_ , THTensor * input)",56, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( diag)( THTensor * r_ , THTensor * t , int k)",142, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( eye)( THTensor * r_ , int64_t n , int64_t m)",62, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( range)( THTensor * r_ , accreal xmin , accreal xmax , accreal step)",80, 14, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( arange)( THTensor * r_ , accreal xmin , accreal xmax , accreal step)",81, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( randperm)( THTensor * r_ , THGenerator * _generator , int64_t n)",75, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quicksortascend)( scalar_t * arr , int64_t * idx , int64_t elements , int64_t stride)",102, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quicksortdescend)( scalar_t * arr , int64_t * idx , int64_t elements , int64_t stride)",103, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sort)( THTensor * rt_ , THLongTensor * ri_ , THTensor * t , int dimension , int descendingOrder)",111, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quickselectnoidx)( scalar_t * arr , int64_t k , int64_t elements , int64_t stride)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quickselect)( scalar_t * arr , int64_t * idx , int64_t k , int64_t elements , int64_t stride)",109, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( medianall)( THTensor * tensor)",93, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( mode)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( kthvalue)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int64_t k , int dimension , int keepdim)",120, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( median)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( topk)( THTensor * rt_ , THLongTensor * ri_ , THTensor * t , int64_t k , int dim , int dir , int sorted)",109, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( tril)( THTensor * r_ , THTensor * t , int64_t k)",80, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( triu)( THTensor * r_ , THTensor * t , int64_t k)",80, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cat)( THTensor * r_ , THTensor * ta , THTensor * tb , int dimension)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( check_shape_except_dim)( THTensor * first , THTensor * second , int dimension)",97, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( catArray)( THTensor * result , THTensor ** inputs , int numInputs , int dimension)",109, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( equal)( THTensor * ta , THTensor * tb)",68, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAndAll)( THTensor * tensor)",129, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAnyAll)( THTensor * tensor)",126, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAnd)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAny)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",110, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( atan2)( THTensor * r_ , THTensor * tx , THTensor * ty)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( polygamma)( THTensor * r_ , int64_t n , THTensor * t)",114, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( lerp)( THTensor * r_ , THTensor * a , THTensor * b , scalar_t weight)",89, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( mean)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",121, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( std)( THTensor * r_ , THTensor * t , int dimension , int biased , int keepdim)",105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( var)( THTensor * r_ , THTensor * t , int dimension , int biased , int keepdim)",105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( norm)( THTensor * r_ , THTensor * t , scalar_t value , int dimension , int keepdim)",105, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( normall)( THTensor * tensor , scalar_t value)",92, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( renorm)( THTensor * res , THTensor * src , scalar_t value , int dimension , scalar_t maxnorm)",113, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dist)( THTensor * tensor , THTensor * src , scalar_t value)",87, 21, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( meanall)( THTensor * tensor)",46, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( varall)( THTensor * tensor , int biased)",64, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( stdall)( THTensor * tensor , int biased)",79, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( linspace)( THTensor * r_ , scalar_t a , scalar_t b , int64_t n)",74, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logspace)( THTensor * r_ , scalar_t a , scalar_t b , int64_t n)",75, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( histc)( THTensor * hist , THTensor * tensor , int64_t nbins , scalar_t minvalue , scalar_t maxvalue)",109, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( bhistc)( THTensor * hist , THTensor * tensor , int64_t nbins , scalar_t minvalue , scalar_t maxvalue)",156, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_alpha_small)( scalar_t x , scalar_t alpha , scalar_t beta)",123, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_beta_small)( scalar_t x , scalar_t alpha , scalar_t beta)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_alpha_mid)( double x , double alpha , double beta)",99, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dirichlet_grad_one)( scalar_t x , scalar_t alpha , scalar_t total)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dirichlet_grad)( THTensor * self , THTensor * x , THTensor * alpha , THTensor * total)",106, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( fill)( scalar_t * x , const scalar_t c , const ptrdiff_t n)",73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cadd)( scalar_t * z , const scalar_t * x , const scalar_t * y , const scalar_t c , const ptrdiff_t n)",111, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( adds)( scalar_t * r_ , const scalar_t * t , const scalar_t value , const ptrdiff_t n)",104, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cmul)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",93, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( muls)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cdiv)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",93, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( divs)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( normal_fill)( scalar_t * data , const int64_t size , struct THGenerator * generator , const scalar_t mean , const scalar_t stddev)",75, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( sigmoid)( scalar_t * y , const scalar_t * x , const ptrdiff_t n)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( startup)",60, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( storage)( const THTensor * self)",52, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( storageOffset)( const THTensor * self)",57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimension)( const THTensor * self)",48, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimensionLegacyNoScalars)( const THTensor * self)",63, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimensionLegacyAll)( const THTensor * self)",57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( size)( const THTensor * self , int dim)",94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( stride)( const THTensor * self , int dim)",94, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( data)( const THTensor * self)",50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( new)( void)",71, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithTensor)( THTensor * tensor)",81, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage)( THStorage * storage , ptrdiff_t storageOffset , at :: IntList sizes , at :: IntList strides)",123, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage1d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0)",83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage2d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1)",96, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage3d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2)",112, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage4d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2 , int64_t size3 , int64_t stride3)",83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize)( at :: IntList size , at :: IntList stride)",71, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize1d)( int64_t size0)",50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize2d)( int64_t size0 , int64_t size1)",65, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize3d)( int64_t size0 , int64_t size1 , int64_t size2)",80, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize4d)( int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newClone)( THTensor * self)",50, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newContiguous)( THTensor * self)",51, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newSelect)( THTensor * tensor , int dimension_ , int64_t sliceIndex_)",86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newNarrow)( THTensor * tensor , int dimension_ , int64_t firstIndex_ , int64_t size_)",101, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newTranspose)( THTensor * tensor , int dimension1_ , int dimension2_)",86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newUnfold)( THTensor * tensor , int dimension_ , int64_t size_ , int64_t step_)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newView)( THTensor * tensor , at :: IntList size)",116, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize)( THTensor * self , at :: IntList size , at :: IntList stride)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resizeAs)( THTensor * self , THTensor * src)",75, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize0d)( THTensor * tensor)",47, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize1d)( THTensor * tensor , int64_t size0)",58, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize2d)( THTensor * tensor , int64_t size0 , int64_t size1)",73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize3d)( THTensor * tensor , int64_t size0 , int64_t size1 , int64_t size2)",88, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize4d)( THTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",101, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize5d)( THTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3 , int64_t size4)",116, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set)( THTensor * self , THTensor * src)",57, 28, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",130, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage1d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage2d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage3d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage4d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_ , int64_t size3_ , int64_t stride3_)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( narrow)( THTensor * self , THTensor * src , int dimension , int64_t firstIndex , int64_t size)",103, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( select)( THTensor * self , THTensor * src , int dimension , int64_t sliceIndex)",91, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( transpose)( THTensor * self , THTensor * src , int dimension1 , int dimension2)",111, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( unfold)( THTensor * self , THTensor * src , int dimension , int64_t size , int64_t step)",108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( squeeze)( THTensor * self , THTensor * src)",55, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( squeeze1d)( THTensor * self , THTensor * src , int dimension)",89, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( unsqueeze1d)( THTensor * self , THTensor * src , int dimension)",90, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isTransposed)( const THTensor * self)",52, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isContiguous)( const THTensor * self)",50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isSameSizeAs)( const THTensor * self , const THTensor * src)",71, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isSetTo)( const THTensor * self , const THTensor * src)",79, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nElement)( const THTensor * self)",52, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( retain)( THTensor * self)",41, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( free)( THTensor * self)",37, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( freeCopyTo)( THTensor * self , THTensor * dst)",58, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorageNd)( THTensor * self , THStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",150, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resizeNd)( THTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",101, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set0d)( THTensor * tensor , scalar_t value)",85, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get0d)( const THTensor * tensor)",85, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set1d)( THTensor * tensor , int64_t x0 , scalar_t value)",129, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get1d)( const THTensor * tensor , int64_t x0)",129, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set2d)( THTensor * tensor , int64_t x0 , int64_t x1 , scalar_t value)",126, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get2d)( const THTensor * tensor , int64_t x0 , int64_t x1)",126, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set3d)( THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , scalar_t value)",149, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get3d)( const THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2)",149, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set4d)( THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3 , scalar_t value)",187, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get4d)( const THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3)",187, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( desc)( const THTensor * tensor)",69, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( sizeDesc)( const THTensor * tensor)",80, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gesv)( int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",104, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( trtrs)( char uplo , char trans , char diag , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",128, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gels)( char trans , int m , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , scalar_t * work , int lwork , int * info)",139, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( syev)( char jobz , char uplo , int n , scalar_t * a , int lda , scalar_t * w , scalar_t * work , int lwork , int * info)",123, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( geev)( char jobvl , char jobvr , int n , scalar_t * a , int lda , scalar_t * wr , scalar_t * wi , scalar_t * vl , int ldvl , scalar_t * vr , int ldvr , scalar_t * work , int lwork , int * info)",188, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gesdd)( char jobz , int m , int n , scalar_t * a , int lda , scalar_t * s , scalar_t * u , int ldu , scalar_t * vt , int ldvt , scalar_t * work , int lwork , int * iwork , int * info)",178, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getrf)( int m , int n , scalar_t * a , int lda , int * ipiv , int * info)",80, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getrs)( char trans , int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",117, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getri)( int n , scalar_t * a , int lda , int * ipiv , scalar_t * work , int lwork , int * info)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potrf)( char uplo , int n , scalar_t * a , int lda , int * info)",73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potrs)( char uplo , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",105, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potri)( char uplo , int n , scalar_t * a , int lda , int * info)",73, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( pstrf)( char uplo , int n , scalar_t * a , int lda , int * piv , int * rank , scalar_t tol , scalar_t * work , int * info)",124, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( geqrf)( int m , int n , scalar_t * a , int lda , scalar_t * tau , scalar_t * work , int lwork , int * info)",111, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( orgqr)( int m , int n , int k , scalar_t * a , int lda , scalar_t * tau , scalar_t * work , int lwork , int * info)",118, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( ormqr)( char side , char trans , int m , int n , int k , scalar_t * a , int lda , scalar_t * tau , scalar_t * c , int ldc , scalar_t * work , int lwork , int * info)",163, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( copy_DEFAULT)( scalar_t * x , const scalar_t * y , const ptrdiff_t n)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( fill_DEFAULT)( scalar_t * x , const scalar_t c , const ptrdiff_t n)",81, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cadd_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const scalar_t c , const ptrdiff_t n)",117, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( adds_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",98, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cmul_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( muls_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",98, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cdiv_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( divs_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",98, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( interleaved_normal_fill_16)( scalar_t * data , const scalar_t mean , const scalar_t stddev)",73, 50, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( normal_fill_DEFAULT)( scalar_t * data , int64_t size , THGenerator * generator , const scalar_t mean , const scalar_t stddev)",67, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( data)( const THStorage * self)",50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( size)( const THStorage * self)",50, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( elementSize)()",33, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( new)( void)",60, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize)( ptrdiff_t size)",61, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithAllocator)( ptrdiff_t size , at :: Allocator * allocator)",66, 40, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithMapping)( const char * filename , ptrdiff_t size , int flags)",87, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize1)( scalar_t data0)",52, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize2)( scalar_t data0 , scalar_t data1)",68, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize3)( scalar_t data0 , scalar_t data1 , scalar_t data2)",84, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize4)( scalar_t data0 , scalar_t data1 , scalar_t data2 , scalar_t data3)",100, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( retain)( THStorage * storage)",44, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( free)( THStorage * storage)",42, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithDataAndAllocator)( at :: DataPtr && data , ptrdiff_t size , at :: Allocator * allocator)",83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( resize)( THStorage * storage , ptrdiff_t size)",60, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( fill)( THStorage * storage , scalar_t value)",58, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( set)( THStorage * self , ptrdiff_t idx , scalar_t value)",71, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( get)( const THStorage * self , ptrdiff_t idx)",71, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( swap)( THStorage * storage1 , THStorage * storage2)",64, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorageCopy.cpp,"THStorage_( rawCopy)( THStorage * storage , scalar_t * src)",60, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorageCopy.cpp,"THStorage_( copy)( THStorage * storage , THStorage * src)",68, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"sdot_( const int * n , const float * x , const int * incx , const float * y , const int * incy)",107, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( swap)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",84, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( scal)( int64_t n , scalar_t a , scalar_t * x , int64_t incx)",83, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( copy)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",84, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( axpy)( int64_t n , scalar_t a , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",96, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( dot)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",87, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( gemv)( char trans , int64_t m , int64_t n , scalar_t alpha , scalar_t * a , int64_t lda , scalar_t * x , int64_t incx , scalar_t beta , scalar_t * y , int64_t incy)",164, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( ger)( int64_t m , int64_t n , scalar_t alpha , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy , scalar_t * a , int64_t lda)",136, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( gemm)( char transa , char transb , int64_t m , int64_t n , int64_t k , scalar_t alpha , scalar_t * a , int64_t lda , scalar_t * b , int64_t ldb , scalar_t beta , scalar_t * c , int64_t ldc)",187, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( fill)( THTensor * r_ , scalar_t value)",84, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( zero)( THTensor * r_)",35, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedFill)( THTensor * tensor , THByteTensor * mask , scalar_t value)",81, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedCopy)( THTensor * tensor , THByteTensor * mask , THTensor * src)",87, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedSelect)( THTensor * tensor , THTensor * src , THByteTensor * mask)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( nonzero)( THLongTensor * subscript , THTensor * tensor)",101, 22, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexSelect)( THTensor * tensor , THTensor * src , int dim , THLongTensor * index)",131, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexCopy)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",89, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( dataOffset)( THTensor * tensor , ptrdiff_t linearIndex)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( checkLinearIndex)( int64_t linearIndex , int64_t numel)",123, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( wrapLinearIndex)( int64_t linearIndex , int64_t numel)",87, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( take)( THTensor * r_ , THTensor * src , THLongTensor * index)",80, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( put)( THTensor * tensor , THLongTensor * index , THTensor * src , int accumulate)",99, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexAdd)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",130, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexFill)( THTensor * tensor , int dim , THLongTensor * index , scalar_t val)",133, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( gather)( THTensor * tensor , THTensor * src , int dim , THLongTensor * index)",108, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatter)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",114, 13, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatterAdd)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",114, 13, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatterFill)( THTensor * tensor , int dim , THLongTensor * index , scalar_t val)",113, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( dot)( THTensor * tensor , THTensor * src)",113, 19, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( minall)( THTensor * tensor)",93, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maxall)( THTensor * tensor)",93, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sumall)( THTensor * tensor)",119, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( prodall)( THTensor * tensor)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( add)( THTensor * r_ , THTensor * t , scalar_t value)",143, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sub)( THTensor * r_ , THTensor * t , scalar_t value)",63, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( add_scaled)( THTensor * r_ , THTensor * t , scalar_t value , scalar_t alpha)",86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sub_scaled)( THTensor * r_ , THTensor * t , scalar_t value , scalar_t alpha)",86, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( mul)( THTensor * r_ , THTensor * t , scalar_t value)",143, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( div)( THTensor * r_ , THTensor * t , scalar_t value)",143, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( lshift)( THTensor * r_ , THTensor * t , scalar_t value)",164, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( rshift)( THTensor * r_ , THTensor * t , scalar_t value)",164, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( fmod)( THTensor * r_ , THTensor * t , scalar_t value)",153, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"modulo_wrap( scalar_t a , scalar_t b)",57, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( remainder)( THTensor * r_ , THTensor * t , scalar_t value)",193, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( bitand)( THTensor * r_ , THTensor * t , scalar_t value)",148, 6, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_fill_AVX( double * x , const double c , const ptrdiff_t n)",77, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cdiv_AVX( double * z , const double * x , const double * y , const ptrdiff_t n)",133, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_divs_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",132, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cmul_AVX( double * z , const double * x , const double * y , const ptrdiff_t n)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_muls_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",94, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cadd_AVX( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",111, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_adds_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",94, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_fill_AVX( float * x , const float c , const ptrdiff_t n)",74, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cdiv_AVX( float * z , const float * x , const float * y , const ptrdiff_t n)",129, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_divs_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",128, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cmul_AVX( float * z , const float * x , const float * y , const ptrdiff_t n)",91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_muls_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",90, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cadd_AVX( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",106, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_adds_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",90, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_fill_VSX( double * x , const double c , const ptrdiff_t n)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cadd_VSX( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_adds_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cmul_VSX( double * z , const double * x , const double * y , const ptrdiff_t n)",122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_muls_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cdiv_VSX( double * z , const double * x , const double * y , const ptrdiff_t n)",122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_divs_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",122, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_fill_VSX( float * x , const float c , const ptrdiff_t n)",79, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cadd_VSX( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_adds_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cmul_VSX( float * z , const float * y , const float * x , const ptrdiff_t n)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_muls_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cdiv_VSX( float * z , const float * x , const float * y , const ptrdiff_t n)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_divs_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",121, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_fill( double * x , const double c , const ptrdiff_t n)",78, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_fill( float * x , const float c , const ptrdiff_t n)",75, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cadd( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",113, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cadd( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",107, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_adds( double * y , const double * x , const double c , const ptrdiff_t n)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_adds( float * y , const float * x , const float c , const ptrdiff_t n)",91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cmul( double * z , const double * x , const double * y , const ptrdiff_t n)",97, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cmul( float * z , const float * x , const float * y , const ptrdiff_t n)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_muls( double * y , const double * x , const double c , const ptrdiff_t n)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_muls( float * y , const float * x , const float c , const ptrdiff_t n)",91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cdiv( double * z , const double * x , const double * y , const ptrdiff_t n)",97, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cdiv( float * z , const float * x , const float * y , const ptrdiff_t n)",92, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_divs( double * y , const double * x , const double c , const ptrdiff_t n)",95, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_divs( float * y , const float * x , const float c , const ptrdiff_t n)",91, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"randDouble()",80, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"near( double a , double b)",171, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_fill_VSX()",96, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_fill_VSX()",95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cadd_VSX()",98, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cadd_VSX()",97, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_adds_VSX()",95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_adds_VSX()",94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cmul_VSX()",95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cmul_VSX()",94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_muls_VSX()",95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_muls_VSX()",94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cdiv_VSX()",95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cdiv_VSX()",94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_divs_VSX()",95, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_divs_VSX()",94, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"main()",63, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THDoubleVector_cadd_AVX2( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",112, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_cadd_AVX2( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",107, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"normal_fill_16_AVX2( float * data , const __m256 * two_pi , const __m256 * one , const __m256 * minus_two , const __m256 * mean , const __m256 * stddev)",82, 2, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_normal_fill_AVX2( float * data , const int64_t size , THGenerator * generator , const float mean , const float stddev)",82, 4, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_sigmoid_AVX2( float * y , const float * x , const ptrdiff_t n)",79, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_fill_NEON( float * x , const float c , const ptrdiff_t n)",82, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cmul_NEON( float * z , const float * x , const float * y , const ptrdiff_t n)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_muls_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",98, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cadd_NEON( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",114, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_adds_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",98, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cdiv_NEON( float * z , const float * x , const float * y , const ptrdiff_t n)",99, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_divs_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",98, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"destroyGenerator( THCState * state , THCGenerator * gen)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"createSeed( std :: random_device & rd)",64, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_init( THCState * state , int devices , int current_device)",89, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_shutdown( THCState * state)",56, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_rawGenerator( THCState * state)",74, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_getGenerator( THCState * state)",56, 4, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_generatorStates( THCState * state)",62, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_seed( THCState * state)",41, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_seedAll( THCState * state)",44, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_manualSeed( THCState * state , uint64_t seed)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_manualSeedAll( THCState * state , uint64_t seed)",61, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_initialSeed( THCState * state)",53, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimension( THCState * state , const THCTensor * self)",67, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimensionLegacyNoScalars( THCState * state , const THCTensor * self)",82, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimensionLegacyAll( THCState * state , const THCTensor * self)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_size( THCState * state , const THCTensor * self , int dim)",74, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_sizeLegacyNoScalars( THCState * state , const THCTensor * self , int dim)",89, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_stride( THCState * state , const THCTensor * self , int dim)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_strideLegacyNoScalars( THCState * state , const THCTensor * self , int dim)",91, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_new( THCState * state , caffe2 :: TypeMeta type_meta)",72, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resize( THCState * state , THCTensor * self , at :: IntList size , at :: IntList stride)",96, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resizeAs( THCState * state , THCTensor * self , THCTensor * src)",81, 4, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resizeNd( THCState * state , THCTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",118, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_set( THCState * state , THCTensor * self , THCTensor * src)",69, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_setStorage( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",148, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_setStorageNd( THCState * state , THCTensor * self , THCStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",168, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_squeeze1d( THCState * state , THCTensor * self , THCTensor * src , int dimension)",90, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_unsqueeze1d( THCState * state , THCTensor * self , THCTensor * src , int dimension)",92, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_allContiguous( THCState * state , THCTensor ** inputs , int numInputs)",83, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nElement( THCState * state , const THCTensor * self)",71, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_retain( THCState * state , THCTensor * self)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_free( THCState * state , THCTensor * self)",56, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_getDevice( THCState * state , const THCTensor * tensor)",70, 2, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_allSameDevice( THCState * state , THCTensor ** inputs , int numInputs)",84, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_canUse32BitIndexMath( THCState * state , const THCTensor * t , ptrdiff_t max_elem)",95, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_all32BitIndexable( THCState * state , THCTensor ** inputs , int numInputs)",87, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_preserveReduceDimSemantics( THCState * state , THCTensor * tensor , int in_dims , int64_t dimension , int keepdim)",89, 42, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"compareSizeAndStride( const void * a , const void * b)",57, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_maybeOverlappingIndices( THCState * state , const THCTensor * t)",78, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"BlockSize::BlockSize( size_t size , void * ptr = NULL)",67, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"Block::Block( size_t size , void * ptr , bool allocated)",79, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"BlockComparator( const BlockSize & a , const BlockSize & b)",68, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::HostAllocator()",50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::malloc( void ** ptr , size_t size)",68, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::free( void * ptr)",77, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::recordEvent( void * ptr , at :: cuda :: CUDAStream stream)",66, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::processEvents()",78, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::emptyCache()",73, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::insertEvents( Block & block)",80, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator_recordEvent( void * ptr , at :: cuda :: CUDAStream stream)",88, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator_emptyCache()",42, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostDeleter( void * ptr)",47, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator::allocate( size_t size) const",68, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator::raw_deleter() const",50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"getTHCCachingHostAllocator()",46, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::DeviceStats()",52, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::increaseAllocated( size_t delta)",77, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::decreaseAllocated( size_t delta)",41, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::increaseCached( size_t delta)",68, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::decreaseCached( size_t delta)",38, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"Block::Block( int device , cudaStream_t stream , size_t size , char * ptr = NULL)",75, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"BlockComparator( const Block * a , const Block * b)",60, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"format_size( uint64_t size)",48, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::THCCachingAllocator()",39, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::get_stats_for_device( int device)",50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::malloc( void ** devPtr , size_t size , cudaStream_t stream)",89, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free( void * ptr)",72, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::emptyCache()",73, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::getBaseAllocation( void * ptr , size_t * outSize)",54, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::cacheInfoAux( FreeBlocks & blocks , int dev_id , size_t * total , size_t * largest)",84, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::cacheInfo( int dev_id , size_t * total , size_t * largest)",61, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::recordStream( void * ptr , at :: cuda :: CUDAStream stream)",77, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free_block( Block * block)",61, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::try_merge_blocks( Block * dst , Block * src , FreeBlocks & free_blocks)",73, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::round_size( size_t size)",58, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::cuda_malloc_retry( int device , void ** devPtr , size_t size)",78, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free_cached_blocks( int device)",50, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free_blocks( FreeBlocks & blocks , FreeBlocks :: iterator it , FreeBlocks :: iterator end)",90, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::find_allocated_block( void * ptr)",43, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::insert_events( Block * block)",79, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::process_events()",78, 4, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"CudaCachingDeleter( void * ptr)",44, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"CudaCachingAllocator::allocate( size_t size) const",82, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"CudaCachingAllocator::raw_deleter() const",50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_get( void)",53, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_emptyCache( void)",52, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_cacheInfo( int dev_id , size_t * cachedAndFree , size_t * largestBlock)",102, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_getBaseAllocation( void * ptr , size_t * size)",77, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_recordStream( void * ptr , at :: cuda :: CUDAStream stream)",86, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_getCudaFreeMutex()",59, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"assertValidDevice( int device)",81, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_currentMemoryAllocated( int device)",74, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_maxMemoryAllocated( int device)",78, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_currentMemoryCached( int device)",71, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_maxMemoryCached( int device)",75, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCaching_CUDAIpcDevptr( std :: string handle)",88, 2, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::~THCIpcDeleter()",35, 0, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"deleteTHCIpcDeleter( void * ptr)",43, 2, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::makeDataPtr( std :: shared_ptr<void> basePtr , void * data)",94, 2, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::THCIpcDeleter( std :: shared_ptr<void> basePtr)",60, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_free( THCState * state)",36, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_alloc( void)",58, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaInit( THCState * state)",94, 6, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaShutdown( THCState * state)",72, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getPeerToPeerAccess( THCState * state , int dev , int devToAccess)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentDeviceProperties( THCState * state)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getDeviceProperties( THCState * state , int device)",81, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getRngState( THCState * state)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCudaHostAllocator( THCState * state)",60, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getNumDevices( THCState * state)",44, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getDeviceResourcePtr( THCState * state , int device)",73, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentStreamOnDevice( THCState * state , int device)",78, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentStream( THCState * state)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentBlasHandle( THCState * state)",91, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentSparseHandle( THCState * state)",91, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentDeviceScratchSpaceSize( THCState * state)",81, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCudaCheck( cudaError_t err , const char * file , const int line)",117, 6, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCudaCheckWarn( cudaError_t err , const char * file , const int line)",119, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCublasCheck( cublasStatus_t status , const char * file , const int line)",78, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCusparseCheck( cusparseStatus_t status , const char * file , const int line)",82, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaMalloc( THCState * state , size_t size)",62, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaFree( THCState * state , void * ptr)",51, 2, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaHostAlloc( THCState * state , size_t size)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaHostRecord( THCState * state , void * ptr)",80, 4, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaMemGetInfo( THCState * state , size_t * freeBytes , size_t * totalBytes , size_t * largestBlock)",108, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_resize( THCState * state , THCStorage * self , ptrdiff_t size)",88, 4, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_getDevice( THCState * state , const THCStorage * storage)",71, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_new( THCState * state , caffe2 :: TypeMeta data_type)",61, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( storage)( THCState * state , const THCTensor * self)",72, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( storageOffset)( THCState * state , const THCTensor * self)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimension)( THCState * state , const THCTensor * self)",67, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimensionLegacyNoScalars)( THCState * state , const THCTensor * self)",82, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimensionLegacyAll)( THCState * state , const THCTensor * self)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( size)( THCState * state , const THCTensor * self , int dim)",74, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( sizeLegacyNoScalars)( THCState * state , const THCTensor * self , int dim)",89, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( stride)( THCState * state , const THCTensor * self , int dim)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( strideLegacyNoScalars)( THCState * state , const THCTensor * self , int dim)",91, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( data)( THCState * state , const THCTensor * self)",92, 4, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( new)( THCState * state)",75, 4, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithTensor)( THCState * state , THCTensor * tensor)",82, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , at :: IntList sizes , at :: IntList strides)",143, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage1d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0)",103, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage2d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1)",104, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage3d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2)",120, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage4d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2 , int64_t size3 , int64_t stride3)",103, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize)( THCState * state , at :: IntList size , at :: IntList stride)",90, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize1d)( THCState * state , int64_t size0)",69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize2d)( THCState * state , int64_t size0 , int64_t size1)",84, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize3d)( THCState * state , int64_t size0 , int64_t size1 , int64_t size2)",99, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize4d)( THCState * state , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",114, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newClone)( THCState * state , THCTensor * self)",66, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newContiguous)( THCState * state , THCTensor * self)",71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newSelect)( THCState * state , THCTensor * tensor , int dimension_ , int64_t sliceIndex_)",106, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newNarrow)( THCState * state , THCTensor * tensor , int dimension_ , int64_t firstIndex_ , int64_t size_)",121, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newTranspose)( THCState * state , THCTensor * tensor , int dimension1_ , int dimension2_)",106, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newUnfold)( THCState * state , THCTensor * tensor , int dimension_ , int64_t size_ , int64_t step_)",115, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newView)( THCState * state , THCTensor * tensor , at :: IntList size)",124, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newFoldBatchDim)( THCState * state , THCTensor * input)",87, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize)( THCState * state , THCTensor * self , at :: IntList size , at :: IntList stride)",96, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resizeAs)( THCState * state , THCTensor * self , THCTensor * src)",76, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize0d)( THCState * state , THCTensor * tensor)",62, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize1d)( THCState * state , THCTensor * tensor , int64_t size0)",77, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize2d)( THCState * state , THCTensor * tensor , int64_t size0 , int64_t size1)",92, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize3d)( THCState * state , THCTensor * tensor , int64_t size0 , int64_t size1 , int64_t size2)",107, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize4d)( THCState * state , THCTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",120, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize5d)( THCState * state , THCTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3 , int64_t size4)",135, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set)( THCState * state , THCTensor * self , THCTensor * src)",71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",152, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage1d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_)",112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage2d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_)",112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage3d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_)",112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage4d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_ , int64_t size3_ , int64_t stride3_)",112, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( narrow)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t firstIndex , int64_t size)",123, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( select)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t sliceIndex)",109, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( transpose)( THCState * state , THCTensor * self , THCTensor * src , int dimension1 , int dimension2)",111, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( unfold)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t size , int64_t step)",117, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( squeeze)( THCState * state , THCTensor * self , THCTensor * src)",75, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( squeeze1d)( THCState * state , THCTensor * self , THCTensor * src , int dimension)",92, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( unsqueeze1d)( THCState * state , THCTensor * self , THCTensor * src , int dimension)",94, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isContiguous)( THCState * state , const THCTensor * self)",69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isSetTo)( THCState * state , const THCTensor * self , const THCTensor * src)",86, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isSameSizeAs)( THCState * state , const THCTensor * self , const THCTensor * src)",91, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nElement)( THCState * state , const THCTensor * self)",71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( retain)( THCState * state , THCTensor * self)",58, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( free)( THCState * state , THCTensor * self)",56, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( freeCopyTo)( THCState * state , THCTensor * self , THCTensor * dst)",78, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorageNd)( THCState * state , THCTensor * self , THCStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",170, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resizeNd)( THCState * state , THCTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",120, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set0d)( THCState * state , THCTensor * tensor , scalar_t value)",92, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get0d)( THCState * state , const THCTensor * tensor)",95, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set1d)( THCState * state , THCTensor * tensor , int64_t x0 , scalar_t value)",137, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get1d)( THCState * state , const THCTensor * tensor , int64_t x0)",137, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set2d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , scalar_t value)",134, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get2d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1)",134, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set3d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , scalar_t value)",155, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get3d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2)",155, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set4d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3 , scalar_t value)",187, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get4d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3)",187, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( checkGPU)( THCState * state , unsigned int nTensors , ...)",70, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( sizeDesc)( THCState * state , const THCTensor * tensor)",77, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorageCopy.cpp,"THCStorage_( copyCPU)( THCState * state , THCStorage * self , struct THStorage * src)",84, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorageCopy.cpp,"THStorage_( copyCuda)( THCState * state , THStorage * self , struct THCStorage * src)",84, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( data)( THCState * state , const THCStorage * self)",69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( size)( THCState * state , const THCStorage * self)",69, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( elementSize)( THCState * state)",46, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( set)( THCState * state , THCStorage * self , ptrdiff_t index , scalar_t value)",96, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( get)( THCState * state , const THCStorage * self , ptrdiff_t index)",96, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( new)( THCState * state)",61, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize)( THCState * state , ptrdiff_t size)",70, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithAllocator)( THCState * state , ptrdiff_t size , at :: Allocator * allocator)",75, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize1)( THCState * state , scalar_t data0)",71, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize2)( THCState * state , scalar_t data0 , scalar_t data1)",87, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize3)( THCState * state , scalar_t data0 , scalar_t data1 , scalar_t data2)",103, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize4)( THCState * state , scalar_t data0 , scalar_t data1 , scalar_t data2 , scalar_t data3)",119, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithMapping)( THCState * state , const char * fileName , ptrdiff_t size , int isShared)",109, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithDataAndAllocator)( THCState * state , at :: DataPtr && data , ptrdiff_t size , at :: Allocator * allocator)",61, 2, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( retain)( THCState * state , THCStorage * self)",60, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( free)( THCState * state , THCStorage * self)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::sparseTensorIdToDeviceType( TensorTypeId type_id)",90, 6, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::SparseTensorImpl( at :: TensorTypeId type_id , const caffe2 :: TypeMeta & data_type)",130, 4, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::sizes() const",42, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::strides() const",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::is_contiguous() const",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::stride( int64_t d) const",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::resize_dim( int64_t ndim)",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_size( int64_t dim , int64_t new_size)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_stride( int64_t dim , int64_t new_stride)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_storage_offset( int64_t storage_offset)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::dim() const",40, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::maybe_zero_dim( bool condition_when_zero_dim)",93, 11, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::storage() const",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::storage_offset() const",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_indices_and_values_unsafe( const Tensor & indices , const Tensor & values)",185, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGeneral.cpp,"at::set_num_threads( int num_threads_)",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGeneral.cpp,"at::get_num_threads()",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::errorHandler( const char * msg , void * data)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::argErrorHandler( int arg , const char * msg , void * data)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::Context()",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::globalContext()",33, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::userEnabledCuDNN() const",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setUserEnabledCuDNN( bool e)",44, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::deterministicCuDNN() const",43, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setDeterministicCuDNN( bool b)",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::benchmarkCuDNN() const",39, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setBenchmarkCuDNN( bool b)",42, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasMKL() const",31, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasLAPACK() const",34, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setFlushDenormal( bool on)",42, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( TensorOptions options)",94, 12, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( const TensorImpl * impl)",80, 12, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( const Tensor & t)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getLegacyTHDispatcher( TensorOptions options)",71, 12, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getLegacyTHDispatcher( const TensorImpl * impl)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getCPUAllocator()",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::LegacyDeviceTypeInit( LegacyDeviceTypeInitArgs)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initCPU() const",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initCUDA() const",36, 4, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initHIP() const",35, 4, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initComplex() const",39, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorGeometry.cpp,"at::TensorGeometry::is_contiguous() const",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/Utils.cpp,"at::_crash_if_asan( int arg)",30, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::UndefinedType()",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::scalarType() const",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::typeMeta() const",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::backend() const",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::allocator() const",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::getDeviceFromPtr( void *) const",62, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",118, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::unsafeStorageFromTH( void * th_pointer , bool retain) const",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::storageWithAllocator( int64_t size , Allocator * allocator) const",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::unsafeTensorFromTH( void * th_pointer , bool retain) const",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::generator() const",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toString() const",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::ID() const",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::elementSizeInBytes() const",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toBackend( Backend b) const",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toScalarType( ScalarType s) const",83, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::operator < <( std :: ostream & out , TensorGeometryArg t)",69, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDim( CheckedFrom c , const TensorGeometryArg & t , int64_t dim)",78, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDimRange( CheckedFrom c , const TensorGeometryArg & t , int64_t dim_start , int64_t dim_end)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkContiguous( CheckedFrom c , const TensorGeometryArg & t)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllContiguous( CheckedFrom c , at :: ArrayRef<TensorArg> ts)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSize( CheckedFrom c , const TensorGeometryArg & t , IntList sizes)",80, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSize( CheckedFrom c , const TensorGeometryArg & t , int64_t dim , int64_t size)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSame( CheckedFrom c , ArrayRef<TensorArg> tensors , void(*fn)(CheckedFrom,const TensorArg&,const TensorArg&))",124, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameSize( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameSize( CheckedFrom c , ArrayRef<TensorArg> tensors)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkNumel( CheckedFrom c , const TensorGeometryArg & t , int64_t numel)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameNumel( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameNumel( CheckedFrom c , ArrayRef<TensorArg> tensors)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameGPU( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",84, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameGPU( CheckedFrom c , ArrayRef<TensorArg> tensors)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameType( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkScalarType( CheckedFrom c , const TensorArg & t , ScalarType ty)",79, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkScalarTypes( CheckedFrom c , const TensorArg & t , at :: ArrayRef<ScalarType> l)",77, 6, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameType( CheckedFrom c , ArrayRef<TensorArg> tensors)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameDim( CheckedFrom c , const TensorGeometryArg & t1 , const TensorGeometryArg & t2)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDefined( CheckedFrom c , const TensorArg & t)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllDefined( CheckedFrom c , ArrayRef<TensorArg> ts)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkBackend( CheckedFrom c , const Tensor & t , Backend backend)",81, 4, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkBackend( CheckedFrom c , ArrayRef<Tensor> tensors , at :: Backend backend)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::maybe_data_ptr( const Tensor & tensor)",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::maybe_data_ptr( const TensorArg & tensor)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::geometry_is_contiguous( IntList sizes , IntList strides)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::CPUGenerator( Context * context_)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::~CPUGenerator()",33, 4, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::copy( const Generator & from)",69, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::free()",37, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::seed()",35, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::initialSeed()",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::manualSeed( uint64_t seed)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::manualSeedAll( uint64_t seed)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::unsafeGetTH()",37, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::allocator() const",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::getDeviceFromPtr( void * data) const",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::generator() const",77, 2, 0
repos/cpp/pytorch/aten/src/ATen/LegacyTHDispatch.cpp,"at::globalLegacyTHDispatch()",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/ExpandUtils.cpp,"at::infer_size( IntList a , IntList b)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/ExpandUtils.cpp,"at::inferExpandGeometry( IntList tensor_sizes , IntList tensor_strides , IntList sizes)",81, 32, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getDLDataType( const Type & type)",74, 6, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getDLContext( const Type & type , const int64_t & device_id)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getATenDeviceType( const DLContext & ctx)",93, 6, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::toScalarType( const DLDataType & dtype)",91, 10, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::deleter( DLManagedTensor * arg)",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::toDLPack( const Tensor & src)",86, 2, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::fromDLPack( const DLManagedTensor * src)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/LegacyTHDispatcherDerived.cpp,"at::LegacyTHDispatcher( $ { Backend } TensorId())",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/RegisterCPU.cpp,"at::register_cpu_types( Context * context)",89, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",103, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::scalarType() const",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::typeMeta() const",52, 4, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::backend() const",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::toString() const",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::ID() const",29, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::elementSizeInBytes() const",45, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::copy_( Tensor & self , const Tensor & src , bool non_blocking) const",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::copy( const Tensor & src , bool non_blocking , optional<Device> to_device) const",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::backward( Tensor & self , c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::set_data( Tensor & self , Tensor new_data) const",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::toBackend( Backend b) const",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::toScalarType( ScalarType s) const",62, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::defaultStrides( IntList sizes)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::computeStorageSize( IntList sizes , IntList strides)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorFromBlob( void * data , IntList sizes , const std :: function<void(void*)> & deleter) const",115, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorFromBlob( void * data , IntList sizes , IntList strides , const std :: function<void(void*)> & deleter) const",132, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorWithAllocator( IntList sizes , Allocator * allocator) const",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorWithAllocator( IntList sizes , IntList strides , Allocator * allocator) const",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",116, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::storageWithAllocator( int64_t size , Allocator * allocator) const",86, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::unsafeTensorFromTH( void * th_pointer , bool retain) const",121, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::unsafeStorageFromTH( void * th_pointer , bool retain) const",99, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/RegisterCUDA.cpp,"at::register_cuda_types( Context * context)",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",103, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::scalarType() const",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::typeMeta() const",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::backend() const",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::toString() const",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::ID() const",29, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::elementSizeInBytes() const",45, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Types.cpp,"at::native::getCudnnDataType( const at :: Tensor & tensor)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Types.cpp,"at::native::cudnn_version()",26, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::getDataType( const at :: Type & t)",93, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::getDataType( const at :: Tensor & t)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::set( const at :: Tensor & t , size_t pad)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::set( cudnnDataType_t datatype , IntList t_sizes , IntList t_strides , size_t pad)",103, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::cudnnTypeToString( cudnnDataType_t dtype)",70, 6, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::operator < <( std :: ostream & out , const TensorDescriptor & d)",87, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::print()",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::FilterDescriptor::set( const at :: Tensor & t , int64_t pad)",93, 4, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::Handle()",42, 4, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::~Handle()",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::getCudnnHandle()",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/VariableHooksInterface.cpp,"at::detail::getVariableHooks()",89, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::ivalue::ConstantString::create( std :: string str_)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::printList( std :: ostream & out , const List & v , const std :: string start , const std :: string finish)",87, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::operator < <( std :: ostream & out , const IValue & v)",72, 8, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::IValue::dump() const",30, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"c10::operator < <( std :: ostream & out , Backend b)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::defaultfloat( std :: ios_base & __base)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::FormatGuard::FormatGuard( std :: ostream & out)",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::FormatGuard::~FormatGuard()",24, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::operator < <( std :: ostream & out , const Type & t)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printFormat( std :: ostream & stream , const Tensor & self)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printIndent( std :: ostream & stream , int64_t indent)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::printScale( std :: ostream & stream , double scale)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printMatrix( std :: ostream & stream , const Tensor & self , int64_t linesize , int64_t indent)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printTensor( std :: ostream & stream , Tensor & self , int64_t linesize)",73, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::print( std :: ostream & stream , const Tensor & tensor_ , int64_t linesize)",104, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::print() const",81, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::toString() const",40, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::domain_prefix()",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::symbol( const std :: string & s)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::string( Symbol sym)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::ns( Symbol sym)",49, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::_symbol( const std :: string & s)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::customString( Symbol sym)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::globalStrings()",43, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::fromQualString( const std :: string & s)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toUnqualString() const",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toQualString() const",46, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toDisplayString() const",71, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::ns() const",36, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::domainString() const",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::fromDomainAndUnqualString( const std :: string & d , const std :: string & s)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::ThreadPool( std :: size_t pool_size , int numa_node_id)",75, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::~ThreadPool()",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::size() const",34, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::numAvailable() const",42, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::inThreadPool() const",57, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::run( const std :: function<void()> & func)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::waitWorkComplete()",45, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::main_loop( std :: size_t index)",68, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::global_work_queue()",36, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::operator < <( std :: ostream & out , const Type & t)",77, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::DynamicType::get()",45, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::UndefinedTensorType::get()",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::NumberType::get()",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::IntType::get()",41, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::FloatType::get()",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::BoolType::get()",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::NoneType::get()",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::GeneratorType::get()",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::StringType::get()",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::DeviceObjType::get()",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::OptionalType::ofTensor()",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofTensors()",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofInts()",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofFloats()",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofBools()",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::inferTypeFrom( const IValue & value)",80, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::unifyTypes( const TypePtr & t1 , const TypePtr & t2)",116, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::matchTypeVariables( TypePtr formal , TypePtr actual , TypeEnv & type_env)",109, 6, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::evalTypeVariables( TypePtr type , std :: unordered_map<std::string,TypePtr> & type_env)",112, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::typeKindToString( TypeKind kind)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::Type::isSubtypeOf( const TypePtr rhs) const",54, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/register_symbols.cpp,"c10::InternedStrings::InternedStrings()",70, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyDeviceTypeInit.cpp,"at::getLegacyDeviceTypeInit()",122, 4, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::globalLegacyTypeDispatch()",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/TensorImpl_test.cpp,"TEST( TensorImplTest , Caffe2Constructor)",42, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Range.cpp,"at::operator < <( std :: ostream & out , const Range & range)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::allocator() const",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::getDeviceFromPtr( void * data) const",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::generator() const",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::CUDAGenerator( Context * context_)",49, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::~CUDAGenerator()",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::copy( const Generator & from)",69, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::free()",46, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::seed()",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::initialSeed()",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::manualSeed( uint64_t seed)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::manualSeedAll( uint64_t seed)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::unsafeGetTH()",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/PinnedMemoryAllocator.cpp,"at::cuda::getPinnedMemoryAllocator()",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::warp_size()",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentDeviceProperties()",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getDeviceProperties( int64_t device)",87, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCUDADeviceAllocator()",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentCUDASparseHandle()",77, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentCUDABlasHandle()",75, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::initCUDA() const",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::initCUDAGenerator( Context * context) const",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasCUDA() const",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasMAGMA() const",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasCuDNN() const",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::current_device() const",44, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::getPinnedMemoryAllocator() const",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::registerCUDATypes( Context * context) const",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::compiledWithCuDNN() const",44, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::compiledWithMIOpen() const",45, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::supportsDilatedConvolutionWithCuDNN() const",74, 6, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::versionCuDNN() const",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::batchnormMinEpsilonCuDNN() const",81, 6, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTGetPlanCacheMaxSize() const",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTSetPlanCacheMaxSize( int64_t max_size) const",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTGetPlanCacheSize() const",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTClearPlanCache() const",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::getNumGPUs() const",80, 8, 0
repos/cpp/pytorch/aten/src/ATen/detail/ComplexHooksInterface.cpp,"at::detail::getComplexHooks()",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/detail/HIPHooksInterface.cpp,"at::detail::getHIPHooks()",72, 4, 0
repos/cpp/pytorch/aten/src/ATen/detail/CUDAHooksInterface.cpp,"at::detail::getCUDAHooks()",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestEmptyTensor( Type & T)",39, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2Basic( Type & T)",75, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2WithScalar( Type & T)",77, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2OldFallback( Type & T)",36, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2MismatchedSizes( Type & T)",40, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3Basic( Type & T)",60, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3WithScalar( Type & T)",74, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3OldFallback( Type & T)",37, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3MismatchedSizes( Type & T)",40, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2Basic( Type & T)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2WithScalar( Type & T)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2ExpandError( Type & T)",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3Basic( Type & T)",67, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3WithScalar( Type & T)",73, 22, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3ExpandError( Type & T)",38, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimBasic( Type & T)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimWithScalar( Type & T)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimWithMismatchedSizes( Type & T)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TEST( BroadcastTest , Broadcast)",41, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"Foo::apply( Tensor a , Tensor b)",67, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"Foo<Half>::apply( Tensor a , Tensor b)",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"test_overflow()",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"TEST( TestScalar , TestScalar)",72, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakPointerGetsInvalidated)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakPointerLock)",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakUpdatesRefcountsTest)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"fill_tensor( int64_t scalar , Tensor & t_)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"test( Type & type , IntList shape , int64_t a = 0 , int64_t b = 1)",81, 8, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Contiguous2D)",38, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Small2D)",32, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _2D)",32, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _3D)",33, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Medium3D)",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _10D)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestSimpleCase( Type & T)",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestExpressionSpecification( Type & T)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestEmptyTensor( Type & T)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestScalarVs1Dim1Size( Type & T)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TEST( TestWrapdim , TestWrapdim)",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_cudnn_test.cpp,"TEST( CUDNNTest , CUDNNTestCUDA)",96, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/atest.cpp,"trace()",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/atest.cpp,"TEST( atest , atest)",81, 10, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , TestParallel)",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , NestedParallel)",84, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , Exceptions)",67, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"requireEqualTensorList( TensorList t1 , TensorList t2)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestSplit( Type & T , Tensor & t)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestChunk( Type & T , Tensor & t)",51, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestStack( Type & T , Tensor & t)",74, 8, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestSize( Type & T , Tensor & t)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestMatmul( Type & T , Tensor & t , Type & AccT)",81, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestStandardGammaGrad( Type & T , Tensor & t)",67, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestWhere( Type & T , Tensor & t)",63, 6, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"test( Type & T , Type & AccT)",33, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TEST( TestNative , NativeTestCPU)",35, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TEST( TestNative , NativeTestGPU)",39, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"require_equal_size_dim( const Tensor & lhs , const Tensor & rhs)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"should_expand( const IntList & from_size , const IntList & to_size)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"test( Type & T)",83, 8, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"TEST( TestScalarTensor , TestScalarTensorCPU)",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"TEST( TestScalarTensor , TestScalarTensorCUDA)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/tbb_init_test.cpp,"test( int given_num_threads)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/tbb_init_test.cpp,"main()",28, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CopyAndMoveTest)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , GetAndSetTest)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"thread_fun( at :: optional<at::cuda::CUDAStream> & cur_thread_stream)",73, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , MultithreadGetAndSetTest)",74, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CUDAGuardTest)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , StreamPoolTest)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , MultiGPUTest)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CUDAEventSyncTest)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CrossDeviceTest)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/verify_api_visibility.cpp,"main()",22, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Arithmetic)",33, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Comparisions)",31, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Cast)",38, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Construction)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"to_string( const Half & h)",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Half2String)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , HalfNumericLimits)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , CommonMath)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"makeRandomNumber()",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"testCudaRNGMultithread()",45, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"TEST( Cuda_RNGTest , MultithreadRNGTest)",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestResize( Type & type)",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestOnesAndDot( Type & type)",69, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSort( Type & type)",75, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestRandperm( Type & type)",62, 4, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"SendContext()",75, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAdd( Type & type)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestLoadsOfAdds( Type & type)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestLoadOfAddsWithCopy( Type & type)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIsContiguous( Type & type)",36, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestPermute( Type & type)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestMm( Type & type)",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSqueeze( Type & type)",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestCopy( Type & type)",34, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestCopyBroadcasting( Type & type)",40, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAbsValue( Type & type)",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAddingAValueWithScalar( Type & type)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSelect( Type & type)",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestZeroDim( Type & type)",71, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestTensorFromTH()",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestToCFloat()",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestToString()",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingByScalar()",71, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingByZerodimTensor()",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingMixedDevice( Type & type)",43, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestDispatch()",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"test( Type & type)",36, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TEST( BasicTest , BasicTestCPU)",32, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TEST( BasicTest , BasicTestCUDA)",33, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/undefined_tensor_test.cpp,"TEST( TestUndefined , UndefinedTest)",95, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/dlconvertor_test.cpp,"TEST( TestDlconvertor , TestDlconvertor)",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , Contiguous2D)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , Contiguous3D)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , PartialCollapse3D)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , StridedCollapse2D)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , PartialStridedCollapse4D)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , CollapsesZerosAndOnes)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , CollapseToPointTensor)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , ExcludingInContiguous4D)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , RovingExclusion)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , InvalidExclusion)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/test_install/main.cpp,"main()",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Types.cpp,"at::native::getMiopenDataType( const at :: Tensor & tensor)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Types.cpp,"at::native::miopen_version()",87, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::getDataType( const at :: Type & t)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::getDataType( const at :: Tensor & t)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::set( const at :: Tensor & t , size_t pad)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::set( miopenDataType_t datatype , IntList t_sizes , IntList t_strides , size_t pad)",104, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::miopenTypeToString( miopenDataType_t dtype)",70, 6, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::operator < <( std :: ostream & out , const TensorDescriptor & d)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::print()",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::FilterDescriptor::set( const at :: Tensor & t , int64_t pad)",95, 4, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::Handle::Handle()",41, 4, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::Handle::~Handle()",29, 6, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::getMiopenHandle()",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/cpu/FlushDenormal.cpp,"at::cpu::set_flush_denormal( bool on)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_out( Tensor & result , const Tensor & self , const Tensor & other , Scalar alpha)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add( const Tensor & self , const Tensor & other , Scalar alpha)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_( Tensor & self , const Tensor & other , Scalar alpha)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_out( Tensor & result , const Tensor & self , const Tensor & other)",77, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div( const Tensor & self , const Tensor & other)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_( Tensor & self , const Tensor & other)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_out( Tensor & result , const Tensor & self , const Tensor & other)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul( const Tensor & self , const Tensor & other)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_( Tensor & self , const Tensor & other)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_out( Tensor & result , const Tensor & self , const Tensor & other , Scalar alpha)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub( const Tensor & self , const Tensor & other , Scalar alpha)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_( Tensor & self , const Tensor & other , Scalar alpha)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::rsub( const Tensor & self , const Tensor & other , Scalar alpha)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::wrapped_scalar_tensor( Scalar scalar)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add( const Tensor & self , Scalar other , Scalar alpha)",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_( Tensor & self , Scalar other , Scalar alpha)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div( const Tensor & self , Scalar other)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_( Tensor & self , Scalar other)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul( const Tensor & self , Scalar other)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_( Tensor & self , Scalar other)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub( const Tensor & self , Scalar other , Scalar alpha)",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_( Tensor & self , Scalar other , Scalar alpha)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::rsub( const Tensor & self , Scalar other , Scalar alpha)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Resize.cpp,"at::native::resize_cpu_( Tensor & self , IntList size)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::get_target_prime( target_t * target , int64_t offset , int64_t stride , int64_t idx , int64_t BLANK)",119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_cpu_template( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK)",161, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_backward_cpu_template( const Tensor & grad_out , const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , const Tensor & neg_log_likelihood , const Tensor & log_alpha , int64_t BLANK)",157, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_cpu( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK)",152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_backward_cpu( const Tensor & grad , const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , const Tensor & neg_log_likelihood , const Tensor & log_alpha , int64_t BLANK)",151, 0, 1
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK , int64_t reduction)",147, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss( const Tensor & log_probs , const Tensor & targets , const Tensor & input_lengths , const Tensor & target_lengths , int64_t BLANK , int64_t reduction)",159, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::relu( const Tensor & self)",36, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::relu_( Tensor & self)",37, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::selu( const Tensor & self)",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::selu_( Tensor & self)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::celu( const Tensor & self , Scalar alpha)",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::celu_( Tensor & self , Scalar alpha)",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::rrelu( const Tensor & self , Scalar lower , Scalar upper , bool training , Generator * generator)",104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::rrelu_( Tensor & self , Scalar lower , Scalar upper , bool training , Generator * generator)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_out( optional<Tensor> opt_result , const Tensor & self , Scalar threshold , Scalar value , const Tensor & other)",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold( const Tensor & self , Scalar threshold , Scalar value)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_( Tensor & self , Scalar threshold , Scalar value)",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_out( Tensor & result , const Tensor & self , Scalar threshold , Scalar value)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_backward( const Tensor & grad , const Tensor & self , Scalar threshold)",86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_kernel_share_weights( Tensor & result , const Tensor & input , const Tensor & weight)",74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_kernel_multi_weights( Tensor & result , const Tensor & input , const Tensor & weight , int64_t input_dim0_size , int64_t channel_size , int64_t input_stride0 , int64_t input_stride1)",78, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu( const Tensor & self , const Tensor & weight_)",116, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_backward_kernel_share_weights( const Tensor & input , const Tensor & weight , const Tensor & grad_out , Tensor & input_grad , Tensor & weight_grad)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_backward_kernel_multi_weights( const Tensor & input , const Tensor & weight , const Tensor & grad_out , Tensor & input_grad , Tensor & weight_grad_collector , int64_t input_dim0_size , int64_t channel_size , int64_t input_stride0 , int64_t input_stride1)",85, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_backward_cpu( const Tensor & grad_out_ , const Tensor & self , const Tensor & weight_)",116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::hardshrink_cpu( const Tensor & self , Scalar lambd)",103, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::hardshrink_backward_cpu( const Tensor & grad , const Tensor & self , Scalar lambd)",103, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::check_dims_match_num_input_features( const char * arg_name , int64_t expected , int64_t actual)",100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::repeat_if_defined( const Tensor & t , int64_t repeat)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::conditional_accessor_1d( const Tensor & t)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu_template( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool train , double momentum , double eps)",129, 31, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_backward_cpu_template( const Tensor & grad_out_ , const Tensor & input , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_invstd , bool train , double eps , std :: array<bool,3> grad_input_mask)",175, 68, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double momentum , double eps , bool cudnn_enabled)",97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::instance_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool use_input_stats , double momentum , double eps , bool cudnn_enabled)",99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::layer_norm( const Tensor & input , IntList normalized_shape , const Tensor & weight , const Tensor & bias , double eps , bool cudnn_enabled)",85, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::group_norm( const Tensor & input , int64_t num_groups , const Tensor & weight , const Tensor & bias , double eps , bool cudnn_enabled)",93, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu( const Tensor & self , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool train , double momentum , double eps)",117, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_backward_cpu( const Tensor & grad_out , const Tensor & self , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_invstd , bool train , double eps , std :: array<bool,3> grad_input_mask)",166, 59, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::data_ptr( const Tensor & self)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , Storage source)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , Storage source , int64_t storage_offset , IntList size , IntList stride)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , const Tensor & source)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self)",41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::is_set_to( const Tensor & self , const Tensor & tensor)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_fill_( Tensor & self , const Tensor & mask , Scalar value)",73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_fill_( Tensor & self , const Tensor & mask , const Tensor & value)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_scatter_( Tensor & self , const Tensor & mask , const Tensor & source)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::view( const Tensor & self , IntList size)",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::put_( Tensor & self , const Tensor & index , const Tensor & source , bool accumulate)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_add_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_fill_( Tensor & self , int64_t dim , const Tensor & index , Scalar value)",86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_fill_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & value)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & src)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_( Tensor & self , int64_t dim , const Tensor & index , Scalar value)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_add_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & src)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_( Tensor & self , Scalar other)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_( Tensor & self , const Tensor & other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_( Tensor & self , Scalar other)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_( Tensor & self , const Tensor & other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_( Tensor & self , Scalar other)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_( Tensor & self , const Tensor & other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_( Tensor & self , Scalar other)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_( Tensor & self , const Tensor & other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_( Tensor & self , Scalar other)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_( Tensor & self , const Tensor & other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_( Tensor & self , Scalar other)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_( Tensor & self , const Tensor & other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma_( Tensor & self)",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2_( Tensor & self , const Tensor & other)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::tril_( Tensor & self , int64_t diagonal)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::triu_( Tensor & self , int64_t diagonal)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma_( Tensor & self)",45, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma_( Tensor & self , int64_t n)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv_( Tensor & self)",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac_( Tensor & self)",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm_( Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal_( Tensor & self)",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg_( Tensor & self)",41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_( Tensor & self , Scalar exponent)",51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_( Tensor & self , const Tensor & exponent)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lerp_( Tensor & self , const Tensor & end , Scalar weight)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign_( Tensor & self)",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_( Tensor & self , Scalar other)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_( Tensor & self , const Tensor & other)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_( Tensor & self , Scalar other)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_( Tensor & self , const Tensor & other)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm_out( Tensor & result , const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm( const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul_( Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv_( Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , int64_t from , int64_t to , Generator * generator)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , int64_t to , Generator * generator)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , Generator * generator)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::uniform_( Tensor & self , double from , double to , Generator * generator)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_( Tensor & self , double mean , double std , Generator * generator)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cauchy_( Tensor & self , double median , double sigma , Generator * generator)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::log_normal_( Tensor & self , double mean , double std , Generator * generator)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::exponential_( Tensor & self , double lambd , Generator * generator)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geometric_( Tensor & self , double p , Generator * generator)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::diag_out( Tensor & result , const Tensor & self , int64_t diagonal)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::diag( const Tensor & self , int64_t diagonal)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cross_out( Tensor & result , const Tensor & self , const Tensor & other , int64_t dim)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cross( const Tensor & self , const Tensor & other , int64_t dim)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::triu_out( Tensor & result , const Tensor & self , int64_t diagonal)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::triu( const Tensor & self , int64_t diagonal)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::tril_out( Tensor & result , const Tensor & self , int64_t diagonal)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::tril( const Tensor & self , int64_t diagonal)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trace( const Tensor & self)",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_out( Tensor & result , const Tensor & self , Scalar other)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne( const Tensor & self , Scalar other)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_out( Tensor & result , const Tensor & self , const Tensor & other)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne( const Tensor & self , const Tensor & other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_out( Tensor & result , const Tensor & self , Scalar other)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq( const Tensor & self , Scalar other)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_out( Tensor & result , const Tensor & self , const Tensor & other)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq( const Tensor & self , const Tensor & other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_out( Tensor & result , const Tensor & self , Scalar other)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge( const Tensor & self , Scalar other)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_out( Tensor & result , const Tensor & self , const Tensor & other)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge( const Tensor & self , const Tensor & other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_out( Tensor & result , const Tensor & self , Scalar other)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le( const Tensor & self , Scalar other)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_out( Tensor & result , const Tensor & self , const Tensor & other)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le( const Tensor & self , const Tensor & other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_out( Tensor & result , const Tensor & self , Scalar other)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt( const Tensor & self , Scalar other)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_out( Tensor & result , const Tensor & self , const Tensor & other)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt( const Tensor & self , const Tensor & other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_out( Tensor & result , const Tensor & self , Scalar other)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt( const Tensor & self , Scalar other)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_out( Tensor & result , const Tensor & self , const Tensor & other)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt( const Tensor & self , const Tensor & other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::take_out( Tensor & result , const Tensor & self , const Tensor & index)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::take( const Tensor & self , const Tensor & index)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_select_out( Tensor & result , const Tensor & self , int64_t dim , const Tensor & index)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_select( const Tensor & self , int64_t dim , const Tensor & index)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_select_out( Tensor & result , const Tensor & self , const Tensor & mask)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_select( const Tensor & self , const Tensor & mask)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::nonzero_out( Tensor & result , const Tensor & self)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::nonzero( const Tensor & self)",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gather_out( Tensor & result , const Tensor & self , int64_t dim , const Tensor & index)",95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gather( const Tensor & self , int64_t dim , const Tensor & index)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul_out( Tensor & result , const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",123, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul( const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv_out( Tensor & result , const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",123, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv( const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gels_out( Tensor & X , Tensor & qr , const Tensor & self , const Tensor & A)",105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gels( const Tensor & self , const Tensor & A)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trtrs_out( Tensor & X , Tensor & M , const Tensor & self , const Tensor & A , bool upper , bool transpose , bool unitriangular)",153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trtrs( const Tensor & self , const Tensor & A , bool upper , bool transpose , bool unitriangular)",121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::symeig_out( Tensor & e , Tensor & V , const Tensor & self , bool eigenvectors , bool upper)",119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::symeig( const Tensor & self , bool eigenvectors , bool upper)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eig_out( Tensor & e , Tensor & v , const Tensor & self , bool eigenvectors)",104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eig( const Tensor & self , bool eigenvectors)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::svd_out( Tensor & U , Tensor & S , Tensor & V , const Tensor & self , bool some , bool compute_uv)",134, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::svd( const Tensor & self , bool some , bool compute_uv)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::potri_out( Tensor & result , const Tensor & self , bool upper)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::potri( const Tensor & self , bool upper)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pstrf_out( Tensor & u , Tensor & piv , const Tensor & self , bool upper , Scalar tol)",113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pstrf( const Tensor & self , bool upper , Scalar tol)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::qr_out( Tensor & Q , Tensor & R , const Tensor & self)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::qr( const Tensor & self)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geqrf_out( Tensor & result0 , Tensor & result1 , const Tensor & self)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geqrf( const Tensor & self)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::orgqr_out( Tensor & result , const Tensor & self , const Tensor & input2)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::orgqr( const Tensor & self , const Tensor & input2)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ormqr_out( Tensor & result , const Tensor & self , const Tensor & input2 , const Tensor & input3 , bool left , bool transpose)",132, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ormqr( const Tensor & self , const Tensor & input2 , const Tensor & input3 , bool left , bool transpose)",109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact_out( Tensor & A_LU , Tensor & pivots , const Tensor & self , bool pivot)",110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact( const Tensor & self , bool pivot)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact_with_info_out( Tensor & A_LU , Tensor & pivots , Tensor & info , const Tensor & self , bool pivot)",144, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact_with_info( const Tensor & self , bool pivot)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrisolve_out( Tensor & result , const Tensor & self , const Tensor & LU_data , const Tensor & LU_pivots)",113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrisolve( const Tensor & self , const Tensor & LU_data , const Tensor & LU_pivots)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::multinomial_out( Tensor & result , const Tensor & self , int64_t num_samples , bool replacement , Generator * generator)",127, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::multinomial( const Tensor & self , int64_t num_samples , bool replacement , Generator * generator)",104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma_out( Tensor & result , const Tensor & self)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma( const Tensor & self)",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma_out( Tensor & result , const Tensor & self)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma( const Tensor & self)",44, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma_out( Tensor & result , int64_t n , const Tensor & self)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma( int64_t n , const Tensor & self)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv_out( Tensor & result , const Tensor & self)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv( const Tensor & self)",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac_out( Tensor & result , const Tensor & self)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac( const Tensor & self)",41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::dist( const Tensor & self , const Tensor & other , Scalar p)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal_out( Tensor & result , const Tensor & self)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal( const Tensor & self)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg_out( Tensor & result , const Tensor & self)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg( const Tensor & self)",40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2_out( Tensor & result , const Tensor & self , const Tensor & other)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2( const Tensor & self , const Tensor & other)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lerp_out( Tensor & result , const Tensor & self , const Tensor & end , Scalar weight)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lerp( const Tensor & self , const Tensor & end , Scalar weight)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::histc_out( Tensor & result , const Tensor & self , int64_t bins , Scalar min , Scalar max)",97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::histc( const Tensor & self , int64_t bins , Scalar min , Scalar max)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign_out( Tensor & result , const Tensor & self)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign( const Tensor & self)",41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_out( Tensor & result , const Tensor & self , Scalar other)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod( const Tensor & self , Scalar other)",49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_out( Tensor & result , const Tensor & self , const Tensor & other)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod( const Tensor & self , const Tensor & other)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_out( Tensor & result , const Tensor & self , Scalar other)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder( const Tensor & self , Scalar other)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_out( Tensor & result , const Tensor & self , const Tensor & other)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder( const Tensor & self , const Tensor & other)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min_out( Tensor & result , const Tensor & self , const Tensor & other)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min( const Tensor & self , const Tensor & other)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min( const Tensor & self)",40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max_out( Tensor & result , const Tensor & self , const Tensor & other)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max( const Tensor & self , const Tensor & other)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max( const Tensor & self)",40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::median( const Tensor & self)",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sort_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool descending)",127, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sort( const Tensor & self , int64_t dim , bool descending)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::topk_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t k , int64_t dim , bool largest , bool sorted)",148, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::topk( const Tensor & self , int64_t k , int64_t dim , bool largest , bool sorted)",105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::all( const Tensor & self)",40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::any( const Tensor & self)",40, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm_out( Tensor & result , const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm( const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::unfold( const Tensor & self , int64_t dimension , int64_t size , int64_t step)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::equal( const Tensor & self , const Tensor & other)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_out( Tensor & result , const Tensor & self , const Tensor & exponent)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow( const Tensor & self , const Tensor & exponent)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_out( Tensor & result , Scalar self , const Tensor & exponent)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow( Scalar self , const Tensor & exponent)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , const Tensor & mean , double std , Generator * generator)",95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( const Tensor & mean , double std , Generator * generator)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , double mean , const Tensor & std , Generator * generator)",95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( double mean , const Tensor & std , Generator * generator)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , const Tensor & mean , const Tensor & std , Generator * generator)",103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( const Tensor & mean , const Tensor & std , Generator * generator)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::alias( const Tensor & self)",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_dirichlet_grad_out( Tensor & output , const Tensor & x , const Tensor & alpha , const Tensor & total)",110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_dirichlet_grad( const Tensor & x , const Tensor & alpha , const Tensor & total)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__and__( const Tensor & self , Scalar other)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__and__( const Tensor & self , const Tensor & other)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__or__( const Tensor & self , Scalar other)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__or__( const Tensor & self , const Tensor & other)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__xor__( const Tensor & self , Scalar other)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__xor__( const Tensor & self , const Tensor & other)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__lshift__( const Tensor & self , Scalar other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__lshift__( const Tensor & self , const Tensor & other)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__rshift__( const Tensor & self , Scalar other)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__rshift__( const Tensor & self , const Tensor & other)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__iand__( Tensor & self , Scalar other)",49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__iand__( Tensor & self , const Tensor & other)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ior__( Tensor & self , Scalar other)",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ior__( Tensor & self , const Tensor & other)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ixor__( Tensor & self , Scalar other)",49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ixor__( Tensor & self , const Tensor & other)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ilshift__( Tensor & self , Scalar other)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ilshift__( Tensor & self , const Tensor & other)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__irshift__( Tensor & self , Scalar other)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__irshift__( Tensor & self , const Tensor & other)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::norm_except_dim( const Tensor & v , int64_t pow , int64_t dim)",97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::_weight_norm( const Tensor & v_in , const Tensor & g_in , int64_t dim)",95, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::_weight_norm_differentiable_backward( const Tensor & grad_w , const Tensor & saved_v , const Tensor & saved_g , const Tensor & saved_norms , int64_t dim)",101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::operator < <( std :: ostream & out , const ConvParams & params)",73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_strided() const",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_dilated() const",46, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_padded() const",45, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_output_padding_neg() const",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_output_padding_big() const",84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_padding_neg() const",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::view1d_as_2d()",54, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_cudnn( const at :: Tensor & input) const",101, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_miopen( const at :: Tensor & input) const",135, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_mkldnn( const at :: Tensor & input) const",77, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_depthwise( const at :: Tensor & input , const at :: Tensor & weight) const",102, 9, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::check_input_shape_forward( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , int64_t groups , bool transposed)",101, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::view4d( const at :: Tensor & tensor)",72, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::view3d( const at :: Tensor & tensor)",72, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::subtensor( at :: Tensor & tensor , int dim , int groups , int g)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv1d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , int64_t groups)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv2d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , int64_t groups)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv3d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , int64_t groups)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose1d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , int64_t groups , IntList dilation)",97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose2d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , int64_t groups , IntList dilation)",97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose3d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , int64_t groups , IntList dilation)",97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::convolution( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , bool transposed , IntList output_padding , int64_t groups)",99, 26, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::convolution_expand_param_if_needed( IntList list_param , const char * param_name , int64_t expected_dim)",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution( const Tensor & input_r , const Tensor & weight_r , const Tensor & bias_r , IntList stride_ , IntList padding_ , IntList dilation_ , bool transposed_ , IntList output_padding_ , int64_t groups_ , bool benchmark , bool deterministic , bool cudnn_enabled)",137, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution_nogroup( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , bool transposed , IntList output_padding)",67, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::subvariable( const Tensor & var , int dim , int groups , int g)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution_double_backward( const Tensor & ggI , const Tensor & ggW_r , const Tensor & ggb , const Tensor & gO_r , const Tensor & weight_r , const Tensor & input , IntList stride_ , IntList padding_ , IntList dilation_ , bool transposed_ , IntList output_padding_ , int64_t groups_ , bool benchmark , bool deterministic , bool cudnn_enabled , std :: array<bool,3> output_mask)",306, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::window_function_checks( const char * function_name , const TensorOptions & options , int64_t window_length)",66, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::getFactoryType( const TensorOptions & options)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar start , Scalar end , const TensorOptions & options)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar start , Scalar end)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar start , Scalar end , Scalar step)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar end , const TensorOptions & options)",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar end)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::_dim_arange( const Tensor & like , int64_t dim)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_cpu( IntList size , const TensorOptions & options)",89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_out( Tensor & result , IntList size)",59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_strided( IntList size , IntList stride , const TensorOptions & options)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_like( const Tensor & self)",51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_like( const Tensor & self , const TensorOptions & options)",85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye( int64_t n , const TensorOptions & options)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye( int64_t n , int64_t m , const TensorOptions & options)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye_out_cpu( Tensor & result , int64_t n)",49, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye_out_cpu( Tensor & result , int64_t n , int64_t m)",70, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full( IntList size , Scalar fill_value , const TensorOptions & options)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_out( Tensor & result , IntList size , Scalar fill_value)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_like( const Tensor & self , Scalar fill_value)",62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_like( const Tensor & self , Scalar fill_value , const TensorOptions & options)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace( Scalar start , Scalar end , const TensorOptions & options)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace( Scalar start , Scalar end , int64_t steps , const TensorOptions & options)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace_out( Tensor & result , Scalar start , Scalar end)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace_out( Tensor & result , Scalar start , Scalar end , int64_t steps)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace( Scalar start , Scalar end , const TensorOptions & options)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace( Scalar start , Scalar end , int64_t steps , const TensorOptions & options)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace_out( Tensor & result , Scalar start , Scalar end)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace_out( Tensor & result , Scalar start , Scalar end , int64_t steps)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones( IntList size , const TensorOptions & options)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_out( Tensor & result , IntList size)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_like( const Tensor & self)",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_like( const Tensor & self , const TensorOptions & options)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::scalar_tensor( Scalar s , const TensorOptions & options)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand( IntList size , const TensorOptions & options)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand( IntList size , Generator * generator , const TensorOptions & options)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_out( Tensor & result , IntList size)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_out( Tensor & result , IntList size , Generator * generator)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_like( const Tensor & self)",50, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_like( const Tensor & self , const TensorOptions & options)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t high , IntList size , const TensorOptions & options)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t high , IntList size , Generator * generator , const TensorOptions & options)",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t low , int64_t high , IntList size , const TensorOptions & options)",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t low , int64_t high , IntList size , Generator * generator , const TensorOptions & options)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t high , IntList size)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t high , IntList size , Generator * generator)",45, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t low , int64_t high , IntList size)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t low , int64_t high , IntList size , Generator * generator)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t high)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t low , int64_t high)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t high , const TensorOptions & options)",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t low , int64_t high , const TensorOptions & options)",69, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn( IntList size , const TensorOptions & options)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn( IntList size , Generator * generator , const TensorOptions & options)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_out( Tensor & result , IntList size)",51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_out( Tensor & result , IntList size , Generator * generator)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_like( const Tensor & self)",51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_like( const Tensor & self , const TensorOptions & options)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_cpu( Tensor & result , int64_t n , THGenerator * generator)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::get_generator( at :: Generator * gen)",71, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm( int64_t n , const TensorOptions & options)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm( int64_t n , Generator * generator , const TensorOptions & options)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_out( Tensor & result , int64_t n)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_out_cpu( Tensor & result , int64_t n , Generator * generator)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range( Scalar start , Scalar end , const TensorOptions & options)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range_out( Tensor & result , Scalar start , Scalar end)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range_out( Tensor & result , Scalar start , Scalar end , Scalar step)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::get_tril_size( int64_t row , int64_t col , int64_t offset)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::check_args( int64_t row , int64_t col , const TensorOptions & options)",62, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tril_indices( int64_t row , int64_t col , int64_t offset , const TensorOptions & options)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::triu_indices( int64_t row , int64_t col , int64_t offset , const TensorOptions & options)",81, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros( IntList size , const TensorOptions & options)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_out( Tensor & result , IntList size)",59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_like( const Tensor & self)",51, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_like( const Tensor & self , const TensorOptions & options)",85, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::bartlett_window( int64_t window_length , const TensorOptions & options)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::bartlett_window( int64_t window_length , bool periodic , const TensorOptions & options)",106, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::blackman_window( int64_t window_length , const TensorOptions & options)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::blackman_window( int64_t window_length , bool periodic , const TensorOptions & options)",108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , const TensorOptions & options)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , const TensorOptions & options)",57, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , double alpha , const TensorOptions & options)",63, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , double alpha , double beta , const TensorOptions & options)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hann_window( int64_t window_length , const TensorOptions & options)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hann_window( int64_t window_length , bool periodic , const TensorOptions & options)",70, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tensor_cpu( ArrayRef<T> values , const TensorOptions & options)",79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tensor_cuda( ArrayRef<T> values , const TensorOptions & options)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::flip_cpu_kernel( const int64_t total_dims , const std :: vector<int64_t> & stride_contiguous_v , const std :: bitset<dim_bitset_size> & flip_dims_b , const Tensor & in_tensor , Tensor & out_tensor)",113, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::flip_cpu( const Tensor & self , IntList dims)",105, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::roll_cpu( const Tensor & self , IntList shifts , IntList dims)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::rot90( const Tensor & self , int64_t k , IntList dims)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/DispatchStub.cpp,"at::native::compute_cpu_capability()",72, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/DispatchStub.cpp,"at::native::get_cpu_capability()",62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::TensorIterator::parallel_reduce( const loop2d_t & loop)",100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::use_two_pass_reduction( TensorIterator & iter)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::two_pass_reduction( TensorIterator & iter , const loop2d_t & loop)",92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::find_split_dim( TensorIterator & iter)",75, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::round_columns( TensorIterator & iter , int dim , int multiple , int64_t begin , int64_t end)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::parallel_dim_reduction( TensorIterator & iter , const loop2d_t & loop)",87, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::TensorIterator::foreach_reduced_elt( const loop_subiter_t & loop)",107, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::host_softmax( Tensor output , const Tensor & input , const int64_t dim)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::host_softmax_backward( Tensor & gI , const Tensor & grad , const Tensor & output , int64_t dim)",81, 18, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax_cpu( const Tensor & input_ , const int64_t dim_ , const bool half_to_float)",95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax_cpu( const Tensor & input_ , const int64_t dim_ , const bool half_to_float)",95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax_backward_cpu( const Tensor & grad_ , const Tensor & output_ , int64_t dim_ , const Tensor & input_)",77, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax_backward_cpu( const Tensor & grad_ , const Tensor & output_ , int64_t dim_ , const Tensor & input_)",76, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax( const Tensor & input_ , const int64_t dim_)",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax( const Tensor & input_ , const int64_t dim_ , ScalarType dtype)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax( const Tensor & input_ , const int64_t dim_)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax( const Tensor & input_ , const int64_t dim_ , ScalarType dtype)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv( int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf( int m , int n , scalar_t * a , int lda , int * ipiv , int * info)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri( int n , scalar_t * a , int lda , int * ipiv , scalar_t * work , int lwork , int * info)",97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackPotrs( char uplo , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky( char uplo , int n , scalar_t * a , int lda , int * info)",73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv<double>( int n , int nrhs , double * a , int lda , int * ipiv , double * b , int ldb , int * info)",116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv<float>( int n , int nrhs , float * a , int lda , int * ipiv , float * b , int ldb , int * info)",113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri<double>( int n , double * a , int lda , int * ipiv , double * work , int lwork , int * info)",112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri<float>( int n , float * a , int lda , int * ipiv , float * work , int lwork , int * info)",109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf<double>( int m , int n , double * a , int lda , int * ipiv , int * info)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf<float>( int m , int n , float * a , int lda , int * ipiv , int * info)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackPotrs<double>( char uplo , int n , int nrhs , double * a , int lda , double * b , int ldb , int * info)",117, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackPotrs<float>( char uplo , int n , int nrhs , float * a , int lda , float * b , int ldb , int * info)",114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky<double>( char uplo , int n , double * a , int lda , int * info)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky<float>( char uplo , int n , float * a , int lda , int * info)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_gesv( Tensor & b , Tensor & A , std :: vector<int64_t> & infos)",96, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_gesv_helper_cpu( const Tensor & self , const Tensor & A)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::gesv( const Tensor & self , const Tensor & A)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::gesv_out( Tensor & solution , Tensor & lu , const Tensor & self , const Tensor & A)",106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_inverse( Tensor & self , std :: vector<int64_t> & infos)",106, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_inverse_helper_cpu( const Tensor & self)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::inverse( const Tensor & self)",51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::inverse_out( Tensor & result , const Tensor & self)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_potrs( Tensor & b , Tensor & A , bool upper , std :: vector<int64_t> & infos)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_potrs_helper_cpu( const Tensor & self , const Tensor & A , bool upper)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::potrs( const Tensor & self , const Tensor & A , bool upper)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::potrs_out( Tensor & result , const Tensor & self , const Tensor & A , bool upper)",86, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_cholesky( Tensor & self , bool upper , std :: vector<int64_t> & infos)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_cholesky_helper_cpu( const Tensor & self , bool upper)",63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky( const Tensor & self , bool upper)",88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky_out( Tensor & result , const Tensor & self , bool upper)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::invalid_mask( const Tensor & self , int64_t idx , const Tensor & mask , int64_t maskIdx)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::checkIndexTensorTypes( TensorList indices)",72, 15, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::expandByteTensors( const Tensor & self , TensorList indices)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::hasContiguousSubspace( TensorList tl)",74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::transposeToFront( Tensor self , TensorList indices)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::computeLinearStride( const Tensor & tensor)",103, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::unsqueezeN( const Tensor & src , int64_t before , int64_t after)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::wrapIndexOnce( const Tensor & index , int64_t dim , int64_t dim_size)",99, 13, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::computeLinearIndex( const Tensor & src , TensorList indices)",96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::makeLinearIndex( Tensor self , TensorList orig)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::all_strides_match( TensorList tensors)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::shapes_as_str( TensorList tensors)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::restride_src( const Tensor & src , int64_t dims_before , int64_t dims_indexed , IntList replacement_shape)",97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::reshape_indexer( const Tensor & index , int64_t dims_before , int64_t dims_after)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::AdvancedIndex::AdvancedIndex( const Tensor & src , TensorList indices_list)",101, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_info( Tensor self , TensorList orig)",80, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_index_iterator( const AdvancedIndex & info)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_index_put_iterator( const AdvancedIndex & info , const Tensor & value)",113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index( const Tensor & self , TensorList indices)",100, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_put( const Tensor & self , TensorList indices , const Tensor & value , bool accumulate)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_put_( Tensor & self , TensorList indices , const Tensor & value , bool accumulate)",100, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_copy_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",127, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_lu_det_P_diag_U_info( const Tensor & self)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::det( const Tensor & self)",86, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::logdet( const Tensor & self)",89, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::slogdet( const Tensor & self)",90, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::pinverse( const Tensor & self , double rcond)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_matrix_rank_helper( const Tensor & self , bool symmetric)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_rank( const Tensor & self , double tol , bool symmetric)",87, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_rank( const Tensor & self , bool symmetric)",94, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::check_1d( const Tensor & t , const char * arg , const char * fn)",91, 1, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::ger( const Tensor & self , const Tensor & vec2)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::ger_out( Tensor & result , const Tensor & self , const Tensor & vec2)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mm( const Tensor & self , const Tensor & mat2)",76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mm_out( Tensor & result , const Tensor & self , const Tensor & mat2)",83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mv( const Tensor & self , const Tensor & vec)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mv_out( Tensor & result , const Tensor & self , const Tensor & vec)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv( const Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv_( Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv_out( Tensor & result , const Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr( const Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr_( Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr_out( Tensor & result , const Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",122, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_cpu_kernel( const Tensor & result , const Tensor & self , const Tensor & mat2 , Scalar beta_ , Scalar alpha_)",124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_out_or_baddbmm_( Tensor & self_or_result , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha , bool is_bmm_out)",156, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_cpu( const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_out_cpu( Tensor & result , const Tensor & self_ , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",134, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm__cpu( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",108, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_cpu( const Tensor & self , const Tensor & mat2)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_out_cpu( Tensor & result , const Tensor & batch1 , const Tensor & batch2)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::dot( const Tensor & self , const Tensor & tensor)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::dot_out( Tensor & result , const Tensor & self , const Tensor & tensor)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul( c10 :: optional<Tensor> out_opt , const Tensor & tensor1 , const Tensor & tensor2)",115, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul( const Tensor & tensor1 , const Tensor & tensor2)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul_out( Tensor & result , const Tensor & tensor1 , const Tensor & tensor2)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_power( const Tensor & a , int64_t n)",80, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm( const Tensor & self)",44, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm( const Tensor & self , IntList dim , bool keepdim)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::nuclear_norm( const Tensor & self , bool keepdim)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::nuclear_norm_out( Tensor & result , const Tensor & self , bool keepdim)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_chain_matmul_general( TensorList matrices , std :: vector<std::vector<int64_t>> & order , int64_t i , int64_t j)",135, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_chain_matmul_three_matrices( TensorList matrices)",99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::chain_matmul( TensorList matrices)",96, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"_copy__cpu( at :: Tensor & self , const at :: Tensor & src)",74, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"_copy__cpu( at :: Tensor & self , const at :: Tensor & src)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"copy_transpose_valid( const at :: Tensor & self , const at :: Tensor & src)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_s_copy__cpu( Tensor & self , const Tensor & src , bool non_blocking)",80, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_copy_same_type_transpose_( Tensor & self , const Tensor & src)",78, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_copy_same_type__cpu( Tensor & self , const Tensor & src)",80, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::make_feature_noise( const Tensor & input)",93, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::is_fused_kernel_acceptable( const Tensor & input , double p)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::multiply( Tensor & input , const Tensor & noise)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::multiply( const Tensor & input , const Tensor & noise)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::_dropout_impl( T & input , double p , bool train)",92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::dropout( const Tensor & input , double p , bool train)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::dropout_( Tensor & input , double p , bool train)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_dropout( const Tensor & input , double p , bool train)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_dropout_( Tensor & input , double p , bool train)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::alpha_dropout( const Tensor & input , double p , bool train)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::alpha_dropout_( Tensor & input , double p , bool train)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_alpha_dropout( const Tensor & input , double p , bool train)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_alpha_dropout_( Tensor & input , double p , bool train)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::start_index( int a , int b , int c)",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::end_index( int a , int b , int c)",53, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t sizeD , int64_t isizeH , int64_t isizeW , int64_t osizeH , int64_t osizeW , int64_t istrideD , int64_t istrideH , int64_t istrideW)",87, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_cpu_template( at :: Tensor & output , at :: Tensor const & input , IntList output_size)",110, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_frame( scalar_t * gradInput_p , scalar_t * gradOutput_p , int64_t sizeD , int64_t isizeH , int64_t isizeW , int64_t osizeH , int64_t osizeW)",73, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_cpu_template( Tensor & gradInput , const Tensor & gradOutput_ , const Tensor & input)",91, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_cpu( Tensor & output , const Tensor & input , IntList output_size)",42, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_cpu( at :: Tensor const & input , IntList output_size)",51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_cpu( Tensor & gradInput , const Tensor & gradOutput , const Tensor & input)",51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_cpu( const Tensor & gradOutput , const Tensor & input)",51, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RoiPooling.cpp,"at::native::RoiPooling2d_forward_cpu( const Tensor & input , const Tensor & rois , int64_t pooledHeight , int64_t pooledWidth , double spatialScale)",113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RoiPooling.cpp,"at::native::RoiPooling2d_backward_cpu( const Tensor & input , const Tensor & rois , int64_t pooledHeight , int64_t pooledWidth , double spatialScale , const Tensor & gradOutput , const Tensor & argmaxes)",34, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::integer_upcast( const Tensor & self , optional<ScalarType> dtype)",113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::make_dim_mask( IntList dims , int ndim)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::allocate_reduction_result( Tensor & result , const Tensor & self , DimMask mask , bool keepdim , ScalarType dtype)",68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::review_reduce_result( const Tensor & result , int ndim , DimMask mask , bool keepdim)",97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::make_reduction( const char * name , Tensor & result , const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",82, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::n_dim_size( const Tensor & self , IntList dim)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim , optional<ScalarType> dtype)",91, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim , ScalarType dtype)",69, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim , optional<ScalarType> dtype)",112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim , optional<ScalarType> dtype)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim , ScalarType dtype)",70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim)",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim , optional<ScalarType> dtype)",113, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , optional<ScalarType> dtype)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , ScalarType dtype)",62, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::get_dtype( Tensor & result , const Tensor & self , optional<ScalarType> dtype , bool promote_integers = false)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> opt_dtype)",72, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , ScalarType dtype)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> opt_dtype)",73, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , ScalarType dtype)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , ScalarType dtype)",86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , ScalarType dtype)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim , ScalarType dtype)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , bool keepdim)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , ScalarType dtype)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , bool keepdim)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , ScalarType dtype)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , bool keepdim , ScalarType dtype)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , bool keepdim)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , ScalarType dtype)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::logsumexp_out( Tensor & result , const Tensor & self , int64_t dim_ , bool keepdim)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::logsumexp( const Tensor & self , int64_t dim_ , bool keepdim)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_norm_out_cpu( Tensor & result , const Tensor & self , Scalar p , int64_t dim_ , bool keepdim)",98, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm_out( Tensor & result , const Tensor & self , Scalar p , int64_t dim , bool keepdim)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_norm( const Tensor & self , Scalar p)",104, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , Scalar p , int64_t dim , bool keepdim)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , Scalar p)",44, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all( const Tensor & self , int64_t dim , bool keepdim)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any( const Tensor & self , int64_t dim , bool keepdim)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var( const Tensor & self , bool unbiased)",104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var( const Tensor & self , int64_t dim , bool unbiased , bool keepdim)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var_out( Tensor & result , const Tensor & self , int64_t dim , bool unbiased , bool keepdim)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std( const Tensor & self , bool unbiased)",104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std( const Tensor & self , IntList dim , bool unbiased , bool keepdim)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std_out( Tensor & result , const Tensor & self , IntList dim , bool unbiased , bool keepdim)",111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::make_offset2bag( const Tensor & offsets , const Tensor & indices , Tensor & offset2bag)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::index_select_add( const Tensor & select_indices , const Tensor & add_indices , const Tensor & src , Tensor & output)",81, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::make_bag_size( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & bag_size)",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::apply_bag_size( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & output , const Tensor & bag_size)",75, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::apply_bag_size_backward( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & output , const Tensor & offset2bag , const Tensor & bag_size)",81, 38, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::embedding_bag_cpu_max( const Tensor & weight , const Tensor & indices , const Tensor & offset2bag , const Tensor & output , const Tensor & bag_size , const Tensor & offsets)",144, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::embedding_bag( const Tensor & weight , const Tensor & indices , const Tensor & offsets , const bool scale_grad_by_freq , const int64_t mode , bool sparse)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_cpu( const Tensor & weight , const Tensor & indices , const Tensor & offsets , const bool scale_grad_by_freq , const int64_t mode , bool sparse)",104, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_backward( const Tensor & grad , const Tensor & indices , const Tensor & offsets , const Tensor & offset2bag , const Tensor & bag_size_ , const Tensor & max_indices_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode , bool sparse)",82, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_dense_backward_cpu( const Tensor & grad_ , const Tensor & indices_ , const Tensor & offsets_ , const Tensor & offset2bag__ , const Tensor & bag_size_ , const Tensor & max_indices_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode)",92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_sparse_backward( const Tensor & grad_ , const Tensor & indices , const Tensor & offsets , const Tensor & offset2bag , const Tensor & bag_size_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",139, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",116, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",180, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss( const Tensor & self , const Tensor & target , int64_t reduction)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",145, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",118, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss( const Tensor & self , const Tensor & target , int64_t reduction)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",144, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",117, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",114, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss( const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",118, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss( const Tensor & self , const Tensor & target , int64_t reduction)",95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_forward_out( Tensor & output , Tensor & is_target , const Tensor & self , const Tensor & target , int64_t reduction)",167, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_forward( const Tensor & self , const Tensor & target , int64_t reduction)",122, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction , const Tensor & is_target)",185, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction , const Tensor & is_target)",158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",149, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",126, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_forward_out( Tensor & output , Tensor & total_weight , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",201, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_forward( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",153, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",219, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",192, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",151, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_forward_out( Tensor & output , Tensor & total_weight , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",203, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_forward( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",155, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",221, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",194, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",110, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss( const Tensor & self , const Tensor & target , int64_t reduction)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",110, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss( const Tensor & self , const Tensor & target , int64_t reduction)",89, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",126, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_out( Tensor & output , const Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu( const Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_backward_out( Tensor & grad_input , const Tensor & grad_output , Scalar alpha , Scalar scale , Scalar input_scale , const Tensor & output)",148, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_backward( const Tensor & grad_output , Scalar alpha , Scalar scale , Scalar input_scale , const Tensor & output)",121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_( Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_out( Tensor & output , const Tensor & self , int64_t dim)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu( const Tensor & self , int64_t dim)",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , int64_t dim)",111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_backward( const Tensor & grad_output , const Tensor & self , int64_t dim)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_out( Tensor & output , const Tensor & self , Scalar min_val , Scalar max_val)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh( const Tensor & self , Scalar min_val , Scalar max_val)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar min_val , Scalar max_val)",135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_backward( const Tensor & grad_output , const Tensor & self , Scalar min_val , Scalar max_val)",108, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_( Tensor & self , Scalar min_val , Scalar max_val)",74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_out( Tensor & output , const Tensor & self , Scalar negative_slope)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu( const Tensor & self , Scalar negative_slope)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar negative_slope)",128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_backward( const Tensor & grad_output , const Tensor & self , Scalar negative_slope)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_( Tensor & self , Scalar negative_slope)",74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_out( Tensor & output , const Tensor & self)",91, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid( const Tensor & self)",53, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_forward_out( Tensor & output , Tensor & buffer , const Tensor & self)",111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_forward( const Tensor & self)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & buffer)",129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_backward( const Tensor & grad_output , const Tensor & self , const Tensor & buffer)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_out( Tensor & output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise( const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training)",176, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_backward( const Tensor & grad_output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training)",149, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_( Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",132, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_out( Tensor & output , const Tensor & self , Scalar beta , Scalar threshold)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus( const Tensor & self , Scalar beta , Scalar threshold)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar beta , Scalar threshold , const Tensor & output)",157, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_backward( const Tensor & grad_output , const Tensor & self , Scalar beta , Scalar threshold , const Tensor & output)",130, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_out( Tensor & output , const Tensor & self , Scalar lambd)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink( const Tensor & self , Scalar lambd)",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar lambd)",119, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_backward( const Tensor & grad_output , const Tensor & self , Scalar lambd)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_out( Tensor & output , const Tensor & self , IntList output_size)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d( const Tensor & self , IntList output_size)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self)",114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_backward( const Tensor & grad_output , const Tensor & self)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_out( Tensor & output , Tensor & indices , const Tensor & self , IntList output_size)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d( const Tensor & self , IntList output_size)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices)",138, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices)",111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_out( Tensor & output , Tensor & indices , const Tensor & self , IntList output_size)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d( const Tensor & self , IntList output_size)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices)",138, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices)",111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_out( Tensor & output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",199, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",172, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_out( Tensor & output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",199, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",172, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d_out( Tensor & output , Tensor & indices , const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & random_samples)",187, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d( const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & random_samples)",144, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & indices)",182, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & indices)",155, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_out( Tensor & output , Tensor & indices , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",204, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",161, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",230, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",203, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_out( Tensor & output , Tensor & indices , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",204, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",161, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",230, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",203, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_out( Tensor & output , const Tensor & self , const Tensor & indices , IntList output_size)",111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d( const Tensor & self , const Tensor & indices , IntList output_size)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size)",152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size)",125, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_out( Tensor & output , const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",144, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d( const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",121, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",185, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",158, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d_out( Tensor & output , const Tensor & self , IntList padding)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d( const Tensor & self , IntList padding)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d_out( Tensor & output , const Tensor & self , IntList padding)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d( const Tensor & self , IntList padding)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d_out( Tensor & output , const Tensor & self , IntList padding)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d( const Tensor & self , IntList padding)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d_out( Tensor & output , const Tensor & self , IntList padding)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d( const Tensor & self , IntList padding)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d_out( Tensor & output , const Tensor & self , IntList padding)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d( const Tensor & self , IntList padding)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_out( Tensor & output , const Tensor & self , IntList output_size , bool align_corners)",112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d( const Tensor & self , IntList output_size , bool align_corners)",92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_backward( const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",125, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_out( Tensor & output , const Tensor & self , IntList output_size , bool align_corners)",114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d( const Tensor & self , IntList output_size , bool align_corners)",94, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",154, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_backward( const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",127, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_out( Tensor & output , const Tensor & self , IntList output_size , bool align_corners)",115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d( const Tensor & self , IntList output_size , bool align_corners)",95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",155, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_backward( const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",128, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_out( Tensor & output , const Tensor & self , IntList output_size)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d( const Tensor & self , IntList output_size)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_backward( const Tensor & grad_output , IntList output_size , IntList input_size)",106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_out( Tensor & output , const Tensor & self , IntList output_size)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d( const Tensor & self , IntList output_size)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_backward( const Tensor & grad_output , IntList output_size , IntList input_size)",106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_out( Tensor & output , const Tensor & self , IntList output_size)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d( const Tensor & self , IntList output_size)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_backward( const Tensor & grad_output , IntList output_size , IntList input_size)",106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::sigmoid_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & output)",104, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::sigmoid_backward( const Tensor & grad_output , const Tensor & output)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::tanh_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & output)",101, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::tanh_backward( const Tensor & grad_output , const Tensor & output)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",215, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",192, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",286, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",226, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & columns , const Tensor & ones)",352, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",311, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",215, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",192, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",292, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",226, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & finput , const Tensor & fgrad_input)",358, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",317, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",163, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",140, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",240, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",174, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input)",306, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",265, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",163, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",191, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",168, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_forward_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",199, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",176, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_backward_out( Tensor & grad_input , Tensor & grad_weight , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation)",254, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , std :: array<bool,2> output_mask)",235, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",140, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",240, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",174, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input)",306, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",265, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",189, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",166, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",260, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",200, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones)",326, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",285, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",189, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",166, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",260, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",200, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones)",326, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",285, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding( const Tensor & weight , const Tensor & indices , int64_t padding_idx , bool scale_grad_by_freq , bool sparse)",78, 17, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_backward( const Tensor & grad , const Tensor & indices , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq , bool sparse)",70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_sparse_backward( const Tensor & grad_ , const Tensor & indices_ , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq)",87, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_dense_backward_cpu( const Tensor & grad_ , const Tensor & indices , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_renorm_cpu_( Tensor & self , const Tensor & indices , double max_norm , double norm_type)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::linear( const Tensor & input , const Tensor & weight , const Tensor & bias)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::sumproduct_pair( const Tensor & left_ , const Tensor & right_ , IntList sum_dims_ , bool keepdim)",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::einsum( std :: string eqn , TensorList tensors)",191, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::_trilinear( const Tensor & i1_ , const Tensor & i2_ , const Tensor & i3_ , IntList expand1_ , IntList expand2_ , IntList expand3_ , IntList sumdim_ , int64_t unroll_dim)",106, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::bilinear( const Tensor & input1 , const Tensor & input2 , const Tensor & weight , const Tensor & bias)",130, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::tensordot( const Tensor & input1 , const Tensor & input2 , IntList dims1 , IntList dims2)",111, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::is_same_size( const Tensor & self , const Tensor & other)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::size( const Tensor & self , int64_t dim)",100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::stride( const Tensor & self , int64_t dim)",100, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::cudnn_is_acceptable( const Tensor & self)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::detach( const Tensor & self)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::detach_( Tensor & self)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::contiguous( const Tensor & self)",41, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Scalar.cpp,"at::native::item( const Tensor & self)",92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Scalar.cpp,"at::native::_local_scalar_dense_cpu( const Tensor & self)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::ensure_has_index( Device device)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to_impl( const Tensor & self , const TensorOptions & options , bool non_blocking)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , const TensorOptions & options , bool non_blocking , bool copy)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , Device device , ScalarType dtype , bool non_blocking , bool copy)",95, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , ScalarType dtype , bool non_blocking , bool copy)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , const Tensor & other , bool non_blocking , bool copy)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::reorder_dimensions()",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::compute_result_type( at :: ArrayRef<OperandInfo> operands , const F & predicate)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_types()",85, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_common_type()",90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compatible_stride( int element_size) const",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::invert_perm( IntList input) const",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::allocate_outputs()",86, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::coalesce_dimensions()",77, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::numel() const",40, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_dim_strides( int dim) const",67, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_data_ptrs( ArrayRef<char*> base , IntList counter) const",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_base_ptrs() const",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_dim_reduced( int dim) const",72, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::permute_dimensions( IntList perm)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::num_output_elements() const",68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::num_reduce_dims() const",47, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::loop_wrapper( const loop_t & loop)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::for_each( const loop_t & loop)",52, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::for_each( const loop2d_t & loop)",87, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_strides() const",59, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::serial_for_each( const loop_t & loop , Range range) const",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::serial_for_each( const loop2d_t & loop , Range range) const",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_trivial_1d() const",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_scalar( int arg) const",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_cpu_scalar( int arg) const",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::data_ptr( int arg) const",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::remove_operand( int arg)",47, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::replace_operand( int arg , void * data , IntList stride)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::remove_dimension( int dim)",58, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::narrow( int dim , int64_t start , int64_t size)",68, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::select_all_keeping_dim( int start_dim , IntList indices)",80, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::binary_op( Tensor & out , const Tensor & a , const Tensor & b)",107, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::reduce_op( Tensor & out , const Tensor & a)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::mark_outputs()",60, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_shape()",94, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::compute_stride( const Tensor & tensor , IntList shape)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_strides()",59, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::can_use_32bit_indexing() const",62, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::split( int dim)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_dim_to_split() const",58, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::with_32bit_indexing() const",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::Builder::build()",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::iterator( const TensorIterator & iter)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::operator ++()",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::operator *() const",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::begin() const",59, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::end() const",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::DimCounter( IntList shape , Range range)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::is_done() const",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::increment( const std :: array<int64_t,2> & step)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::max_2d_step() const",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/ConstantPadNd.cpp,"at::native::constant_pad_nd( const Tensor & self , IntList pad , Scalar value)",102, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::pairwise_distance( const Tensor & x1 , const Tensor & x2 , double p , double eps , bool keepdim)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::pdist( const Tensor & self , const double p)",103, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::_pdist_forward( const Tensor & self , const double p)",115, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::_pdist_backward( const Tensor & grad , const Tensor & self , const double p , const Tensor & pdist)",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::cosine_similarity( const Tensor & x1 , const Tensor & x2 , int64_t dim , double eps)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"sample_poisson( double lambda , THGenerator * generator)",78, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli( const Tensor & self , Generator * gen)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli( const Tensor & self , double p , Generator * gen)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_out( Tensor & result , const Tensor & self , Generator * gen)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_tensor_cpu_( Tensor & self , const Tensor & p_ , Generator * gen)",104, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_scalar_cpu_( Tensor & self , double p , Generator * gen)",96, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_standard_gamma_grad_cpu( const Tensor & self , const Tensor & output)",84, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_s_poisson_cpu( const Tensor & lambda , Generator * gen)",97, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_s_gamma_cpu( const Tensor & alpha , Generator * gen)",147, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::_type_has_native( const Type & dtype)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::_has_native( const Tensor & self)",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::clone( const Tensor & self)",40, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::resize_as_( Tensor & self , const Tensor & the_template)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::pow_out( Tensor & result , const Tensor & self , Scalar exponent)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::pow( const Tensor & self , Scalar exponent)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::zero_( Tensor & self)",40, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm_out( Tensor & result , const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",123, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm( const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",102, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm_( Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",98, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::linspace_from_neg_one( const Tensor & grid , int64_t num_steps)",74, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::make_base_grid_4D( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::make_base_grid_5D( const Tensor & theta , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",96, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_4D( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_5D( const Tensor & theta , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator( const Tensor & theta , IntList size)",80, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_4D_backward( const Tensor & grad_grid , int64_t N , int64_t C , int64_t H , int64_t W)",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_5D_backward( const Tensor & grad_grid , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",65, 24, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_backward( const Tensor & grad , IntList size)",76, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_cpu_template( const Tensor & self , const bool sorted , const bool return_inverse)",82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu_impl( ForwardIt first , ForwardIt last , std :: vector<int64_t> & indices , Tensor inverse_indices_vec)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu_template( const Tensor & self , const int64_t dim , const bool return_inverse)",83, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_cpu( const Tensor & self , const bool sorted , const bool return_inverse)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu( const Tensor & self , const int64_t dim , const bool sorted , const bool return_inverse)",103, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::checkLongTensor( const Tensor & tensor)",92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pack_padded_sequence( const Tensor & _input , const Tensor & _lengths , bool batch_first)",115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pack_padded_sequence_backward( const Tensor & grad , at :: IntList input_size , const Tensor & _batch_sizes , bool batch_first)",130, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pad_packed_sequence( const Tensor & data , const Tensor & _batch_sizes , bool batch_first , Scalar padding_value , int64_t total_length)",160, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp( const Tensor & self , optional<Scalar> min , optional<Scalar> max)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp_max( const Tensor & self , Scalar max)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp_min( const Tensor & self , Scalar min)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp__cpu( Tensor & self , optional<Scalar> min , optional<Scalar> max)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_out_cpu( Tensor & result , const Tensor & self , optional<Scalar> min , optional<Scalar> max)",65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_max__cpu( Tensor & self , Scalar max)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_max_out_cpu( Tensor & result , const Tensor & self , Scalar max)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_min__cpu( Tensor & self , Scalar min)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_min_out_cpu( Tensor & result , const Tensor & self , Scalar min)",77, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::fill_( Tensor & self , Scalar value)",49, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::fill_( Tensor & self , const Tensor & value)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::mvlgamma( const Tensor & self , int64_t p)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::mvlgamma_( Tensor & self , int64_t p)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"where_cpu( at :: Tensor & ret , const at :: Tensor & condition , const at :: Tensor & self , const at :: Tensor & other)",64, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::allclose( const Tensor & self , const Tensor & other , double rtol , double atol , bool equal_nan)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::isclose( const Tensor & self , const Tensor & other , double rtol , double atol , bool equal_nan)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::is_nonzero( const Tensor & self)",76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::where( const Tensor & condition , const Tensor & self , const Tensor & other)",93, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_s_where_cpu( const Tensor & condition , const Tensor & self , const Tensor & other)",88, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::kthvalue( const Tensor & self , int64_t k , int64_t dim , bool keepdim)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::kthvalue_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t k , int64_t dim , bool keepdim)",103, 43, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::median( const Tensor & self , int64_t dim , bool keepdim)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::median_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool keepdim)",97, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::mode( const Tensor & self , int64_t dim , bool keepdim)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::mode_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool keepdim)",95, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_max_out_cpu( Tensor & max , Tensor & max_indices , const Tensor & self , int64_t dim , bool keepdim)",89, 40, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max( const Tensor & self , int64_t dim , bool keepdim)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max_out( Tensor & max , Tensor & max_indices , const Tensor & self , int64_t dim , bool keepdim)",94, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max_values( const Tensor & self , int64_t dim , bool keepdim)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_min_out_cpu( Tensor & min , Tensor & min_indices , const Tensor & self , int64_t dim , bool keepdim)",89, 40, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min( const Tensor & self , int64_t dim , bool keepdim)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min_out( Tensor & min , Tensor & min_indices , const Tensor & self , int64_t dim , bool keepdim)",94, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min_values( const Tensor & self , int64_t dim , bool keepdim)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmax( const Tensor & self , int64_t dim , bool keepdim)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmax( const Tensor & self)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmin( const Tensor & self , int64_t dim , bool keepdim)",63, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmin( const Tensor & self)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_argmax( const Tensor & self , int64_t dim , bool keepdim)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_argmin( const Tensor & self , int64_t dim , bool keepdim)",64, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConvolutionTBC.cpp,"at::native::conv_tbc( const Tensor & self , const Tensor & weight , const Tensor & bias , int64_t pad)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConvolutionTBC.cpp,"at::native::conv_tbc_backward( const Tensor & dOutput , const Tensor & input , const Tensor & weight , const Tensor & bias , int64_t pad)",154, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Memory.cpp,"at::native::pin_memory( const Tensor & self)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/PixelShuffle.cpp,"at::native::pixel_shuffle( const Tensor & self , int64_t upscale_factor)",135, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::clip_coordinates( scalar_t in , int64_t clip_limit)",100, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::clip_coordinates_set_grad( scalar_t in , int64_t clip_limit , scalar_t * grad_in)",84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::reflect_coordinates( scalar_t in , int64_t clip_limit)",83, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::reflect_coordinates_set_grad( scalar_t in , int64_t clip_limit , scalar_t * grad_in)",87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::within_bounds_2d( int64_t h , int64_t w , int64_t H , int64_t W)",84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::within_bounds_3d( int64_t d , int64_t h , int64_t w , int64_t D , int64_t H , int64_t W)",106, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::safe_add_2d( scalar_t * data , int64_t h , int64_t w , int64_t sH , int64_t sW , int64_t H , int64_t W , scalar_t delta)",79, 33, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::safe_add_3d( scalar_t * data , int64_t d , int64_t h , int64_t w , int64_t sD , int64_t sH , int64_t sW , int64_t D , int64_t H , int64_t W , scalar_t delta)",82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_cpu_impl( const Tensor & input , const Tensor & grid , GridSamplerInterpolation interpolation_mode , GridSamplerPadding padding_mode)",112, 18, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_backward_cpu_impl( const Tensor & grad_output , const Tensor & input , const Tensor & grid , GridSamplerInterpolation interpolation_mode , GridSamplerPadding padding_mode)",126, 16, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_2d_cpu( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_cpu( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",79, 27, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_2d_backward_cpu( const Tensor & grad_output , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_backward_cpu( const Tensor & grad_output , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",105, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::tanh_f::operator ( )( const Tensor & t) const",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::relu_f::operator ( )( const Tensor & t) const",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedSequence::PackedSequence( Tensor _data , Tensor _batch_sizes)",70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::CellParams( const Tensor & _w_ih , const Tensor & _w_hh , const Tensor & _b_ih , const Tensor & _b_hh)",97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::pair_vec( const std :: vector<T> & vals)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::unpair_vec( std :: vector<pair_of<T>> && vals)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gather_params( TensorList params , bool has_biases)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_as_output( const Tensor & t)",55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_as_output( const tpair_of<Tensor> & t)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::project( at :: ArrayRef<tpair_of<Tensor>> tuples)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_concat( at :: ArrayRef<Tensor> hiddens)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_concat( at :: ArrayRef<tpair_of<Tensor>> hiddens)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_slice( const Tensor & t , int64_t start , int64_t end)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_slice( const tpair_of<Tensor> & t , int64_t start , int64_t end)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::Cell::~Cell()",104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::SimpleCell::operator ( )( const Tensor & input , const hidden_type & hidden , const CellParams & params) const",119, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::LSTMCell::operator ( )( const Tensor & input , const hidden_type & hidden , const CellParams & params) const",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::GRUCell::operator ( )( const Tensor & input , const hidden_type & hidden , const CellParams & params) const",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::Layer::~Layer()",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::FullLayer( Cell<hidden_type> & cell)",37, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::operator ( )( std :: vector<Tensor> step_inputs , const hidden_type & input_hidden , const CellParams & params) const",135, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::operator ( )( const Tensor & inputs , const hidden_type & input_hidden , const CellParams & params) const",123, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::FullBidirectionalLayer( Cell<dir_hidden_type> & cell)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::operator ( )( const Tensor & input , const hidden_type & input_hidden , const param_type & params) const",122, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::reverse( std :: vector<Tensor> && x) const",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedLayer::PackedLayer( Cell<hidden_type> & cell)",39, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const CellParams & params) const",130, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::ReversedPackedLayer::ReversedPackedLayer( Cell<hidden_type> & cell)",47, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::ReversedPackedLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const CellParams & params) const",130, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedBidirectionalLayer::PackedBidirectionalLayer( Cell<dir_hidden_type> & cell)",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedBidirectionalLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const param_type & params) const",130, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::dropout( const Tensor & input , double p)",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::dropout( const PackedSequence & input , double p)",74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::apply_layer_stack( const Layer<io_type,hidden_type,weight_type> & layer , const io_type & input , const std :: vector<hidden_type> & hiddens , const std :: vector<weight_type> & weights , int64_t num_layers , double dropout_p , bool train)",100, 18, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_rnn_impl( const io_type & input , const std :: vector<CellParams> & params , const std :: vector<typename CellType::hidden_type> & hiddens , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",135, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_rnn_impl_with_concat( const io_type & input , const std :: vector<CellParams> & params , const std :: vector<typename CellType::hidden_type> & hiddens , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",127, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_lstm_impl( const io_type & input , const std :: vector<CellParams> & params , const Tensor & hx , const Tensor & cx , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",127, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm( const Tensor & _input , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",98, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm( const Tensor & data , const Tensor & batch_sizes , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",93, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm_cell( const Tensor & input , TensorList hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",95, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gru_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::rnn_tanh_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::rnn_relu_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"apply_loss_reduction( const at :: Tensor & unreduced , int64_t reduction)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::cosine_embedding_loss( const Tensor & input1 , const Tensor & input2 , const Tensor & target , double margin , int64_t reduction)",131, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::hinge_embedding_loss( const Tensor & self , const Tensor & target , double margin , int64_t reduction)",106, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::triplet_margin_loss( const Tensor & anchor , const Tensor & positive , const Tensor & negative , double margin , double p , double eps , bool swap , int64_t reduction)",112, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::margin_ranking_loss( const Tensor & input1 , const Tensor & input2 , const Tensor & target , double margin , int64_t reduction)",129, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::kl_div( const Tensor & input , const Tensor & target , int64_t reduction)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::kl_div_backward_cpu( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",111, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::binary_cross_entropy_with_logits( const Tensor & input , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",152, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::binary_cross_entropy_with_logits_backward( const Tensor & grad , const Tensor & input , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",181, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SummaryOps.cpp,"at::native::_bincount_cpu_template( const Tensor & self , const Tensor & weights , int64_t minlength)",74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/SummaryOps.cpp,"at::native::_bincount_cpu( const Tensor & self , const Tensor & weights , int64_t minlength)",106, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_cuda( const Tensor & self)",35, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_distributed( const Tensor & self)",42, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_complex( const Tensor & self)",54, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_floating_point( const Tensor & self)",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_signed( const Tensor & self)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_sparse( const Tensor & self)",37, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::type_as( const Tensor & self , const Tensor & other)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::check1d( const char * function_name , const char * argument_name , IntList x)",56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::adaptive_avg_pool1d( const Tensor & self , IntList output_size)",71, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::adaptive_max_pool1d( const Tensor & self , IntList output_size)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool1d_with_indices( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",65, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::avg_pool1d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool1d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",64, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool2d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",64, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool3d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",64, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_reshape_from_tensor( const Tensor & self , const Tensor & shape_tensor)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_shape_as_tensor( const Tensor & self)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::broadcast_tensors( TensorList tensors)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_cat_no_zero_dim( TensorList tensors)",86, 13, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat_out( Tensor & result , TensorList tensors , int64_t dim)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sizes_match_except( IntList s1 , IntList s2 , int64_t dim_except)",109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_cat_sparse_dims( Tensor const & t , int64_t pos , IntList sizes , int64_t wrapped , int64_t sparse_dim , int64_t dense_dim)",105, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat_sparse( TensorList tensors , int64_t dim)",137, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat( TensorList tensors , int64_t dim)",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::chunk( const Tensor & self , int64_t chunks , int64_t dim)",101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diagflat( const Tensor & self , int64_t offset)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diagonal( const Tensor & self , int64_t offset , int64_t dim1_ , int64_t dim2_)",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diag_embed( const Tensor & self , int64_t offset , int64_t dim1_ , int64_t dim2_)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::expand( const Tensor & self , IntList size , bool implicit)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::expand_as( const Tensor & self , const Tensor & other)",60, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided( const Tensor & self , IntList size , IntList stride , int64_t storage_offset)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided_( Tensor & self , IntList size , IntList stride , int64_t storage_offset)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided( const Tensor & self , IntList size , IntList stride)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided_( Tensor & self , IntList size , IntList stride)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow_copy_sparse( const Tensor & self , int64_t dim , int64_t start , int64_t length)",111, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow_copy_dense( const Tensor & self , int64_t dim , int64_t start , int64_t length)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow( const Tensor & self , int64_t dim , int64_t start , int64_t length)",100, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::permute( const Tensor & self , IntList dims)",53, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::repeat( const Tensor & self , IntList repeats)",106, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::reshape( const Tensor & self , IntList proposed_shape)",84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::reshape_as( const Tensor & self , const Tensor & other)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::select( const Tensor & self , int64_t dim , int64_t index)",75, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::slice( const Tensor & self , int64_t dim , int64_t start , int64_t end , int64_t step)",90, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::split( const Tensor & self , int64_t split_size , int64_t dim)",107, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::split_with_sizes( const Tensor & self , IntList split_sizes , int64_t dim)",100, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::get_stack_inputs( TensorList tensors , int64_t dim)",86, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::stack( TensorList tensors , int64_t dim)",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::stack_out( Tensor & result , TensorList tensors , int64_t dim)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sparse_transpose_( Tensor & self , int64_t dim0 , int64_t dim1)",110, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::transpose_( Tensor & self , int64_t dim0 , int64_t dim1)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::transpose( const Tensor & self , int64_t dim0 , int64_t dim1)",69, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_t( const Tensor & self , const char * fn)",85, 13, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::t( const Tensor & self)",32, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::t_( Tensor & self)",32, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferSqueezeGeometry( const Tensor & tensor)",46, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferSqueezeGeometry( const Tensor & tensor , int64_t dim)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferUnsqueezeGeometry( const Tensor & tensor , int64_t dim)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze( const Tensor & self)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze( const Tensor & self , int64_t dim)",58, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze_( Tensor & self)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze_( Tensor & self , int64_t dim)",59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_unsafe_view( const Tensor & self , IntList size)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze_sparse( Tensor const & self , int64_t dim)",116, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze( const Tensor & self , int64_t dim)",60, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze_( Tensor & self , int64_t dim)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::flatten( const Tensor & self , int64_t start_dim , int64_t end_dim)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::view_as( const Tensor & self , const Tensor & other)",58, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::numel( const Tensor & self)",46, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unbind( const Tensor & self , int64_t dim)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::meshgrid( TensorList tensors)",118, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_fft( const Tensor & self , const int64_t signal_ndim , const bool complex_input , const bool complex_output , const bool inverse , IntList signal_sizes , const bool normalized , const bool onesided)",112, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_get_plan_cache_max_size()",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_set_plan_cache_max_size( int64_t max_size)",61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_get_plan_cache_size()",57, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_clear_plan_cache()",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::fft( const Tensor & self , const int64_t signal_ndim , const bool normalized)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::ifft( const Tensor & self , const int64_t signal_ndim , const bool normalized)",84, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::rfft( const Tensor & self , const int64_t signal_ndim , const bool normalized , const bool onesided)",82, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::irfft( const Tensor & self , const int64_t signal_ndim , const bool normalized , const bool onesided , IntList signal_sizes)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::stft( const Tensor & self , const int64_t n_fft , const int64_t hop_length , const int64_t win_length , const Tensor & window , const bool normalized , const bool onesided)",91, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::_baddbmm_mkl_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::gemm_batched( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const float alpha , const float ** A , const float ** B , const float beta , float ** C)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::gemm_batched( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const double alpha , const double ** A , const double ** B , const double beta , double ** C)",94, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::baddbmm_mkl_template( const Tensor & res , const Tensor & mat1 , const Tensor & mat2 , Scalar beta_ , Scalar alpha_)",130, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::_baddbmm_mkl_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",109, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_mkl( const Tensor & input , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntList checked_signal_sizes , bool normalized , bool onesided , IntList output_sizes)",60, 16, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_fill_with_conjugate_symmetry_slice( Tensor & output , int64_t signal_ndim , int64_t size_last_dim , int64_t start_last_dim_idx , int64_t i , int64_t num)",89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_fill_with_conjugate_symmetry_( Tensor & input , int64_t signal_ndim , int64_t size_last_dim , int64_t last_dim_start_slice)",93, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_mkl( const Tensor & self , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntList checked_signal_sizes , bool normalized , bool onesided , IntList output_sizes)",103, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp,"at::native::_cudnn_ctc_loss( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK , bool deterministic)",175, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp,"at::native::_cudnn_ctc_loss( const Tensor & log_probs_t , const Tensor & targets_t , IntList input_lengths_ , IntList target_lengths_ , int64_t BLANK , bool deterministic)",181, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_forward( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_backward( const Tensor & grad_theta , int64_t N , int64_t C , int64_t H , int64_t W)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::setSamplerDescriptor( SpatialTransformerDescriptor & desc , cudnnDataType_t dataType , int N , int C , int H , int W)",62, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_forward( const Tensor & theta_t , int64_t N , int64_t C , int64_t H , int64_t W)",83, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_backward( const Tensor & grad_grid_t , int64_t N , int64_t C , int64_t H , int64_t W)",84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_forward( const Tensor & input_t , const Tensor & grid_t)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_backward( const Tensor & input_t , const Tensor & grid_t , const Tensor & grad_output_t)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::setSamplerDescriptor( SpatialTransformerDescriptor & desc , cudnnDataType_t dataType , const at :: Tensor & tensor)",114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::checkGridSize( CheckedFrom c , TensorArg grid , TensorArg input)",70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_forward( const Tensor & input_t , const Tensor & grid_t)",84, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_backward( const Tensor & input_t , const Tensor & grid_t , const Tensor & grad_output_t)",87, 12, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_flatten_weight( TensorList weight_arr , int64_t weight_stride0 , int64_t input_size , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , bool fn_bidirectional)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn( const Tensor & input_r , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf_r , const Tensor & hx , const Tensor & cx , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state)",68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward( const Tensor & input , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output , const Tensor & grad_output_r , const Tensor & grad_hy_r , const Tensor & grad_cy_r , int64_t mode , int64_t hidden_size , int64_t num_layers , bool batch_first , double dropout , bool train , bool bidirectional , IntList batch_sizes , const Tensor & dropout_state , const Tensor & reserve , std :: array<bool,4> output_mask)",130, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_init_dropout_state( double dropout , bool train , int64_t dropout_seed , const TensorOptions & options)",115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::DropoutDescriptorParams()",33, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::set( bool train_ , double dropout_ , Tensor dropout_state_)",68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::descriptor( cudnnHandle_t handle) const",63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::num_directions() const",37, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_mode( int64_t fn_mode)",60, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_bidirectional( bool fn_bidirectional)",85, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_algo( cudnnRNNAlgo_t algo)",40, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set( int64_t mode , int64_t hidden_size , int64_t num_layers , bool bidirectional , cudnnDataType_t datatype)",116, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::descriptor( cudnnHandle_t handle , DropoutDescriptor && dropout_desc) const",127, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::descriptor( cudnnHandle_t handle) const",59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::rnn_descriptor_sequence( const Tensor & tensor , IntList batch_sizes)",101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::rnn_descriptor( const Tensor & tensor , int64_t N)",82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::is_input_packed() const",38, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::set( IntList input_sizes , IntList batch_sizes_ , bool batch_first)",76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::descriptors( Tensor x) const",64, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::RNNDescriptors( const RNNParams & fn , cudnnHandle_t handle , Tensor x , Tensor y , Tensor hx , Tensor cx)",106, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_descs( const std :: vector<TensorDescriptor> & descs)",97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_x_descs()",57, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_y_descs()",57, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_num_weights( cudnnHandle_t handle , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , cudnnDataType_t datatype)",107, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_num_linear_layers( cudnnRNNMode_t mode)",53, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_parameters( cudnnHandle_t handle , const RNNDescriptorParams & rnn , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , const FilterDescriptor & w_desc , const Tensor & weight_buf)",115, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_expected_data_ptrs( const Tensor & weight_buf , cudnnHandle_t handle , const RNNDescriptorParams & rnn , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , cudnnDataType_t datatype)",99, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_viewOrCopyParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to , bool copy)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_copyParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_viewParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_input_size( const TensorDescriptorListParams & tensors)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_hidden_size( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_output_size( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",113, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_algo( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",136, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_flatten_weight( TensorList weight_arr , int64_t weight_stride0 , int64_t input_size , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , bool fn_bidirectional)",108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn( const Tensor & input_r , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf_r , const Tensor & hx , const Tensor & cx , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state)",125, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward_input( const Tensor & input_r , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output_r , const Tensor & grad_output_r , const Tensor & grad_hy , const Tensor & grad_cy , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state , const Tensor & fn_reserve , std :: array<bool,3> output_mask)",97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward_weight( const Tensor & input_r , TensorList weight_arr , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output_r , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state , const Tensor & fn_reserve)",129, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward( const Tensor & input , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output , const Tensor & grad_output_r , const Tensor & grad_hy_r , const Tensor & grad_cy_r , int64_t mode , int64_t hidden_size , int64_t num_layers , bool batch_first , double dropout , bool train , bool bidirectional , IntList batch_sizes , const Tensor & dropout_state , const Tensor & reserve , std :: array<bool,4> output_mask)",294, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_init_dropout_state( double dropout , bool train , int64_t dropout_seed , const TensorOptions & options)",115, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::unpack_hidden( const Tensor & hidden)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::unpack_hidden( const std :: tuple<Tensor,Tensor> & hidden)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden( const Tensor & hx , const Tensor & cx)",102, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden<Tensor>( const Tensor & hx , const Tensor & cx)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden<std::tuple<Tensor,Tensor>>( const Tensor & hx , const Tensor & cx)",105, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutState::lock()",82, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutState::unlock()",23, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_dropout_state( double dropout_p , bool train , TensorOptions options)",104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::try_get_weight_buf( const Tensor & input , TensorList parameters , bool has_biases , cudnnRNNMode_t mode , int64_t hidden_size , int64_t num_layers , bool bidirectional)",90, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_impl( const Tensor & input , const Tensor & _batch_sizes , const hidden_type & hidden , TensorList params , bool has_biases , cudnnRNNMode_t mode , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_impl( const Tensor & input , const hidden_type & hidden , TensorList params , bool has_biases , cudnnRNNMode_t mode , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::lstm_cudnn( Tensor & output , Tensor & hy , Tensor & cy , const Tensor & input , TensorList hx , TensorList params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::lstm_packed_cudnn( Tensor & output , Tensor & hy , Tensor & cy , const Tensor & data , const Tensor & batch_sizes , TensorList hx , TensorList params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",84, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",86, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_bias( const at :: Tensor & grad_output)",85, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_input( const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",90, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",97, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",95, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_output_size( IntList input_size , IntList weight_size , IntList padding , IntList stride , IntList dilation , int64_t groups)",70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_input_size( IntList output_size , IntList weight_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_weight_size( IntList input_size , IntList output_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::narrowGroup( const Tensor & t , int dim , int group_idx , int64_t groups)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::check_args( CheckedFrom c , IntList args , size_t expected_size , const char * arg_name)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::convolution_shape_check( CheckedFrom c , const TensorGeometryArg & input , const TensorGeometryArg & weight , const TensorGeometryArg & output , IntList padding , IntList stride , IntList dilation , int64_t groups)",102, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::setConvolutionParams( ConvolutionParams * params , const at :: Tensor & input , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool deterministic)",70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::ConvolutionArgs::ConvolutionArgs( const Tensor & input , const Tensor & output , const Tensor & weight)",132, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::BenchmarkCache::find( const ConvolutionParams & params , T * results)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::BenchmarkCache::insert( const ConvolutionParams & params , const T & results)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::Workspace::Workspace( size_t size)",63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::Workspace::~Workspace()",56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionFwdAlgo_t algo , size_t * sz)",52, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdDataAlgo_t algo , size_t * sz)",57, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdFilterAlgo_t algo , size_t * sz)",59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getMaxWorkspaceSize( const ConvolutionArgs & args , const algo_t * algo , int n_algo)",90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getBestAlgorithm( perf_t * perfResults , bool deterministic , int n_algo)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::cache()",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::findAlgorithm( const ConvolutionArgs & args)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::getAlgorithm( const ConvolutionArgs & args , algo_t * algo)",81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::getWorkspaceSize( const ConvolutionArgs & args , algo_t algo , size_t * workspaceSize)",60, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::cache()",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::findAlgorithm( const ConvolutionArgs & args)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::getAlgorithm( const ConvolutionArgs & args , algo_t * algo)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdDataAlgo_t algo , size_t * workspaceSize)",65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::cache()",70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::findAlgorithm( const ConvolutionArgs & args)",96, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::getAlgorithm( const ConvolutionArgs & args , algo_t * algo)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::getWorkspaceSize( const ConvolutionArgs & args , algo_t algo , size_t * workspaceSize)",96, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::findAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",88, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::chooseAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_add_bias_( CheckedFrom c , const TensorArg & output , const TensorArg & bias)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_input( const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",87, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",164, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",134, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( CheckedFrom c , IntList input_size , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_forward( CheckedFrom c , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",98, 36, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( IntList input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",154, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",100, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( CheckedFrom c , IntList weight_size , const TensorArg & grad_output , const TensorArg & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",71, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",71, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_bias( const Tensor & grad_output_t)",99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double exponential_average_factor , double epsilon)",79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm_backward( const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var , double epsilon)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::expandScale( const Tensor & t , int64_t dim)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , const Tensor & running_mean_t , const Tensor & running_var_t , bool training , double exponential_average_factor , double epsilon)",119, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm_backward( const Tensor & input_t , const Tensor & grad_output_t , const Tensor & weight_t , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean_t , const Tensor & save_var_t , double epsilon)",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp__cuda( Tensor & self , optional<Scalar> min , optional<Scalar> max)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_out_cuda( Tensor & result , const Tensor & self , optional<Scalar> min , optional<Scalar> max)",65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_max__cuda( Tensor & self , Scalar max)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_max_out_cuda( Tensor & result , const Tensor & self , Scalar max)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_min__cuda( Tensor & self , Scalar min)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_min_out_cuda( Tensor & result , const Tensor & self , Scalar min)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double exponential_average_factor , double epsilon)",79, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm_backward( const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var , double epsilon)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::expandScale( const Tensor & t , int64_t dim)",52, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , const Tensor & running_mean_t , const Tensor & running_var_t , bool training , double exponential_average_factor , double epsilon)",119, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm_backward( const Tensor & input_t , const Tensor & grad_output_t , const Tensor & weight_t , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean_t , const Tensor & save_var_t , double epsilon)",116, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",89, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_bias( const at :: Tensor & grad_output)",87, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",86, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_input( const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",92, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",99, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",95, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_output_size( IntList input_size , IntList weight_size , IntList padding , IntList stride , IntList dilation , int64_t groups)",70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_input_size( IntList output_size , IntList weight_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_weight_size( IntList input_size , IntList output_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::narrowGroup( const Tensor & t , int dim , int group_idx , int64_t groups)",78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::check_args( CheckedFrom c , IntList args , size_t expected_size , const char * arg_name)",96, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::convolution_shape_check( CheckedFrom c , const TensorGeometryArg & input , const TensorGeometryArg & weight , const TensorGeometryArg & output , IntList padding , IntList stride , IntList dilation , int64_t groups)",102, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::setConvolutionParams( ConvolutionParams * params , const at :: Tensor & input , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool deterministic)",56, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ConvolutionArgs::ConvolutionArgs( const Tensor & input , const Tensor & output , const Tensor & weight)",132, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ParamsHash::operator ( )( const ConvolutionParams & params) const",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ParamsEqual::operator ( )( const ConvolutionParams & a , const ConvolutionParams & b) const",82, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::BenchmarkCache::find( const ConvolutionParams & params , T * results)",59, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::BenchmarkCache::insert( const ConvolutionParams & params , const T & results)",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::Workspace::Workspace( size_t size)",63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::Workspace::~Workspace()",56, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvFwdAlgorithm_t)",65, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvBwdDataAlgorithm_t)",69, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvBwdWeightsAlgorithm_t)",72, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getBestAlgorithm( perf_t * perfResults , bool deterministic , int n_algo)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::cache()",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::cache()",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",63, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::cache()",70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",64, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::findAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",80, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::chooseAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_add_bias_( CheckedFrom c , const TensorArg & output , const TensorArg & bias)",97, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",88, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",84, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_input( const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",87, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",165, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",134, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( CheckedFrom c , IntList input_size , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_forward( CheckedFrom c , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",98, 36, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( IntList input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",155, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",100, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( CheckedFrom c , IntList weight_size , const TensorArg & grad_output , const TensorArg & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",98, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",71, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",71, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_bias( const Tensor & grad_output_t)",101, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/Activation.cpp,"at::native::threshold_kernel( TensorIterator & iter , Scalar threshold_scalar , Scalar value_scalar)",99, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::add_kernel( TensorIterator & iter , Scalar alpha_scalar)",73, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::sub_kernel( TensorIterator & iter , Scalar alpha_scalar)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::mul_kernel( TensorIterator & iter)",65, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::div_kernel( TensorIterator & iter)",88, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::Indexer::Indexer( int64_t num_indexers , char ** indexers , const int64_t * indexer_strides , IntList original_sizes , IntList original_strides)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::Indexer::get( int64_t idx)",81, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::is_constant_index( int ntensor , const int64_t * strides)",69, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::cpu_index_kernel( TensorIterator & iter , IntList index_size , IntList index_stride , const func_t & f , bool serial_execution = false)",90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::index_kernel( TensorIterator & iter , IntList index_size , IntList index_stride)",106, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::index_put_kernel( TensorIterator & iter , IntList index_size , IntList index_stride , bool accumulate)",108, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/CopyKernel.cpp,"at::native::copy_kernel_impl( Tensor & dst , const Tensor & src)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocationBase::ComputeLocationBase( int64_t size)",59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocationBase::unnormalize( const Vec & in) const",48, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Zeros>::apply( const Vec & in) const",42, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Zeros>::apply_get_grad( const Vec & in) const",67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::ComputeLocation( int64_t size)",50, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::apply( const Vec & in) const",68, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::apply_get_grad( const Vec & in) const",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::ComputeLocation( int64_t size)",66, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::apply( const Vec & in) const",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::apply_get_grad( const Vec & in) const",84, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::mask_scatter_add( const scalar_t * src , scalar_t * base_addr , const int_same_size_t<scalar_t> * offsets , const int_same_size_t<scalar_t> * mask , int64_t len)",71, 17, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::ApplyGridSample( const TensorAccessor<scalar_t,4> & input)",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::compute_interp_params( const Vec & x , const Vec & y) const",81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::forward( TensorAccessor<scalar_t,3> & out_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",103, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::backward( TensorAccessor<scalar_t,3> & gInp_slice , TensorAccessor<scalar_t,3> & gGrid_slice , const TensorAccessor<scalar_t,3> & gOut_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",103, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::ApplyGridSample( const TensorAccessor<scalar_t,4> & input)",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::forward( TensorAccessor<scalar_t,3> & out_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::backward( TensorAccessor<scalar_t,3> & gInp_slice , TensorAccessor<scalar_t,3> & gGrid_slice , const TensorAccessor<scalar_t,3> & gOut_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",91, 32, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sample_2d_grid_slice_iterator( const TensorAccessor<scalar_t,3> & grid_slice , const ApplyFn & apply_fn)",92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sampler_2d_cpu_kernel_impl( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",115, 38, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sampler_2d_backward_cpu_kernel_impl( const Tensor & grad_output_ , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",116, 38, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( scalar_t val)",28, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( float val)",26, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( double val)",26, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::Reduction::apply( Tensor & res , Tensor & res_indices , const Tensor & self , c10 :: optional<int64_t> dim , bool greater)",75, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::max_kernel_impl( Tensor & max , Tensor & max_indices , const Tensor & self , c10 :: optional<int64_t> dim)",76, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::min_kernel_impl( Tensor & min , Tensor & min_indices , const Tensor & self , c10 :: optional<int64_t> dim)",77, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::_sigmoid( float * x , float * y , int64_t size)",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::_sigmoid( double * x , double * y , int64_t size)",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::sigmoid_kernel( Tensor & result , const Tensor & self)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::bernoulli_mkl_kernel( Tensor & output , const double p , Generator * gen)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::bernoulli_mkl_kernel( Tensor & self , const double p , Generator * gen)",80, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_log_softmax_lastdim( scalar_t * input_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",78, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_softmax_lastdim( scalar_t * input_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",74, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_host_softmax_backward_lastdim( scalar_t * grad_input_data_base , scalar_t * grad_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",76, 16, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::vec_host_softmax_lastdim::apply( Tensor & output , const Tensor & input)",68, 10, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::vec_host_softmax_backward_lastdim::apply( Tensor & grad_input , const Tensor & grad , const Tensor & output)",72, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::softmax_lastdim_kernel_impl( Tensor & result , const Tensor & self)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::log_softmax_lastdim_kernel_impl( Tensor & result , const Tensor & self)",71, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::softmax_backward_lastdim_kernel_impl( Tensor & grad_input , const Tensor & grad , const Tensor & output)",67, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::log_softmax_backward_lastdim_kernel_impl( Tensor & grad_input , const Tensor & grad , const Tensor & output)",69, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::sign( Vec val)",74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::map( const Vec & diff , const Vec & p)",112, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::red( const Vec & agg , const Vec & up)",78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::finish( const scalar_t agg , const scalar_t p)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::map( const Vec & diff , const Vec & p)",74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::red( const Vec & agg , const Vec & up)",78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::finish( const scalar_t agg , const scalar_t p)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",139, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::lttdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",219, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::map( const Vec & diff , const Vec & p)",81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::red( const Vec & agg , const Vec & up)",78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::finish( const scalar_t agg , const scalar_t p)",99, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",168, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::map( const Vec & diff , const Vec & p)",81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::red( const Vec & agg , const Vec & up)",78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::finish( const scalar_t agg , const scalar_t p)",107, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",213, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::map( const Vec & diff , const Vec & p)",74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::red( const Vec & agg , const Vec & up)",94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::finish( const scalar_t agg , const scalar_t p)",88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",215, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_parallel( Tensor & result , const Tensor & self , const scalar_t p)",97, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply( Tensor & result , const Tensor & self , const scalar_t p)",76, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::backward_down_column( const scalar_t * self_i , scalar_t * res_i , const scalar_t * grad_k , const scalar_t * dist_k , const Vec & pvec , int64_t n , int64_t m , int64_t gs , int64_t count = Vec :: size)",215, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_backward_parallel( Tensor & result , const Tensor & grad , const Tensor & self , const scalar_t p , const Tensor & dist)",141, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply_backward( Tensor & result , const Tensor & grad , const Tensor & self , const double p , const Tensor & dist)",123, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::pdist_forward_kernel_impl( Tensor & result , const Tensor & self , const double p)",85, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::pdist_backward_kernel_impl( Tensor & result , const Tensor & grad , const Tensor & self , const double p , const Tensor & dist)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::sum_kernel_impl( TensorIterator & iter)",70, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::WelfordData::WelfordData()",43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::WelfordData::WelfordData( double mean , double m2 , int64_t n)",79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::std_kernel_impl( TensorIterator & iter , bool unbiased)",67, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::prod_kernel_impl( TensorIterator & iter)",69, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::round_down( int64_t a , int64_t m)",57, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::apply( Tensor & res , const Tensor & self , Scalar p , c10 :: optional<int64_t> dim)",69, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::reduce_all( const scalar_t * data_ , int64_t size , float pval)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::norm_reduce( const scalar_t * data , int64_t n , int64_t stride , float pval)",96, 6, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::norm_reduce_sequential( const scalar_t * data , int64_t n , int64_t stride , float pval)",104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::norm_reduce128( const scalar_t * data , int64_t n , float pval)",80, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::norm_kernel_impl( Tensor & result , const Tensor & self , Scalar p , c10 :: optional<int64_t> dim)",58, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups)",81, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_weights( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",92, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , std :: array<bool,3> output_mask)",105, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::conv_output_size( IntList input_size , IntList weight_size , IntList padding , IntList stride , IntList dilation , int64_t groups)",71, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_weights( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , std :: array<bool,3> output_mask)",103, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_to_csr( const int64_t * indices , int64_t dim , int64_t nnz)",83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::zero_sparse_( SparseTensor & self)",55, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::wrapped_scalar_tensor( Scalar s)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_zerodim( SparseTensor & r , const SparseTensor & t , const Tensor & value)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_scalar( SparseTensor & r , const SparseTensor & t , Scalar value)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::log1p_out_sparse( SparseTensor & r , const SparseTensor & t)",90, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::log1p_sparse_( SparseTensor & t)",94, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::pow_out_sparse_scalar( SparseTensor & r , const SparseTensor & t_ , Scalar value)",128, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::pow_sparse_scalar( const SparseTensor & t , Scalar value)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::div_out_sparse_zerodim( SparseTensor & r , const SparseTensor & t , const Tensor & value)",100, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::div_out_sparse_scalar( SparseTensor & r , const SparseTensor & t , Scalar value)",92, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::norm_sparse( const SparseTensor & self , Scalar value)",61, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_out_sparse_cpu( SparseTensor & r , const SparseTensor & t , const SparseTensor & src , Scalar value)",208, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_dense_sparse_worker_cpu( Tensor & r , Scalar value , const SparseTensor & sparse , const Tensor & indices , const Tensor & values)",133, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_out_dense_sparse_cpu( Tensor & r , const Tensor & dense , SparseTensorRef sparse__ , Scalar value)",136, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_cpu( SparseTensor & r , const Tensor & t_ , const Tensor & src_)",140, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_out_sparse_dense_worker( int64_t nnz , int64_t dim_i , int64_t dim_j , int64_t dim_k , Tensor & r , Scalar beta , const Tensor & t , Scalar alpha , const Tensor & csr , const Tensor & indices , const Tensor & values , const Tensor & dense)",237, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_out_sparse_dense_cpu( Tensor & r , const Tensor & t , const SparseTensor & sparse_ , const Tensor & dense , Scalar beta , Scalar alpha)",125, 8, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_sparse_dense_cpu( const Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",66, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_sparse_dense_cpu_( Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_addmm( const Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",60, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_mm( const SparseTensor & sparse , const Tensor & dense)",74, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::hspmm_out_sparse_cpu( SparseTensor & r , const SparseTensor & sparse_ , const Tensor & dense)",136, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::hspmm_sparse_cpu( const SparseTensor & sparse , const Tensor & dense)",81, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sspaddmm_out_cpu( SparseTensor & r , const SparseTensor & t , const SparseTensor & sparse_ , const Tensor & dense , Scalar beta , Scalar alpha)",103, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sspaddmm_out_only_sparse( Tensor & result , const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",73, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::smm( const Tensor & self , const Tensor & mat2)",58, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::sspaddmm( const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",76, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input)",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , ScalarType dtype)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , IntList dims_to_sum , ScalarType dtype)",87, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , IntList dims_to_sum)",160, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum_backward_cpu( const Tensor & grad_ , const SparseTensor & input_ , IntList dims_to_sum)",210, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_dim_sparse( const SparseTensor & self)",54, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_dim_sparse( const SparseTensor & self)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::is_coalesced_sparse( const SparseTensor & self)",53, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_nnz_sparse( const SparseTensor & self)",48, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_indices_sparse( const SparseTensor & self)",51, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_values_sparse( const SparseTensor & self)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_coalesced_sparse_( SparseTensor & self , bool coalesced)",65, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::indices_sparse( const Tensor & self)",90, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::values_sparse( const Tensor & self)",89, 11, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_sparse( const TensorOptions & options)",56, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_with_dims_sparse( int64_t sparse_dim , int64_t dense_dim , ArrayRef<int64_t> size , const TensorOptions & options)",165, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_with_dims_and_tensor_sparse( int64_t sparse_dim , int64_t dense_dim , ArrayRef<int64_t> size , const LongTensor & indices , const Tensor & values , const TensorOptions & options)",63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::empty_sparse( IntList size , const TensorOptions & options)",66, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( ArrayRef<int64_t> size , const TensorOptions & options)",94, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::expand_values_if_needed( const Tensor & values)",71, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( const Tensor & indices , const Tensor & values_ , const TensorOptions & options)",127, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( const Tensor & indices , const Tensor & values_ , ArrayRef<int64_t> size , const TensorOptions & options)",129, 15, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_sparse_coo_tensor_unsafe( const Tensor & indices , const Tensor & values_ , ArrayRef<int64_t> size , const TensorOptions & options)",135, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::clone_sparse( const SparseTensor & self)",112, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_resize_( SparseTensor & self , ArrayRef<int64_t> size , int64_t sparse_dim , int64_t dense_dim)",114, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_resize_and_clear_( SparseTensor & self , ArrayRef<int64_t> size , int64_t sparse_dim , int64_t dense_dim)",124, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_is_same_size_as_sparse( const SparseTensor & self , const SparseTensor & src)",125, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::resize_as_sparse_( SparseTensor & self , const SparseTensor & src)",79, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_to_sparse( const Tensor & self)",50, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_to_sparse( const Tensor & self , int64_t sparse_dim)",102, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_to_dense( const SparseTensor & self)",73, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::copy_sparse_( SparseTensor & self , const SparseTensor & src , bool non_blocking)",93, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::coalesce_sparse_cpu( const SparseTensor & self)",118, 14, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_out_cpu_kernel( Tensor & r_values , const Tensor & t , const int64_t r_nnz , const int64_t sparse_dim , const LongTensor & mask_indices)",68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_out_cpu( SparseTensor & r , const Tensor & t , const SparseTensor & mask)",108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_cpu( const Tensor & t , SparseTensorRef mask)",70, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,"at::native::sparse_mask_out_cuda( SparseTensor & r , const Tensor & t , const SparseTensor & mask)",108, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,"at::native::sparse_mask_cuda( const Tensor & t , SparseTensorRef mask)",71, 0, 0
