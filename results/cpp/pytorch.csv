File, Function, Length, Total Width, Leading Space(s), Leading Tab(s)
repos/cpp/pytorch/torch/abi-check.cpp,"main()",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGetRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGetNumProcesses()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllReduceMultiGPU( THDTensorDescriptor * data , size_t len , THDReduceOp operation , THDGroup group)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllReduce( THDTensorDescriptor & desc , THDReduceOp operation , THDGroup group)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDReduceMultiGPU( THDTensorDescriptor * desc , size_t len , THDReduceOp operation , int dst_rank , THDGroup group)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDReduce( THDTensorDescriptor & desc , THDReduceOp operation , int dst_rank , THDGroup group)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBroadcastMultiGPU( THDTensorDescriptor * desc , size_t len , int src_rank , THDGroup group)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBroadcast( THDTensorDescriptor & desc , int src_rank , THDGroup group)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDIsend( THDTensorDescriptor & desc , int dst_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDIrecv( THDTensorDescriptor & desc , int src_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDSend( THDTensorDescriptor & desc , int dst_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRecvAnySource( THDTensorDescriptor & desc)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRecv( THDTensorDescriptor & desc , int src_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllGatherMultiGPU( THDTensorDescriptor * output , size_t outputLen , THDTensorDescriptor * input , size_t inputLen , THDGroup group)",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDAllGather( THDTensorDescriptor * output , size_t len , THDTensorDescriptor & input , THDGroup group)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGatherSend( THDTensorDescriptor & input , int dst_rank , THDGroup group)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDGatherRecv( THDTensorDescriptor * output , size_t len , THDTensorDescriptor & input , THDGroup group)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDScatterSend( THDTensorDescriptor * input , size_t len , THDTensorDescriptor & output , THDGroup group)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDScatterRecv( THDTensorDescriptor & output , int src_rank , THDGroup group)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDBarrier( THDGroup group)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDNewGroup( const int * ranks , size_t len)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRequest_isCompleted( THDRequest * request)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/Collectives.cpp,"THDRequest_wait( THDRequest * request)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDProcessGroupInit( THDChannelType channel_type , std :: string init_method = "env://" , int world_size = - 1 , std :: string group_name = "" , int rank = - 1)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDProcessGroupDestroy()",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/process_group/General.cpp,"THDClearGroupCache( THDGroup group)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"master()",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"worker( int id)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_smoke.cpp,"main()",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"test( std :: shared_ptr<thd::DataChannel> data_channel)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"run_all_tests( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"init_gloo_master( int workers)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"init_gloo_worker( unsigned int id , int workers)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_gloo_cache.cpp,"main( void)",22, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_accept_timeout.cpp,"master()",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_accept_timeout.cpp,"main()",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/tensor_smoke.cpp,"main()",35, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_tensor( std :: shared_ptr<thd::DataChannel> data_channel)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_tensor_any_source( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_scalar( std :: shared_ptr<thd::DataChannel> data_channel)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_broadcast( std :: shared_ptr<thd::DataChannel> data_channel)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"_test_reduce_helper( std :: shared_ptr<thd::DataChannel> data_channel , THDReduceOp op_type , int64_t init_value , int64_t expected_value)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_reduce( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"_test_allReduce_helper( std :: shared_ptr<thd::DataChannel> data_channel , THDReduceOp op_type , int64_t init_value , int64_t expected_value)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allReduce( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_scatter( std :: shared_ptr<thd::DataChannel> data_channel)",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_gather( std :: shared_ptr<thd::DataChannel> data_channel)",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allGather( std :: shared_ptr<thd::DataChannel> data_channel)",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_barrier( std :: shared_ptr<thd::DataChannel> data_channel)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_isend( std :: shared_ptr<thd::DataChannel> data_channel)",23, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_irecv( std :: shared_ptr<thd::DataChannel> data_channel)",25, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_interlaces( std :: shared_ptr<thd::DataChannel> data_channel)",34, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_broadcast_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_reduce_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",24, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allReduce_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_scatter_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",27, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_gather_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",31, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_allGather_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",23, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_barrier_group( std :: shared_ptr<thd::DataChannel> data_channel , THDGroup group , std :: vector<thd::rank_type> group_ranks)",31, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_send_recv_invalid_rank( std :: shared_ptr<thd::DataChannel> data_channel)",23, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_empty_group( std :: shared_ptr<thd::DataChannel> data_channel)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_process_not_in_group( std :: shared_ptr<thd::DataChannel> data_channel)",29, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_tensors_do_not_match_group_size( std :: shared_ptr<thd::DataChannel> data_channel)",28, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"test_tensors_are_not_the_same( std :: shared_ptr<thd::DataChannel> data_channel)",29, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"run_all_tests( std :: shared_ptr<thd::DataChannel> data_channel , int workers)",33, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_tcp_master( int workers)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_tcp_worker( unsigned int id , int workers)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_gloo_master( int workers)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_gloo_worker( unsigned int id , int workers)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"init_mpi_process()",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_collectives.cpp,"main( int argc , char const * argv [ ])",66, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"master()",22, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"worker( int id)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_tcp_slow_master.cpp,"main()",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/test/data_channel_mpi_smoke.cpp,"main( int argc , char ** argv)",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannelRequest.cpp,"THDRequest_free( void * request)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDSetCudaStatePtr( THCState ** state)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDRegisterCudaStream( cudaStream_t stream)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/Cuda.cpp,"THDGetStreamId( cudaStream_t stream)",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::setSocketNoDelay( int socket)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::getSocketPort( int fd)",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::splitAddress( const std :: string & addr)",26, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::sockaddrToString( struct sockaddr * addr)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::listen( port_type port)",51, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::connect( const std :: string & address , port_type port , bool wait , int timeout)",99, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/ChannelUtils.cpp,"thd::accept( int listen_socket , int timeout)",36, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::newChannel( THDChannelType type , std :: string init_method , int world_size , std :: string group_name , int rank)",38, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::Group()",1, 31, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::Group( std :: vector<rank_type> ranks , rank_type max_rank)",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::~Group()",1, 32, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::size() const",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::mustGetGroupRank( rank_type global_rank) const",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::getGroupRank( rank_type global_rank) const",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::mustGetGlobalRank( rank_type group_rank) const",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/DataChannel.cpp,"thd::DataChannel::Group::getGlobalRank( rank_type group_rank) const",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::sendPeerName( int socket)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::getInterfaceAddresses()",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::discoverWorkers( int listen_socket , rank_type world_size)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::discoverMaster( std :: vector<std::string> addresses , port_type port)",25, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodUtils.cpp,"thd::getRank( const std :: vector<int> & ranks , int assigned_rank , size_t order)",26, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::mustGetEnv( const char * env)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::loadWorkerEnv()",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::maybeLoadEnv( const char * env_name , int value , std :: string parameter_name)",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodEnv.cpp,"thd::init::initEnv( std :: string argument , int world_size_r , std :: string group_name , int rank)",30, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethod.cpp,"thd::getInitConfig( std :: string argument , int world_size , std :: string group_name , int rank)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::lockLoop( int fd , struct flock & oflock)",11, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::lockFile( int fd)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::unlockFile( int fd)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::waitForGroup( std :: string file_path , std :: string group_name , std :: fstream & file)",46, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::waitForData( int fd , std :: fstream & file , rank_type world_size)",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::parseFile( std :: fstream & file , rank_type world_size , std :: string group_name)",46, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodFile.cpp,"thd::init::initFile( std :: string argument , int world_size_r , std :: string group_name , int assigned_rank)",82, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::getRandomString()",22, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::MulticastMessage( std :: string group_name , port_type port , int rank)",6, 22, 8, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::MulticastMessage( std :: string msg)",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::MulticastMessage::pack()",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::isMulticastAddress( struct sockaddr * address)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::bindMulticastSocket( struct sockaddr * address , struct sockaddr_storage * sock_addr , int timeout_sec = 1 , int ttl = 1)",69, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::getMessages( struct sockaddr * addr , rank_type world_size , std :: string group_name , std :: string packed_msg)",68, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCPMaster( std :: string address , std :: string str_port , rank_type world_size , int assigned_rank)",29, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCPMulticast( std :: string group_name , rank_type world_size , int assigned_rank , struct sockaddr * addr)",69, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/init_methods/InitMethodTCP.cpp,"thd::init::initTCP( std :: string argument , int world_size_r , std :: string group_name , int rank)",48, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::_getNcclDataType( at :: ScalarType type)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::getDevicesList( const std :: string & deviceSeq)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::DataChannelNccl( InitMethod :: Config config , int timeout)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcastUniqueNcclId( ncclUniqueId * ncclId)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::~DataChannelNccl()",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_destroySockets()",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::destroy()",24, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_destroyNcclResources( THDGroup groupId)",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::clearGroupCache( THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::init()",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::getRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::getNumProcesses()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_getNcclResourcePair( std :: vector<at::Tensor> & input , THDGroup groupId)",87, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_tensorCheckHelper( const std :: vector<at::Tensor> & input , const std :: vector<at::Tensor> & output , size_t outputOverInput)",77, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup groupId)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup groupId)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",43, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::broadcast( at :: Tensor & data , rank_type srcRank , THDGroup groupId)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::barrier( THDGroup groupId)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::newGroup( const std :: vector<rank_type> & ranks)",28, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::_checkGroupIdValid( THDGroup groupId)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dstRank , THDGroup groupId)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type srcRank , THDGroup groupId)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::send( Scalar & data , rank_type dstRank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::send( at :: Tensor & data , rank_type dstRank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( Scalar & data , rank_type srcRank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( at :: Tensor & data)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::receive( at :: Tensor & data , rank_type srcRank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::isend( at :: Tensor & data , rank_type dstRank)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp,"thd::DataChannelNccl::ireceive( at :: Tensor & data , rank_type srcRank)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::log2ceil( uint32_t value)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::pow2( T value)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::RequestTCP( QueueWorker :: Request && request)",2, 38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::~RequestTCP()",1, 45, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::isCompleted()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::RequestTCP::wait()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::DataChannelTCP( InitMethod :: Config config)",2, 36, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::DataChannelTCP( InitMethod :: Config config , int timeout)",28, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::~DataChannelTCP()",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::destroy()",1, 34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::initWorker()",54, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::initMaster()",55, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::init()",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::getRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::getNumProcesses()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",48, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",78, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",51, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",48, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::send( Scalar & data , rank_type dst_rank)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::send( at :: Tensor & data , rank_type dst_rank)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( Scalar & data , rank_type src_rank)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( at :: Tensor & data)",37, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::receive( at :: Tensor & data , rank_type src_rank)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::isend( at :: Tensor & data , rank_type dst_rank)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::ireceive( at :: Tensor & data , rank_type src_rank)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::barrier( THDGroup group_id)",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::newGroup( const std :: vector<rank_type> & ranks)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_send( const Scalar & data , rank_type dst_rank)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_send( const at :: Tensor & data , rank_type dst_rank)",23, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_receive( Scalar & data , rank_type src_rank)",27, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_receive( const at :: Tensor & data , rank_type src_rank)",31, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::_reduce( at :: Tensor & result , at :: Tensor & data , THDReduceOp operation) const",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelTCP.cpp,"thd::DataChannelTCP::clearGroupCache( THDGroup group_id)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::StoreDeamon( int listen_socket)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::~StoreDeamon()",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::join()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::deamon()",42, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::query( rank_type rank)",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::StoreDeamon::checkAndUpdate( std :: vector<std::string> & keys) const",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::Store( const std :: string & addr , port_type port , int listen_socket)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::~Store()",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::set( const std :: string & key , const std :: vector<char> & data)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::get( const std :: string & key)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/Store.cpp,"thd::Store::wait( const std :: vector<std::string> & keys)",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::RequestMPI()",1, 44, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::~RequestMPI()",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::isCompleted()",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::wait()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::save_buffer( std :: shared_ptr<T> ptr)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::save_tensor_buffer( at :: Tensor & t)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::RequestMPI::new_request()",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::DataChannelMPI()",1, 67, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::~DataChannelMPI()",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::destroy()",1, 34, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::init()",38, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::getRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::getNumProcesses()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::_newLikeFlat( std :: vector<at::Tensor> & tensors) const",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",31, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",45, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",45, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",25, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::send( Scalar & data , rank_type dst_rank)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::send( at :: Tensor & data , rank_type dst_rank)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( Scalar & data , rank_type src_rank)",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( at :: Tensor & data)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::receive( at :: Tensor & data , rank_type src_rank)",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::barrier( THDGroup group_id)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::isend( at :: Tensor & data , rank_type dst_rank)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::ireceive( at :: Tensor & data , rank_type src_rank)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::newGroup( const std :: vector<rank_type> & ranks)",43, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp,"thd::DataChannelMPI::clearGroupCache( THDGroup group_id)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::RequestGloo( QueueWorker :: Request && request)",2, 38, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::~RequestGloo()",1, 48, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::isCompleted()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::RequestGloo::wait()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::Group::Group( const std :: string & addr , port_type port , std :: vector<rank_type> ranks , rank_type max_rank , int store_socket)",8, 53, 4, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::DataChannelGloo( InitMethod :: Config config)",43, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::~DataChannelGloo()",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::destroy()",1, 35, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::init()",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::getRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::getNumProcesses()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGatherT( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",34, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGather( std :: vector<at::Tensor> & output , at :: Tensor & input , THDGroup group_id)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::gather( std :: vector<at::Tensor> & output , at :: Tensor & input , rank_type dst_rank , THDGroup group_id)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::scatter( std :: vector<at::Tensor> & input , at :: Tensor & output , rank_type src_rank , THDGroup group_id)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduceT( at :: Tensor & t , THDReduceOp operation , THDGroup group_id)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduce( at :: Tensor & data , THDReduceOp operation , THDGroup group_id)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::reduce( at :: Tensor & data , THDReduceOp operation , rank_type dst_rank , THDGroup group_id)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcastT( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",26, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcast( at :: Tensor & data , rank_type src_rank , THDGroup group_id)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::send( Scalar & data , rank_type dst_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::send( at :: Tensor & data , rank_type dst_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( Scalar & data , rank_type src_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( at :: Tensor & data)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::receive( at :: Tensor & data , rank_type src_rank)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::isend( at :: Tensor & data , rank_type dst_rank)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::ireceive( at :: Tensor & data , rank_type src_rank)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allReduce( std :: vector<at::Tensor> & data , THDReduceOp operation , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::allGather( std :: vector<at::Tensor> & output , std :: vector<at::Tensor> & input , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::reduce( std :: vector<at::Tensor> & data , THDReduceOp operation , rank_type dstRank , THDGroup groupId)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::broadcast( std :: vector<at::Tensor> & data , rank_type srcRank , THDGroup groupId)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::clearGroupCache( THDGroup group_id)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::barrier( THDGroup group_id)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/THD/base/data_channels/DataChannelGloo.cpp,"thd::DataChannelGloo::newGroup( const std :: vector<rank_type> & ranks)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::~Work()",1, 31, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::isCompleted()",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::isSuccess() const",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::exception() const",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::sourceRank() const",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::synchronize()",1, 42, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::wait()",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::Work::finish( std :: exception_ptr exception)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::ProcessGroup( int rank , int size)",1, 77, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroup.cpp,"c10d::ProcessGroup::~ProcessGroup()",1, 33, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::setSocketNoDelay( int socket)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::getSocketPort( int fd)",22, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::sockaddrToString( struct :: sockaddr * addr)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::listen( PortType port)",52, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::connect( const std :: string & address , PortType port , bool wait , const std :: chrono :: milliseconds & timeout)",104, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Utils.cpp,"c10d::tcputil::accept( int listenSocket , const std :: chrono :: milliseconds & timeout)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::cudaAwareMpiCheck()",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSingleTensorHelper( const at :: Tensor & tensor)",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSingleTensor( const std :: vector<at::Tensor> & tensors)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::checkSameSizeAndType( const at :: Tensor & tensor , const std :: vector<at::Tensor> & tensors)",11, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::AsyncWork( at :: Tensor tensor , MPI_Request request)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::isCompleted()",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::isSuccess() const",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::sourceRank() const",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::wait()",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::AsyncWork::populateException()",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::mpiExit()",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::initMPIOnce()",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::createProcessGroupMPI( std :: vector<int> ranks)",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::ProcessGroupMPI( int rank , int size , MPI_Comm pgComm)",34, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::~ProcessGroupMPI()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::destroy()",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::abort()",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::runLoop()",29, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::enqueue( std :: unique_ptr<WorkEntry> entry)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::broadcast( std :: vector<at::Tensor> & tensors , const BroadcastOptions & opts)",22, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::allreduce( std :: vector<at::Tensor> & tensors , const AllreduceOptions & opts)",24, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::reduce( std :: vector<at::Tensor> & tensors , const ReduceOptions & opts)",29, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::allgather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const AllgatherOptions & opts)",45, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::gather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const GatherOptions & opts)",68, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::scatter( std :: vector<at::Tensor> & outputTensors , std :: vector<std::vector<at::Tensor>> & inputTensors , const ScatterOptions & opts)",66, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::send( std :: vector<at::Tensor> & tensors , int dstRank , int tag)",27, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::recv( std :: vector<at::Tensor> & tensors , int srcRank , int tag)",27, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::recvAnysource( std :: vector<at::Tensor> & tensors , int tag)",26, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::barrier( const BarrierOptions & opts)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp,"c10d::ProcessGroupMPI::getGroupRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Store.cpp,"c10d::Store::~Store()",1, 19, 0, 0
repos/cpp/pytorch/torch/lib/c10d/Store.cpp,"c10d::Store::setTimeout( const std :: chrono :: seconds & timeoutSec)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::GlooStore( const std :: shared_ptr<::c10d::Store> & store)",1, 76, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::set( const std :: string & key , const std :: vector<char> & value)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::get( const std :: string & key)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::wait( const std :: vector<std::string> & keys)",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::GlooStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",5, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::toFunction( const ReduceOp & r)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setInputs( O & opts , std :: vector<at::Tensor> & tensors)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setInput( O & opts , at :: Tensor & tensor)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setOutputs( O & opts , std :: vector<at::Tensor> & tensors)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::setOutput( O & opts , at :: Tensor & tensor)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::pinnedLike( at :: Tensor & tensor)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::initializeStreamsEvents( std :: vector<at::Tensor> & tensors , std :: vector<at::cuda::CUDAStream> & streams , std :: vector<at::cuda::CUDAEvent> & events)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::initializeStreamsEvents( std :: vector<std::vector<at::Tensor>> & tensors , std :: vector<at::cuda::CUDAStream> & streams , std :: vector<at::cuda::CUDAEvent> & events)",32, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::SendWork::SendWork( at :: Tensor & tensor , std :: unique_ptr<::gloo::transport::UnboundBuffer> buffer)",4, 53, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::SendWork::wait()",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::RecvWork( at :: Tensor & tensor , std :: unique_ptr<::gloo::transport::UnboundBuffer> buffer)",4, 67, 4, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::sourceRank() const",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::RecvWork::wait()",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::Options::Options()",4, 37, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::ProcessGroupGloo( const std :: shared_ptr<Store> & store , int rank , int size , Options options)",32, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::~ProcessGroupGloo()",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::nextTag()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::runLoop( int workerIndex)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::enqueue( std :: shared_ptr<AsyncWork> work)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::AsyncBroadcastWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , uint32_t tag)",11, 20, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::broadcast( at :: Tensor & tensor)",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastWork::run()",11, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::AsyncBroadcastCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , uint32_t tag)",17, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::run()",19, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBroadcastCUDAWork::synchronize()",9, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::broadcast( std :: vector<at::Tensor> & inputs , const BroadcastOptions & opts)",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::AsyncAllreduceWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , ReduceOp reduceOp , uint32_t tag)",6, 74, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::allreduce( std :: vector<at::Tensor> & tensors)",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::run()",11, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::getFunction( gloo :: AllreduceOptions :: Func & fn , const ReduceOp op)",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceWork::getFunction( const at :: ScalarType & dtype , const ReduceOp op)",7, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::AsyncAllreduceCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , ReduceOp reduceOp , uint32_t tag)",16, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::run()",23, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllreduceCUDAWork::synchronize()",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::allreduce( std :: vector<at::Tensor> & inputs , const AllreduceOptions & opts)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::AsyncReduceWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , ReduceOp reduceOp , uint32_t tag)",13, 20, 8, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::reduce( std :: vector<at::Tensor> & tensors)",9, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::run()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::getFunction( gloo :: ReduceOptions :: Func & fn , const ReduceOp op)",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceWork::getFunction( const at :: ScalarType & dtype , const ReduceOp op)",7, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::AsyncReduceCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & inputs , int rootRank , int rootTensor , ReduceOp reduceOp , uint32_t tag)",18, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::run()",19, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncReduceCUDAWork::synchronize()",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::reduce( std :: vector<at::Tensor> & inputs , const ReduceOptions & opts)",49, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::AsyncAllgatherWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , uint32_t tag)",6, 72, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::allgather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs)",25, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherWork::run()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::AsyncAllgatherCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , uint32_t tag)",25, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::run()",26, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncAllgatherCUDAWork::synchronize()",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::allgather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , const AllgatherOptions & opts)",65, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::AsyncGatherWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , int root , uint32_t tag)",11, 20, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::gather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs)",27, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherWork::run()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::AsyncGatherCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , int root , uint32_t tag)",26, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::run()",26, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncGatherCUDAWork::synchronize()",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::gather( std :: vector<std::vector<at::Tensor>> & outputs , std :: vector<at::Tensor> & inputs , const GatherOptions & opts)",56, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::AsyncScatterWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , int root , uint32_t tag)",11, 20, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::scatter( std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs)",17, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterWork::run()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::AsyncScatterCUDAWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , int root , uint32_t tag)",27, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::run()",23, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncScatterCUDAWork::synchronize()",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::scatter( std :: vector<at::Tensor> & outputs , std :: vector<std::vector<at::Tensor>> & inputs , const ScatterOptions & opts)",55, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::checkSingleTensor( std :: vector<at::Tensor> & tensors)",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::checkTag( int32_t tag)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::send( std :: vector<at::Tensor> & tensors , int dstRank , int tag)",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::recv( std :: vector<at::Tensor> & tensors , int srcRank , int tag)",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::recvAnysource( std :: vector<at::Tensor> & tensors , int tag)",27, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBarrierWork::AsyncBarrierWork( const std :: shared_ptr<gloo::Context> & context , std :: vector<std::weak_ptr<AsyncWork>> priorWork , uint32_t tag)",5, 71, 6, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::AsyncBarrierWork::run()",13, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::barrier( const BarrierOptions & opts)",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp,"c10d::ProcessGroupGloo::getGroupRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::PrefixStore( const std :: string & prefix , Store & store)",2, 40, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::joinKey( const std :: string & key)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::joinKeys( const std :: vector<std::string> & keys)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::set( const std :: string & key , const std :: vector<uint8_t> & value)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::get( const std :: string & key)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::add( const std :: string & key , int64_t value)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::check( const std :: vector<std::string> & keys)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::wait( const std :: vector<std::string> & keys)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/PrefixStore.cpp,"c10d::PrefixStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::syscall( F fn)",11, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::Lock( int fd , int operation)",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::~Lock()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::Lock( Lock && other)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::unlock()",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::Lock::flock( int operation)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::File( const std :: string & path , int flags , std :: chrono :: milliseconds timeout)",22, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::~File()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::lockShared()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::lockExclusive()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::seek( off_t offset , int whence)",5, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::tell()",5, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::size()",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const void * buf , size_t count)",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( void * buf , size_t count)",8, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const std :: string & str)",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::write( const std :: vector<uint8_t> & data)",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( std :: string & str)",7, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::File::read( std :: vector<uint8_t> & data)",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::refresh( File & file , off_t pos , std :: unordered_map<std::string,std::vector<uint8_t>> & cache)",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::FileStore( const std :: string & path , int numWorkers)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::~FileStore()",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::set( const std :: string & key , const std :: vector<uint8_t> & value)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::get( const std :: string & key)",28, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::addHelper( const std :: string & key , int64_t i)",20, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::add( const std :: string & key , int64_t i)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::check( const std :: vector<std::string> & keys)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::wait( const std :: vector<std::string> & keys)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/FileStore.cpp,"c10d::FileStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getNcclDataType( at :: ScalarType type)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getKeyFromDevices( const std :: vector<at::Device> & devices)",11, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::getDeviceList( const std :: vector<at::Tensor> & tensors)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::syncStreams( const std :: vector<at::Device> & devices , std :: vector<at::cuda::CUDAEvent> & ncclEvents , std :: vector<at::cuda::CUDAStream> & ncclStreams)",11, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::WorkNCCL( const std :: vector<at::Device> & devices)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL()",1, 43, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::isCompleted()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::isSuccess() const",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::exception() const",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecution()",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::synchronize()",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::WorkNCCL::wait()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::ProcessGroupNCCL( const std :: shared_ptr<Store> & store , int rank , int size , const std :: string & groupName)",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::~ProcessGroupNCCL()",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::broadcastUniqueNCCLID( ncclUniqueId * ncclID)",33, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::getNCCLComm( const std :: string & devicesKey , const std :: vector<at::Device> & devices)",69, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::tensorCheckHelper( const std :: vector<at::Tensor> & input , const std :: vector<at::Tensor> & output , int outputOverInput)",75, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::allreduce( std :: vector<at::Tensor> & tensors , const AllreduceOptions & opts)",46, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::broadcast( std :: vector<at::Tensor> & tensors , const BroadcastOptions & opts)",47, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::reduce( std :: vector<at::Tensor> & tensors , const ReduceOptions & opts)",49, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::allgather( std :: vector<std::vector<at::Tensor>> & outputTensors , std :: vector<at::Tensor> & inputTensors , const AllgatherOptions & opts)",73, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::barrier( const BarrierOptions & opts)",39, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::gather( std :: vector<std::vector<at::Tensor>> & , std :: vector<at::Tensor> & , const GatherOptions &)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::scatter( std :: vector<at::Tensor> & , std :: vector<std::vector<at::Tensor>> & , const ScatterOptions &)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::send( std :: vector<at::Tensor> & , int , int)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::recv( std :: vector<at::Tensor> & , int , int)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::recvAnysource( std :: vector<at::Tensor> & , int)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp,"c10d::ProcessGroupNCCL::getGroupRank()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::TCPStoreDaemon( int storeListenSocket)",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::~TCPStoreDaemon()",18, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::join()",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::run()",100, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::stop()",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::query( int socket)",23, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::wakeupWaitingClients( const std :: string & key)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::setHandler( int socket)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::addHandler( int socket)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::getHandler( int socket) const",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::checkHandler( int socket) const",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::waitHandler( int socket)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStoreDaemon::checkKeys( const std :: vector<std::string> & keys) const",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::TCPStore( const std :: string & masterAddr , PortType masterPort , bool isServer)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::~TCPStore()",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::set( const std :: string & key , const std :: vector<uint8_t> & data)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::get( const std :: string & key)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::add( const std :: string & key , int64_t value)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::check( const std :: vector<std::string> & keys)",16, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::wait( const std :: vector<std::string> & keys)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/TCPStore.cpp,"c10d::TCPStore::wait( const std :: vector<std::string> & keys , const std :: chrono :: milliseconds & timeout)",25, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/example/allreduce.cpp,"main( int argc , char ** argv)",27, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"initialize( const std :: string & path , int N , Args && ... args)",17, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::AsyncTest( const std :: string & path)",1, 54, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::AsyncTest( AsyncTest && other)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::getProcessGroup()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncTest::start( int rank , int size)",12, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::AsyncInputIsOutputTest( const std :: string & path , int numTensors)",28, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::wait( std :: shared_ptr<ProcessGroup::Work> & work)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncInputIsOutputTest::getTensors()",13, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncAllreduceTest::AsyncAllreduceTest( const std :: string & path , int numTensors)",2, 52, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncAllreduceTest::run()",19, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncBroadcastTest::AsyncBroadcastTest( const std :: string & path , int numTensors)",2, 52, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"AsyncBroadcastTest::run( int rootRank , int rootTensor)",22, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"runAsyncAllreduceTest( const std :: string & path , size_t numProcesses , size_t numTensors)",31, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"runAsyncBroadcastTest( const std :: string & path , size_t numProcesses , size_t numTensors)",36, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,"main( int argc , char ** argv)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::NCCLTestBase( const std :: string & path)",1, 57, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::NCCLTestBase( NCCLTestBase && other)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::getProcessGroup()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTestBase::initialize( int rank , int size)",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::NCCLTest( const std :: string & path , int worldSize)",31, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::wait( std :: shared_ptr<ProcessGroup::Work> & work)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::getTensors()",14, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::getOutputTensors()",18, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"NCCLTest::numDevices() const",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllreduceNCCLTest::AllreduceNCCLTest( const std :: string & path , int worldSize)",2, 37, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllreduceNCCLTest::run()",19, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"BroadcastNCCLTest::BroadcastNCCLTest( const std :: string & path , int worldSize)",2, 37, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"BroadcastNCCLTest::run( int rootRank , int rootTensor)",22, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"ReduceNCCLTest::ReduceNCCLTest( const std :: string & path , int worldSize)",2, 37, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"ReduceNCCLTest::run( int rootRank , int rootTensor)",22, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllgatherNCCLTest::AllgatherNCCLTest( const std :: string & path , int worldSize)",2, 37, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"AllgatherNCCLTest::run()",19, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testAllreduce( const std :: string & path , int rank , int size)",22, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testBroadcast( const std :: string & path , int rank , int size)",29, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testReduce( const std :: string & path , int rank , int size)",30, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"testAllgather( const std :: string & path , int rank , int size)",25, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupNCCLTest.cpp,"main( int argc , char ** argv)",25, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"tmppath()",19, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"testHelper( const std :: string prefix = "")",55, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/FileStoreTest.cpp,"main( int argc , char ** argv)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/TCPStoreTest.cpp,"testHelper( const std :: string & prefix = "")",82, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/TCPStoreTest.cpp,"main( int argc , char ** argv)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::SignalTest( const std :: string & path)",1, 55, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::~SignalTest()",5, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::arm( int pid , int signal)",6, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"SignalTest::run( int rank , int size)",30, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testSignal( const std :: string & path , int signal)",14, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::initialize( const std :: string & path , int num)",19, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::CollectiveTest( const std :: string & path)",1, 59, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::CollectiveTest( CollectiveTest && other)",4, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::getProcessGroup()",3, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"CollectiveTest::start( int rank , int size)",13, 4, 2, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"copyTensors( const std :: vector<std::vector<at::Tensor>> & inputs)",13, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testAllreduce( const std :: string & path , const at :: Backend b)",35, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testBroadcast( const std :: string & path , const at :: Backend b)",55, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"testBarrier( const std :: string & path)",15, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupGlooTest.cpp,"main( int argc , char ** argv)",53, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"waitWork( std :: shared_ptr<c10d::ProcessGroupMPI> pg , std :: vector<std::shared_ptr<c10d::ProcessGroup::Work>> works)",12, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testAllreduce( int iter = 1000)",32, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testBroadcast( int iter = 10000)",35, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testReduce( int iter = 10000)",35, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testAllgather( int iter = 10000)",43, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testGather( int iter = 10000)",49, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testScatter( int iter = 1)",48, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"testSendRecv( bool recvAnysource , int iter = 10000)",58, 2, 0, 0
repos/cpp/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp,"main( int argc , char ** argv)",23, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"libshm_init( const char * manager_exec_path)",2, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"deleteTHManagedMapAllocator( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"THManagedMapAllocator::makeDataPtr( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm_windows/core.cpp,"THManagedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"get_alloc_info( const char * filename)",11, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"start_manager()",40, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"get_manager_socket( const std :: string & manager_handle)",10, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"libshm_init( const char * manager_exec_path)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocatorInit::THManagedMapAllocatorInit( const char * manager_handle , const char * filename)",21, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::THManagedMapAllocator( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",2, 108, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::close()",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"deleteTHManagedMapAllocator( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::makeDataPtr( const char * manager_handle , const char * filename , int flags , ptrdiff_t size)",4, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/core.cpp,"THManagedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"ClientSession::ClientSession( ManagerSocket s)",1, 66, 2, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"register_fd( int fd)",6, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"unregister_fd( int fd)",7, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"print_init_message( const char * message)",5, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"object_exists( const char * name)",9, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"free_used_object( const std :: string & name)",8, 2, 0, 0
repos/cpp/pytorch/torch/lib/libshm/manager.cpp,"main( int argc , char * argv [ ])",84, 2, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_New( void * data , void(*destructor)(void*))",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_check( PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_get( PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_dealloc( THPWrapper * self)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/PtrWrapper.cpp,"THPWrapper_init( PyObject * module)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_New( at :: Layout layout , const std :: string & name)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_repr( THPLayout * self)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Layout.cpp,"THPLayout_init( PyObject * module)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/stub.cpp,"init_C()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/stub.cpp,"PyInit__C()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_New()",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_NewWithGenerator( at :: Generator & cdata)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_dealloc( THPGenerator * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_getState( THPGenerator * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_setState( THPGenerator * self , PyObject * _new_state)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_manualSeed( THPGenerator * self , PyObject * seed)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_seed( THPGenerator * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_initialSeed( THPGenerator * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Generator.cpp,"THPGenerator_init( PyObject * module)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_New( at :: ScalarType scalar_type , const std :: string & name)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_is_floating_point( THPDtype * self)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_reduce( THPDtype * self)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_repr( THPDtype * self)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Dtype.cpp,"THPDtype_init( PyObject * module)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_getCallable( PyObject * arg , PyObject ** result)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackSize( PyObject * arg)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_tryUnpackLongs( PyObject * arg , THLongStoragePtr & result)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackLongs( PyObject * arg)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_tryUnpackLongVarArgs( PyObject * args , int ignore_first , THLongStoragePtr & result)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_checkIntTuple( PyObject * arg)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_unpackIntTuple( PyObject * arg)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_setError( const char * format , ...)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_addPyMethodDefs( std :: vector<PyMethodDef> & vector , PyMethodDef * methods)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"classOrTypename( PyObject * obj)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_dispatchStateless( PyObject * tensor , const char * name , PyObject * args , PyObject * kwargs)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPUtils_invalidArguments( PyObject * given_args , PyObject * given_kwargs , const char * function_name , size_t num_options , ...)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THPGenerator>::free()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"setBackCompatBroadcastWarn( bool warn)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"getBackCompatBroadcastWarn()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"setBackCompatKeepdimWarn( bool warn)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"getBackCompatKeepdimWarn()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"maybeThrowBackCompatKeepdimWarn( char * func)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THTensor>::free()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils.cpp,"THPPointer<THPStorage>::free()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Storage.cpp,"THPPointer<THStorage>::free()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialRead<int>( int fildes , void * buf , size_t nbytes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialRead<PyObject*>( PyObject * fildes , void * buf , size_t nbytes)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialWrite<int>( int fildes , void * buf , size_t nbytes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialWrite<PyObject*>( PyObject * fildes , void * buf , size_t nbytes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"isUnsupportedOperation()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonReadBuffered( PyObject * fildes , void * buf , size_t raw_nbytes)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonIO( PyObject * fildes , void * buf , size_t nbytes , bool is_read)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonReadInto( PyObject * fildes , void * buf , size_t nbytes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doPartialPythonWrite( PyObject * fildes , void * buf , size_t nbytes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doRead( io fildes , void * raw_buf , size_t nbytes)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/serialization.cpp,"doWrite( io fildes , void * raw_buf , size_t nbytes)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"THPException_init( PyObject * module)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::replaceAll( std :: string & str , const std :: string & old_str , const std :: string & new_str)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::processErrorMsg( std :: string str)",76, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::formatMessage( const char * format , va_list fmt_args)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::IndexError::IndexError( const char * format , ...)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::TypeError::TypeError( const char * format , ...)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Exceptions.cpp,"torch::ValueError::ValueError( const char * format , ...)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_initNames( PyObject * self , PyObject * arg)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_initExtension( PyObject * _unused , PyObject * shm_manager_path)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfCsrcASAN( PyObject * module , PyObject * arg)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfCsrcUBSAN( PyObject * module , PyObject * arg)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_crashIfATenASAN( PyObject * module , PyObject * arg)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getNumThreads( PyObject * module)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setNumThreads( PyObject * module , PyObject * arg)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDefaultTensorType( PyObject * _unused , PyObject * type)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDefaultDtype( PyObject * _unused , PyObject * dtype)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_safeCall( PyObject * _unused , PyObject * args , PyObject * kwargs)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_addDocStr( PyObject * _unused , PyObject * args)",50, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_inferSize( PyObject * _unused , PyObject * args)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBackcompatBroadcastWarn( PyObject * module , PyObject * arg)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getBackcompatBroadcastWarn( PyObject * module)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBackcompatKeepdimWarn( PyObject * module , PyObject * arg)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getBackcompatKeepdimWarn( PyObject * module)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_hasDistributed( PyObject * _unused)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"DLPack_Capsule_Destructor( PyObject * data)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_toDLPack( PyObject * _unused , PyObject * data)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_fromDLPack( PyObject * _unused , PyObject * data)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setUserEnabledCuDNN( PyObject * _unused , PyObject * arg)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_userEnabledCuDNN( PyObject * _unused)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setDeterministicCuDNN( PyObject * _unused , PyObject * arg)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_deterministicCuDNN( PyObject * _unused)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setBenchmarkCuDNN( PyObject * _unused , PyObject * arg)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_benchmarkCuDNN( PyObject * _unused)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_setFlushDenormal( PyObject * _unused , PyObject * arg)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_getDefaultDtype( PyObject * _unused , PyObject * arg)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THPModule_isDefaultTypeCuda( PyObject * _unused , PyObject * arg)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THCUDNN_cudnn_version( PyObject * self , PyObject * args)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"THCUDNN_methods()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"warning_handler( const c10 :: SourceLocation & source_location , const char * msg)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"initModule()",134, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"pytorch_duplicate_guard()",8, 3, 0, 0
repos/cpp/pytorch/torch/csrc/Module.cpp,"call_duplicate_guard::call_duplicate_guard()",1, 56, 2, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes16( void * ptr)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes32( void * ptr)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"swapBytes64( void * ptr)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt16LE( const uint8_t * data)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt16BE( const uint8_t * data)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt32LE( const uint8_t * data)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt32BE( const uint8_t * data)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt64LE( const uint8_t * data)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"decodeUInt64BE( const uint8_t * data)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_nativeByteOrder()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt16Buffer( int16_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt32Buffer( int32_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeInt64Buffer( int64_t * dst , const uint8_t * src , THPByteOrder order , size_t len)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeHalfBuffer( THHalf * dst , const uint8_t * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeFloatBuffer( float * dst , const uint8_t * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_decodeDoubleBuffer( double * dst , const uint8_t * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt16Buffer( uint8_t * dst , const int16_t * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt32Buffer( uint8_t * dst , const int32_t * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeInt64Buffer( uint8_t * dst , const int64_t * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeFloatBuffer( uint8_t * dst , const float * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/byte_order.cpp,"THP_encodeDoubleBuffer( uint8_t * dst , const double * src , THPByteOrder order , size_t len)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"setSignalHandler( int signal , void(*handler)(int,siginfo_t*,void*) , struct sigaction * old_sa_ptr)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"handler_SIGTERM( int sig , siginfo_t * info , void * ctx)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerSignalHandlers( PyObject * module , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_errorIfAnyWorkerFails( PyObject * module)",43, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_updateWorkerPIDs( PyObject * module , PyObject * args)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_removeWorkerPIDs( PyObject * module , PyObject * loader_id)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_setWorkerSignalHandlers( PyObject * module , PyObject * _ignored)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_updateWorkerPIDs( PyObject * module , PyObject * _ignored)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_removeWorkerPIDs( PyObject * module , PyObject * _ignored)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DataLoader.cpp,"THPModule_errorIfAnyWorkerFails( PyObject * module , PyObject * _ignored)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_New( const at :: Device & device)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_repr( THPDevice * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_str( THPDevice * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_type( THPDevice * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_index( THPDevice * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_hash( THPDevice * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_rc( PyObject * a , PyObject * b , int op)",33, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_reduce( THPDevice * self)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Device.cpp,"THPDevice_init( PyObject * module)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::get_backend( bool is_cuda , bool is_sparse)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::get_type( const std :: string & name , bool is_cuda , bool is_sparse)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getPyTypeObject( const at :: Storage & storage)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerStoragePyTypeObject( PyTypeObject * pytype , const std :: string & name , bool is_cuda , bool is_sparse)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerDtypeObject( THPDtype * dtype , at :: ScalarType scalarType)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::registerLayoutObject( THPLayout * layout , at :: Backend backend)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getVariableType( at :: ScalarType scalarType , const THPLayout & layout , const at :: Device & device)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getDtype( at :: ScalarType scalarType)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getLayout( at :: Backend backend)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::getDeviceType( const at :: Type & type)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::createPyObject( const at :: Storage & storage)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::isStorage( PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/DynamicTypes.cpp,"torch::createStorage( PyObject * obj)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_New( const at :: ScalarType & type)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_New( const at :: ScalarType & type)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_str( THPFInfo * self)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_str( THPIInfo * self)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_compare( THPDTypeInfo * a , THPDTypeInfo * b , int op)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_bits( THPDTypeInfo * self , void *)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_eps( THPFInfo * self , void *)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_max( THPFInfo * self , void *)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPIInfo_max( THPFInfo * self , void *)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPFInfo_tiny( THPFInfo * self , void *)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/TypeInfo.cpp,"THPDTypeInfo_init( PyObject * module)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/nvrtc.cpp,"PyInit__nvrtc( void)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_New( const torch :: autograd :: Variable & var)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_NewFromSizes( int dim , const int64_t * sizes)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"isTracedZeroDimVar( PyObject * item)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",31, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_repr( THPSize * self)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"wrap_tuple_fn( Args ... args)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_numel( THPSize * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/Size.cpp,"THPSize_init( PyObject * module)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/multiprocessing/init.cpp,"torch::multiprocessing::multiprocessing_init( PyObject * _unused)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/multiprocessing/init.cpp,"torch::multiprocessing::python_functions()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/onnx/init.cpp,"torch::onnx::initONNXBindings( PyObject * module)",33, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::throw_nccl_error( ncclResult_t status)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::NcclCommList( const std :: vector<int> & devices)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::~NcclCommList()",24, 4, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::NcclCommList::ref() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_get_communicators( TensorList inputs)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_get_data_type( const Type & type)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::detail::_check_inputs( TensorList inputs , TensorList outputs , int input_multiplier , int output_multiplier)",66, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::is_available( TensorList tensors)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::version()",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::get_max_count()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::broadcast( TensorList tensors , const stream_list & streams , const comm_list & user_comms)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::reduce( const std :: vector<at::Tensor> & inputs , std :: vector<at::Tensor> & outputs , int32_t root , int32_t op , const stream_list & streams , const comm_list & user_comms)",46, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/nccl.cpp,"torch::cuda::nccl::reduce( std :: vector<at::Tensor> & inputs , int32_t root , int32_t op , const stream_list & streams , const comm_list & user_comms)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_version( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_unique_id( PyObject * self , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_nccl_comm( PyObject * capsule)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"destroy_nccl_comm( PyObject * capsule)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_streams( PyObject * obj , size_t size)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"unpack_comms( PyObject * obj , size_t size)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_init_rank( PyObject * self , PyObject * args)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_reduce( PyObject * self , PyObject * args)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_all_reduce( PyObject * self , PyObject * args)",55, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_broadcast( PyObject * self , PyObject * args)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_all_gather( PyObject * self , PyObject * args)",61, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"THCPModule_nccl_reduce_scatter( PyObject * self , PyObject * args)",53, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_nccl.cpp,"extract_tensors( PyObject * obj)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"torch::cuda::unique_type_checker::show( const at :: Type & t)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/cuda/comm.cpp,"torch::cuda::broadcast( const Tensor & tensor , IntList devices)",208, 28, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/utils.cpp,"THPUtils_PySequence_to_CUDAStreamList( PyObject * obj)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Stream.cpp,"THCPStream_init( PyObject * module)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/python_comm.cpp,"torch::cuda::python::initCommMethods( PyObject * module)",52, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setDevice( int device)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setDevice_wrap( PyObject * self , PyObject * arg)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDevice_wrap( PyObject * self)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDeviceCount_wrap( PyObject * self)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCurrentStream_wrap( PyObject * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setStream_wrap( PyObject * self , PyObject * obj)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_isDriverSufficient( PyObject * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getDriverVersion( PyObject * self)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCompiledVersion( PyObject * self)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getRNGState( PyObject * _unused)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_setRNGState( PyObject * _unused , PyObject * obj)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_manualSeed( PyObject * _unused , PyObject * seed)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_manualSeedAll( PyObject * _unused , PyObject * seed)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_seed( PyObject * _unused)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_seedAll( PyObject * _unused)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_initialSeed( PyObject * _unused)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaHostAllocator( PyObject * _unused)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaSynchronize( PyObject * _unused)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaSleep( PyObject * _unused , PyObject * cycles)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaLockMutex( PyObject * module)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_cudaUnlockMutex( PyObject * module)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_emptyCache( PyObject * _unused)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_memoryAllocated( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_maxMemoryAllocated( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_memoryCached( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_maxMemoryCached( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"bindCudaDeviceProperties( PyObject * module)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_initExtension( PyObject * self)",44, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_useNccl()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_getCurrentBlasHandle_wrap( PyObject * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"THCPModule_methods()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/cuda/Module.cpp,"torch::cuda::initModule( PyObject * module)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_initProcessGroup( PyObject * _unused , PyObject * args)",29, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_destroyProcessGroup( PyObject * _unused)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_registerStream( PyObject * _unused , PyObject * _stream)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_getRank( PyObject * _unused)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_getNumProcesses( PyObject * _unused)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_makeDescriptor( PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_unpackRequest( PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_getReduceOp( PyObject * obj)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"_getGroup( PyObject * obj)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_clearGroupCache( PyObject * _unused , PyObject * args)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_isend( PyObject * _unused , PyObject * args)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_irecv( PyObject * _unused , PyObject * args)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_send( PyObject * _unused , PyObject * args)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_recvAnySource( PyObject * _unused , PyObject * _tensor)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_recv( PyObject * _unused , PyObject * args)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allReduceMultiGPU( PyObject * _unused , PyObject * args)",52, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_reduceMultiGPU( PyObject * _unused , PyObject * args)",56, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_broadcastMultiGPU( PyObject * _unused , PyObject * args)",53, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allGatherMultiGPU( PyObject * _unused , PyObject * args)",80, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allReduce( PyObject * _unused , PyObject * args)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_reduce( PyObject * _unused , PyObject * args)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_broadcast( PyObject * _unused , PyObject * args)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_allGather( PyObject * _unused , PyObject * args)",49, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_gatherSend( PyObject * _unused , PyObject * args)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_gatherRecv( PyObject * _unused , PyObject * args)",48, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_scatterSend( PyObject * _unused , PyObject * args)",48, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_scatterRecv( PyObject * _unused , PyObject * args)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_barrier( PyObject * _unused , PyObject * _group)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_newGroup( PyObject * _unused , PyObject * args)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_requestIsCompleted( PyObject * _unused , PyObject * _req)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_requestWait( PyObject * _unused , PyObject * _req)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_initExtension( PyObject * _unused , PyObject * args)",37, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/Module.cpp,"THDPModule_methods()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/init.cpp,"torch::distributed::c10d::c10d_init( PyObject * _unused)",450, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/init.cpp,"torch::distributed::c10d::python_functions()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::copyBroadcastTensorsToReplicas( const std :: vector<std::vector<at::Tensor>> & broadcastTensors , std :: vector<std::vector<at::Tensor>> & replicaData)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::bucketTensors( std :: vector<at::Tensor> & tensors , int64_t bucketSize , bool fineGrained)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::distBroadcastCoalesced( ProcessGroup & processGroup , std :: vector<at::Tensor> & tensors , int64_t bufferSize , bool fineGrained)",40, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::syncParams( ProcessGroup & processGroup , std :: vector<std::vector<at::Tensor>> & parameterData , std :: vector<std::vector<at::Tensor>> & bufferData , const std :: vector<int64_t> & devices , int64_t broadcastBucketSize , bool broadcastBuffers)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::queueReduction( ProcessGroup & processGroup , std :: vector<std::vector<at::Tensor>> & gradsBatch , const std :: vector<int64_t> & devices)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/distributed/c10d/ddp.cpp,"c10d::syncReduction( std :: shared_ptr<ProcessGroup::Work> & reductionWork , std :: vector<at::Tensor> & gradsBatch , at :: Tensor & gradsBatchCoalesced)",31, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::device_count()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::is_available()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/cuda.cpp,"torch::cuda::cudnn_is_available()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/jit.cpp,"torch::jit::compile( const std :: string & source)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::check_is_little_endian()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::flip_endianness( uint32_t value)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_int32( std :: ifstream & stream)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::expect_int32( std :: ifstream & stream , uint32_t expected)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::join_paths( std :: string head , const std :: string & tail)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_images( const std :: string & root , bool train)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::read_targets( const std :: string & root , bool train)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::MNIST( const std :: string & root , Mode mode)",3, 60, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::get( size_t index)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::size() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::is_train() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::images() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp,"torch::data::datasets::MNIST::targets() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::BatchSize( size_t size)",1, 51, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::size() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::BatchSize::operator size_t() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::StreamSampler( size_t epoch_size)",1, 77, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::reset( optional<size_t> new_size)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::next( size_t batch_size)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::save( serialize :: OutputArchive & archive) const",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/stream.cpp,"torch::data::samplers::StreamSampler::load( serialize :: InputArchive & archive)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::SequentialSampler( size_t size)",1, 67, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::reset( optional<size_t> new_size)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::next( size_t batch_size)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::save( serialize :: OutputArchive & archive) const",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::load( serialize :: InputArchive & archive)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/sequential.cpp,"torch::data::samplers::SequentialSampler::index() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::RandomSampler( int64_t size , Dtype index_dtype)",2, 54, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::reset( optional<size_t> new_size)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::next( size_t batch_size)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::save( serialize :: OutputArchive & archive) const",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::load( serialize :: InputArchive & archive)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/data/samplers/random.cpp,"torch::data::samplers::RandomSampler::index() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::AdamOptions::AdamOptions( double learning_rate)",2, 39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::step()",38, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::save( serialize :: OutputArchive & archive) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adam.cpp,"torch::optim::Adam::load( serialize :: InputArchive & archive)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/serialize.cpp,"torch::optim::detail::serialize( serialize :: OutputArchive & archive , const std :: string & key , const std :: vector<int64_t> & steps)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/serialize.cpp,"torch::optim::detail::serialize( serialize :: InputArchive & archive , const std :: string & key , std :: vector<int64_t> & steps)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::OptimizerBase( std :: vector<Tensor> parameters)",2, 44, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::add_parameters( const std :: vector<Tensor> & parameters)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::zero_grad()",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::parameters() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::parameters()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::size() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::buffer_at( std :: vector<Tensor> & buffers , size_t index)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::save( serialize :: OutputArchive & archive) const",1, 69, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::OptimizerBase::load( serialize :: InputArchive & archive)",1, 62, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::operator < <( serialize :: OutputArchive & archive , const OptimizerBase & optimizer)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/optimizer.cpp,"torch::optim::detail::operator > >( serialize :: InputArchive & archive , OptimizerBase & optimizer)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSpropOptions::RMSpropOptions( double learning_rate)",2, 39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::step()",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::save( serialize :: OutputArchive & archive) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/rmsprop.cpp,"torch::optim::RMSprop::load( serialize :: InputArchive & archive)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGSOptions::LBFGSOptions( double learning_rate)",2, 39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::gather_flat_grad()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::add_grad( const torch :: Tensor & step_size , const Tensor & update)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::step( LossClosure closure)",119, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::save( serialize :: OutputArchive & archive) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/lbfgs.cpp,"torch::optim::LBFGS::load( serialize :: InputArchive & archive)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::AdagradOptions::AdagradOptions( double learning_rate)",2, 39, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::step()",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::save( serialize :: OutputArchive & archive) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/adagrad.cpp,"torch::optim::Adagrad::load( serialize :: InputArchive & archive)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGDOptions::SGDOptions( double learning_rate)",1, 80, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::step()",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::save( serialize :: OutputArchive & archive) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/optim/sgd.cpp,"torch::optim::SGD::load( serialize :: InputArchive & archive)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/python/init.cpp,"torch::python::bind_ordered_dict( py :: module module , const char * dict_name)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/python/init.cpp,"torch::python::init_bindings( PyObject * module)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::InputArchive()",2, 58, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::read( const std :: string & key , Tensor & tensor , bool is_buffer)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::read( const std :: string & key , InputArchive & archive)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::load_from( const std :: string & filename , c10 :: optional<torch::Device> device)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/input-archive.cpp,"torch::serialize::InputArchive::load_from( std :: istream & stream , c10 :: optional<torch::Device> device)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::OutputArchive()",2, 58, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::write( const std :: string & key , const Tensor & tensor , bool is_buffer)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::write( const std :: string & key , OutputArchive & nested_archive)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::save_to( const std :: string & filename)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/serialize/output-archive.cpp,"torch::serialize::OutputArchive::save_to( std :: ostream & stream)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::Fan::Fan( Tensor & tensor)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::constant_( Tensor tensor , Scalar value)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::dirac_( Tensor tensor)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::eye_( Tensor matrix)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::normal_( Tensor tensor , double mean , double std)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::ones_( Tensor tensor)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::orthogonal_( Tensor tensor , double gain)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::sparse_( Tensor tensor , double sparsity , double std)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::uniform_( Tensor tensor , double low , double high)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::xavier_normal_( Tensor tensor , double gain)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::xavier_uniform_( Tensor tensor , double gain)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/init.cpp,"torch::nn::init::zeros_( Tensor tensor)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::join_name( const std :: string & name_prefix , const std :: string & name)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::extend( std :: vector<Tensor> & vector , const OrderedDict<std::string,Tensor> & dict)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::Module()",2, 78, 4, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::Module( std :: string name)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::name() const",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::clone( const optional<Device> & device) const",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ModuleApplyFunction & function)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ConstModuleApplyFunction & function) const",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const NamedModuleApplyFunction & function , const std :: string & name_prefix)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ConstNamedModuleApplyFunction & function , const std :: string & name_prefix) const",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const ModulePointerApplyFunction & function) const",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply( const NamedModulePointerApplyFunction & function , const std :: string & name_prefix) const",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::parameters( bool recurse) const",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_parameters( bool recurse) const",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::buffers( bool recurse) const",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_buffers( bool recurse) const",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::modules( bool include_self) const",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_modules( const std :: string & name_prefix , bool include_self) const",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::children() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::named_children() const",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::train( bool on)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::eval()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Device device , torch :: Dtype dtype , bool non_blocking)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Dtype dtype , bool non_blocking)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::to( torch :: Device device , bool non_blocking)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::is_training() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::zero_grad()",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::save( serialize :: OutputArchive & archive) const",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::load( serialize :: InputArchive & archive)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::register_parameter( std :: string name , Tensor tensor , bool requires_grad)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::register_buffer( std :: string name , Tensor tensor)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::clone_( Module & other , const optional<Device> & device)",1, 70, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::apply_to_submodules( const NamedModulePointerApplyFunction & function , const std :: string & name_prefix) const",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::Module::shared_from_this_checked() const",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator < <( serialize :: OutputArchive & archive , const std :: shared_ptr<nn::Module> & module)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/module.cpp,"torch::nn::operator > >( serialize :: InputArchive & archive , const std :: shared_ptr<nn::Module> & module)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormOptions::BatchNormOptions( int64_t features)",1, 78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::BatchNormImpl( BatchNormOptions options)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::reset()",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::forward( const Tensor & input)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/batchnorm.cpp,"torch::nn::BatchNormImpl::pure_forward( const Tensor & input , const Tensor & mean , const Tensor & variance)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingOptions::EmbeddingOptions( int64_t count , int64_t dimension)",2, 46, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::EmbeddingImpl( EmbeddingOptions options)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::reset()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/embedding.cpp,"torch::nn::EmbeddingImpl::forward( const Tensor & input)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::ConvImpl( ConvOptions<D> options)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::ConvImpl<D,Derived>::reset()",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv1dImpl::forward( const Tensor & input)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv2dImpl::forward( const Tensor & input)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/conv.cpp,"torch::nn::Conv3dImpl::forward( const Tensor & input)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::FunctionalImpl( Function function)",2, 40, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::reset()",1, 32, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::forward( Tensor input)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/functional.cpp,"torch::nn::FunctionalImpl::operator ( )( Tensor input)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearOptions::LinearOptions( int64_t in , int64_t out)",1, 78, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::LinearImpl( LinearOptions options)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::reset()",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/linear.cpp,"torch::nn::LinearImpl::forward( const Tensor & input)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNOptionsBase::RNNOptionsBase( int64_t input_size , int64_t hidden_size)",2, 60, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::RNNImplBase( const RNNOptionsBase & options_ , optional<CuDNNMode> cudnn_mode , int64_t number_of_gates)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::reset()",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Device device , torch :: Dtype dtype , bool non_blocking)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Dtype dtype , bool non_blocking)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::to( torch :: Device device , bool non_blocking)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::flatten_parameters()",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::generic_forward( std :: function<RNNFunctionSignature> function , const Tensor & input , Tensor state)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::flat_weights() const",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::detail::RNNImplBase<Derived>::any_parameters_alias() const",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::RNNOptions( int64_t input_size , int64_t hidden_size)",2, 60, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::tanh()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNOptions::relu()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::RNNImpl( const RNNOptions & options)",10, 26, 6, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::RNNImpl::forward( const Tensor & input , Tensor state)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::LSTMImpl::LSTMImpl( const LSTMOptions & options)",5, 36, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::LSTMImpl::forward( const Tensor & input , Tensor state)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::GRUImpl::GRUImpl( const GRUOptions & options)",5, 36, 10, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/rnn.cpp,"torch::nn::GRUImpl::forward( const Tensor & input , Tensor state)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::detail::DropoutImplBase<Derived>::DropoutImplBase( DropoutOptions options_)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::detail::DropoutImplBase<Derived>::reset()",1, 42, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutOptions::DropoutOptions( double rate)",1, 61, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::DropoutImpl::forward( const Tensor & input)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/api/src/nn/modules/dropout.cpp,"torch::nn::FeatureDropoutImpl::forward( const Tensor & input)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/object_ptr.cpp,"THPPointer<PyObject>::free()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::TupleParser( PyObject * args , int num_args)",8, 3, 1, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( bool & x , const std :: string & param_name)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( int & x , const std :: string & param_name)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( double & x , const std :: string & param_name)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( std :: vector<int> & x , const std :: string & param_name)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::parse( std :: string & x , const std :: string & param_name)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::next_arg()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tuple_parser.cpp,"torch::TupleParser::invalid_type( const std :: string & expected , const std :: string & param_name)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_list.cpp,"torch::utils::recursive_to_list( char * data , IntList sizes , IntList strides , int64_t dim , ScalarType scalarType , int64_t elementSize)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_list.cpp,"torch::utils::tensor_to_list( const Tensor & tensor)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::backend_to_string( const at :: Type & type)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::type_to_string( const at :: Type & type)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::type_from_string( const std :: string & str)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_types.cpp,"torch::utils::all_declared_types()",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_layouts.cpp,"torch::utils::initializeLayouts()",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/cuda_lazy_init.cpp,"torch::utils::cuda_lazy_init()",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::py_typename( PyObject * object)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SimpleType::SimpleType( std :: string & name)",1, 48, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SimpleType::is_matching( PyObject * object)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::MultiType::MultiType( std :: initializer_list<std::string> accepted_types)",2, 30, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::MultiType::is_matching( PyObject * object)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::NullableType::NullableType( std :: unique_ptr<Type> type)",1, 70, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::NullableType::is_matching( PyObject * object)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::TupleType::TupleType( std :: vector<std::unique_ptr<Type>> types)",2, 32, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::TupleType::is_matching( PyObject * object)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SequenceType::SequenceType( std :: unique_ptr<Type> type)",2, 30, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::SequenceType::is_matching( PyObject * object)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Argument::Argument( std :: string name , std :: unique_ptr<Type> type)",2, 55, 2, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( std :: vector<Argument> arguments , bool is_variadic , bool has_out)",2, 86, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( bool is_variadic , bool has_out)",2, 66, 6, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::Option::Option( Option && other)",3, 31, 4, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_splitString( const std :: string & s , const std :: string & delim)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_buildType( std :: string type_name , bool is_nullable)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_parseOption( const std :: string & _option_str , const std :: unordered_map<std::string,PyObject*> & kwargs)",63, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_argcountMatch( const Option & option , const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_formattedArgDesc( const Option & option , const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",51, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_argDesc( const std :: vector<PyObject*> & arguments , const std :: unordered_map<std::string,PyObject*> & kwargs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::_tryMatchKwargs( const Option & option , const std :: unordered_map<std::string,PyObject*> & kwargs)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/invalid_arguments.cpp,"torch::format_invalid_args( PyObject * given_args , PyObject * given_kwargs , const std :: string & function_name , const std :: vector<std::string> & options)",80, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::maybe_initialize_cuda( const Type & type)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::maybe_initialize_cuda( const Device device)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_zeros( const Type & type , optional<Device> device , IntList sizes)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_ones( const Type & type , optional<Device> device , IntList sizes)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::dispatch_full( const Type & type , Scalar fill_value , optional<Device> device , IntList sizes)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_sizes( const Type & type , optional<Device> device , IntList sizes)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_storage( const Type & type , Storage storage)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_with_tensor( const Type & type , const Tensor & other)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::compute_sizes( PyObject * seq)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::infer_scalar_type( PyObject * obj)",51, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::recursive_store( char * data , IntList sizes , IntList strides , int64_t dim , ScalarType scalarType , int elementSize , PyObject * obj)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::internal_new_from_data( const Type & type , c10 :: optional<Device> device_opt , PyObject * data , bool copy_variables , bool copy_numpy , bool type_inference)",47, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_from_data_copy( const Type & type , c10 :: optional<Device> device , PyObject * data)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_new_from_sequence( const Type & type , c10 :: optional<Device> device , PyObject * data)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::check_legacy_ctor_device( const Type & type , c10 :: optional<Device> device)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_sparse_tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",40, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_sparse_tensor_new( const Type & type , PyObject * args , PyObject * kwargs)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::typeWithDefault( PythonArgs & r , int64_t dtype_idx , int64_t device_idx , const Type & type)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_tensor_new( const Type & type , PyObject * args , PyObject * kwargs)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::legacy_new_from_data( const Type & type , c10 :: optional<Device> device , PyObject * data)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::sparse_coo_tensor_ctor( const Type & default_type , PyObject * args , PyObject * kwargs)",35, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::tensor_ctor( const Type & type , PyObject * args , PyObject * kwargs)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::as_tensor( const Type & type , PyObject * args , PyObject * kwargs)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_tensor( const Type & type , PyObject * args , PyObject * kwargs)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_empty( const Type & type , PyObject * args , PyObject * kwargs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_full( const Type & type , PyObject * args , PyObject * kwargs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_ones( const Type & type , PyObject * args , PyObject * kwargs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_new.cpp,"torch::utils::new_zeros( const Type & type , PyObject * args , PyObject * kwargs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::StridedData::StridedData( const Tensor & tensor)",4, 57, 4, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::StridedData::step( int dim)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::recursive_apply( IntList sizes , ScalarType scalarType , int64_t dim , PyObject * fn , std :: array<StridedData,N> strided_data)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::apply_( Tensor & self , PyObject * fn)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::map_( Tensor & self , const Tensor & other_ , PyObject * fn)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_apply.cpp,"torch::utils::map2_( Tensor & self , const Tensor & x_ , const Tensor & y_ , PyObject * fn)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_dtypes.cpp,"torch::utils::getDtypeNames( at :: ScalarType scalarType)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_dtypes.cpp,"torch::utils::initializeDtypes()",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_to_numpy( const at :: Tensor & tensor)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_from_numpy( PyObject * obj)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::is_numpy_scalar( PyObject * obj)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::to_numpy_shape( IntList x)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::to_aten_shape( int ndim , npy_intp * values)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_to_numpy( const at :: Tensor & tensor)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::tensor_from_numpy( PyObject * obj)",44, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::aten_to_dtype( const at :: Type & type)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::numpy_dtype_to_aten( int dtype)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_numpy.cpp,"torch::utils::is_numpy_scalar( PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::should_allow_numbers_as_tensors( const std :: string & name)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::FunctionParameter( const std :: string & fmt , bool keyword_only)",49, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::check( PyObject * obj)",50, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::type_name() const",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::parse_as_integer( const std :: string & s)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::parse_intlist_args( const std :: string & s , int64_t size)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionParameter::set_default_str( const std :: string & str)",53, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::FunctionSignature( const std :: string & fmt)",65, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::toString() const",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::extra_args( const FunctionSignature & signature , ssize_t nargs)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::missing_args( const FunctionSignature & signature , int idx)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::find_param( FunctionSignature & signature , PyObject * name)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::extra_kwargs( FunctionSignature & signature , PyObject * kwargs , ssize_t num_pos_args)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::FunctionSignature::parse( PyObject * args , PyObject * kwargs , PyObject * dst [ ] , bool raise_exception)",92, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::PythonArgParser( std :: vector<std::string> fmts , bool traceable)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::raw_parse( PyObject * args , PyObject * kwargs , PyObject * parsed_args [ ])",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/python_arg_parser.cpp,"torch::PythonArgParser::print_error( PyObject * args , PyObject * kwargs , PyObject * parsed_args [ ])",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::take_tensors( TensorList tensors , size_t size_limit , bool fine_grained)",55, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::reorder_tensors_like( std :: vector<Tensor> & tensors , TensorList order)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::get_indices( const at :: Tensor & t)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::get_values( const at :: Tensor & t)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::flatten_sparse_tensors( at :: TensorList tensors)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/utils/tensor_flatten.cpp,"torch::utils::unflatten_sparse_tensors( const at :: Tensor & flat_indices , const at :: Tensor & flat_values , at :: TensorList tensors)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::PyTensorType::aten_type()",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::unavailable_type( const PyTensorType & type)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_instancecheck( PyTensorType * self , PyObject * arg)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_dtype( PyTensorType * self)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_layout( PyTensorType * self)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_is_cuda( PyTensorType * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::Tensor_is_sparse( PyTensorType * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_initialize_metaclass( PyTypeObject & metaclass)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_initialize_tensor_type( PyTypeObject & type , const char * name , PyObject * tp_dict)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_module( Backend backend)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_name( Backend backend , ScalarType scalarType)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_storage_obj( const Type & type)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_type( PyTensorType & type_obj , Backend backend , ScalarType scalarType)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_name( PyTensorType & type_obj , const std :: string & name)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_tensor_dict()",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::initialize_aten_types( std :: vector<PyTensorType> & tensor_types)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::initialize_python_bindings()",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_bind_tensor_types( const std :: vector<PyTensorType> & tensor_types)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::PyTensorType_Check( PyObject * obj)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_tensor_type( THPDtype * dtype , THPLayout * layout , bool is_cuda)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_set_default_tensor_type( PyObject * obj)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::py_set_default_dtype( PyObject * obj)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::set_default_tensor_type( const at :: Type & type)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::get_default_tensor_type()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/tensor/python_tensor.cpp,"torch::tensors::getDevice( const at :: Tensor & tensor)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::noop( const Node * n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::checkImplicitTensorToNum( at :: Tensor t , bool toInt)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::normalizeIndex(int64_t idx,int64_t list_size)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::getItem(TList&list,int64_t idx)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listAppend(const Node*node)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSelect(const Node*node)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listLen(const Node*node)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listEq(const Node*node)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listNe(const Node*node)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::tensor_list_equal(Shared<TensorList>a,Shared<TensorList>b)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listEq<Shared<TensorList>>(const Node*node)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listNe<Shared<TensorList>>(const Node*node)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listAdd(const Node*node)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSlice(const Node*node)",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::listSetItem(const Node*node)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::_check_size_factor(size_t dim,const IValue&size,const IValue&scale_factor)",17, 6, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::_output_size(const at::Tensor&input,size_t dim,const IValue&size,const IValue&scale_factors)",21, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::interpolate(const at::Tensor&input,const IValue&size,const IValue&scale_factors,const std::string&mode,c10::optional<bool>align_corners)",56, 86, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::interpolate_op(const Node*n)",13, 14, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::convert_scale_factor_to_double(const IValue&int_ivalue)",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_nearest_op(const Node*n)",12, 14, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_op(const Node*n)",14, 14, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::upsample_bilinear_op(const Node*n)",12, 14, 4, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::leaky_relu(const at::Tensor&tensor,double scalar)",3, 65, 0, 0
repos/cpp/pytorch/torch/csrc/jit/register_prim_ops.cpp,"torch::jit::cat(const std::vector<at::Tensor>&tensors)",3, 57, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::getNodeStackTraceString( const Node * n)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::validateBlock( Block * b , onnx_torch :: OperatorExportTypes operator_export_type)",48, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::validateGraph( const std :: shared_ptr<Graph> & graph , onnx_torch :: OperatorExportTypes operator_export_type)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::get_model_proto()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeIntermediateValueInfo( onnx :: GraphProto * graph_proto , const Value * n)",2, 63, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ATenTypeToOnnxType( at :: ScalarType at_type)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncoderBase( onnx_torch :: OperatorExportTypes operator_export_type , bool strip_doc)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeValueInfo( onnx :: GraphProto * graph_proto , onnx :: ValueInfoProto * v , const Value * n)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeGraph( onnx :: GraphProto * graph_proto , const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::EncodeBlock( onnx :: GraphProto * graph_proto , const Block * block , const std :: vector<at::Tensor> & initializers)",101, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::EncoderBase::AddAttribute( onnx :: NodeProto * node_proto , const jit :: Node * node , const jit :: Symbol name)",60, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::get_raw_data_export_map()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::GraphEncoder( const std :: shared_ptr<Graph> & graph , int64_t onnx_opset_version , onnx_torch :: OperatorExportTypes operator_export_type , const std :: vector<at::Tensor> & initializers , bool defer_weight_export , bool strip_doc)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::GraphEncoder::EncodeTensor( onnx :: TensorProto * tensor_proto , const at :: Tensor & tensor , const c10 :: optional<std::string> external_ref)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::ScriptModuleSerializer( const std :: string & filename)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::ScriptModuleSerializer( std :: ostream * ofs)",2, 30, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::serialize( const script :: Module & module)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertModel( const script :: Module & module , torch :: ModelDef * model_def)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::addTensor( const at :: Tensor & tensor)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertAndWriteTensor( size_t tensor_id , const at :: Tensor & tensor , torch :: TensorDef * tensor_proto , std :: unordered_map<const void*,std::string> & storageMap)",54, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::writeTensorTable( torch :: ModelDef * model_def)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertModule( const script :: Module & module , const std :: string & prefix , const std :: string & name , torch :: ModuleDef * module_def)",35, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ScriptModuleSerializer::convertParameter( const script :: NamedParameter & param , torch :: ParameterDef * param_def)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::idt( size_t indent)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::nlidt( size_t indent)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TensorProto & tensor , std :: ostream & stream)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TensorShapeProto & shape , std :: ostream & stream)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TypeProto_Tensor & tensor_type , std :: ostream & stream)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: TypeProto & type , std :: ostream & stream)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: ValueInfoProto & value_info , std :: ostream & stream)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: AttributeProto & attr , std :: ostream & stream , size_t indent)",47, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: NodeProto & node , std :: ostream & stream , size_t indent)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: GraphProto & graph , std :: ostream & stream , size_t indent)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: OperatorSetIdProto & operator_set_id , std :: ostream & stream)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::dump( const onnx :: ModelProto & model , std :: ostream & stream , size_t indent)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::prettyPrint( const onnx :: ModelProto & model)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::pretty_print_onnx( const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers , int64_t onnx_opset_version , bool defer_weight_export , :: torch :: onnx :: OperatorExportTypes operator_export_type , bool google_printer)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::export_onnx( const std :: shared_ptr<Graph> & graph , const std :: vector<at::Tensor> & initializers , int64_t onnx_opset_version , bool defer_weight_export , :: torch :: onnx :: OperatorExportTypes operator_export_type)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ExportModule( const script :: Module & module , std :: ostream & out)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/export.cpp,"torch::jit::ExportModule( const script :: Module & module , const std :: string & filename)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::loadPythonClasses()",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::runJITCPPTests()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/init.cpp,"torch::jit::initJITBindings( PyObject * module)",278, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printValueRef( std :: ostream & out , const Value * n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const std :: vector<T> & nodes)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printValueRefs( std :: ostream & out , const at :: ArrayRef<T> & nodes)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const at :: ArrayRef<const Value*> & nodes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const at :: ArrayRef<Value*> & nodes)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::const_value_list_with_types::const_value_list_with_types( ArrayRef<const Value*> values , bool use_newlines = false)",2, 52, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const_value_list_with_types l)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printAttributes( std :: ostream & out , const Node * n , bool ignore_subgraph = false)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::indent( std :: ostream & out , size_t level)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::printNode( std :: ostream & out , size_t level , const Node * n , std :: vector<const Node*> * groups)",41, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const Node & n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::operator < <( std :: ostream & out , const Graph & g)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::prettyPrint( std :: ostream & out)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::dumpPretty()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::checkSameDevice( const Node * node)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::lint() const",67, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::lint() const",140, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::dump() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::LintGraph( std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::Block( Graph * graph_ , Node * node_)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::reIndexTopology()",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::cloneFrom( Block * src , std :: function<Value*(Value*)> value_map)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Block::destroy()",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::copy()",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::mustBeNone() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::uniqueNameBase() const",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::setUniqueName( const std :: string & name)",42, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::copyMetadata( Value * from)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::replaceFirstUseWith( Value * newValue)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Value::replaceAllUsesWith( Value * newValue)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::findArgument( const FunctionSchema & the_schema , Symbol name)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::get( Symbol name) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::namedInput( Symbol name) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::matches( const char * signature_literal , at :: ArrayRef<Symbol> const_inputs) const",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::dump() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::findSchema() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::maybeSchema() const",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isNondeterministic() const",38, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::assignTopoPosition()",43, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::Node( Graph * graph_ , NodeKind kind_)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::eraseOutput( size_t i)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addBlock()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::eraseBlock( size_t i)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::destroy()",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::cloneFrom( Node * s)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceAllUsesWith( Node * n)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertInput( size_t i , Value * value)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addInput( Value * value)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceInput( size_t i , Value * newValue)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::replaceInputWith( Value * from , Value * to)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::addOutput()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertOutput( size_t i)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isBeforeOrAfter( const Node * n , MoveSide moveSide) const",38, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isBefore( const Node * n) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::isAfter( const Node * n) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertBefore( Node * n)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::insertAfter( Node * n)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveAfterTopologicallyValid( Node * n , const AliasDb & aliasDb)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::couldMoveAfterTopologically( Node * n , const AliasDb & aliasDb)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveBeforeTopologicallyValid( Node * n , const AliasDb & aliasDb)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::couldMoveBeforeTopologically( Node * n , const AliasDb & aliasDb)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::WorkingSet( Node * mover , const AliasDb & aliasDb)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::add( Node * n)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::eraseMover()",22, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::nodes()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::dependsOn( Node * n) const",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::hasDataDependency( Node * n) const",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::hasMutabilityDependency( Node * n) const",24, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::producesFor( Node * n) const",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::consumesFrom( Node * n) const",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::getUsersSameBlock( Node * n) const",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::getWritersSameBlock( Node * n) const",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::WorkingSet::findSameBlock( Node * target , Node * n)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::tryMove( Node * movePoint , MoveSide moveSide , const AliasDb & aliasDb , bool dryRun)",87, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::move( Node * movePoint , MoveSide moveSide)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveAfter( Node * n)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::moveBefore( Node * n)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeInput( size_t i)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeAllInputs()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::findUseForInput( size_t i)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::dropInput( size_t i)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Node::removeFromList()",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::fakeRange()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::insert( Symbol opname , at :: ArrayRef<NamedValue> args , at :: ArrayRef<NamedValue> kwargs , const c10 :: optional<SourceRange> & range)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::create( NodeKind kind , size_t num_outputs)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::create( NodeKind kind , ArrayRef<Value*> inputs , size_t num_outputs)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createUndefined()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNone( TypePtr typ)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNoneGenerator()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createFusionGroup()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTuple( at :: ArrayRef<Value*> values)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleUnpack( Value * v)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleIndex( Value * tup , int64_t index)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createTupleSlice( Value * tup , int64_t beg , int64_t end)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createList( const TypePtr & elem_type , at :: ArrayRef<Value*> values)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createListUnpack( Value * v , size_t size)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createNumToTensor( Value * value)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createImplicitTensorToNum( const TypePtr & type , Value * value)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::createClone( Node * n , const std :: function<Value*(Value*)> & value_map , bool copy_blocks)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::insertConstant( IValue val , c10 :: optional<SourceRange> loc , c10 :: optional<ScopePtr> scope)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::toString() const",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::~Graph()",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeNode( Node * n)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeValue( Value * v)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::Graph::freeBlock( Block * b)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::defaultAllocPythonOp( Graph * g)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::allocPythonOp( Graph * g)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/ir.cpp,"torch::jit::setAllocPythonOp( PythonOp *(*v)(Graph*g))",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::SchemaParser( const std :: string & str)",2, 14, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDeclaration()",37, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDeclarations()",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseIdent()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseBaseType()",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseAliasAnnotation()",29, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseType()",47, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseArgument( size_t idx , bool is_return , bool kwarg_only)",41, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseSingleConstant( TypeKind kind)",42, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::convertToList( TypeKind kind , const SourceRange & range , std :: vector<IValue> vs)",18, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseConstantList( TypeKind kind)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseTensorDefault( const SourceRange & range)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseDefaultValue( const TypePtr & arg_type , c10 :: optional<int32_t> arg_N)",37, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::script::SchemaParser::parseList( int begin , int sep , int end , const std :: function<void()> & callback)",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::canonicalSchemaString( const FunctionSchema & schema)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::registerPendingOperators()",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::registerOperator( Operator && op)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::lookupByLiteral( const char * name)",19, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorRegistry::getOperators( Symbol name)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getRegistry()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::registerOperator( Operator && op)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getAllOperatorsFor( Symbol name)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::sig( const char * signature)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::parseSchema( const std :: string & schema)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::Operator::matches( const Node * node) const",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::findOperatorFor( const Node * node)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::getOperatorFor( const Node * node)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorSet::OperatorSet( std :: initializer_list<const char*> sig_literals)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/operator.cpp,"torch::jit::OperatorSet::find( const Node * n) const",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::createTripCountConjunctiveCondition( Graph * g , Value * cur_trip_count , Value * max_trip_count , Value * cond)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::desugarTripCounts( Block * b)",53, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::flattenIO( Graph & graph)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::dropUnused( Block * b)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::findLastUses( Graph & g)",108, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::PreprocessGraph::PreprocessGraph( Graph & g)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::ContainerTensor()",2, 97, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::sizes() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::strides() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::dim() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::ContainerTensor::storage() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::relativeJump( int from_inst , int to_inst)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::CodeImpl( const std :: shared_ptr<Graph> & graph_)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJumpFalse( int from_inst , int to_inst)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJumpTrue( int from_inst , int to_inst)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::createJump( int from_inst , int to_inst)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertNodesFromBlock( Block * block)",78, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertInstruction( Node * n)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertInstruction( Symbol sym , std :: shared_ptr<SourceLocation> debug_location , ArrayRef<Value*> inputs , ArrayRef<uint8_t> move_flags , ArrayRef<Value*> outputs)",23, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::moveFlags( Node * n)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::moveFlags( Block * b)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::insertAssign( std :: shared_ptr<SourceLocation> debug_location , ArrayRef<Value*> inputs , ArrayRef<uint8_t> move_flags , ArrayRef<Value*> outputs)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::get( const ListHandle<int> & list , int i) const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::get( const ListHandle<bool> & list , int i) const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listBegin( ListHandle<int> & list)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listInsert( ListHandle<int> & list , int value)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listBegin( ListHandle<bool> & list)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::listInsert( ListHandle<bool> & list , int value)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::aliasRegistersTo( ArrayRef<Value*> new_allocations , ArrayRef<Value*> existing_allocations)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::getOrAllocateRegister( Value * n , bool required = false)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::grad_executors()",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::dumpInstruction( std :: ostream & out , size_t pc) const",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::CodeImpl::dump( std :: ostream & out) const",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::InterpreterStateImpl( const Code & code)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::intrusive_from_this()",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::runImpl( Stack & stack)",63, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::getOrCreateFuture()",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::runAsync( Stack & stack)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::run( Stack & stack)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::get( const ListHandle<int> & list , int i)",3, 5, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::get( const ListHandle<bool> & list , int i)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterStateImpl::loadTensorsFromRegisters( const UseList & uses , Stack & stack)",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::operator < <( std :: ostream & out , const Code & code)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::Code::Code( const std :: shared_ptr<Graph> & graph)",2, 36, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::Code::grad_executors()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::InterpreterState( const Code & code)",2, 62, 2, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::run( Stack & stack)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::runAsync( Stack & stack)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::getFuture()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/interpreter.cpp,"torch::jit::InterpreterState::InterpreterState( c10 :: intrusive_ptr<c10::intrusive_ptr_target> pImpl_)",2, 34, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_interpreter.cpp,"torch::jit::createPythonOperation( const Node * op_)",40, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/hooks_for_testing.cpp,"torch::jit::didFinishEmitModule( std :: shared_ptr<script::Module> module)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/hooks_for_testing.cpp,"torch::jit::setEmitModuleHook( std :: function<void(std::shared_ptr<script::Module>module)> cb)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::wrapDim( int64_t & dim , const std :: vector<int64_t> & sizes)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::isDifferentiable( Node * n)",112, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::isDifferentiable( Graph & g)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::gradientForNode( Node * node , ArrayRef<Value*> grad_values)",403, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::linearGradientForNode( Node * node , ArrayRef<Value*> grad_values)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::ReverseDetails::ReverseDetails( value_map && grad_map , Block * reverse_block)",3, 38, 4, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::createAutogradAdd( Value * a , Value * b)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::addReverseInline( Gradient & grad_desc)",70, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::getReverseCaptures( Gradient & grad_desc)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::liftConstants( Gradient & grad_desc , ReverseDetails & rev_info)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::deduplicateSizeCaptures( Gradient & grad_desc , ReverseDetails & rev_info)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::eliminateDeadCode( ReverseDetails & rev_info)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::Optimize( Gradient & grad_desc , ReverseDetails & rev_info)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::lambdaLiftReverse( Gradient & grad_desc , ReverseDetails & rev_info)",105, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/autodiff.cpp,"torch::jit::differentiate( std :: shared_ptr<Graph> & graph)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::getPythonName( const PyObject * obj_)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::printPyObject( std :: ostream & out , const THPObjectPtr & obj)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::ConcretePythonOp( Graph * graph)",2, 22, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::name() const",8, 3, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::cloneFrom( Node * other_)",11, 3, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::allocNewInstance( Graph * g)",3, 3, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::autogradFunction() const",21, 3, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::ConcretePythonOp::writeScalars( std :: ostream & out) const",10, 3, 1, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::pythonAllocPythonOp( Graph * g)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_ir.cpp,"torch::jit::initPythonIRBindings( PyObject * module_)",345, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::ExecutionPlan( std :: shared_ptr<Graph> graph)",3, 33, 4, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::run( Stack & stack) const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::operator bool() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::ExecutionPlan::getDebugState()",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::DifferentiableGraphBackward( GraphExecutor executor , size_t capture_size)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::apply( variable_list && inputs)",39, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphBackward::capture( const IValue & val , bool is_output)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::DifferentiableGraphOp( Gradient grad)",6, 55, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::operator ( )( Stack & stack) const",49, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::detachVariables( Stack & stack) const",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::captureInputs( DifferentiableGraphBackward & grad_fn , at :: ArrayRef<IValue> inputs) const",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::DifferentiableGraphOp::captureOutputs( DifferentiableGraphBackward & grad_fn , at :: ArrayRef<IValue> outputs) const",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::packGradient( Gradient gradient , Node * dnode)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::getGradient( const Node * n)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::detail::getGradExecutor( Operation & op)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::prepareGraph( std :: shared_ptr<Graph> & graph)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::countFlatInputs( const TypePtr & ptr)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::countFlatInputs( const std :: shared_ptr<Graph> & graph)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::hasMutableOperators( Block * block)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::GraphExecutorImpl( std :: shared_ptr<Graph> graph , bool optimize)",8, 54, 8, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::run( Stack & stack)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::graphFor( const Stack & stack) const",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getDebugState()",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::debugDisableAutodiffSubgraphInlining()",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getOrCompileFallback()",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::getOrCompile( const Stack & stack)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::compileSpec( const ArgumentSpec & spec)",42, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runOptimization( std :: shared_ptr<Graph> & graph , const ArgumentSpec & spec)",18, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runNondiffOptimization( std :: shared_ptr<Graph> & graph)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::needsGradient( const std :: shared_ptr<const Graph> & graph)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::mayIntroduceGradient( const Block * b)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutorImpl::runTraced( Stack & stack)",31, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::GraphExecutor( std :: shared_ptr<Graph> graph , bool optimize)",2, 62, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::run( Stack & inputs)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::graph() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::graphFor( const Stack & inputs) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::getDebugState()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::GraphExecutor::debugDisableAutodiffSubgraphInlining()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/graph_executor.cpp,"torch::jit::runRequiredPasses( const std :: shared_ptr<Graph> & g)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( const std :: string & filename)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::ScriptModuleDeserializer( std :: istream * is)",2, 29, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::deserialize( ModuleLookup module_lookup , c10 :: optional<at::Device> device)",38, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::loadTensorTable( torch :: ModelDef * model_def)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::loadTensor( const torch :: TensorDef & tensor_proto , std :: unordered_map<std::string,at::Storage> & storageMap)",64, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::ScriptModuleDeserializer::convertModule( const torch :: ModuleDef & module_def)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( ModuleLookup module_lookup , std :: istream & in , c10 :: optional<at::Device> device)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::import_ir_module( ModuleLookup module_lookup , const std :: string & filename , c10 :: optional<at::Device> device)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( std :: istream & in , c10 :: optional<at::Device> device)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import.cpp,"torch::jit::load( const std :: string & filename , c10 :: optional<at::Device> device)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::cast_handle_sequence( std :: vector<py::handle> objs)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::flatten_rec( PyObject * obj , ParsedArgs & args)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::flatten( py :: handle obj)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::cast_sequence( std :: vector<py::object> objs)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::unflatten_rec( ArrayRef<Variable> :: iterator & var_it , ArrayRef<Variable> :: iterator & var_it_end , std :: string :: const_iterator & desc_it)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_arg_flatten.cpp,"torch::jit::python::unflatten( ArrayRef<Variable> vars , const IODescriptor & desc)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::push( Symbol name)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::getRoot()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::getDepth()",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/scope.cpp,"torch::jit::Scope::namesFromRoot( const std :: string & separator) const",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::detail::genericAddInput( Node * n , T value)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::detail::badArgType( const T & v)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , int64_t value)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , bool value)",1, 108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , double value)",1, 108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: Scalar & value)",1, 108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const c10 :: optional<at::Scalar> & value)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const std :: string & value)",1, 108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: Tensor & value)",1, 108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: SparseTensorRef & value)",1, 108, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Generator * value)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Device value)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: Layout value)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: ScalarType value)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: TensorList value)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const at :: TensorOptions & options)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , at :: IntList value)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addInputs( Node * n , const char * name , const ArrayRef<double> & value)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addOutput( Node * node , const at :: Tensor & output)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::addOutput( Node * node , const std :: vector<at::Tensor> & outputs)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getTracingState()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setTracingState( std :: shared_ptr<TracingState> state)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::TracingState::TracingState()",2, 28, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::getSizeOf( const autograd :: Variable & var , int64_t dim)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ArgumentStash::stashIntListElem( const std :: string & arg_name , size_t size , size_t idx , const Variable & var)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::ArgumentStash::stashValue( const std :: string & arg_name , size_t idx , const Variable & var , const TypePtr & type)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::defaultRecordSourceLocation( Node * n)",1, 45, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::recordSourceLocation( Node * n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setRecordSourceLocation( void(*v)(Node*))",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::defaultWarn( const std :: string & str)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::_do_warn( const char * _reason , const char * _kind)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/tracer.cpp,"torch::jit::tracer::setWarn( warn_fn_type fn)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::tensorEqual( const at :: Tensor & lhs , const at :: Tensor & rhs)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::tensorListEqual( const std :: vector<at::Tensor> & lhs , const std :: vector<at::Tensor> & rhs)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::attributesEqualCSE( const Node * lhs , const Node * rhs)",46, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::HashNode::operator ( )( const Node * k) const",6, 3, 0, 0
repos/cpp/pytorch/torch/csrc/jit/node_hashing.cpp,"torch::jit::EqualNode::operator ( )( const Node * lhs , const Node * rhs) const",25, 3, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::getPythonInterpreterStackTrace()",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::createGraphByTracing( const py :: function & func , Stack trace_inputs , const py :: function & var_name_lookup_fn , bool force_outplace , const c10 :: optional<size_t> & num_real_inputs)",38, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::preRecordPythonTrace( THPObjectPtr pyobj , const std :: string & arg_types , at :: ArrayRef<Variable> inputs , pyobj_list scalar_args)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::pythonRecordSourceLocation( Node * n)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::pythonWarn( const std :: string & reason)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/python_tracer.cpp,"torch::jit::tracer::initPythonTracerBindings( PyObject * module)",67, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/constants.cpp,"torch::jit::insertConstant( Graph & g , const IValue & val , c10 :: optional<SourceRange> loc , c10 :: optional<ScopePtr> scope)",59, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/constants.cpp,"torch::jit::toIValue(const Value*v)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::ModuleAccessorValue( std :: shared_ptr<script::Module> module)",2, 33, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ModuleAccessorValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::OpsValue( size_t version)",2, 25, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::OpsValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::ConstantValue( IValue value)",2, 32, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::kind() const",1, 59, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantValue::asValue( const SourceRange & loc , script :: Method & m)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::ConstantTableValue( ArrayRef<at::Tensor> constants)",2, 29, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::ConstantTableValue::attr( const SourceRange & loc , script :: Method & m , const std :: string & field)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::parseVersionNumber( script :: Lexer & L)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/import_method.cpp,"torch::jit::import_methods( const std :: shared_ptr<script::Module> & mod , const std :: string & src , const std :: vector<at::Tensor> & constant_table)",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( at :: Tensor data , at :: Tensor mask , at :: Tensor dims)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( const at :: Tensor & data , int64_t batch_size)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::BatchTensor( const std :: vector<at::Tensor> & datalist , at :: Tensor dims)",29, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::BatchTensor::examples()",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/batched/BatchTensor.cpp,"torch::jit::initBatchTensorBindings( PyObject * module)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::removeTupleNodes( Node * n , bool must_remove_tuples)",33, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::VisitNode( Node * n , Node * insert_point)",62, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerAllTuples( Block * block)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::EnsureNoTuples( ArrayRef<Value*> values)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::EnsureNoTuples( Block * block)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerAllTuples( std :: shared_ptr<Graph> & graph)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerSimpleTuples( Block * block)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_tuples.cpp,"torch::jit::LowerSimpleTuples( std :: shared_ptr<Graph> & graph)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::ToONNX( std :: shared_ptr<Graph> & graph , :: torch :: onnx :: OperatorExportTypes operator_export_type)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx.cpp,"torch::jit::BlockToONNX( Block * old_block , Block * new_block , :: torch :: onnx :: OperatorExportTypes operator_export_type , std :: unordered_map<Value*,Value*> env)",175, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimizeImpl( Block * block , bool addmm_fusion_enabled)",114, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimize( Block * block , bool addmm_fusion_enabled)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/peephole.cpp,"torch::jit::PeepholeOptimize( const std :: shared_ptr<Graph> & graph , bool addmm_fusion_enabled)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/lower_grad_of.cpp,"torch::jit::LowerGradOf( Graph & g)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::isSimpleMap( Node * node)",85, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::broadcastSizes( at :: ArrayRef<Value*> sizes)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::GraphFuser( Block * block , std :: shared_ptr<Graph> graph)",2, 51, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tensorInputs( Node * node)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusable( Node * node)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableCatNode( Node * node)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableAsExitNode( Node * node)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::isFusableOnlyAsExitNode( Node * node)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::calculatesSize( Node * node)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::allUsersAreThisConsumerOrCalcSizes( Node * consumer , Value * producer)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mustRemainAsFusionGroupOutput( Value * producer)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::getSubgraph( Node * n)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mergeFusionGroups( Node * consumer_group , Node * producer_group)",54, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::mergeNodeIntoGroup( Node * group , Node * n)",56, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::createSingletonFusionGroup( Node * n)",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::insertAt( Node ** insertion_point , Node * n)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tryFuse( Node * consumer , Value * producer , const AliasDb & aliasDb)",62, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::canFuseChunk( Node * consumer , Value * producer)",23, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::findFusedChunk( Node * group , Value * input)",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseChunkByReusingExistingFusedChunk( Node * group , Node * chunk , Node * existingFusedChunk)",23, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::fuseChunk( Node * consumer , Value * producer)",18, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::sortReverseTopological( ArrayRef<Value*> inputs)",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::scanNodeForChunks( Node * consumer)",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::insertExplicitBroadcast( Node * node)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::promoteChunkToBroadcastingChunk( Node * chunk)",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::tryToMoveChunk( Node * consumer , Value * producer)",129, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::scanNode( Node * consumer , const AliasDb & aliasDb)",29, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::replaceIntermediateBroadcastingChunks()",30, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::usedOnlyInSize( Value * v)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::buildShapeExpressions( Node * fusion_group)",61, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::removeOutputsUsedOnlyInSize( Node * fusion_group)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::GraphFuser::run()",48, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::PeepholeOptimizeShapeExpressions( Block * block)",50, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/graph_fuser.cpp,"torch::jit::FuseGraph( std :: shared_ptr<Graph> & graph)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::isValidArgumentForRunning( Value * v)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::isValidReturnForRunning( Value * v)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::ShapePropagator( std :: shared_ptr<Graph> graph)",2, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnBlock( Block * block , bool insert_expands = true)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::setUnshapedType( Node * node)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::wrapDim( int64_t dim , at :: IntList sizes)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::jitDeviceIndexToDevice( int device)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::representativeValue( Value * v)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::gatherTensorTypes( Node * node)",26, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::mergeTypes( ArrayRef<Value*> lhs , ArrayRef<Value*> rhs , ArrayRef<Value*> outputs)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::broadcastBinary( Node * node , std :: vector<CompleteTensorTypePtr> & types , size_t idx1 , size_t idx2)",28, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::dependsOnMutation( Node * node)",32, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::canPropagateShapeByRunningIt( Node * node)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnNodeByRunningIt( Node * node)",26, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateCatShape( Node * cat_node)",62, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateShapeOnNode( Node * node , bool insert_expands = true)",122, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::determineListSize( Value * list)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateTensorShapeOnNode( Node * node , bool insert_expands)",858, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::ShapePropagator::PropagateCompleteShapeOnNode( Node * node , bool insert_expands , std :: vector<CompleteTensorTypePtr> tensor_types)",227, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::PropagateInputShapes( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( at :: ArrayRef<Value*> vals)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( Block * b)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/shape_analysis.cpp,"torch::jit::EraseShapeInformation( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::ChunkOutput::ChunkOutput( Value * v , size_t o)",2, 28, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::getChunkOutputs( Node * chunk)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::CanonicalizeOps( Block * block)",62, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize_ops.cpp,"torch::jit::CanonicalizeOps( const std :: shared_ptr<Graph> & graph)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::canRunWithAutograd( Node * node)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::scanNode( Node * node , size_t threshold)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::InlineAutodiffSubgraphs( Block * block , size_t threshold)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,"torch::jit::InlineAutodiffSubgraphs( std :: shared_ptr<Graph> & graph , size_t threshold)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::runNode( Node * n)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::propagateNode( Node * n)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::inlineIf( Block * body , Node * n)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::isTrueConstant( Value * val)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::inlineIf( Node * n)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::removeExtraNodeOutputs( Node * n)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( Node * n , const AliasDb & aliasDb , bool recurse)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( Block * block , const AliasDb & aliasDb , bool recurse)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_propagation.cpp,"torch::jit::ConstantPropagation( std :: shared_ptr<Graph> & graph)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::isInplaceOp( const Node * node)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::RemoveInplaceOps( Block * block)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_inplace_ops.cpp,"torch::jit::RemoveInplaceOps( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/erase_number_types.cpp,"torch::jit::EraseNumberTypesOnBlock( Block * block)",44, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/erase_number_types.cpp,"torch::jit::EraseNumberTypes( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/canonicalize.cpp,"torch::jit::Canonicalize( const std :: shared_ptr<Graph> & graph , bool keep_unique_names)",35, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_pooling.cpp,"torch::jit::ConstantPooling( Block * block , std :: unordered_set<Node*,HashNode,EqualNode> & constants)",31, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/constant_pooling.cpp,"torch::jit::ConstantPooling( const std :: shared_ptr<Graph> & graph)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isTrueConstant( Value * val)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isForLoop( Node * node)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::limitedBlockSize( Block * body , int64_t limit)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::isSmallBlock( Block * body)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::inlineBody( Node * loop)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::repeatBody( Block * body , int64_t times)",52, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::replaceLoopCounter( Node * loop)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::unroll( Node * loop)",40, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::UnrollLoops( Block * block)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/loop_unrolling.cpp,"torch::jit::UnrollLoops( std :: shared_ptr<Graph> & graph)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::getRequiresGrad( Value * value)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( Value * value , bool req_value)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( at :: ArrayRef<Value*> outputs , const std :: vector<bool> & values)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::setRequiresGrad( Node * node , const std :: vector<bool> & values)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::bitwiseOr( std :: vector<bool> a , const std :: vector<bool> & b)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGradSimpleNode( Node * node)",33, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( Node * node)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( Block * block)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/requires_grad_analysis.cpp,"torch::jit::PropagateRequiresGrad( std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::have_same_shape( at :: TensorList inputs)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::shape_is_fast_for_reduce( const at :: Tensor & lhs , const at :: Tensor & rhs)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::mm( Node * mm)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::transpose( Node * t , TreeToken & inp_token)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::add( Node * add , TreeToken & l , TreeToken & r)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::operator bool()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::TreeToken::removeTransposesAndGatherMatmuls()",29, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMMTreeReduce( Block * block)",55, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::shape_is_fast_for_side( const at :: Tensor & other_side_input)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::gatherIndependentMMUses( Value * value , const AliasDb & alias_db)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMMSide( Block * block , const AliasDb & alias_db)",46, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::hasMutableOperators( Block * block)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/batch_mm.cpp,"torch::jit::BatchMM( std :: shared_ptr<Graph> & graph)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inplace_check.cpp,"torch::jit::CheckInplace( Block * block)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/inplace_check.cpp,"torch::jit::CheckInplace( std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isPrint( char s)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::printQuotedString( std :: ostream & stmt , const std :: string & str)",51, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isValidIdentifierChar( char c , size_t pos)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::isValidIdentifier( const std :: string & name)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::QualifiedName( QualifiedNamePtr prefix , std :: string name)",2, 58, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::create( QualifiedNamePtr prefix , std :: string name)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::create( std :: string name)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::str() const",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::QualifiedName::emit( std :: ostream & out) const",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::createTensorToParameterNameMap( const script :: Module & module , const QualifiedNamePtr & prefix , std :: unordered_map<at::Tensor*,QualifiedNamePtr> & result)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::isConstantLike( Node * n)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::canInline( Value * v)",23, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanValue( Node * block_point , Value * v)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::previousNonConstant( Node * n)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanNode( Node * n)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::scanBlock( Block * b)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::getOrAddTensorConstant( at :: Tensor t)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::buildConstantList( Node * n , std :: vector<Node*> & constants)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::buildConstantList( Block * b , std :: vector<Node*> & constants)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genNameImpl( const std :: string & candidate , std :: unordered_set<std::string> & used)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genName( const std :: string & candidate)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genMethodName( const std :: string & candidate)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::makeValidIdentifier( const std :: string & candidate)",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::genUniqueNameFor( Value * v)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::useOf( Value * v) const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValue( Value * v , const std :: string & s)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValue( Value * v , Value * w)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::assignValuesToTheirUniqueNames( at :: ArrayRef<Value*> values)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::indent()",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::WithIndented()",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::zipWith( at :: ArrayRef<T0> list_a , at :: ArrayRef<T1> list_b , F action) const",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printValueList( std :: ostream & stmt , at :: ArrayRef<Value*> list , const char * begin = "" , const char * end = "")",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printAssignment( at :: ArrayRef<Value*> lhs , at :: ArrayRef<Value*> rhs)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printIf( IfView stmt)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::shouldEmitAsForLoop( LoopView stmt)",24, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printLoop( LoopView stmt)",47, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printNode( Node * node , bool print_const)",53, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMaybeAnnotatedConstantList( std :: ostream & stmt , const char * the_type , size_t list_size , const IValue & the_list)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printConstant( std :: ostream & stmt , const IValue & v)",29, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printRHS( std :: ostream & stmt , Node * node)",142, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printBlock( Block * root , bool block_has_other_statements)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printDefaultValue( std :: ostream & stmt , const IValue & value)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printFunctionDefinition( Graph & graph , const std :: string & name , const std :: vector<c10::optional<IValue>> & defaults = { } , const std :: vector<std::string> & param_names = { })",55, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::PythonPrintPass( std :: ostream & out_ , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",5, 91, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::resultType( const Graph & graph)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printFunction( Graph & graph , const std :: string & name , const std :: vector<c10::optional<IValue>> & defaults = { } , const std :: vector<std::string> & param_names = { })",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMethod( script :: Method & method)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printMethod( script :: Method & method , const std :: unordered_map<at::Tensor*,QualifiedNamePtr> & parameter_names)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrintPass::printModule( script :: Module & module)",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const Graph & graph , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const script :: Method & method , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::PythonPrint( std :: ostream & out , const script :: Module & module , std :: vector<at::Tensor> & tensor_table , bool enforce_importable)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/python_print.cpp,"torch::jit::printerHasSpecialCaseFor( Symbol sym)",49, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_expands.cpp,"torch::jit::RemoveExpands( Block * block)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/remove_expands.cpp,"torch::jit::RemoveExpands( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::getBatchOperator( const std :: string & name , int64_t num_inputs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitAten( Node * n , Block * block , Block * res_block)",52, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitConstant( Node * n , Block * block , Block * res_block)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitNumToTensor( Node * n , Block * block , Block * res_block)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitTensorToNum( Node * n , Block * block , Block * res_block)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitListConstruct( Node * n , Block * block , Block * res_block)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitIf( Node * n , Block * block , Block * res_block)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::visitLoop( Node * n , Block * block , Block * res_block)",125, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::ToBatch::toBatch( Block * block , Block * res_block)",65, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::to_batch_graph( std :: shared_ptr<Graph> & graph)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/to_batch.cpp,"torch::jit::initRegisterBatchOpsBindings( PyObject * module)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/specialize_undef.cpp,"torch::jit::specializeUndef( Graph & g)",92, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::shouldAnnotate( const TypePtr & type)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::shouldAnnotate( const Value * v)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::AliasDb( std :: shared_ptr<Graph> graph)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::buildWildcardIndex( const Block * b)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWildcard( const Node * n) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWildcardImpl( const Node * n) const",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesTo( Node * n , const Value * v) const",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::hasWrites( Node * n) const",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::writesToInputAlias( Node * n) const",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWriters( const Node * n) const",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getAliases( const Value * v) const",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getWrites( Node * n) const",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::dump() const",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( const std :: shared_ptr<Graph> & graph)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( Block * block)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyze( Node * node)",126, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeIf( Node * node)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeLoop( Node * node)",38, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeSubgraph( Node * node)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeCreator( Node * node)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeExtractor( Node * node)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeChunk( Node * node)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::analyzeBroadcastingChunk( Node * node)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::getFreshAlias( bool isGraphInput)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::addAlias( const Value * value , AliasInfo alias)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::addAlias( const Value * value , Symbol alias)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::addAlias( const Value * value , const Value * from)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::mapAliases( at :: ArrayRef<Value*> to , at :: ArrayRef<Value*> from)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/alias_analysis.cpp,"torch::jit::AliasDb::giveFreshAlias( const Value * value)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/common_subexpression_elimination.cpp,"torch::jit::EliminateCommonSubexpression( Block * block , const AliasDb & aliasDb , std :: function<Node*(Node*)> parent_lookup_fn)",49, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/common_subexpression_elimination.cpp,"torch::jit::EliminateCommonSubexpression( std :: shared_ptr<Graph> & graph)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::SubgraphSlicer( Block * block , std :: shared_ptr<Graph> graph , size_t minSubgraphSize)",7, 45, 8, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::run( std :: vector<Node*> & diffGraphs)",53, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::inlineIfTooSmall( Node * n)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::sortReverseTopological( ArrayRef<Value*> inputs)",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::shouldConsiderForMerge( Node * node)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::scanNode( Node * consumer , const AliasDb & aliasDb)",20, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::SubgraphSlicer::tryMerge( Node * consumer , Node * producer , const AliasDb & aliasDb)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,"torch::jit::CreateAutodiffSubgraphs( const std :: shared_ptr<Graph> & graph , size_t threshold)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::DeadCodeEliminator( std :: shared_ptr<Graph> graph)",2, 53, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::run( Block * block , bool recurse)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::setDeleteCallback( std :: function<void(const std::unordered_set<const Value*>&)> deleteCallback)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::setLastWildcard()",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::markReturnNode( Node * node)",43, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::mark( Block * block)",19, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::markIfLive( Node * node)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::mark( Node * node)",32, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::sweep( Block * block , bool recurse)",18, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::hasUntrackedMutation( Node * node)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::hasSideEffects( Node * node)",20, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::removeDeadBlockOutputs( Node * node)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::DeadCodeEliminator::removeDeadLoopOutputs( Node * node)",19, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( Block * block , bool recurse)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/dead_code_elimination.cpp,"torch::jit::EliminateDeadCode( Block * block , std :: function<void(const std::unordered_set<const Value*>&)> cb)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isRNN( const Node * node)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isNopTranspose( const std :: vector<int64_t> & perm)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::composeTransposes( const std :: vector<int64_t> & t1 , const std :: vector<int64_t> & t2)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::getBroadcastPositions( Node * node)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fusibleExpandTo( at :: IntList from , at :: IntList to)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseBroadcast( Block * b)",46, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseConsecutiveTransposes( Block * b)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::eliminateNopTranspose( Block * b)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fuseTransposeIntoGemm( Block * b)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::pushPackingPastRnn( Block * b)",81, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::removeNopPacking( Block * graph)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::hackFixupPadPackedShapes( Block * graph)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultRNNState( Graph * graph , Node * n , int input_index)",67, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultRnnHiddenState( Block * b)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::fixDefaultLstmCellState( Block * b)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::isSafeToSpeculate( Node * n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::speculateOps( Block * block)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::replaceInputWithList( Node * node , size_t i , ArrayRef<Value*> to)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::eraseListConstruct( Block * block)",57, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/peephole.cpp,"torch::jit::PeepholeOptimizeONNX( std :: shared_ptr<Graph> & graph)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,"torch::jit::FixupONNXLoops( Block * block)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,"torch::jit::FixupONNXLoops( std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,"torch::jit::PrepareDivisionForONNXOnBlock( Block * block)",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,"torch::jit::PrepareDivisionForONNX( const std :: shared_ptr<Graph> & graph)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepCopy( const IValue & self)",42, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepCopy( const Stack & stack)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::deepEquals( const IValue & lhs , const IValue & rhs)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::AliasAndIValue::AliasAndIValue( c10 :: optional<at::AliasInfo> aliasInfo , IValue iValue)",4, 70, 6, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkInputPreconditions( const Stack & inputs)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkAliases( const std :: vector<AliasAndIValue> & inputs , const std :: vector<AliasAndIValue> & outputs)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkWrites( const std :: vector<AliasAndIValue> & inputs , const std :: vector<IValue> & deepCopiedInputs)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::findNodeForOp( const Graph & g , const std :: string & unqualifiedOpName)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::toIValueProp( const Value * v)",40, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/check_alias_annotation.cpp,"torch::jit::checkAliasAnnotation( const std :: shared_ptr<Graph> & graph , std :: vector<IValue> pythonInputs , const std :: string & unqualifiedOpName)",62, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::isSubgraphNodeKind( Symbol s)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::isSubgraphNodeKind( Node * n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::mergeSubgraph( Node * mergeTo , Node * mergeFrom)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::getSubgraph( Node * n)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::unmergeSubgraph( Node * subgraphNode)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::mergeNodeIntoSubgraph( Node * toMerge , Node * subgraphNode)",82, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/passes/utils/subgraph_utils.cpp,"torch::jit::SubgraphUtils::createSingletonSubgraph( Node * n , Symbol subgraphKind)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::typeString( py :: handle h)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::toSimple( Value * v)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::PythonValue( py :: object self)",2, 29, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::getSchema( const size_t n_args , const size_t n_binders)",44, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs_ , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::kind() const",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonValue::getattr( const SourceRange & loc , const std :: string & name)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonModuleValue::PythonModuleValue( py :: object mod)",1, 78, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENPythonModuleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::ConstantPythonTupleValue( py :: object tup)",1, 85, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENConstantPythonTupleValue::asValue( const SourceRange & loc , Method & m)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::ModuleValue( std :: shared_ptr<Module> module)",2, 33, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",39, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::ModuleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint = { })",19, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::BooleanDispatchValue( py :: dict dispatched_fn)",2, 52, 6, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::removeIndex( at :: ArrayRef<NamedValue> arr , size_t index)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::VISIBILITY_HIDDENBooleanDispatchValue::call( const SourceRange & loc , Method & caller , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",35, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::toSugaredValue( py :: object obj , Method & m , SourceRange loc , bool is_constant , bool is_submodule)",86, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::unpackVariableTensorList( std :: vector<at::Tensor> outputs)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::gatherParametersAndBuffers( std :: vector<at::Tensor*> & values , const Module & m)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::pythonResolver( const ResolutionCallback & rcb)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::getSchemaWithNameAndDefaults( const SourceRange & range , const FunctionSchema & schema , const at :: optional<std::string> & new_name , const FunctionDefaults & default_args)",37, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/init.cpp,"torch::jit::script::initJitScriptBindings( PyObject * module)",256, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::SharedParserData::isUnary( int kind , int * prec)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::SharedParserData::isBinary( int kind , int * prec)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::stringToKind( const std :: string & str)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::kindToString( int kind)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/lexer.cpp,"torch::jit::script::sharedParserData()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::getAllBuiltinFunctionsFor( Symbol name)",21, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::loadSource( const std :: string & source)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::BuiltinFunctionRegistry::loadBuiltinFunctions()",24, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/builtin_functions.cpp,"torch::jit::script::getAllBuiltinFunctionsFor( Symbol name)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::mergeTypesFromTypeComment( const Decl & decl , const Decl & type_annotation_decl , bool is_method)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::ParserImpl( const std :: string & str)",2, 46, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseIdent()",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::createApply( const Expr & expr)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::followsTuple( int kind)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExpOrExpTuple()",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseBaseExp()",52, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAssignmentOp()",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseTrinary( TreeRef true_branch , const SourceRange & range , int binary_prec)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExp()",1, 42, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseExp( int precedence)",43, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseList( int begin , int sep , int end , T(ParserImpl::*parse)())",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseConst()",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseConcatenatedStringLiterals()",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAttributeValue()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseOperatorArguments( TreeList & inputs , TreeList & attributes)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSubscriptExp()",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseSubscript( const TreeRef & value)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseParam()",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseBareTypeAnnotation()",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseTypeComment()",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseAssign( const Expr & lhs)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseStmt()",53, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseOptionalIdentList()",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseIf( bool expect_if = true)",20, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseWhile()",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseFor()",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseStatements( bool expect_indent = true)",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseDecl()",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::parseFunction( bool is_method)",18, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::lexer()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::c( int kind , const SourceRange & range , TreeList && trees)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::ParserImpl::makeList( const SourceRange & range , TreeList && trees)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::Parser( const std :: string & src)",2, 32, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseFunction( bool is_method)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::lexer()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/parser.cpp,"torch::jit::script::Parser::parseTypeComment()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::SourceRangeFactory::SourceRangeFactory( std :: string source)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::SourceRangeFactory::create( int line , int start_col , int end_col)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::wrap_list( const SourceRange & fallback_pos , std :: vector<T> && vec)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::wrap_maybe( const SourceRange & fallback_pos , T * val)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/python_tree_views.cpp,"torch::jit::script::initTreeViewBindings( PyObject * module)",184, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::placeholderCreator( Method &)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::try_emit_call_to( Graph & graph , const SourceRange & loc , Method & callee , c10 :: optional<NamedValue> self , ArrayRef<NamedValue> args , ArrayRef<NamedValue> kwargs , std :: stringstream & failure_messages , Method * caller , bool conv_tensors_to_nums)",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Method::emit_call_to( const SourceRange & loc , Method & callee , ArrayRef<NamedValue> args , ArrayRef<NamedValue> kwargs)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Method::ensure_defined()",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: Device device , at :: ScalarType dtype , bool non_blocking)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: ScalarType dtype , bool non_blocking)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to( at :: Device device , bool non_blocking)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::save( std :: ostream & out)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::save( const std :: string & filename)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/module.cpp,"torch::jit::script::Module::to_impl( const c10 :: optional<at::Device> & device , const c10 :: optional<at::ScalarType> & dtype , bool non_blocking)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::NoneValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::GetAttrValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::PrintValue::kind() const",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::PrintValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::CastValue::CastValue( TypePtr type , c10 :: Symbol method)",3, 30, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::CastValue::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::asSimple( const SugaredValuePtr & value)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::meaningfulName( const std :: string & name)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::Environment( Method & method , Resolver resolver , Block * b , std :: shared_ptr<Environment> next = nullptr)",2, 86, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setVariableTypeError( const std :: string & name , const std :: string & msg)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findVariableTypeError( const std :: string & name)",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInThisFrame( const std :: string & name)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInParentFrame( const std :: string & name)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::findInAnyFrame( const std :: string & name)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getValueInThisFrame( const SourceRange & loc , const std :: string & name)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::createCapturedInput( Value * orig , const std :: string & name)",21, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::createCapturedInputIfNeeded( const SourceRange & loc , const std :: string & ident)",16, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::block()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getBlockOwningKind()",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setVar( const SourceRange & loc , const std :: string & name , Value * value)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::setSugaredVar( const SourceRange & loc , const std :: string & name , SugaredValuePtr value)",49, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getSugaredVar( const Ident & ident , bool required = true)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getVar( const Ident & ident)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getSugaredVar( const std :: string & ident , const SourceRange & range , bool required = true)",33, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::getVar( const std :: string & ident , const SourceRange & range)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::deleteExtraInputs( const SourceRange & loc)",22, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::Environment::definedVariables()",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::packOutputs( Graph & g , at :: ArrayRef<Value*> values)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::createTupleUnpack( Value * v)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::unwrapOptional( TypePtr opt_type)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isIntOrFloatUsedAsList( const Value * value , const Argument & arg)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::convertibleToList( const TypePtr & type , const TypePtr & list_type_)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryConvertToType( const SourceRange & loc , Graph & graph , const TypePtr & concrete_type , Value * value , bool allow_conversions)",43, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryMatchArgument( const Argument & arg , Graph & graph , const SourceRange & loc , const NamedValue & named_value , const std :: function<std::ostream&()> & err , bool allow_conversions , TypeEnv & type_env)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::findInputWithName( const std :: string & name , at :: ArrayRef<NamedValue> kwargs)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryCreateList( const TypePtr & elem_type , Graph & graph , const SourceRange & loc , at :: ArrayRef<NamedValue> varargs , const std :: function<std::ostream&()> & err , bool convert_tensor_to_num , TypeEnv & type_env)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::materializeConstant( T val , Graph & graph , const SourceRange & r , std :: unordered_map<T,Value*> & map)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::tryMatchSchema( const FunctionSchema & schema , const SourceRange & loc , Graph & graph , c10 :: optional<NamedValue> self , at :: ArrayRef<NamedValue> args , at :: ArrayRef<NamedValue> kwargs , std :: ostream & failure_messages , bool allow_conversions)",117, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::prefixLine( const std :: string & str , const std :: string & prefix)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::emitBuiltinNode( const MatchedSchema & matched_schema , const SourceRange & loc , Graph & graph , Symbol name)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::emitBuiltinCall( const SourceRange & loc , Graph & graph , Symbol name , const c10 :: optional<NamedValue> & self , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , bool required)",62, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::ensureInt( const SourceRange & range , Value * v)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::BuiltinFunction::call( const SourceRange & loc , Method & m , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes , size_t n_binders)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isSupportedListElementType( const TypePtr & type)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::to_ir( Def def_ , Resolver resolver_ , SugaredValuePtr self_ , Method & method)",40, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::pushFrame( Block * b)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::popFrame()",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::evaluateDefaults( const SourceRange & r , const std :: vector<Expr> & default_types , const std :: vector<Expr> & default_exprs)",29, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::parseArgsFromDecl( const Decl & decl)",49, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::parseReturnsFromDecl( const Decl & decl)",26, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::extractSchemaFromDef( const Def & def)",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitFormalArguments( const SugaredValuePtr & self , const FunctionSchema & schema)",33, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitReturn( c10 :: optional<Return> return_stmt_ , const FunctionSchema & schema)",45, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStatements( const List<Stmt> & statements)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStatements( List<Stmt> :: const_iterator begin , List<Stmt> :: const_iterator end)",49, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSingleIfBranch( Block * b , const List<Stmt> & branch)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::create( Symbol kind , const SourceRange & loc , size_t n_outputs)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTernaryIf( const TernaryIf & expr)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitShortCircuitIf( const SourceRange & loc , const TreeRef & first_expr , const TreeRef & second_expr , bool is_or)",22, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIfExpr( const SourceRange & range , Value * cond_value , std :: function<Value*()> true_expr , std :: function<Value*()> false_expr)",32, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitCond( const Expr & cond)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIfElseBlocks( Value * cond_value , const If & stmt)",84, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIf( const If & stmt)",55, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitLoopCommon( SourceRange range , c10 :: optional<Expr> max_trip_count , c10 :: optional<Expr> cond , const List<Stmt> & body , c10 :: optional<Ident> itr_ident)",71, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitForRange( const SourceRange & range , const Ident & target , const List<Expr> & args , const List<Stmt> & body)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitFor( const For & stmt)",49, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitWhile( const While & stmt)",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitRaise( const SourceRange & loc)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAssert( const Assert & stmt)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::calcNumStarredUnpack( const List<Expr> & lhs , const SourceRange & r)",27, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getAugOp( const AugAssign & stmt , bool isTensor)",15, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignment( const AugAssign & stmt)",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToSelectVar( const AugAssign & stmt)",24, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToVar( const AugAssign & stmt)",30, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAugAssignmentToSubscript( const AugAssign & stmt)",80, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscriptAssign( const SourceRange & stmtRange , const Subscript & lhs , const Expr & rhs)",7, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscriptAssign( const SourceRange & stmtRange , const Subscript & lhs , const NamedValue & rhs)",54, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleAssign( const TupleLiteral & tl , const Expr & rhs)",56, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAssignment( const Assign & stmt)",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNodeKind( int kind , int ninputs)",54, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNamedValues( const TreeList & trees , bool maybe_unpack)",18, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getNamedValues( const List<Expr> & trees , bool maybe_unpack)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getValues( const TreeList & trees , bool maybe_unpack)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getValues( const List<Expr> & trees , bool maybe_unpack)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitAttributes( const List<Attribute> & attributes)",5, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitApplyExpr( Apply & apply , size_t n_binders)",56, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitExpr( const Expr & tree , TypePtr type_hint = nullptr)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::reverseComparision( NodeKind kind)",12, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSugaredExpr( const Expr & tree , size_t n_binders , TypePtr type_hint = nullptr)",17, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitNegate( const TreeRef & tree)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitForkExpr( SourceRange loc , const std :: shared_ptr<SugaredValue> & forked , at :: ArrayRef<NamedValue> inputs , at :: ArrayRef<NamedValue> attributes)",42, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSimpleExpr( const TreeRef & tree , const TypePtr & type_hint = nullptr)",107, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitConst( const Const & c)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitStringLiteral( const StringLiteral & c)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSelect( const SourceRange & loc , Value * input , int64_t dim , Value * index)",9, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSlice( const SourceRange & loc , Value * input , c10 :: optional<int64_t> dim , const SliceExpr & slice)",33, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIndex( const SourceRange & loc , Value * input , at :: ArrayRef<Value*> indices)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitIntAndSliceIndexing( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",41, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitMultidimSlicing( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",21, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitBasicSlice( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",14, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::getTupleIndexVal( const SourceRange & loc , const TupleTypePtr & tuple_type , Value * idx_val , bool allow_out_of_bounds)",25, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleIndex( const SourceRange & loc , Value * tuple_val , Value * idx_val)",8, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitTupleSlice( const SourceRange & loc , const NamedValue & tuple_val , const NamedValue & beg_val , const at :: optional<NamedValue> & end_val)",20, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscript( const Subscript & subscript)",6, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitSubscript( const SourceRange & loc , Value * sliceable , const List<Expr> & subscript_exprs)",13, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::to_ir::emitBasicGather( const SourceRange & loc , Value * gatherable , const List<Expr> & subscript_exprs)",21, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::builtin_cast_methods()",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::SimpleValue::attr( const SourceRange & loc , Method & m , const std :: string & field)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::inlineCallTo( Graph & g , Graph & callee , ArrayRef<Value*> inputs)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::defineMethodsInModule( const std :: shared_ptr<Module> & m , const std :: vector<Def> & definitions , const std :: vector<Resolver> & resolvers , const SugaredValuePtr & self)",37, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::ident_to_type_lut()",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::subscript_to_type_fns()",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::isTorch( const Expr & expr)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::parseBaseTypeName( const Expr & expr)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::parseTypeFromExpr( const Expr & expr)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::handleBroadcastList( const Expr & expr)",49, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::defineMethodsInModule( std :: shared_ptr<Module> m , const std :: string & source , const Resolver & resolver , const SugaredValuePtr & self)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/script/compiler.cpp,"torch::jit::script::SimpleValue::asTuple( const SourceRange & loc , Method & m , const c10 :: optional<size_t> & size_hint)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::getKernelCache()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::debugNumCachedKernelSpecs()",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::normalizeGraphForCache( const std :: shared_ptr<Graph> & graph)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::store( std :: shared_ptr<Graph> graph)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::nolock_retrieve( KernelCacheImpl & cache , const int64_t key)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::retrieve( const int64_t key)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/kernel_cache.cpp,"torch::jit::fuser::lookupGraph( std :: shared_ptr<Graph> graph)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::registerFusion( const Node * fusion_group)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::runFusion( const int64_t key , Stack & stack)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::canFuseOnCPU()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::canFuseOnGPU()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::overrideCanFuseOnCPU( bool value)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::debugLaunchGraph( Graph & graph , at :: ArrayRef<at::Tensor> inputs)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/interface.cpp,"torch::jit::nCompiledKernels()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::getMapSize( const KernelSpec & spec , at :: TensorList args , at :: IntList arg_subset)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::canRunKernel( const KernelSpec & spec , at :: TensorList args)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::expandArgs( const KernelSpec & spec , std :: vector<at::Tensor> & args , std :: vector<int64_t> & map_size)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::computeNumel( const at :: ArrayRef<int64_t> & sizes)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::computeMapSize( const at :: Tensor & tensor , const PartitionDesc & chunkDesc)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::compressContiguous( const at :: IntList & sizes , const at :: IntList & strides , const std :: vector<bool> & cont , uint32_t * c_sizes , uint32_t * c_strides)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::launchFusion( const FusedKernel & fusion , const at :: Device device , const at :: ArrayRef<at::Tensor> & inputs , std :: vector<at::Tensor> & outputs)",116, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/executor.cpp,"torch::jit::fuser::runFusion( const int64_t key , Stack & stack)",60, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::valueName( const Value * n)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const int64_t v)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const bool v)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarValue( const double v)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::scalarTypeName( const at :: ScalarType type)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::calcScalarTypeName( const at :: ScalarType type)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::variableType( const std :: shared_ptr<c10::Type> & t)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::typeCastedValueName( const std :: shared_ptr<c10::Type> & t , const at :: ScalarType outtype , const std :: string & vn)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::encodeRHS( const Node * n)",107, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::usedInFusedChunk( const Value * input)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::emitIndexingFor( std :: ostream & out , const std :: string & tensor , const int ndim , const bool last_is_cont)",20, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/codegen.cpp,"torch::jit::fuser::generateKernel( const std :: string & name , const Graph & graph , const std :: vector<TensorDesc> & input_desc , const std :: vector<TensorDesc> & output_desc , const bool use_cuda)",184, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::nCompiledKernels()",1, 60, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::debugFuser()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::usedInFusedChunk( const Value * input)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::setInputChunkDescriptors( KernelSpec & spec)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::getInputDependencies( const Value * output)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::setInputBroadcastGroups( KernelSpec & spec)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::upfrontCompilation( KernelSpec & spec)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::registerFusion( const Node * fusion_group)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/compiler.cpp,"torch::jit::fuser::compileKernel( const KernelSpec & spec , const ArgSpec & arg_spec , const std :: vector<int64_t> & map_size , const at :: Device device)",75, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/fallback.cpp,"torch::jit::fuser::runFallback( int64_t key , Stack & stack)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::checkCUDAVersion( const cudaDeviceProp & prop)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::getMajorMinor( const cudaDeviceProp * const prop , int & major , int & minor)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::FusedKernelCUDA( int16_t device , std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",79, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::ceilDiv( const int a , const int b)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp,"torch::jit::fuser::cuda::FusedKernelCUDA::launch_raw( const uint32_t numel , std :: vector<void*> & arguments) const",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::programExists( const std :: string & program)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::CompilerConfig::CompilerConfig()",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::getConfig()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::runCompiler( const std :: string & cpp_file , const std :: string & so_file)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::disas( const std :: string & so_file)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/jit/fuser/cpu/fused_kernel.cpp,"torch::jit::fuser::cpu::FusedKernelCPU::FusedKernelCPU( std :: string name , std :: string code , std :: vector<TensorDesc> input_desc , std :: vector<TensorDesc> output_desc , std :: vector<PartitionDesc> chunk_desc , std :: vector<PartitionDesc> concat_desc , bool has_random)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( New)( THWStorage * ptr)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( dealloc)( THPStorage * self)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( newWithAllocator)( int64_t size , at :: Allocator * allocator)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( pynew)( PyTypeObject * type , PyObject * args , PyObject * kwargs)",102, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( length)( THPStorage * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( get)( THPStorage * self , PyObject * index)",49, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( set)( THPStorage * self , PyObject * index , PyObject * value)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( initCopyMethods)()",41, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( init)( PyObject * module)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/Storage.cpp,"THPStorage_( postInit)( PyObject * module)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/serialization.cpp,"THPStorage_( writeFileRaw)( THWStorage * self , io fd)",40, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/serialization.cpp,"THPStorage_( readFileRaw)( io file , THWStorage * _storage)",58, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedDecref)( THPStorage * self)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedIncref)( THPStorage * self)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( __newHandle)()",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newFilenameStorage)( ptrdiff_t size)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( pyNewFilenameStorage)( PyObject * _unused , PyObject * args)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareFilename)( THPStorage * self)",33, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedFilename)( PyObject * _unused , PyObject * args)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newFdStorage)( ptrdiff_t size)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( pyNewFdStorage)( PyObject * _unused , PyObject * args)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareFd)( THPStorage * self)",28, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedFd)( PyObject * _unused , PyObject * args)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( shareCuda)( THPStorage * self)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newSharedCuda)( PyObject * _unused , PyObject * args)",46, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( weakRef)( THPStorage * self , PyObject * args)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( newWithWeakPtr)( PyObject * _unused , PyObject * arg)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( freeWeakRef)( PyObject * _unused , PyObject * arg)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( expired)( PyObject * _unused , PyObject * arg)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( sharedFd)( THPStorage * self)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageSharing.cpp,"THPStorage_( isShared)( THPStorage * self)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( size)( THPStorage * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( dataPtr)( THPStorage * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( copy_)( PyObject * self , PyObject * args , PyObject * kwargs)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( isPinned)( THPStorage * self)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( elementSize)( THPStorage * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( new)( THPStorage * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( resize_)( THPStorage * self , PyObject * number_arg)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fill_)( THPStorage * self , PyObject * number_arg)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fromBuffer)( PyObject * _unused , PyObject * args , PyObject * keywds)",91, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( fromFile)( PyObject * _unused , PyObject * args , PyObject * keywds)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( writeFile)( THPStorage * self , PyObject * args)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( newWithFile)( PyObject * _unused , PyObject * file)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( setFromFile)( THPStorage * self , PyObject * args)",35, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( getDevice)( THPStorage * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/generic/StorageMethods.cpp,"THPStorage_( _setCdata)( THPStorage * self , PyObject * new_cdata)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::PyFunctionPreHook( PyObject * dict , int value_idx)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::~PyFunctionPreHook()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPreHook::operator ( )( const variable_list & values)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::PyFunctionPostHook( PyObject * dict)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::~PyFunctionPostHook()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"torch::autograd::PyFunctionPostHook::operator ( )( const variable_list & _outputs , const variable_list & _inputs)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"wrap_variables( const variable_list & c_variables)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"unwrap_variables( PyObject * py_variables)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"check_result( PyObject * prev , PyObject * result , PyObject * hook)",22, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"check_single_result( PyObject * _original , PyObject * _result , PyObject * hook)",44, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_hook.cpp,"hook_name( PyObject * hook)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/saved_variable.cpp,"torch::autograd::SavedVariable::SavedVariable( const Variable & variable , bool is_output)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/saved_variable.cpp,"torch::autograd::SavedVariable::unpack( std :: shared_ptr<Function> saved_for) const",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::VariableInfo::VariableInfo( const Variable & var)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::VariableInfo::zeros( at :: OptionalDeviceGuard & device_guard) const",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::legacy_apply( const variable_list & inputs)",43, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::apply( variable_list && inputs)",94, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::is_traceable()",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::release_variables()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::name() const",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"torch::autograd::PyFunction::get_shared_ptr()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_traverse( THPFunction * self , visitproc visit , void * arg)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_clear( THPFunction * self)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_dealloc( THPFunction * self)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_mark_dirty( THPFunction * self)",24, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_wrap_outputs( THPFunction * self , PyObject * inputs_tuple , PyObject * raw_output , PyObject * outputs , bool is_executable)",95, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_save_variables( THPFunction * self)",29, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_parse_non_differentiable( THPFunction * self)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"unpack_input( PyObject * args)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_assert_not_tracing( const char * name , const variable_list & input_vars)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trace_pre_record( PyObject * op_obj , PyObject * input_objects , const variable_list & input_vars)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trace_post_record( Node * node , PyObject * op_obj , const variable_list & input_vars , PyObject * output_objects , bool is_inplace , bool unpack_output)",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"process_outputs( PyObject * op_obj , THPFunction * grad_fn , const UnpackedInput & unpacked , PyObject * inputs , THPObjectPtr && raw_output , bool is_executable , Node * node)",43, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_do_forward( THPFunction * self , PyObject * _inputs)",30, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_apply( PyObject * cls , PyObject * inputs)",50, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_prepare_grads( THPFunction * self , THPObjectPtr & raw_grads , bool is_grad_output)",31, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"_trim_grad_input( THPFunction * self , THPObjectPtr & grad_input)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_do_backward( THPFunction * self , PyObject * args)",50, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction__register_hook_dict( THPFunction * self , PyObject * _var)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_register_hook( THPFunction * self , PyObject * hook)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"unpack_saved_variables( THPFunction * self , const std :: function<PyObject*(const Variable&)> & unpack_fn)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_saved_tensors( THPFunction * self , void * _unused)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_saved_variables( THPFunction * self , void * _unused)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_next_functions( THPFunction * self , void * _unused)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_metadata( THPFunction * self , void * _unused)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getObject( PyObject * obj , void * _unused)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"setObject( PyObject * obj , PyObject * value , void * _unused)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getMember( PyObject * obj , void * _unused)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getImplMember( PyObject * obj , void * _unused)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"getRequiresGrad( PyObject * obj , void * _unused)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_initModule( PyObject * module)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"Decref::operator ( )( PyFunction * p) const",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_function.cpp,"THPFunction_asFunction( THPFunction * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::getEventList()",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::mark( std :: string name , bool include_cuda)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::c_str( const char * str)",1, 51, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::c_str( std :: string & str)",1, 60, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::pushRangeImpl( T name , const char * msg = "" , int64_t sequence_nr = - 1)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::pushRange( std :: string name)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::popRange()",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( Function * fn)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( std :: string name)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( const char * name)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::RecordFunction::RecordFunction( const char * name , int64_t current_sequence_nr)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::onEachDevice( std :: function<void(int)> op)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::enableProfiler( ProfilerState new_state)",32, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/profiler.cpp,"torch::autograd::profiler::disableProfiler()",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/grad_mode.cpp,"torch::autograd::GradMode::is_enabled()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/grad_mode.cpp,"torch::autograd::GradMode::set_enabled( bool enabled)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"THPAutograd_initExtension( PyObject * _unused)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::set_grad_enabled( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::is_grad_enabled( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::set_anomaly_mode_enabled( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::is_anomaly_mode_enabled( PyObject * _unused , PyObject * arg)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/init.cpp,"torch::autograd::python_functions()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::peek_at_next_sequence_nr()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::get_next_sequence_nr()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::name() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::Function::metadata()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::gatherFunctions( Function * func , std :: vector<std::shared_ptr<Function>> & stack)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/function.cpp,"torch::autograd::deleteFunction( Function * function)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::add( size_t pos , Variable var)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::device() const",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/input_buffer.cpp,"torch::autograd::InputBuffer::variables( InputBuffer && g)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_call( PyObject * self , PyObject * args , PyObject * kwargs)",44, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_traverse( PyObject * self , visitproc visit , void * arg)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_clear( PyObject * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_dealloc( PyObject * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_next_functions( THPCppFunction * self , PyObject * hook)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_metadata( THPCppFunction * self , void * _unused)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_requires_grad( THPCppFunction * self)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_register_hook_dict( PyObject * self , PyObject * _var)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_register_hook( PyObject * self , PyObject * hook)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::THPCppFunction_name( PyObject * self)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::_initFunctionPyTypeObject( PyTypeObject & type , const char * name , PyGetSetDef * function_properties , PyMethodDef * function_methods)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::DefaultFunctionType::DefaultFunctionType()",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::functionToPyObject( const std :: shared_ptr<Function> & cdata)",37, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::registerCppFunction( const std :: type_info & type , PyTypeObject * pytype)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_cpp_function.cpp,"torch::autograd::registerFunctionHook( Function & fn , PyObject * hook)",25, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::Impl( at :: Tensor data , bool requires_grad , Edge gradient_edge)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::numel() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::sizes() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::strides() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::is_contiguous() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::dim() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::size( int64_t d) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::stride( int64_t d) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::resize_dim( int64_t ndim)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_size( int64_t dim , int64_t new_size)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_stride( int64_t dim , int64_t new_stride)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_storage_offset( int64_t storage_offset)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::slow_data() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::storage() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::storage_offset() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::get_device_slow() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::get_grad_accumulator()",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::detach_()",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::backward( c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::set_data( Tensor new_data)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::Impl::release_resources()",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::DifferentiableViewImpl( Variable base , at :: Tensor data , Edge gradient_edge)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::get_grad_fn()",23, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::rebase_history( Edge gradient_edge)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::DifferentiableViewImpl::release_resources()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/variable.cpp,"torch::autograd::Variable::rebase_history( Edge gradient_edge)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::FunctionTask::FunctionTask( GraphTask * base , std :: shared_ptr<Function> fn , InputBuffer inputs)",4, 35, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::CompareFunctionTaskTime::operator ( )( FunctionTask const & t1 , FunctionTask const & t2)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::ExecInfo::Capture::Capture( int input_idx , int output_idx)",1, 95, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::ExecInfo::should_execute() const",3, 6, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::can_checkpoint()",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::GraphTask( bool keep_graph , bool grad_mode)",6, 26, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ReadyQueue::push( FunctionTask item)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ReadyQueue::pop()",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_init( int device)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_main( GraphTask * graph_task)",43, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::thread_on_exception( FunctionTask & task , std :: exception & e)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_pre_hooks( Function & fn , variable_list inputs)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_post_hooks( Function & fn , variable_list outputs , const variable_list & inputs)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::is_compatible_type( const at :: Type & expected , const at :: Type & actual)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::validate_outputs( const edge_list & edges , variable_list & grads , const F & format_error)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::call_function( FunctionTask & task)",45, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::evaluate_function( FunctionTask & task)",87, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::compute_dependencies( Function * root , GraphTask & task)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::ClearCallbacks( std :: vector<std::function<void()>> & callbacks , std :: mutex & callbacks_lock)",4, 50, 4, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::~ClearCallbacks()",1, 33, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::ClearCallbacks::clear()",4, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::execute( const edge_list & roots , const variable_list & inputs , bool keep_graph , bool create_graph , const edge_list & outputs)",65, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::get_base_engine()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::set_default_engine_stub( EngineStub stub)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::get_default_engine()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::queue_callback( std :: function<void()> callback)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::is_checkpoint_valid()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::ready_queue( int device)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::Engine::start_threads()",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/engine.cpp,"torch::autograd::GraphTask::init_to_execute( Function & graph_root , const edge_list & outputs)",63, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp,"torch::autograd::PyAnomalyMetadata::store_stack()",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp,"torch::autograd::PyAnomalyMetadata::print_stack()",29, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_legacy_variable.cpp,"torch::autograd::THPVariable_pynew( PyTypeObject * type , PyObject * args , PyObject * kwds)",65, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_legacy_variable.cpp,"torch::autograd::init_legacy_variable( PyObject * module)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_length( PyObject * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::count_specified_dimensions( PyObject * index)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::invalid_index( PyObject * obj)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySlice( const Variable & self , int64_t dim , PyObject * slice , bool ensure_view = false)",21, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySelect( const Variable & self , int64_t dim , int64_t index)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::sequenceToVariable( const at :: Type & type , PyObject * seq)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::valueToTensor( const at :: Type & type , PyObject * value)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::boolToIndexingTensor( const Variable & self , bool value)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::applySlicing( const Variable & self , PyObject * index , variable_list & outIndices)",58, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::typeConvertIndices( const Variable & self , const variable_list & indices)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::dispatch_index( const Variable & self , const variable_list & indices)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::dispatch_index_put_( Variable & self , const variable_list & indices , const Variable & value)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::treatSequenceAsTuple( PyObject * index)",39, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::wrapTuple( PyObject * index)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_getitem( PyObject * self , PyObject * index)",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::slicePrefix1sSize( IntList sizes)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::copy_to( Variable dst , const Variable & src)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable_indexing.cpp,"torch::autograd::THPVariable_setitem( PyObject * self , PyObject * index , PyObject * py_value)",50, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::VariableType( Context * context , TypeExtendedInterface * baseType)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::scalarType() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::typeMeta() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::backend() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allocator() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::getDeviceFromPtr( void * data) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unsafeStorageFromTH( void * th_pointer , bool retain) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::storageWithAllocator( int64_t size , Allocator * allocator) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unsafeTensorFromTH( void * th_pointer , bool retain) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::generator() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toString() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::elementSizeInBytes() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toBackend( Backend b) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::toScalarType( ScalarType s) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::ID() const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::register_variable_type_for( TypeExtendedInterface * baseType)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableTypeRegistry::VariableTypeRegistry()",11, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::VariableHooks( at :: VariableHooksArgs)",1, 42, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::registerVariableTypeFor( at :: LegacyTypeDispatch * context , at :: Backend backend , at :: ScalarType scalar_type) const",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableHooks::getVariableTypeFromBaseType( const at :: Type & baseType) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::isVariableType( const at :: Type & type)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::getVariableTypeFromBaseType( const at :: Type & baseType)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::allTypesForBackends( at :: ArrayRef<at::Backend> backends)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allCPUTypes()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::allCUDATypes()",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::checked_cast_variable( const Tensor & t , const char * name , int pos)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::checked_cast_variable( Tensor & t , const char * name , int pos)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( const Tensor & t , const char * name , int pos)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( Tensor & t , const char * name , int pos)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( SparseTensorRef t , const char * name , int pos)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack_opt( const Tensor & t , const char * name , int pos)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::unpack( at :: TensorList tl , const char * name , int pos)",15, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::backward( Tensor & self , c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::set_data( Tensor & self , Tensor new_data) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::s_copy_( Tensor & self , const Tensor & src , bool non_blocking) const",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::_s_copy_from( const Tensor & self , const Tensor & dst , bool non_blocking) const",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::resize_( Tensor & self , IntList size) const",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::resize_as_( Tensor & self , const Tensor & the_template) const",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::detach( const Tensor & self) const",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/VariableTypeManual.cpp,"torch::autograd::VariableType::detach_( Tensor & self) const",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"get_python_engine()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::thread_init( int device)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::thread_on_exception( FunctionTask & task , std :: exception & e)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::make_anomaly_metadata()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"torch::autograd::python::PythonEngine::execute( const edge_list & roots , const variable_list & inputs , bool keep_graph , bool create_graph , const edge_list & outputs)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"_maybe_reinitialize_engine_after_fork()",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_run_backward( THPEngine * self , PyObject * args , PyObject * kwargs)",100, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_queue_callback( PyObject * self , PyObject * _callback)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_is_checkpoint_valid( PyObject * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_new( PyTypeObject * type , PyObject * args , PyObject * kwargs)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"child_atfork()",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_engine.cpp,"THPEngine_initModule( PyObject * module)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_NewWithVar( PyTypeObject * type , Variable var)",17, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_Wrap( Variable var)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_traverse( THPVariable * self , visitproc visit , void * arg)",26, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_clear( THPVariable * self)",12, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_dealloc( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_pynew( PyTypeObject * type , PyObject * args , PyObject * kwargs)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_make_subclass( PyObject * _ignored , PyObject * args , PyObject * kwargs)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_cdata( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_version( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_grad_fn( THPVariable * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_grad_fn( THPVariable * self , PyObject * obj)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_leaf( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_data( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_data( THPVariable * self , PyObject * data)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_grad( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_grad( THPVariable * self , PyObject * py_grad)",36, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_volatile( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_volatile( THPVariable * self , PyObject * obj)",4, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_output_nr( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_requires_grad( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_requires_grad( THPVariable * self , PyObject * obj)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_name( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_backwards_hooks( THPVariable * self)",10, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_set_backwards_hooks( THPVariable * self , PyObject * obj)",16, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_base( THPVariable * self)",9, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_get_shape( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_cuda( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_is_sparse( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_dtype( THPVariable * self)",7, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_layout( THPVariable * self)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_device( THPVariable * self)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/python_variable.cpp,"THPVariable_initModule( PyObject * module)",13, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/basic_ops.cpp,"torch::autograd::Error::apply( variable_list && inputs)",3, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/basic_ops.cpp,"torch::autograd::DelayedError::apply( variable_list && inputs)",11, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp,"torch::autograd::AccumulateGrad::AccumulateGrad( Variable variable_)",5, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp,"torch::autograd::AccumulateGrad::apply( variable_list && grads)",52, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"DelayedErrorCtor::operator ( )( PyObject * args)",10, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"NoCtor::operator ( )( PyObject * args)",3, 4, 2, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"addClass( PyObject * module , PyTypeObject & type , const char * name , PyGetSetDef * function_properties = nullptr , PyMethodDef * function_methods = nullptr)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"getTupleAttr( PyObject * obj , void * _unused)",14, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"getValueAttr( PyObject * obj , void * _unused)",8, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"accumulateGradVar( PyObject * _self , void * _unused)",6, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/init.cpp,"THPAutograd_initFunctions()",33, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Scatter::Scatter( std :: vector<at::Device> devices , const c10 :: optional<std::vector<int64_t>> & chunk_sizes , int64_t dim , const c10 :: optional<std::vector<c10::optional<at::cuda::CUDAStream>>> & streams , bool unsqueeze_scalars)",11, 47, 6, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Scatter::apply( variable_list && inputs)",37, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Gather::Gather( const at :: Device & destination_device , int64_t dim)",2, 60, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/comm.cpp,"torch::autograd::Gather::apply( variable_list && inputs)",58, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/utils.cpp,"torch::autograd::wrap_outputs( const variable_list & inputs , tensor_list && outputs , const function_constructor & ctr)",27, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/utils.cpp,"torch::autograd::check_input_variables( const char * name , const variable_list & inputs , int args , int required_args)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopyBackwards::apply( variable_list && grads)",19, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::CopySlices( const Variable & base_var , at :: TensorGeometry view_ , std :: shared_ptr<Function> fn_)",18, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::apply( variable_list && inputs)",34, 2, 0, 0
repos/cpp/pytorch/torch/csrc/autograd/functions/tensor.cpp,"torch::autograd::CopySlices::release_variables()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"init_MakeDataPyExt()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"resizeJPEG( PyObject * self , PyObject * args)",34, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::DecoderThread( PyObject * py_list_src , int start_img , int end_img , int target_size , bool crop_to_square)",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::~DecoderThread()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::run()",6, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::getTargetList()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/make-data/pyext/src/pyext.cpp,"DecoderThread::makeJPEG( int idx)",45, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"sqrt( int _X)",1, 48, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"log( int _X)",1, 46, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_init( MTYPE * data , int64 numRows , int64 numCols , bool transpose , bool ownsData)",6, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( int64 numRows , int64 numCols)",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( int64 numRows , int64 numCols , bool transpose)",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( const Matrix & like)",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( MTYPE * data , int64 numRows , int64 numCols)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( MTYPE * data , int64 numRows , int64 numCols , bool transpose)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::Matrix( const PyArrayObject * src)",20, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::~Matrix()",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_updateDims( int64 numRows , int64 numCols)",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_checkBounds( int64 startRow , int64 endRow , int64 startCol , int64 endCol) const",6, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::slice( int64 startRow , int64 endRow , int64 startCol , int64 endCol) const",13, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::slice( int64 startRow , int64 endRow , int64 startCol , int64 endCol , Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceRows( int64 startRow , int64 endRow) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceRows( int64 startRow , int64 endRow , Matrix & target) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceCols( int64 startCol , int64 endCol) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sliceCols( int64 startCol , int64 endCol , Matrix & target) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtractFromScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtractFromScalar( MTYPE scalar , Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThanScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThanScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equalsScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThanScalar( MTYPE scalar , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThanScalar( MTYPE scalar , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equalsScalar( MTYPE scalar , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , Matrix & target)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scale)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m , Matrix & target)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m , MTYPE scale)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::subtract( const Matrix & m , MTYPE scale , Matrix & target)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scaleM , Matrix & target)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scaleThis , MTYPE scaleM)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::add( const Matrix & m , MTYPE scaleThis , MTYPE scaleM , Matrix & target)",20, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addScalar( MTYPE scalar , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWithScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWithScalar( MTYPE scalar , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWithScalar( MTYPE scalar)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWithScalar( MTYPE scalar , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThan( Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::biggerThan( Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThan( Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::smallerThan( Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equals( Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::equals( Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::notEquals( Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::notEquals( Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWith( Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::minWith( Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWith( Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::maxWith( Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec , MTYPE scale , Matrix & target)",19, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec , MTYPE scale)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addVector( const Matrix & vec , Matrix & target)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMultByVector( const Matrix & vec)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMultByVector( const Matrix & vec , Matrix & target)",14, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b , MTYPE scale)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b , Matrix & target) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::rightMult( const Matrix & b , MTYPE scaleAB , Matrix & target) const",6, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addProduct( const Matrix & a , const Matrix & b , MTYPE scaleAB , MTYPE scaleThis)",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::addProduct( const Matrix & a , const Matrix & b)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::transpose() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::transpose( bool hard) const",12, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::tile( int64 timesY , int64 timesX) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::tile( int64 timesY , int64 timesX , Matrix & target) const",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_tileTo2( Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::resize( int64 newNumRows , int64 newNumCols)",11, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::resize( const Matrix & like)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::scale( MTYPE alpha)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::scale( MTYPE alpha , Matrix & target)",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::copy( Matrix & dest , int64 srcStartRow , int64 srcEndRow , int64 srcStartCol , int64 srcEndCol , int64 destStartRow , int64 destStartCol) const",26, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::copy() const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::copy( Matrix & target) const",8, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_copyAllTo( Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::min() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::min( int64 axis) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::min( int64 axis , Matrix & target) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::max() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::max( int64 axis) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::max( int64 axis , Matrix & target) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sum() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::norm() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::norm2() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sum( int64 axis) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::sum( int64 axis , Matrix & target) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregate( int64 axis , Matrix & target , MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",13, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregateRow( int64 row , MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregateCol( int64 col , MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_aggregate( MTYPE(*agg_func)(MTYPE,MTYPE) , MTYPE initialValue) const",8, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::printShape( const char * name) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::print() const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::print( int64 rows , int64 cols) const",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::print( int64 startRow , int64 rows , int64 startCol , int64 cols) const",8, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::apply( Matrix :: FUNCTION f)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::apply( Matrix :: FUNCTION f , Matrix & target)",30, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMult( const Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivide( const Matrix & a , Matrix & target) const",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseMult( const Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivide( const Matrix & a)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::randomizeUniform()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::randomizeNormal()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::randomizeNormal( MTYPE , MTYPE)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivideByVector( const Matrix & vec)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::eltWiseDivideByVector( const Matrix & vec , Matrix & target)",9, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_divideByVector( const Matrix & vec , Matrix & target)",6, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::reshape( int64 numRows , int64 numCols)",5, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::reshaped( int64 numRows , int64 numCols)",4, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop( MTYPE(*func)(MTYPE) , Matrix & target)",6, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop( MTYPE(*func)(MTYPE))",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop2( const Matrix & a , MTYPE(*func)(MTYPE,MTYPE) , Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop2( const Matrix & a , MTYPE(*func)(MTYPE,MTYPE,MTYPE) , MTYPE scalar , Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoop2( const Matrix & a , MTYPE(*func)(MTYPE,MTYPE,MTYPE,MTYPE) , MTYPE scalar1 , MTYPE scalar2 , Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::_applyLoopScalar( const MTYPE scalar , MTYPE(*func)(MTYPE,MTYPE) , Matrix & target) const",7, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::hasNan() const",10, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/util/src/matrix.cpp,"Matrix::hasInf() const",10, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::DecoderThread( PyObject * pyList , Matrix & target , int start_img , int end_img , int img_size , int inner_size , bool test , bool multiview)",8, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::~DecoderThread()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::run()",26, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::decodeJpeg( int idx , int & width , int & height)",30, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::randUniform()",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::randUniform( double min , double max)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::crop( int64 i , int64 src_width , int64 src_height , bool flip)",3, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/cuda-convnet2/cudaconvnet/src/jpeg.cpp,"DecoderThread::crop( int64 i , int64 src_width , int64 src_height , bool flip , int64 crop_start_x , int64 crop_start_y)",22, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op.cpp,"caffe2::detail::workspaceInfo<CPUContext>( const CPUContext &)",8, 2, 0, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op.cpp,"caffe2::GetCTCGradient::GetGradientDefs()",4, 4, 2, 0
repos/cpp/pytorch/caffe2/contrib/warpctc/ctc_op_gpu.cpp,"caffe2::detail::workspaceInfo<CUDAContext>( const CUDAContext & context)",6, 2, 0, 0
repos/cpp/pytorch/c10/DeviceType.cpp,"c10::DeviceTypeName( DeviceType d , bool lower_case)",32, 2, 0, 0
repos/cpp/pytorch/c10/DeviceType.cpp,"c10::isValidDeviceType( DeviceType d)",15, 2, 0, 0
repos/cpp/pytorch/c10/DeviceType.cpp,"c10::operator < <( std :: ostream & stream , DeviceType type)",4, 2, 0, 0
repos/cpp/pytorch/c10/Stream.cpp,"c10::operator < <( std :: ostream & stream , const Stream & s)",4, 2, 0, 0
repos/cpp/pytorch/c10/Half.cpp,"c10::operator < <( std :: ostream & out , const Half & value)",4, 2, 0, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::parse_type( const std :: string & device_string)",22, 2, 0, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::Device::validate()",6, 2, 0, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::Device::Device( const std :: string & device_string)",23, 2, 0, 0
repos/cpp/pytorch/c10/Device.cpp,"c10::operator < <( std :: ostream & stream , const Device & device)",7, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::UndefinedTensorImpl()",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::sizes() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::size( int64_t d) const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::stride( int64_t d) const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::dim() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::storage() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::storage_offset() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/UndefinedTensorImpl.cpp,"c10::UndefinedTensorImpl::strides() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/DefaultDtype.cpp,"c10::set_default_dtype( caffe2 :: TypeMeta dtype)",3, 2, 0, 0
repos/cpp/pytorch/c10/core/DefaultDtype.cpp,"c10::get_default_dtype()",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::grad()",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::grad() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( TensorTypeId type_id , const caffe2 :: TypeMeta & data_type , Allocator * allocator , bool is_variable)",8, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( Storage && storage , TensorTypeId type_id , bool is_variable)",2, 79, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::TensorImpl( Storage && storage , TensorTypeId type_id , const caffe2 :: TypeMeta & data_type , bool is_variable)",10, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::sizes() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::strides() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::compute_contiguous() const",17, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::release_resources()",5, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::dim() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::size( int64_t d) const",4, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::stride( int64_t d) const",4, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::maybe_zero_dim( bool condition_when_zero_dim)",7, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::TensorImpl::storage() const",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::deletePlacementDeleteContext( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorImpl.cpp,"c10::PlacementDeleteContext::makeDataPtr( at :: DataPtr && data_ptr , PlacementDtor placement_dtor , size_t size , at :: Device device)",11, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::TensorTypeIds()",1, 60, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::singleton()",4, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdCreator::TensorTypeIdCreator()",1, 60, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdCreator::create()",14, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::TensorTypeIdRegistry()",1, 81, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::registerId( c10 :: TensorTypeId id)",4, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistry::deregisterId( c10 :: TensorTypeId id)",4, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::createAndRegister()",5, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIds::deregister( c10 :: TensorTypeId id)",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistrar::TensorTypeIdRegistrar()",2, 61, 4, 0
repos/cpp/pytorch/c10/core/TensorTypeIdRegistration.cpp,"c10::TensorTypeIdRegistrar::~TensorTypeIdRegistrar()",3, 2, 0, 0
repos/cpp/pytorch/c10/core/CopyBytes.cpp,"c10::_CopyBytesFunctionRegisterer::_CopyBytesFunctionRegisterer( DeviceType fromType , DeviceType toType , CopyBytesFunction func_sync , CopyBytesFunction func_async)",19, 2, 0, 0
repos/cpp/pytorch/c10/core/CopyBytes.cpp,"c10::CopyBytes( size_t nbytes , const void * src , Device src_device , void * dst , Device dst_device , bool async)",17, 2, 0, 0
repos/cpp/pytorch/c10/core/Scalar.cpp,"c10::Scalar::operator -() const",9, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorTypeId.cpp,"c10::operator < <( std :: ostream & str , c10 :: TensorTypeId rhs)",3, 2, 0, 0
repos/cpp/pytorch/c10/core/TensorOptions.cpp,"c10::operator < <( std :: ostream & stream , const TensorOptions & options)",9, 2, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::deleteInefficientStdFunctionContext( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"c10::InefficientStdFunctionContext::makeDataPtr( void * ptr , const std :: function<void(void*)> & deleter , Device device)",9, 2, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"caffe2::SetAllocator( at :: DeviceType t , at :: Allocator * alloc)",3, 2, 0, 0
repos/cpp/pytorch/c10/core/Allocator.cpp,"caffe2::GetAllocator( const at :: DeviceType & t)",5, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStreamInternals::~CUDAStreamInternals()",3, 4, 2, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::operator < <( std :: ostream & stream , StreamIdType s)",17, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::streamIdType( StreamId s)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::streamIdIndex( StreamId s)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::makeStreamId( StreamIdType st , size_t si)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::pointer_within( const T * ptr , const A & arr)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_getStreamId( const CUDAStreamInternals * ptr)",30, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initGlobalStreamState()",18, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initDeviceStreamState( DeviceIndex device_index)",31, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::initCUDAStreamsOnce()",12, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::check_gpu( DeviceIndex device_index)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::get_idx( std :: atomic<uint32_t> & counter)",4, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_internals( CUDAStream s)",17, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream_fromInternals( const CUDAStreamInternals * ptr)",5, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::CUDAStream::stream() const",5, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getStreamFromPool( const bool isHighPriority , DeviceIndex device_index)",18, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getDefaultCUDAStream( DeviceIndex device_index)",6, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::getCurrentCUDAStream( DeviceIndex device_index)",6, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::setCurrentCUDAStream( CUDAStream stream)",6, 2, 0, 0
repos/cpp/pytorch/c10/cuda/CUDAStream.cpp,"c10::cuda::operator < <( std :: ostream & stream , const CUDAStream & s)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/test/impl/CUDATest.cpp,"TEST( CUDATest , SmokeTest)",3, 2, 0, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::c10_cuda_test()",5, 2, 0, 0
repos/cpp/pytorch/c10/cuda/impl/CUDATest.cpp,"c10::cuda::impl::c10_cuda_private_test()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::GlobalInitStream()",4, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::SetUsageMessage( const string & str)",4, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::UsageMessage()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::ParseCommandLineFlags( int * pargc , char ** * pargv)",83, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::CommandLineFlagsHasBeenParsed()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<string>( const string & content , string * value)",6, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<int>( const string & content , int * value)",10, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<int64_t>( const string & content , int64_t * value)",18, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<double>( const string & content , double * value)",12, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_no_gflags.cpp,"c10::C10FlagParser::Parse<bool>( const string & content , bool * value)",22, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::Error( const std :: string & new_msg , const std :: string & backtrace , const void * caller)",8, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::Error( const char * file , const int line , const char * condition , const std :: string & msg , const std :: string & backtrace , const void * caller)",19, 21, 10, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::msg() const",5, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::msg_without_backtrace() const",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Error::AppendMessage( const std :: string & new_msg)",9, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::warn( SourceLocation source_location , std :: string msg)",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::set_warning_handler( handler_t handler)",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::Warning::print_warning( const SourceLocation & source_location , const char * msg)",5, 2, 0, 0
repos/cpp/pytorch/c10/util/Exception.cpp,"c10::GetExceptionString( const std :: exception & e)",7, 2, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::detail::StripBasename( const std :: string & full_path)",9, 2, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::operator < <( std :: ostream & out , const SourceLocation & loc)",4, 2, 0, 0
repos/cpp/pytorch/c10/util/StringUtil.cpp,"c10::ReplaceAll( std :: string & s , const char * from , const char * to)",14, 2, 0, 0
repos/cpp/pytorch/c10/util/Type.cpp,"c10::demangle( const char * name)",26, 2, 0, 0
repos/cpp/pytorch/c10/util/Type.cpp,"c10::demangle( const char * name)",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::enforce_detail::EnforceFailMessage::EnforceFailMessage( std :: string && msg)",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::GetFetchStackTrace()",4, 3, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::SetStackTraceFetcher( std :: function<string(void)> fetcher)",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ThrowEnforceNotMet( const char * file , const int line , const char * condition , const std :: string & msg , const void * caller)",12, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::Error::Error( SourceLocation source_location , const std :: string & msg)",7, 45, 14, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::InitCaffeLogging( int * argc , char ** argv)",17, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::UpdateLoggingLevelsFromFlags()",13, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ShowLogInfoToStderr()",4, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::InitCaffeLogging( int * argc , char ** argv)",19, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::UpdateLoggingLevelsFromFlags()",1, 39, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::ShowLogInfoToStderr()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::MessageLogger::MessageLogger( const char * file , int line , int severity)",31, 2, 0, 0
repos/cpp/pytorch/c10/util/Logging.cpp,"c10::MessageLogger::~MessageLogger()",39, 2, 0, 0
repos/cpp/pytorch/c10/util/UniqueVoidPtr.cpp,"c10::detail::deleteNothing( void *)",1, 29, 0, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::detail::_ThrowRuntimeTypeLogicError( const string & msg)",5, 2, 0, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::TypeMeta::_typeMetaDataInstance<detail::_Uninitialized>()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/typeid.cpp,"caffe2::TypeIdentifier::createTypeId()",11, 2, 0, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::is_python_frame( const FrameInformation & frame)",4, 2, 0, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::parse_frame_information( const std :: string & frame_string)",66, 2, 0, 0
repos/cpp/pytorch/c10/util/Backtrace.cpp,"c10::get_backtrace( size_t frames_to_skip , size_t maximum_number_of_frames , bool skip_python_frames)",79, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::IsNUMAEnabled()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMABind( int numa_node_id)",20, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNUMANode( const void * ptr)",19, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNumNUMANodes()",8, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMAMove( void * ptr , size_t size , int numa_node_id)",28, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetCurrentNUMANode()",9, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::IsNUMAEnabled()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMABind( int numa_node_id)",5, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNUMANode( const void * ptr)",4, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetNumNUMANodes()",4, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::NUMAMove( void * ptr , size_t size , int numa_node_id)",5, 2, 0, 0
repos/cpp/pytorch/c10/util/numa.cpp,"c10::GetCurrentNUMANode()",4, 2, 0, 0
repos/cpp/pytorch/c10/util/SmallVector.cpp,"c10::SmallVectorBase::grow_pod( void * FirstEl , size_t MinSizeInBytes , size_t TSize)",28, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::SetUsageMessage( const string & str)",7, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::UsageMessage()",3, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::ParseCommandLineFlags( int * pargc , char ** * pargv)",6, 2, 0, 0
repos/cpp/pytorch/c10/util/flags_use_gflags.cpp,"c10::CommandLineFlagsHasBeenParsed()",4, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::Foo::Foo( int x)",3, 4, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::Foo::~Foo()",1, 20, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::Bar::Bar( int x)",3, 4, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::AnotherBar::AnotherBar( int x)",3, 4, 2, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::TEST( RegistryTest , CanRunCreator)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::TEST( RegistryTest , ReturnNullOnNonExistingCreator)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooDefault()",4, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooDefaultAgain()",4, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooBarFallback()",4, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::RegisterFooBarPreferred()",4, 2, 0, 0
repos/cpp/pytorch/c10/test/registry_test.cpp,"c10_test::TEST( RegistryTest , RegistryPriorities)",19, 2, 0, 0
repos/cpp/pytorch/c10/test/flags_test.cpp,"c10_test::TEST( FlagsTest , TestGflagsCorrectness)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/DeviceGuard_test.cpp,"TEST( DeviceGuard , ResetDeviceDifferentDeviceType)",12, 2, 0, 0
repos/cpp/pytorch/c10/test/DeviceGuard_test.cpp,"TEST( OptionalDeviceGuard , ResetDeviceDifferentDeviceType)",13, 2, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceTrue)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceFalse)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , TestEnforceEquals)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , EnforceShowcase)",24, 2, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingTest , Join)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/logging_test.cpp,"c10_test::TEST( LoggingDeathTest , TestEnforceUsingFatal)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/core/TensorTypeId_test.cpp,"TEST( TensorTypeIdTest , Printing)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeClass1Parameter::SomeClass1Parameter( int param_)",1, 53, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeClass2Parameters::SomeClass2Parameters( int param1_ , int param2_)",2, 44, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeBaseClass::SomeBaseClass( int v_)",1, 35, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"SomeChildClass::SomeChildClass( int v)",1, 46, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::DestructableMock( bool * resourcesReleased , bool * wasDestructed)",2, 80, 6, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::~DestructableMock()",3, 4, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"DestructableMock::release_resources()",3, 4, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"ChildDestructableMock::ChildDestructableMock( bool * resourcesReleased , bool * wasDestructed)",2, 62, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"NullType1::singleton()",3, 4, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"NullType2::singleton()",3, 4, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith0Parameters)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith1Parameter)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , ClassWith2Parameters)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , TypeIsAutoDeductible)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( MakeIntrusiveTest , CanAssignToBaseClassPtr)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTargetTest , whenAllocatedOnStack_thenDoesntCrash)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCallingGet_thenReturnsObject)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCallingConstGet_thenReturnsObject)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCallingGet_thenReturnsNullptr)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenDereferencing_thenReturnsObject)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenConstDereferencing_thenReturnsObject)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenArrowDereferencing_thenReturnsObject)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenConstArrowDereferencing_thenReturnsObject)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenPointsToSameObject)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenOldInstanceInvalid)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenStaysValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToSelf_thenStaysInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningFromInvalidPtr_thenNewInstanceIsInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenOldInstanceInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenMoveAssigningToDifferentNullptr_thenHasNewNullptr)",12, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenPointsToSameObject)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenOldInstanceValid)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenStaysValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToSelf_thenStaysInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigning_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenOldInstanceInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssigningInvalidPtrToBaseClass_thenNewInstanceIsInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenCopyAssigningToDifferentNullptr_thenHasNewNullptr)",12, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenPointsToSameObject)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenOldInstanceInvalid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructing_thenNewInstanceValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenOldInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenNewInstanceValid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenMoveConstructingToDifferentNullptr_thenHasNewNullptr)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenPointsToSameObject)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenOldInstanceValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructing_thenNewInstanceValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenOldInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNullPtr_whenCopyConstructingToDifferentNullptr_thenHasNewNullptr)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunction)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethod)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionFromInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodFromInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionWithInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodWithInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapFunctionInvalidWithInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , SwapMethodInvalidWithInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInContainer)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInSet)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInUnorderedSet)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInMap)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , CanBePutInUnorderedMap)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_AfterCopyConstructor)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_AfterCopyAssignment)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Equality_Nullptr)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality_NullptrLeft)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , Nonequality_NullptrRight)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsDifferent)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsDifferent_ValidAndInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_AfterCopyConstructor)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_AfterCopyAssignment)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , HashIsSame_BothNullptr)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , OneIsLess)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsLess1)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsLess2)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , NullptrIsNotLessThanNullptr)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenIsInvalid)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenHoldsNullptr)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenDestructed_thenDestructsObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructed_thenDestructsObjectAfterSecondDestructed)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveConstructedToBaseClass_thenDestructsObjectAfterSecondDestructed)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsOldObject)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsOldObject)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithBaseClassCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenMoveAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsObjectAfterSecondDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsObjectAfterSecondDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",20, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssigned_thenDestructsOldObject)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClass_thenDestructsOldObject)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithBaseClassCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCopyAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenCallingReset_thenDestructs)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCallingReset_thenDestructsAfterCopyDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithCopy_whenCallingResetOnCopy_thenDestructsAfterOriginalDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithMoved_whenCallingReset_thenDestructsAfterMovedDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtrWithMoved_whenCallingResetOnMoved_thenDestructsImmediately)",13, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsMoveConstructingToConst)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsCopyConstructingToConst)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsMoveAssigningToConst)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , AllowsCopyAssigningToConst)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNewPtr_thenHasUseCount1)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenNewPtr_thenIsUnique)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenEmptyPtr_thenHasUseCount0)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenEmptyPtr_thenIsNotUnique)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenResetPtr_thenHasUseCount0)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenResetPtr_thenIsNotUnique)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenHasUseCount1)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenIsUnique)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenOldHasUseCount0)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveConstructedPtr_thenOldIsNotUnique)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenHasUseCount1)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenIsUnique)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenOldHasUseCount0)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenMoveAssignedPtr_thenOldIsNotUnique)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenHasUseCount2)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenIsNotUnique)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenOldHasUseCount2)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_thenOldIsNotUnique)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenDestructingCopy_thenHasUseCount1)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenDestructingCopy_thenIsUnique)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenReassigningCopy_thenHasUseCount1)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyConstructedPtr_whenReassigningCopy_thenIsUnique)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_thenHasUseCount2)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_thenIsNotUnique)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenDestructingCopy_thenHasUseCount1)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenDestructingCopy_thenIsUnique)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenReassigningCopy_thenHasUseCount1)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenCopyAssignedPtr_whenReassigningCopy_thenIsUnique)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenDoesntCrash)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",23, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( IntrusivePtrTest , givenStackObject_whenReclaimed_thenCrashes)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"IntrusiveAndWeak::IntrusiveAndWeak( intrusive_ptr<T> ptr_)",1, 79, 2, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_weak_intrusive( Args && ... args)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_weak_only( Args && ... args)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"make_invalid_weak()",3, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCreatingAndDestructing_thenDoesntCrash)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenLocking_thenReturnsCorrectObject)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigning_thenOldInstanceInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToSelf_thenStaysValid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigning_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToSelf_thenStaysInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigning_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigning_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToSelf_thenStaysInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToSelf_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningFromInvalidPtr_thenNewInstanceIsInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningFromWeakOnlyPtr_thenNewInstanceIsInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenMoveAssigningToBaseClass_thenOldInstanceInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToBaseClass_thenNewInstanceIsValid)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningToBaseClass_thenPointsToSameObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenMoveAssigningInvalidPtrToBaseClass_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenMoveAssigningToDifferentNullptr_thenHasNewNullptr)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigning_thenOldInstanceValid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToSelf_thenStaysValid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigning_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToSelf_thenStaysInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigning_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigning_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToSelf_thenStaysInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToSelf_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenValidPtr_whenCopyAssigningToBaseClass_thenOldInstanceInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigningInvalidPtrToBaseClass_thenNewInstanceIsInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToBaseClass_thenNewInstanceIsValid)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenCopyAssigningToBaseClass_thenPointsToSameObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigningWeakOnlyPtrToBaseClass_thenNewInstanceIsValid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenCopyAssigningToDifferentNullptr_thenHasNewNullptr)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenPointsToSameObject)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenOldInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructing_thenNewInstanceValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenPointsToSameObject)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenOldInstanceInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClass_thenNewInstanceValid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructingToBaseClassFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenMoveConstructingToDifferentNullptr_thenHasNewNullptr)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenPointsToSameObject)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenOldInstanceValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructing_thenNewInstanceValid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenPointsToSameObject)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenOldInstanceInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClass_thenNewInstanceInvalid)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromInvalidPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructingToBaseClassFromWeakOnlyPtr_thenNewInstanceInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNullPtr_whenCopyConstructingToDifferentNullptr_thenHasNewNullptr)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunction)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethod)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionFromInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodFromInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWithInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWithInvalid)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionInvalidWithInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodInvalidWithInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionFromWeakOnlyPtr)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodFromWeakOnlyPtr)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWithWeakOnlyPtr)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWithWeakOnlyPtr)",9, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapFunctionWeakOnlyPtrWithWeakOnlyPtr)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , SwapMethodWeakOnlyPtrWithWeakOnlyPtr)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInContainer)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInSet)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInUnorderedSet)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInMap)",13, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , CanBePutInUnorderedMap)",13, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyConstructor)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyAssignment)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_AfterCopyAssignment_WeakOnly)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Equality_Invalid)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_InvalidLeft)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_InvalidRight)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , Nonequality_WeakOnly)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_ValidAndInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_ValidAndWeakOnly)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsDifferent_WeakOnlyAndWeakOnly)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyConstructor)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyConstructor_WeakOnly)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyAssignment)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_AfterCopyAssignment_WeakOnly)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , HashIsSame_BothInvalid)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , OneIsLess)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsLess1)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsLess2)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , InvalidIsNotLessThanInvalid)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingResetOnWeakPtr_thenIsInvalid)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingResetOnStrongPtr_thenIsInvalid)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsMoveConstructingToConst)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsCopyConstructingToConst)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsMoveAssigningToConst)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , AllowsCopyAssigningToConst)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNewPtr_thenHasUseCount1)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenNewPtr_thenIsNotExpired)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_thenHasUseCount0)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenInvalidPtr_thenIsExpired)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_thenHasUseCount0)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_thenIsExpired)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingWeakReset_thenHasUseCount0)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingWeakReset_thenIsExpired)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingStrongReset_thenHasUseCount0)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingStrongReset_thenIsExpired)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenHasUseCount1)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenIsNotExpired)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenOldHasUseCount0)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveConstructedPtr_thenOldIsExpired)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenHasUseCount1)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenIsNotExpired)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenOldHasUseCount0)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenMoveAssignedPtr_thenOldIsExpired)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenHasUseCount1)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenIsNotExpired)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenOldHasUseCount1)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenCopyConstructedPtr_thenOldIsNotExpired)",5, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenLastStrongPointerResets_thenReleasesResources)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenDestructedButStillHasStrongPointers_thenDoesntReleaseResources)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenDestructed_thenDestructsObject)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructed_thenDestructsObjectAfterSecondDestructed)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveConstructedToBaseClass_thenDestructsObjectAfterSecondDestructed)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsOldObject)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsOldObject)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithBaseClassCopy_whenMoveAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenMoveAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssigned_thenDestructsObjectAfterSecondDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenMoveAssignedToBaseClass_thenDestructsObjectAfterSecondDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",18, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyConstructedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndDestructed_thenDestructsObjectAfterLastDestruction)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",20, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClassAndOriginalDestructed_thenDestructsObjectAfterLastDestruction)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssigned_thenDestructsOldObject)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCopyAssignedToBaseClass_thenDestructsOldObject)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithBaseClassCopy_whenCopyAssigned_thenDestructsOldObjectAfterCopyIsDestructed)",22, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCopyAssignedToBaseClass_thenDestructsOldObjectAfterCopyIsDestructed)",21, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenCallingReset_thenDestructs)",10, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCallingReset_thenDestructsAfterCopyDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithCopy_whenCallingResetOnCopy_thenDestructsAfterOriginalDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithMoved_whenCallingReset_thenDestructsAfterMovedDestructed)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtrWithMoved_whenCallingResetOnMoved_thenDestructsImmediately)",13, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenDoesntCrash)",6, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenReleasedAndReclaimed_thenDoesntCrash)",8, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",32, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenWeakOnlyPtr_whenReleasedAndReclaimed_thenIsDestructedAtEnd)",26, 2, 0, 0
repos/cpp/pytorch/c10/test/util/intrusive_ptr_test.cpp,"TEST( WeakIntrusivePtrTest , givenStackObject_whenReclaimed_thenCrashes)",7, 2, 0, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"MovableOnly::MovableOnly( int val_)",1, 78, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"MovableOnly::operator ==( const MovableOnly & lhs , const MovableOnly & rhs)",1, 104, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting()",1, 52, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting( const CopyCounting & rhs)",1, 105, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::CopyCounting( CopyCounting && rhs)",1, 100, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::operator =( const CopyCounting & rhs)",5, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"CopyCounting::operator =( CopyCounting && rhs)",5, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex)",8, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_singleInput)",4, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_movableOnly)",6, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_onlyCopiesIfNecessary)",13, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_onlyMovesIfNecessary)",10, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_extract_arg_by_filtered_index::TEST( MetaprogrammingTest , ExtractArgByFilteredIndex_keepsLValueReferencesIntact)",5, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::map_to_double::operator ( )( T a) const",3, 8, 6, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap)",6, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_emptyInput)",6, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_emptyOutput)",6, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_movableOnly_byRValue)",12, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_movableOnly_byValue)",12, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyCopiesIfNecessary)",18, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyMovesIfNecessary_1)",15, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Metaprogramming_test.cpp,"test_filter_map::TEST( MetaprogrammingTest , FilterMap_onlyMovesIfNecessary_2)",16, 6, 4, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_equality_comparable::operator ==( const EqualityComparable & , const EqualityComparable &)",1, 101, 4, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"std::hash<test_is_hashable::Hashable>::operator ( )( const test_is_hashable :: Hashable &)",1, 76, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_function_type::Functor::operator ( )()",1, 29, 8, 0
repos/cpp/pytorch/c10/test/util/TypeTraits_test.cpp,"test_is_function_type::func()",4, 6, 4, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , TypeMetaStatic)",11, 2, 0, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , Names)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , TypeMeta)",37, 2, 0, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassAllowAssignment::ClassAllowAssignment()",1, 36, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassAllowAssignment::ClassAllowAssignment( const ClassAllowAssignment & src)",1, 70, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::ClassNoAssignment::ClassNoAssignment()",1, 33, 2, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , CtorDtorAndCopy)",28, 2, 0, 0
repos/cpp/pytorch/c10/test/util/typeid_test.cpp,"caffe2::TEST( TypeMetaTest , Float16IsNotUint16)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::map_to_size::operator ( )( T) const",1, 96, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_sametype)",7, 6, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::map_make_shared::operator ( )( T)",3, 8, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_differenttypes)",5, 6, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::Class1::func()",1, 51, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::Class2::func()",1, 56, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::mapper_call_func::operator ( )( T)",1, 100, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_members)",7, 6, 4, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::mapper_call_nonexistent_function::operator ( )( T)",1, 126, 6, 0
repos/cpp/pytorch/c10/test/util/TypeList_test.cpp,"test_map_types_to_values::TEST( TypeListTest , MapTypesToValues_empty)",7, 6, 4, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"half_legacy_impl::halfbits2float( unsigned short h)",30, 3, 0, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"half_legacy_impl::float2halfbits( float src)",51, 3, 0, 0
repos/cpp/pytorch/c10/test/util/Half_test.cpp,"TEST( HalfDoubleConversionTest , Half2Double)",17, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"dev( DeviceIndex index)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , Constructor)",33, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , ConstructorError)",4, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , SetDevice)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , ResetDevice)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineDeviceGuard , SetIndex)",15, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , Constructor)",31, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , NullaryConstructor)",20, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , SetDevice)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineDeviceGuard_test.cpp,"TEST( InlineOptionalDeviceGuard , SetIndex)",14, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"dev( DeviceIndex index)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"stream( DeviceIndex index , StreamId sid)",3, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , Constructor)",17, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamSameSameDevice)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamDifferentSameDevice)",18, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineStreamGuard , ResetStreamDifferentDevice)",20, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , Constructor)",35, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , ResetStreamSameDevice)",16, 2, 0, 0
repos/cpp/pytorch/c10/test/impl/InlineStreamGuard_test.cpp,"TEST( InlineOptionalStreamGuard , ResetStreamDifferentDevice)",18, 2, 0, 0
repos/cpp/pytorch/c10/impl/DeviceGuardImplInterface.cpp,"c10::impl::DeviceGuardImplRegistrar::DeviceGuardImplRegistrar( DeviceType type , const DeviceGuardImplInterface * impl)",3, 2, 0, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::deviceForInputs( Stack & stack , size_t N)",6, 2, 0, 0
repos/cpp/pytorch/tools/jit/templates/register_aten_ops.cpp,"torch::jit::as_bool_array( at :: ArrayRef<int64_t> vec)",6, 2, 0, 0
repos/cpp/pytorch/tools/cwrap/plugins/templates/nn_tail.cpp,"torch::nn::short_name( PyObject * c_module)",15, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_apply_( PyObject * self , PyObject * arg)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_size( PyObject * self , PyObject * args , PyObject * kwargs)",24, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_stride( PyObject * self , PyObject * args , PyObject * kwargs)",22, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_get_device( PyObject * self_ , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage_offset( PyObject * self_ , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_dim( PyObject * self , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_contiguous( const Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_contiguous( PyObject * self , PyObject * args)",24, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_copy_( Tensor & self , const Tensor & other , bool non_blocking)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_copy_( PyObject * self , PyObject * args , PyObject * kwargs)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CDouble( const Tensor & self)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CComplexDouble( const Tensor & self)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to_CLong( const Tensor & self)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_float_scalar( PyObject * self , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_integral_scalar( PyObject * self , PyObject * args)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_index_scalar( PyObject * self , PyObject * args)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_invert( const Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_invert( PyObject * self , PyObject * args)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , Device device , bool non_blocking , bool copy)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , ScalarType dtype , bool non_blocking , bool copy)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_to( const Tensor & self , Device device , ScalarType dtype , bool non_blocking , bool copy)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_cpu( PyObject * self , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_cuda( PyObject * self , PyObject * args , PyObject * kwargs)",16, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_to_type( PyObject * self , ScalarType scalarType)",6, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_byte( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_char( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_double( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_float( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_half( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_int( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_long( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_short( PyObject * self , PyObject * args)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_element_size( PyObject * self , PyObject * args)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_numpy( PyObject * self , PyObject * arg)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_record_stream( PyObject * self , PyObject * arg)",16, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_requires_grad_( PyObject * self , PyObject * args , PyObject * kwargs)",22, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::dispatch_is_contiguous( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_is_contiguous( PyObject * self_ , PyObject * args)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_item( PyObject * self , PyObject * args)",14, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_map_( PyObject * self , PyObject * args , PyObject * kwargs)",16, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_map2_( PyObject * self , PyObject * args , PyObject * kwargs)",17, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new( PyObject * self , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_empty( PyObject * self , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_full( PyObject * self , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_ones( PyObject * self , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_new_zeros( PyObject * self , PyObject * args , PyObject * kwargs)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage( PyObject * self , PyObject * arg)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_storage_type( PyObject * self , PyObject * arg)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_to( PyObject * self , PyObject * args , PyObject * kwargs)",25, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_tolist( PyObject * self , PyObject * args)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_type( PyObject * self , PyObject * args , PyObject * kwargs)",47, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_variable_methods.cpp,"torch::autograd::THPVariable_bool( PyObject * self , PyObject * args)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::check_out_type_matches( Tensor result , ScalarType scalarType , bool scalarType_is_none , const THPLayout & layout , bool layout_is_none , const Device & device , bool device_is_none)",17, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar end , Tensor result)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar end , const TensorOptions & options)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar start , Scalar end , Scalar step , Tensor result)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_arange( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::allIntegral( std :: initializer_list<std::reference_wrapper<Scalar>> l)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_arange( PyObject * self , PyObject * args , PyObject * kwargs)",49, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_range( Scalar start , Scalar end , Scalar step , Tensor result)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_range( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",6, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_range( PyObject * self , PyObject * args , PyObject * kwargs)",30, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , Generator * generator , Tensor result)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , Generator * generator , const TensorOptions & options)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , Tensor result)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t high , IntList size , const TensorOptions & options)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , Generator * generator , Tensor result)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , Generator * generator , const TensorOptions & options)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , Tensor result)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::dispatch_randint( int64_t low , int64_t high , IntList size , const TensorOptions & options)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_randint( PyObject * self_ , PyObject * args , PyObject * kwargs)",96, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_as_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_from_numpy( PyObject * module , PyObject * arg)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable__promote_types( PyObject * self , PyObject * args , PyObject * kwargs)",15, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_sparse_coo_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_tensor( PyObject * self , PyObject * args , PyObject * kwargs)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::THPVariable_get_device( PyObject * self_ , PyObject * args , PyObject * kwargs)",15, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_torch_functions.cpp,"torch::autograd::initTorchFunctions( PyObject * module)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::IndexRangeGenerator::range( size_t range_size)",4, 4, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::IndexRangeGenerator::size()",1, 30, 2, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::copy_range( variable_list & out , IndexRange range , const Tensor & t)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::copy_range( variable_list & out , IndexRange range , at :: ArrayRef<Tensor> t)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::not_implemented( const char * name)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::maybe_multiply( const Tensor & t , const Scalar & s)",14, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_safe_size( IntList sizes , IntList dim)",11, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::norm_backward( const Tensor & grad , const Tensor & self , const Scalar & p_ , const Tensor & norm)",25, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::norm_backward( Tensor grad , const Tensor & self , const Scalar & p_ , Tensor norm , int64_t dim , bool keepdim)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward( Tensor grad , const Tensor & self , const Scalar & exponent_)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_self( Tensor grad , const Tensor & self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_exponent( Tensor grad , const Tensor & self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::pow_backward_exponent( Tensor grad , const Scalar & base , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mvlgamma_backward( Tensor grad , const Tensor & self , int64_t p)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::permute_backwards( const Tensor & grad , IntList fwd_dims)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_multiple( const Tensor & t , IntList dim , size_t n_dims)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_backward( const Tensor & grad , IntList sizes , IntList dims , bool keepdim)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::reverse_list( const IntList list)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::reverse_dim( const Tensor & t , int64_t dim)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_safe_zeros_backward( const Tensor & grad , const Tensor & inp , int64_t dim)",17, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_backward( const Tensor & grad , const Tensor & input , const Tensor & result)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prod_backward( Tensor grad , const Tensor & input , Tensor result , int64_t dim , bool keepdim)",19, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_scan_exclusive( const Tensor & x , int64_t dim)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumprod_backward( const Tensor & grad , const Tensor & input , int64_t dim)",120, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumprod_backward( const Tensor & grad , const Tensor & input , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::gesv_backward_self( const Tensor & grad , const Tensor & self , const Tensor & A)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::gesv_backward_A( const Tensor & grad , const Tensor & self , const Tensor & A , const Tensor & solution)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumsum_backward( const Tensor & x , int64_t dim)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cumsum_backward( const Tensor & x , int64_t dim , ScalarType input_dtype)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::logsumexp_backward( Tensor grad , const Tensor & self , Tensor result , int64_t dim , bool keepdim)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unbind_backward( const variable_list & grads , int64_t dim)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_to( const Tensor & self , IntList sizes)",11, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_to( const Tensor & self , int64_t dim , IntList sizes)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cat_tensors_backward( const Tensor & grad , const std :: vector<std::vector<int64_t>> & sizes , int64_t dim)",17, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::clamp_backward( const Tensor & grad , const Tensor & self , const optional<Scalar> & min , const optional<Scalar> & max)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mm_mat1_backward( const Tensor & grad , const Tensor & mat2 , const Tensor & mat1 , const Scalar & alpha)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mm_mat2_backward( const Tensor & grad , const Tensor & mat1 , IntList sizes , IntList strides , const Scalar & alpha)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_sparse_addmm_sparse_backward( const Tensor & grad , const Tensor & sparse_ , const Tensor & dense , const Scalar & alpha)",6, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::renorm_backward( const Tensor & grad , const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",25, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_tensorlist( TensorList tl)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::repeat_backward( Tensor grad , int64_t input_dims , IntList repeats)",15, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_fused_dropout_backward( Tensor grad , Tensor mask , double p1m)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::select_equals_backward( Tensor grad , const Tensor & input , const Tensor & value)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::index_select_backward( Tensor grad , int64_t dim , Tensor indices , IntList sizes , bool keepdim)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::slice_backward( Tensor grad , IntList input_sizes , int64_t dim , int64_t start , int64_t end , int64_t step)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::select_backward( Tensor grad , IntList input_sizes , int64_t dim , int64_t index)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::trace_backward( const Tensor & grad , IntList sizes)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unfold_backward( const Tensor & grad , IntList input_sizes , int64_t dim , int64_t size , int64_t step)",13, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::var_backward( const Tensor & grad , const Tensor & self , bool unbiased)",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::var_backward( Tensor grad , const Tensor & self , IntList dim , bool unbiased , bool keepdim)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::masked_scatter_backward( const Tensor & grad , const Tensor & mask , IntList sizes)",15, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::cholesky_backward( Tensor grad , bool upper , Tensor L)",35, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::split_with_sizes_backward( const std :: vector<torch::autograd::Variable> & grads , IntList split_sizes , int64_t dim , IntList sizes , const Type & type)",21, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::split_backward( const std :: vector<torch::autograd::Variable> & grads , int64_t split_size , int64_t dim , IntList sizes , const Type & type)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::max_pool_double_backward( const Tensor & grad , const Tensor & indices , int dim)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::glu_double_backward( const Tensor & grad , const Tensor & grad_output , const Tensor & input , int64_t dim)",18, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::glu_double_backward_grad_output( const Tensor & grad , const Tensor & input , int64_t dim)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::kl_div_double_backward_grad_output( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::kl_div_target_backward( Tensor grad_output , Tensor self , Tensor target , int64_t reduction)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::binary_cross_entropy_with_logits_target_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",18, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log_sigmoid_double_backward( const Tensor & grad , const Tensor & input)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::softmax_double_backward( const Tensor & grad , const Tensor & grad_output , int dim , const Tensor & output)",16, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log_softmax_double_backward( const Tensor & grad , const Tensor & grad_output , int dim , const Tensor & output)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::l1_loss_double_backward_grad_output( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::smooth_l1_loss_double_backward( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::smooth_l1_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::diag_backward( const Tensor & grad , IntList input_sizes , int64_t diagonal)",14, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::diagonal_backward( const Tensor & grad , IntList input_sizes , int64_t offset , int64_t dim1 , int64_t dim2)",6, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mse_loss_double_backward( const Tensor & grad , const Tensor & input , int64_t reduction)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::mse_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::soft_margin_loss_double_backward( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",9, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::soft_margin_loss_double_backward_grad_output( const Tensor & grad , const Tensor & grad_output , const Tensor & input , const Tensor & target , int64_t reduction)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::softplus_double_backward( const Tensor & grad , const Tensor & input , Scalar beta , Scalar threshold)",4, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_maybe_overlapping_memory( IntList sizes , IntList strides)",18, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_min_storage_size( IntList sizes , IntList strides , int64_t storage_offset)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::as_strided_backward( Tensor grad , TensorGeometry input_geometry , IntList sizes , IntList strides , int64_t storage_offset)",100, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::atan2_backward( const Tensor & grad , const Tensor & self , const Tensor & other , std :: array<bool,2> output_mask)",6, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::prelu_double_backward( const Tensor & grad_grad_input , const Tensor & grad_grad_weight , const Tensor & grad_out , const Tensor & input_ , const Tensor & weight_)",80, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::svd_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , bool some , bool compute_uv , const Tensor & raw_u , const Tensor & sigma , const Tensor & raw_v)",81, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::symeig_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , bool eigenvectors , bool upper , const Tensor & lambda , const Tensor & v)",34, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::det_backward( const Tensor & grad , const Tensor & self , const Tensor & det)",11, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::logdet_backward( const Tensor & grad , const Tensor & self , const Tensor & logdet)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::slogdet_backward( const std :: vector<torch::autograd::Variable> & grads , const Tensor & self , const Tensor & signdet , const Tensor & logabsdet)",17, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::trtrs_backward( const Tensor & grad_x , const Tensor & grad_m , const Tensor & b , const Tensor & a , const Tensor & x , const bool upper , const bool transpose , const bool unitriangular , std :: array<bool,2> output_mask)",28, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::fft_backward( const Tensor & self , const Tensor & grad , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntList checked_signal_sizes , bool normalized , bool onesided , IntList output_sizes)",100, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sum_exclude_dim1( const Tensor & to_sum , bool keepdim = true)",8, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::unsqueeze_dim1( const Tensor & src , const Tensor & target)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::expand_as_dim1( const Tensor & src , const Tensor & target)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::batchnorm_double_backward( const Tensor & input , const Tensor & gamma , const Tensor & ggI , const Tensor & ggG , const Tensor & ggB , const Tensor & gO , const Tensor & running_mean , const Tensor & running_var , bool training , double eps , const Tensor & save_mean , const Tensor & save_invstd , std :: array<bool,3> output_mask)",126, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::_trilinear_backward( const Tensor & grad_out , const Tensor & i1 , const Tensor & i2 , const Tensor & i3 , IntList expand1 , IntList expand2 , IntList expand3 , IntList sumdim , int64_t unroll_dim , std :: array<bool,3> grad_mask)",12, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::log1p_backward( const Tensor & grad , const Tensor & self)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::sparse_constructor_values_backward( const Tensor & sparse_grad_out , const Tensor & indices , IntList values_shape)",10, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::to_dense_backward( const Tensor & grad , const Tensor & input_)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/Functions.cpp,"torch::autograd::generated::constant_pad_nd_backward( const Tensor & grad , IntList pad)",5, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_functions.cpp,"torch::autograd::generated::addClass( PyTypeObject & type , const char * name , PyGetSetDef * function_properties = NULL , PyMethodDef * function_methods = NULL)",7, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_functions.cpp,"torch::autograd::generated::initialize_autogenerated_functions()",3, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_nn_functions.cpp,"torch::autograd::THPVariable__parse_to( PyObject * module , PyObject * args , PyObject * kwargs)",25, 2, 0, 0
repos/cpp/pytorch/tools/autograd/templates/python_nn_functions.cpp,"torch::autograd::initNNFunctions( PyObject * module)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/common/main.cpp,"add_negative_flag( const std :: string & flag)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/common/main.cpp,"main( int argc , char * argv [ ])",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( NoGradTest , SetsGradModeCorrectly)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"AutogradTest::AutogradTest()",5, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanTakeDerivatives)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanTakeDerivativesOfZeroDimTensors)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST_F( AutogradTest , CanPassCustomGradientInputs)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( NNInitTest , CanInitializeTensorThatRequiresGrad)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/misc.cpp,"TEST( TempFileTest , MatchesExpectedPattern)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , AllocatesTensorOnTheCorrectDevice_MultiCUDA)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDevice_MultiCUDA)",40, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToTensorAndTensorAttributes_MultiCUDA)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDoesNotCopyWhenOptionsAreAllTheSame_CUDA)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_cuda.cpp,"TEST( TensorTest , ToDeviceAndDtype_MultiCUDA)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromSharedPointer)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromConcreteType)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ConstructsFromModuleHolder)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , PushBackAddsAnElement)",18, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , AccessWithAt)",28, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , AccessWithPtr)",29, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardOnEmptySequentialIsDisallowed)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardChainsCorrectly)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CallingForwardWithTheWrongReturnTypeThrows)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , TheReturnTypeOfForwardDefaultsToTensor)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ForwardReturnsTheLastValue)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , SanityCheckForHoldingStandardModules)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , ExtendPushesModulesFromOtherSequential)",45, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , HasReferenceSemantics)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , IsCloneable)",30, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , RegistersElementsAsSubmodules)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/sequential.cpp,"TEST_F( SequentialTest , CloneToDevice_CUDA)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DifferentiableScatter_MultiCUDA)",21, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DifferentiableGather_MultiCUDA)",27, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , Replicate_MultiCUDA)",34, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApply_MultiCUDA)",26, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApplyWithDifferentOutputDevice_MultiCUDA)",25, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , ParallelApplyRethrowsException_MultiCUDA)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DataParallelPlacesTheOutputOnTheRequestedDevice_MultiCUDA)",34, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/parallel.cpp,"TEST_F( ParallelTest , DataParallelUsesAllAvailableCUDADevices_CUDA)",18, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::DummyDataset( size_t size = 100)",1, 60, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::get( size_t index)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"DummyDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DatasetCallsGetCorrectly)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TransformCallsGetApplyCorrectly)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"InfiniteStreamDataset::get_batch( size_t batch_size)",7, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"InfiniteStreamDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , InfiniteStreamDataset)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , NoSequencerIsIdentity)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , OrderedSequencerIsSetUpWell)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , OrderedSequencerReOrdersValues)",30, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , BatchLambdaAppliesFunctionToBatch)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , LambdaAppliesFunctionToExample)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CollateReducesBatch)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CollationReducesBatch)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerReturnsIndicesInOrder)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerReturnsLessValuesForLastBatch)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerResetsWell)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SequentialSamplerResetsWithNewSizeWell)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanSaveAndLoadSequentialSampler)",24, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerReturnsIndicesInCorrectRange)",23, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerReturnsLessValuesForLastBatch)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerResetsWell)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , RandomSamplerResetsWithNewSizeWell)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SavingAndLoadingRandomSamplerYieldsSameSequence)",29, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerReturnsTheBatchSizeAndThenRemainder)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerResetsWell)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StreamSamplerResetsWithNewSizeWell)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorDatasetConstructsFromSingleTensor)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorDatasetConstructsFromInitializerListOfTensors)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StackTransformWorksForExample)",23, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , StackTransformWorksForTensorExample)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"T::operator ( )( torch :: Tensor input)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TensorStringDataset::get( size_t index)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TensorStringDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorTransformWorksForAnyTargetType)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , TensorLambdaWorksforAnyTargetType)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnCopyableDataset::get( size_t index)",4, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnCopyableDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , MapDoesNotCopy)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePushAndPopFromSameThread)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePopWithTimeoutThrowsUponTimeout)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueuePushAndPopFromDifferentThreads)",23, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , QueueClearEmptiesTheQueue)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleCanPushAndPopJob)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleCanPushAndPopResult)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttlePopResultReturnsNulloptWhenNoJobsInFlight)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttleDrainMeansPopResultReturnsNullopt)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , DataShuttlePopResultTimesOut)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::UncopyableDataset( const std :: string &)",1, 56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::get( size_t index)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UncopyableDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SharedBatchDatasetReallyIsShared)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , SharedBatchDatasetDoesNotIncurCopyWhenPassedDatasetObject)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndex::TestIndex( size_t offset , std :: vector<size_t> index)",2, 51, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndex::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::TestIndexDataset( size_t size)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::get_batch( TestIndex index)",7, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::TestIndexSampler( size_t size)",1, 58, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::reset( torch :: optional<size_t> new_size = torch :: nullopt)",1, 76, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::next( size_t batch_size)",9, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::save( torch :: serialize :: OutputArchive & archive) const",1, 72, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TestIndexSampler::load( torch :: serialize :: InputArchive & archive)",1, 65, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataTest , CanUseCustomTypeAsIndexType)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DataLoaderOptionsDefaultAsExpected)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DataLoaderOptionsCoalesceOptionalValues)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , MakeDataLoaderDefaultsAsExpected)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnsizedDataset::get( size_t i)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"UnsizedDataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , MakeDataLoaderThrowsWhenConstructingSamplerWithUnsizedDataset)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsCompareEqualToThemselves)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ValidIteratorsCompareUnequalToEachOther)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , SentinelIteratorsCompareEqualToEachOther)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsCompareEqualToSentinelWhenExhausted)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IteratorsShareState)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanDereferenceIteratorMultipleTimes)",19, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CanUseIteratorAlgorithms)",21, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , CallingBeginWhileOtherIteratorIsInFlightThrows)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IncrementingExhaustedValidIteratorThrows)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DereferencingExhaustedValidIteratorThrows)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , IncrementingSentinelIteratorThrows)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DereferencingSentinelIteratorThrows)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , YieldsCorrectBatchSize)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , ReturnsLastBatchWhenSmallerThanBatchSizeWhenDropLastIsFalse)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , DoesNotReturnLastBatchWhenSmallerThanBatchSizeWhenDropLastIsTrue)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , RespectsTimeout)",33, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"Barrier::Barrier( size_t target)",1, 56, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"Barrier::wait()",8, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::Dataset( const Dataset & other)",4, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::get_batch( torch :: ArrayRef<size_t> indices)",17, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"ordering_test::Dataset::size() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , EnforcesOrderingAmongThreadsWhenConfigured)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , Reset)",21, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/dataloader.cpp,"TEST( DataLoaderTest , TestExceptionsArePropagatedFromWorkers)",25, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"f( T && m)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"f( T && m)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , AllOf)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , AnyOf)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , EnableIfModule)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/static.cpp,"TEST( TestStatic , Apply)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , IsEmptyAfterDefaultConstruction)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , InsertAddsElementsWhenTheyAreYetNotPresent)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , GetReturnsValuesWhenTheyArePresent)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , GetThrowsWhenPassedKeysThatAreNotPresent)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanInitializeFromList)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , InsertThrowsWhenPassedElementsThatArePresent)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FrontReturnsTheFirstItem)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FrontThrowsWhenEmpty)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , BackReturnsTheLastItem)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , BackThrowsWhenEmpty)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FindReturnsPointersToValuesWhenPresent)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , FindReturnsNullPointersWhenPasesdKeysThatAreNotPresent)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorThrowsWhenPassedKeysThatAreNotPresent)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorReturnsItemsPositionallyWhenPassedIntegers)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , SubscriptOperatorsThrowswhenPassedKeysThatAreNotPresent)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , UpdateInsertsAllItemsFromAnotherOrderedDict)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , UpdateAlsoChecksForDuplicates)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanIterateItems)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ClearMakesTheDictEmpty)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanCopyConstruct)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanCopyAssign)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanMoveConstruct)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanMoveAssign)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , CanInsertWithBraces)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ErrorMessagesIncludeTheKeyDescription)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , KeysReturnsAllKeys)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ValuesReturnsAllValues)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/ordered_dict.cpp,"TEST( OrderedDictTest , ItemsReturnsAllItems)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"xor_model()",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"save_and_load( torch :: Tensor input)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Basic)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , BasicToFile)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Resized)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Sliced)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , NonContiguous)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , XOR)",41, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , Optim)",68, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , XOR_CUDA)",59, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/serialize.cpp,"TEST( SerializeTest , CanSerializeModulesWithIntermediateModulesWithoutParametersOrBuffers)",30, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::getState()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::getReward()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::isDone()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::reset()",5, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::CartPole()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"CartPole::step( int action)",38, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"test_mnist( size_t batch_size , size_t number_of_epochs , bool with_cuda , M && model , F && forward_op , O && optimizer)",44, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , CartPole)",97, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , MNIST_CUDA)",35, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/integration.cpp,"TEST_F( IntegrationTest , MNISTBatchNorm_CUDA)",35, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"CPUDevice()",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"CUDADevice( DeviceIndex index)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCUDATypes_CUDA)",20, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options_cuda.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCUDATensors_MultiCUDA)",24, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"test::AGIUnit2::AGIUnit2()",1, 43, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanEnableAndDisableTrainingMode)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ZeroGrad)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ZeroGradWithUndefined)",23, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterModuleThrowsForEmptyOrDottedName)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterModuleThrowsForDuplicateModuleName)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterParameterThrowsForEmptyOrDottedName)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterParameterThrowsForDuplicateModuleName)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterBufferThrowsForEmptyOrDottedName)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , RegisterBufferThrowsForDuplicateModuleName)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CanGetName)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , AsCastsModulesCorrectly)",27, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , Conversion_MultiCUDA)",47, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CallingCloneOnModuleThatDoesNotOverrideCloneThrows)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CallingCloneOnModuleThatDoesOverrideCloneDoesNotThrow)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneCreatesDistinctParameters)",47, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ClonePreservesExternalReferences)",29, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneCopiesTheValuesOfVariablesOfSubmodules)",39, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloneToDevicePreservesTheDeviceOfParameters_CUDA)",31, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , CloningToAParticularDevicePlacesAllParametersThere_MultiCUDA)",31, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"ParameterTestModule::ParameterTestModule()",5, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , HasCorrectNumberOfParameters)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ContainsParametersWithTheCorrectName)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"BufferTestModule::BufferTestModule()",5, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , HasCorrectNumberOfBuffers)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ContainsBuffersWithTheCorrectName)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"AImpl::AImpl()",1, 23, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"AImpl::AImpl( int x)",1, 26, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , DefaultConstructorOfModuleHolderCallsDefaultConstructorOfImpl)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ValueConstructorOfModuleHolderCallsCorrectConstructorInImpl)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NullptrConstructorLeavesTheModuleHolderInEmptyState)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestModule::TestModule( int64_t size)",6, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestModule::forward( torch :: Tensor input)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesReturnsExpectedSubmodulesForFlatModel)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesExcludesSelfWhenIncludeSelfSetToFalse)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesReturnsExpectedNamedSubmodulesForFlatModel)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesExcludesSelfWhenIncludeSelfSetToFalse)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ChildrenReturnsExpectedSubmodulesForFlatModel)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedChildrenReturnsExpectedNamedSubmodulesForFlatModel)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ParametersReturnsExpectedTensorsForFlatModel)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedParametersReturnsExpectedTensorsForFlatModel)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , BuffersReturnsExpectedTensorsForFlatModel)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedBuffersReturnsExpectedTensorsForFlatModel)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TestContainer::TestContainer( int64_t number , std :: vector<TestContainer> modules = { })",8, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"get_test_container_item( std :: shared_ptr<torch::nn::Module> module)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"make_deeply_nested_test_container()",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"make_key_value_pairs_for_deeply_nested_container()",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulesReturnsExpectedSubmodulesForDeepModel)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulesReturnsExpectedNamedSubmodulesForDeepModel)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ChildrensReturnsExpectedSubmodulesForDeepModel)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedChildrensReturnsExpectedNamedSubmodulesForDeepModel)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModuleApplyIteratesCorreclty)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ConstModuleApplyIteratesCorreclty)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModuleApplyIteratesCorreclty)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ConstNamedModuleApplyIteratesCorreclty)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ModulePointerApplyIteratesCorreclty)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , NamedModulePointerApplyIteratesCorreclty)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/module.cpp,"TEST_F( ModuleTest , ThrowsWhenAttemptingtoGetTopLevelModuleAsSharedPtr)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/jit.cpp,"TEST( TorchScriptTest , CanCompileMultipleFunctions)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TestModel::TestModel()",4, 53, 8, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"NestedModel::NestedModel()",4, 69, 8, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv1d)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv2dEven)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv2dUneven)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Conv3d)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , SimpleContainer)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , EmbeddingBasic)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , EmbeddingList)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Dropout)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Parameters)",19, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalCallsSuppliedFunction)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalWithTorchFunction)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , FunctionalArgumentBinding)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormStateful)",25, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormStateless)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , BatchNormPureForward)",13, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear_CUDA)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/modules.cpp,"TEST_F( ModulesTest , Linear2_CUDA)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromInitializerList)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromVector)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromArray)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , CanConstructFromSingleValue)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , ThrowsWhenConstructedWithIncorrectNumberOfArgumentsInInitializerList)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/expanding-array.cpp,"TEST_F( ExpandingArrayTest , ThrowsWhenConstructedWithIncorrectNumberOfArgumentsInVector)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TestValue::TestValue( const int & x)",1, 51, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TestValue::TestValue( int && x)",1, 46, 2, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , ForwardRvaluesCorrectly)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , ForwardLvaluesCorrectly)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/memory.cpp,"TEST( MakeUniqueTest , CanConstructUniquePtrOfArray)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , DefaultsToTheRightValues)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ReturnsTheCorrectType)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , UtilityFunctionsReturnTheRightTensorOptions)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCPUTypes)",19, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromCPUTensors)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( TensorOptionsTest , ConstructsWellFromVariables)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST( DeviceTest , ParsesCorrectlyFromString)",37, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"DefaultDtypeTest::DefaultDtypeTest()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"DefaultDtypeTest::~DefaultDtypeTest()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , CanSetAndGetDefaultDtype)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , NewTensorOptionsHasCorrectDefault)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor_options.cpp,"TEST_F( DefaultDtypeTest , NewTensorsHaveCorrectDefaultDtype)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , SimpleReturnType)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , SimpleReturnTypeAndSingleArgument)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , StringLiteralReturnTypeAndArgument)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , StringReturnTypeWithConstArgument)",10, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , TensorReturnTypeAndStringArgumentsWithFunkyQualifications)",18, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , WrongArgumentType)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , WrongNumberOfArguments)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"M::M( int value_)",1, 68, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"M::forward( float x)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , GetWithCorrectTypeSucceeds)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , GetWithIncorrectTypeThrows)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithBaseClassSucceeds)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithGoodDowncastSuccceeds)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , PtrWithBadDowncastThrows)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , DefaultStateIsEmpty)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , AllMethodsThrowForEmptyAnyModule)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , CanMoveAssignDifferentModules)",20, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , ConstructsFromModuleHolder)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyModuleTest , ConvertsVariableToTensorCorrectly)",18, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::TestValue::TestValue( T && value)",1, 68, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::TestValue::operator ( )()",3, 4, 2, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"torch::nn::make_value( T && value)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesIntWhenCorrectType)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesConstIntWhenCorrectType)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesStringLiteralWhenCorrectType)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesStringWhenCorrectType)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesPointersWhenCorrectType)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , CorrectlyAccessesReferencesWhenCorrectType)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TryGetReturnsNullptrForTheWrongType)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , GetThrowsForTheWrongType)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , MoveConstructionIsAllowed)",6, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , MoveAssignmentIsAllowed)",7, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForInt)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForStringLiteral)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/any.cpp,"TEST_F( AnyValueTest , TypeInfoIsCorrectForString)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"exactly_equal( at :: Tensor left , T right)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"almost_equal( at :: Tensor left , T right , T tolerance = 1e - 4)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToDtype)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToTensorAndTensorAttributes)",22, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToOptionsWithRequiresGrad)",30, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ToDoesNotCopyWhenOptionsAreAllTheSame)",27, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValueForSingleValue)",16, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesForManyValues)",15, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesForManyValuesVariable)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , ContainsCorrectValuesWhenConstructedFromVector)",17, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , UsesOptionsThatAreSupplied)",14, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlob)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlobUsesDeleter)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , FromBlobWithStrides)",25, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , Item)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/tensor.cpp,"TEST( TensorTest , Item_CUDA)",12, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"test_RNN_xor( Func && model_maker , bool cuda = false)",52, 3, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"check_lstm_sizes( RNNOutput output)",18, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , CheckOutputSizes)",19, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , CheckOutputValuesMatchPyTorch)",60, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndLSTM)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndGRU)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNRelu)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNTanh)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , Sizes_CUDA)",21, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndLSTM_CUDA)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndGRU_CUDA)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNRelu_CUDA)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/rnn.cpp,"TEST_F( RNNTest , EndToEndRNNTanh_CUDA)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"test_optimizer_xor( Options options)",41, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"assign_parameter( const Parameters & parameters , const char * name , torch :: Tensor new_tensor)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"check_exact_values( Options options , std :: vector<std::vector<torch::Tensor>> expected_parameters)",57, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , BasicInterface)",26, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_SGD)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_Adagrad)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_RMSprop)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_RMSpropWithMomentum)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_Adam)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , XORConvergence_AdamWithAmsgrad)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_Adam)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdamWithWeightDecay)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdamWithWeightDecayAndAMSGrad)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_Adagrad)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdagradWithWeightDecay)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_AdagradWithWeightDecayAndLRDecay)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSprop)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecay)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecayAndCentered)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_RMSpropWithWeightDecayAndCenteredAndMomentum)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGD)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecay)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecayAndMomentum)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ProducesPyTorchValues_SGDWithWeightDecayAndNesterovMomentum)",5, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ZeroGrad)",26, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , ExternalVectorOfParameters)",21, 2, 0, 0
repos/cpp/pytorch/test/cpp/api/optim.cpp,"TEST( OptimTest , AddParameter_LBFGS)",18, 2, 0, 0
repos/cpp/pytorch/test/cpp/jit/no-gtest.cpp,"torch::jit::runJITCPPTests()",26, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::CPUComplexFloatType()",5, 39, 12, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::empty( IntList size , const TensorOptions & options) const",17, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::ComplexHooks::ComplexHooks( ComplexHooksArgs)",1, 36, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::ComplexHooks::registerComplexTypes( Context * context) const",4, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::scalarType() const",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::typeMeta() const",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::backend() const",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::toString() const",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::ID() const",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"at::CPUComplexFloatType::elementSizeInBytes() const",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/complex_registration_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",1, 45, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension.cpp,"tanh_add( Tensor x , Tensor y)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::Net( int64_t in , int64_t out)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::reset()",4, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::forward( torch :: Tensor x)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::set_bias( torch :: Tensor bias)",4, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::get_bias() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_parameter( const std :: string & name , torch :: Tensor tensor)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_buffer( const std :: string & name , torch :: Tensor tensor)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"Net::add_new_submodule( const std :: string & name)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",9, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/jit_extension2.cpp,"exp_add( Tensor x , Tensor y)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"cudnn_relu_check( const torch :: Tensor & inputs , const torch :: Tensor & outputs)",19, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"cudnn_relu( const torch :: Tensor & inputs , const torch :: Tensor & outputs)",34, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cudnn_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",4, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"sigmoid_add( torch :: Tensor x , torch :: Tensor y)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::MatrixMultiplier( int A , int B)",4, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::forward( torch :: Tensor weights)",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"MatrixMultiplier::get() const",3, 4, 2, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"function_taking_optional( c10 :: optional<torch::Tensor> tensor)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",11, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cuda_extension.cpp,"sigmoid_add( torch :: Tensor x , torch :: Tensor y)",8, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/cuda_extension.cpp,"PYBIND11_MODULE( TORCH_EXTENSION_NAME , m)",3, 2, 0, 0
repos/cpp/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp,"dummy( int)",1, 20, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"helpers::check_all_parameters( const torch :: jit :: script :: Module & module , Predicate predicate)",10, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"get_operator_from_registry_and_execute()",22, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"load_serialized_module_with_custom_op_and_execute( const std :: string & path_to_exported_script_module)",12, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_argument_checking_for_serialized_modules( const std :: string & path_to_exported_script_module)",35, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_move_to_device( const std :: string & path_to_exported_script_module)",21, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"test_move_to_dtype( const std :: string & path_to_exported_script_module)",17, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/test_custom_ops.cpp,"main( int argc , const char * argv [ ])",19, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/op.cpp,"custom_op( torch :: Tensor tensor , double scalar , int64_t repeat)",11, 2, 0, 0
repos/cpp/pytorch/test/custom_operator/op.cpp,"custom_op2( std :: string s1 , std :: string s2)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THDefaultAllocator::allocate( size_t size) const",4, 4, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THDefaultAllocator::raw_deleter() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"getTHDefaultAllocator()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",276, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( const char * filename , int flags , size_t size)",3, 3, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"WaitForReleaseHandle( PVOID lpParam , BOOLEAN TimerOrWaitFired)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::close()",37, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( const char * filename , int flags , size_t size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::THMapAllocator( WithFd , const char * filename , int fd , int flags)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::~THMapAllocator( THMapAllocator * ctx)",1, 56, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocatorArgCheck::THRefcountedMapAllocatorArgCheck( int flags)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( const char * filename , int flags , size_t size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::initializeAlloc()",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::close()",33, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::incref()",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::decref()",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocatorArgCheck::THRefcountedMapAllocatorArgCheck( int flags)",1, 81, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( const char * filename , int flags , size_t size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::THRefcountedMapAllocator( WithFd , const char * filename , int fd , int flags , size_t size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::initializeAlloc()",1, 52, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::~THRefcountedMapAllocator()",1, 57, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"deleteTHMapAllocator( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"deleteTHRefcountedMapAllocator( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::fromDataPtr( const at :: DataPtr & dptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::makeDataPtr( const char * filename , int flags , size_t size , size_t * actual_size_out)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THMapAllocator::makeDataPtr( WithFd , const char * filename , int fd , int flags , size_t size , size_t * actual_size_out)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::makeDataPtr( const char * filename , int flags , size_t size , size_t * actual_size_out)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::makeDataPtr( WithFd , const char * filename , int fd , int flags , size_t size , size_t * actual_size_out)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THAllocator.cpp,"THRefcountedMapAllocator::data() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isOpened( THFile * self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_name( THFile * self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"fread__( void * ptr , size_t size , size_t nitems , FILE * stream)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_mode( const char * mode , int * isReadable , int * isWritable)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_synchronize( THFile * self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_seek( THFile * self , ssize_t position)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_seekEnd( THFile * self)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_position( THFile * self)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_close( THFile * self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_reverseMemory( void * dst , const void * src , ssize_t blockSize , ssize_t numBlocks)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isLittleEndianCPU( void)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_isBigEndianCPU( void)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_nativeEndianEncoding( THFile * self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_littleEndianEncoding( THFile * self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_bigEndianEncoding( THFile * self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_longSize( THFile * self , int size)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_noBuffer( THFile * self)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_free( THFile * self)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_readLong( THFile * self , int64_t * data , ssize_t n)",61, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_writeLong( THFile * self , int64_t * data , ssize_t n)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_readString( THFile * self , const char * format , char ** str_)",88, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_writeString( THFile * self , const char * str , ssize_t size)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THDiskFile_new( const char * name , const char * mode , int isQuiet)",82, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_mode( const char * mode , int * isReadable , int * isWritable)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_free( THFile * self)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THDiskFile.cpp,"THPipeFile_new( const char * name , const char * mode , int isQuiet)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_readStringRaw( THFile * self , const char * format , char ** str_)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_writeStringRaw( THFile * self , const char * str , size_t size)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_synchronize( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_seek( THFile * self , size_t position)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_seekEnd( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_position( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_close( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_free( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_isOpened( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_binary( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_ascii( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_autoSpacing( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_noAutoSpacing( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_quiet( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_pedantic( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THFile.cpp,"THFile_clearError( THFile * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THSize.cpp,"THSize_isSameSizeAs( const int64_t * sizeA , int64_t dimsA , const int64_t * sizeB , int64_t dimsB)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THSize.cpp,"THSize_nElement( int64_t dims , int64_t * size)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"defaultErrorHandlerFunction( const char * msg , void * data)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THError( const char * file , const int line , const char * fmt , ...)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THAssertionFailed( const char * file , const int line , const char * exp , const char * fmt , ...)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetErrorHandler( THErrorHandlerFunction new_handler , void * data)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetDefaultErrorHandler( THErrorHandlerFunction new_handler , void * data)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"defaultArgErrorHandlerFunction( int argNumber , const char * msg , void * data)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THArgCheck( const char * file , int line , int condition , int argNumber , const char * fmt , ...)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetArgErrorHandler( THArgErrorHandlerFunction new_handler , void * data)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetDefaultArgErrorHandler( THArgErrorHandlerFunction new_handler , void * data)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetGCHandler( void(*torchGCFunction_)(void*data) , void * data)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THAllocInternal( ptrdiff_t size)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THAlloc( ptrdiff_t size)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THRealloc( void * ptr , ptrdiff_t size)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THFree( void * ptr)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog10( const double x)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog1p( const double x)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THLog2( const double x)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THExpm1( const double x)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THSetNumThreads( int num_threads)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THGetNumThreads( void)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THGetNumCores( void)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"THInferNumThreads( void)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THGeneral.cpp,"_THSizeDesc( const int64_t * size , const int64_t ndim)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_free( THTensor * self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_setStorage( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_setStorageNd( THTensor * self , THStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_resize( THTensor * self , at :: IntList size , at :: IntList stride)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_resizeNd( THTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_compute_stride( at :: IntList oldshape , at :: IntList oldstride , at :: IntList newshape)",61, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THTensor.cpp,"THTensor_stealAndSetStoragePtr( THTensor * tensor , THStorage * storage)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_new( caffe2 :: TypeMeta data_type)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_free( THStorage * storage)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_size( const THStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_retain( THStorage * storage)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THStorageFunctions.cpp,"THStorage_resize( THStorage * storage , ptrdiff_t size)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_isOpened( THFile * self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_strnextspace( int8_t * str_ , int8_t * c_)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_grow( THMemoryFile * self , ssize_t size)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_mode( const char * mode , int * isReadable , int * isWritable)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_longSize( THFile * self , int size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_storage( THFile * self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_synchronize( THFile * self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_seek( THFile * self , ssize_t position)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_seekEnd( THFile * self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_position( THFile * self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_close( THFile * self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_free( THFile * self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_readLong( THFile * self , int64_t * data , ssize_t n)",79, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_writeLong( THFile * self , int64_t * data , ssize_t n)",89, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_cloneString( const int8_t * str , ssize_t size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_readString( THFile * self , const char * format , char ** str_)",63, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_writeString( THFile * self , const char * str , ssize_t size)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_newWithStorage( THCharStorage * storage , const char * mode)",67, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THMemoryFile.cpp,"THMemoryFile_new( const char * mode)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_newUnseeded()",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_new()",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_copy( THGenerator * self , THGenerator * from)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGenerator_free( THGenerator * self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGeneratorState_isValid( THGeneratorState * _gen_state)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THGeneratorState_copy( THGeneratorState * self , THGeneratorState * from)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"readURandomLong()",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_seed( THGenerator * _generator)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_manualSeed( THGenerator * _generator , uint64_t the_seed_)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_initialSeed( THGenerator * _generator)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_nextState( THGenerator * _generator)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_random( THGenerator * _generator)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_random64( THGenerator * _generator)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"uniform_double( THGenerator * _generator)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"uniform_float( THGenerator * _generator)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_standard_uniform( THGenerator * _generator)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_uniform( THGenerator * _generator , double a , double b)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_uniformFloat( THGenerator * _generator , float a , float b)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_normal( THGenerator * _generator , double mean , double stdv)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_exponential( THGenerator * _generator , double lambda)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_cauchy( THGenerator * _generator , double median , double sigma)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_logNormal( THGenerator * _generator , double mean , double stdv)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_geometric( THGenerator * _generator , double p)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_bernoulli( THGenerator * _generator , double p)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THRandom.cpp,"THRandom_bernoulliFloat( THGenerator * _generator , float p)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THLogAdd( double log_a , double log_b)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THLogSub( double log_a , double log_b)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/THLogAdd.cpp,"THExpMinusApprox( const double x)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validConv2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullConv2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",48, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullXCorr2Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr2DRevptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kr , int64_t kc , int64_t sr , int64_t sc)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",41, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validConv3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",41, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullConv3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( fullXCorr3Dptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",39, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( validXCorr3DRevptr)( scalar_t * r_ , scalar_t alpha , scalar_t * t_ , int64_t it , int64_t ir , int64_t ic , scalar_t * k_ , int64_t kt , int64_t kr , int64_t kc , int64_t st , int64_t sr , int64_t sc)",36, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2d)( scalar_t * output_data , scalar_t alpha , scalar_t * ptr_input , int64_t nInputRows , int64_t nInputCols , scalar_t * ptr_weight , int64_t nKernelRows , int64_t nKernelCols , int64_t srow , int64_t scol , const char * vf , const char * xc)",36, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3d)( scalar_t * output_data , scalar_t alpha , scalar_t * ptr_input , int64_t nInputDepth , int64_t nInputRows , int64_t nInputCols , scalar_t * ptr_weight , int64_t nKernelDepth , int64_t nKernelRows , int64_t nKernelCols , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",36, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( convsize)( int64_t x , int64_t k , int64_t s , const char * vf)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2DRevger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol)",97, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2DRevgerm)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol)",104, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",125, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmv)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",130, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmm)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",143, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",53, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dcmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t srow , int64_t scol , const char * vf , const char * xc)",71, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv2Dmap)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , THTensor * map , int64_t srow , int64_t scol , const char * vf , const char * xc)",80, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3DRevger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol)",79, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dger)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",85, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmv)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",87, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",62, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dcmul)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",79, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorConv.cpp,"THTensor_( conv3Dmap)( THTensor * r_ , scalar_t beta , scalar_t alpha , THTensor * t_ , THTensor * k_ , THTensor * map , int64_t sdepth , int64_t srow , int64_t scol , const char * vf , const char * xc)",89, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"modulo_wrap( scalar_t a , scalar_t b)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( bitor)( THTensor * r_ , THTensor * t , scalar_t value)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( bitxor)( THTensor * r_ , THTensor * t , scalar_t value)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( clamp)( THTensor * r_ , THTensor * t , scalar_t min_value , scalar_t max_value)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cadd)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src)",35, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( csub)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cmul)( THTensor * r_ , THTensor * t , THTensor * src)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( pow)( THTensor * r_ , THTensor * t , scalar_t value)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cpow)( THTensor * r_ , THTensor * t , THTensor * src)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cdiv)( THTensor * r_ , THTensor * t , THTensor * src)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( clshift)( THTensor * r_ , THTensor * t , THTensor * src)",65, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( crshift)( THTensor * r_ , THTensor * t , THTensor * src)",65, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cfmod)( THTensor * r_ , THTensor * t , THTensor * src)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cremainder)( THTensor * r_ , THTensor * t , THTensor * src)",57, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitand)( THTensor * r_ , THTensor * t , THTensor * src)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitor)( THTensor * r_ , THTensor * t , THTensor * src)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( cbitxor)( THTensor * r_ , THTensor * t , THTensor * src)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( tpow)( THTensor * r_ , scalar_t value , THTensor * t)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addcmul)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src1 , THTensor * src2)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addcdiv)( THTensor * r_ , THTensor * t , scalar_t value , THTensor * src1 , THTensor * src2)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addmv)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * mat , THTensor * vec)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( match)( THTensor * r_ , THTensor * m1 , THTensor * m2 , scalar_t gain)",41, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addmm)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * m1 , THTensor * m2)",143, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addr)( THTensor * r_ , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * vec1 , THTensor * vec2)",66, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMath.cpp,"THTensor_( addbmm)( THTensor * result , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * batch1 , THTensor * batch2)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( isTransposedContiguous)( THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( checkTransposed)( THTensor * self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( newTransposedContiguous)( THTensor * self)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( checkLapackClone)( THTensor * result , THTensor * src , int nrows)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( cloneColumnMajorNrows)( THTensor * self , THTensor * src , int nrows)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( cloneColumnMajor)( THTensor * self , THTensor * src)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesv)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a)",53, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( trtrs)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a , const char * uplo , const char * trans , const char * diag)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gels)( THTensor * rb_ , THTensor * ra_ , THTensor * b , THTensor * a)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( geev)( THTensor * re_ , THTensor * rv_ , THTensor * a_ , const char * jobvr)",79, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( syev)( THTensor * re_ , THTensor * rv_ , THTensor * a , const char * jobz , const char * uplo)",46, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesdd)( THTensor * ru_ , THTensor * rs_ , THTensor * rv_ , THTensor * a , const char * some , const char * compute_uv)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( gesdd2)( THTensor * ru_ , THTensor * rs_ , THTensor * rv_ , THTensor * ra_ , THTensor * a , const char * some , const char * compute_uv)",126, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( getri)( THTensor * ra_ , THTensor * a)",43, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( clearUpLoTriangle)( THTensor * a , const char * uplo)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( copyUpLoTriangle)( THTensor * a , const char * uplo)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potrf)( THTensor * ra_ , THTensor * a , const char * uplo)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potrs)( THTensor * rb_ , THTensor * b , THTensor * a , const char * uplo)",47, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( potri)( THTensor * ra_ , THTensor * a , const char * uplo)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( pstrf)( THTensor * ra_ , THIntTensor * rpiv_ , THTensor * a , const char * uplo , scalar_t tol)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( qr)( THTensor * rq_ , THTensor * rr_ , THTensor * a)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( geqrf)( THTensor * ra_ , THTensor * rtau_ , THTensor * a)",40, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( orgqr)( THTensor * ra_ , THTensor * a , THTensor * tau)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( ormqr)( THTensor * ra_ , THTensor * a , THTensor * tau , THTensor * c , const char * side , const char * trans)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( btrifact)( THTensor * ra_ , THIntTensor * rpivots_ , THIntTensor * rinfo_ , int pivot , THTensor * a)",76, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorLapack.cpp,"THTensor_( btrisolve)( THTensor * rb_ , THTensor * b , THTensor * atf , THIntTensor * pivots)",106, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( random)( THTensor * self , THGenerator * _generator)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( clampedRandom)( THTensor * self , THGenerator * _generator , int64_t min , int64_t max)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( cappedRandom)( THTensor * self , THGenerator * _generator , int64_t max)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( geometric)( THTensor * self , THGenerator * _generator , double p)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( uniform)( THTensor * self , THGenerator * _generator , double a , double b)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal)( THTensor * self , THGenerator * _generator , double mean , double stddev)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_means)( THTensor * self , THGenerator * gen , THTensor * means , double stddev)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_stddevs)( THTensor * self , THGenerator * gen , double mean , THTensor * stddevs)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( normal_means_stddevs)( THTensor * self , THGenerator * gen , THTensor * means , THTensor * stddevs)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( exponential)( THTensor * self , THGenerator * _generator , double lambda)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( cauchy)( THTensor * self , THGenerator * _generator , double median , double sigma)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( logNormal)( THTensor * self , THGenerator * _generator , double mean , double stdv)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomialAliasSetup)( THTensor * probs , THLongTensor * J , THTensor * q)",87, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomialAliasDraw)( THLongTensor * self , THGenerator * _generator , THLongTensor * J , THTensor * q)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( multinomial)( THLongTensor * self , THGenerator * _generator , THTensor * prob_dist , int n_sample , int with_replacement)",176, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( getRNGState)( THGenerator * _generator , THTensor * self)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorRandom.cpp,"THTensor_( setRNGState)( THGenerator * _generator , THTensor * self)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( baddbmm)( THTensor * result , scalar_t beta , THTensor * t , scalar_t alpha , THTensor * batch1 , THTensor * batch2)",46, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( numel)( THTensor * t)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( preserveReduceDimSemantics)( THTensor * r_ , int in_dims , int reduce_dimension , int keepdim)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( max)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",82, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( min)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",82, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sum)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",76, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( prod)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",76, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cumsum)( THTensor * r_ , THTensor * t , int dimension)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cumprod)( THTensor * r_ , THTensor * t , int dimension)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sign)( THTensor * r_ , THTensor * t)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( trace)( THTensor * t)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cross)( THTensor * r_ , THTensor * a , THTensor * b , int dimension)",46, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmax)( THTensor * r , THTensor * t , THTensor * src)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmin)( THTensor * r , THTensor * t , THTensor * src)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cmaxValue)( THTensor * r , THTensor * t , scalar_t value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cminValue)( THTensor * r , THTensor * t , scalar_t value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( zerosLike)( THTensor * r_ , THTensor * input)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( onesLike)( THTensor * r_ , THTensor * input)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( diag)( THTensor * r_ , THTensor * t , int k)",48, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( eye)( THTensor * r_ , int64_t n , int64_t m)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( range)( THTensor * r_ , accreal xmin , accreal xmax , accreal step)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( arange)( THTensor * r_ , accreal xmin , accreal xmax , accreal step)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( randperm)( THTensor * r_ , THGenerator * _generator , int64_t n)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quicksortascend)( scalar_t * arr , int64_t * idx , int64_t elements , int64_t stride)",88, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quicksortdescend)( scalar_t * arr , int64_t * idx , int64_t elements , int64_t stride)",88, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( sort)( THTensor * rt_ , THLongTensor * ri_ , THTensor * t , int dimension , int descendingOrder)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quickselectnoidx)( scalar_t * arr , int64_t k , int64_t elements , int64_t stride)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( quickselect)( scalar_t * arr , int64_t * idx , int64_t k , int64_t elements , int64_t stride)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( medianall)( THTensor * tensor)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( mode)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",65, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( kthvalue)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int64_t k , int dimension , int keepdim)",47, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( median)( THTensor * values_ , THLongTensor * indices_ , THTensor * t , int dimension , int keepdim)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( topk)( THTensor * rt_ , THLongTensor * ri_ , THTensor * t , int64_t k , int dim , int dir , int sorted)",67, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( tril)( THTensor * r_ , THTensor * t , int64_t k)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( triu)( THTensor * r_ , THTensor * t , int64_t k)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( cat)( THTensor * r_ , THTensor * ta , THTensor * tb , int dimension)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( check_shape_except_dim)( THTensor * first , THTensor * second , int dimension)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( catArray)( THTensor * result , THTensor ** inputs , int numInputs , int dimension)",94, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( equal)( THTensor * ta , THTensor * tb)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAndAll)( THTensor * tensor)",19, 69, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAnyAll)( THTensor * tensor)",19, 67, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAnd)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",76, 18, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logicalAny)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",76, 18, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( atan2)( THTensor * r_ , THTensor * tx , THTensor * ty)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( polygamma)( THTensor * r_ , int64_t n , THTensor * t)",7, 48, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( lerp)( THTensor * r_ , THTensor * a , THTensor * b , scalar_t weight)",6, 89, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( mean)( THTensor * r_ , THTensor * t , int dimension , int keepdim)",8, 1, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( std)( THTensor * r_ , THTensor * t , int dimension , int biased , int keepdim)",40, 18, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( var)( THTensor * r_ , THTensor * t , int dimension , int biased , int keepdim)",40, 18, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( norm)( THTensor * r_ , THTensor * t , scalar_t value , int dimension , int keepdim)",46, 18, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( normall)( THTensor * tensor , scalar_t value)",27, 11, 3, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( renorm)( THTensor * res , THTensor * src , scalar_t value , int dimension , scalar_t maxnorm)",56, 4, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dist)( THTensor * tensor , THTensor * src , scalar_t value)",26, 47, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( meanall)( THTensor * tensor)",4, 1, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( varall)( THTensor * tensor , int biased)",8, 19, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( stdall)( THTensor * tensor , int biased)",4, 1, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( linspace)( THTensor * r_ , scalar_t a , scalar_t b , int64_t n)",20, 55, 13, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( logspace)( THTensor * r_ , scalar_t a , scalar_t b , int64_t n)",20, 75, 8, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( histc)( THTensor * hist , THTensor * tensor , int64_t nbins , scalar_t minvalue , scalar_t maxvalue)",30, 78, 6, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( bhistc)( THTensor * hist , THTensor * tensor , int64_t nbins , scalar_t minvalue , scalar_t maxvalue)",37, 65, 28, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_alpha_small)( scalar_t x , scalar_t alpha , scalar_t beta)",12, 52, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_beta_small)( scalar_t x , scalar_t alpha , scalar_t beta)",15, 63, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( beta_grad_alpha_mid)( double x , double alpha , double beta)",33, 60, 2, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dirichlet_grad_one)( scalar_t x , scalar_t alpha , scalar_t total)",57, 6, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp,"THTensor_( dirichlet_grad)( THTensor * self , THTensor * x , THTensor * alpha , THTensor * total)",23, 91, 4, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( fill)( scalar_t * x , const scalar_t c , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cadd)( scalar_t * z , const scalar_t * x , const scalar_t * y , const scalar_t c , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( adds)( scalar_t * r_ , const scalar_t * t , const scalar_t value , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cmul)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( muls)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( cdiv)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( divs)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( normal_fill)( scalar_t * data , const int64_t size , struct THGenerator * generator , const scalar_t mean , const scalar_t stddev)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( sigmoid)( scalar_t * y , const scalar_t * x , const ptrdiff_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDispatch.cpp,"THVector_( startup)",17, 3, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( storage)( const THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( storageOffset)( const THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimension)( const THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimensionLegacyNoScalars)( const THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nDimensionLegacyAll)( const THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( size)( const THTensor * self , int dim)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( stride)( const THTensor * self , int dim)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( data)( const THTensor * self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( new)( void)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithTensor)( THTensor * tensor)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage)( THStorage * storage , ptrdiff_t storageOffset , at :: IntList sizes , at :: IntList strides)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage1d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage2d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage3d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithStorage4d)( THStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2 , int64_t size3 , int64_t stride3)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize)( at :: IntList size , at :: IntList stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize1d)( int64_t size0)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize2d)( int64_t size0 , int64_t size1)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize3d)( int64_t size0 , int64_t size1 , int64_t size2)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newWithSize4d)( int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newClone)( THTensor * self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newContiguous)( THTensor * self)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newSelect)( THTensor * tensor , int dimension_ , int64_t sliceIndex_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newNarrow)( THTensor * tensor , int dimension_ , int64_t firstIndex_ , int64_t size_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newTranspose)( THTensor * tensor , int dimension1_ , int dimension2_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newUnfold)( THTensor * tensor , int dimension_ , int64_t size_ , int64_t step_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( newView)( THTensor * tensor , at :: IntList size)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize)( THTensor * self , at :: IntList size , at :: IntList stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resizeAs)( THTensor * self , THTensor * src)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize0d)( THTensor * tensor)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize1d)( THTensor * tensor , int64_t size0)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize2d)( THTensor * tensor , int64_t size0 , int64_t size1)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize3d)( THTensor * tensor , int64_t size0 , int64_t size1 , int64_t size2)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize4d)( THTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resize5d)( THTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3 , int64_t size4)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set)( THTensor * self , THTensor * src)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage1d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage2d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage3d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorage4d)( THTensor * self , THStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_ , int64_t size3_ , int64_t stride3_)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( narrow)( THTensor * self , THTensor * src , int dimension , int64_t firstIndex , int64_t size)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( select)( THTensor * self , THTensor * src , int dimension , int64_t sliceIndex)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( transpose)( THTensor * self , THTensor * src , int dimension1 , int dimension2)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( unfold)( THTensor * self , THTensor * src , int dimension , int64_t size , int64_t step)",35, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( squeeze)( THTensor * self , THTensor * src)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( squeeze1d)( THTensor * self , THTensor * src , int dimension)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( unsqueeze1d)( THTensor * self , THTensor * src , int dimension)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isTransposed)( const THTensor * self)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isContiguous)( const THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isSameSizeAs)( const THTensor * self , const THTensor * src)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( isSetTo)( const THTensor * self , const THTensor * src)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( nElement)( const THTensor * self)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( retain)( THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( free)( THTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( freeCopyTo)( THTensor * self , THTensor * dst)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( setStorageNd)( THTensor * self , THStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( resizeNd)( THTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set0d)( THTensor * tensor , scalar_t value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get0d)( const THTensor * tensor)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set1d)( THTensor * tensor , int64_t x0 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get1d)( const THTensor * tensor , int64_t x0)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set2d)( THTensor * tensor , int64_t x0 , int64_t x1 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get2d)( const THTensor * tensor , int64_t x0 , int64_t x1)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set3d)( THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get3d)( const THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( set4d)( THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( get4d)( const THTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( desc)( const THTensor * tensor)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensor.cpp,"THTensor_( sizeDesc)( const THTensor * tensor)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gesv)( int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( trtrs)( char uplo , char trans , char diag , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gels)( char trans , int m , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( syev)( char jobz , char uplo , int n , scalar_t * a , int lda , scalar_t * w , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( geev)( char jobvl , char jobvr , int n , scalar_t * a , int lda , scalar_t * wr , scalar_t * wi , scalar_t * vl , int ldvl , scalar_t * vr , int ldvr , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( gesdd)( char jobz , int m , int n , scalar_t * a , int lda , scalar_t * s , scalar_t * u , int ldu , scalar_t * vt , int ldvt , scalar_t * work , int lwork , int * iwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getrf)( int m , int n , scalar_t * a , int lda , int * ipiv , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getrs)( char trans , int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( getri)( int n , scalar_t * a , int lda , int * ipiv , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potrf)( char uplo , int n , scalar_t * a , int lda , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potrs)( char uplo , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( potri)( char uplo , int n , scalar_t * a , int lda , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( pstrf)( char uplo , int n , scalar_t * a , int lda , int * piv , int * rank , scalar_t tol , scalar_t * work , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( geqrf)( int m , int n , scalar_t * a , int lda , scalar_t * tau , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( orgqr)( int m , int n , int k , scalar_t * a , int lda , scalar_t * tau , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THLapack.cpp,"THLapack_( ormqr)( char side , char trans , int m , int n , int k , scalar_t * a , int lda , scalar_t * tau , scalar_t * c , int ldc , scalar_t * work , int lwork , int * info)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( copy_DEFAULT)( scalar_t * x , const scalar_t * y , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( fill_DEFAULT)( scalar_t * x , const scalar_t c , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cadd_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const scalar_t c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( adds_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cmul_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( muls_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( cdiv_DEFAULT)( scalar_t * z , const scalar_t * x , const scalar_t * y , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( divs_DEFAULT)( scalar_t * y , const scalar_t * x , const scalar_t c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( interleaved_normal_fill_16)( scalar_t * data , const scalar_t mean , const scalar_t stddev)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THVectorDefault.cpp,"THVector_( normal_fill_DEFAULT)( scalar_t * data , int64_t size , THGenerator * generator , const scalar_t mean , const scalar_t stddev)",33, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( data)( const THStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( size)( const THStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( elementSize)()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( new)( void)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize)( ptrdiff_t size)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithAllocator)( ptrdiff_t size , at :: Allocator * allocator)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithMapping)( const char * filename , ptrdiff_t size , int flags)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize1)( scalar_t data0)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize2)( scalar_t data0 , scalar_t data1)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize3)( scalar_t data0 , scalar_t data1 , scalar_t data2)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithSize4)( scalar_t data0 , scalar_t data1 , scalar_t data2 , scalar_t data3)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( retain)( THStorage * storage)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( free)( THStorage * storage)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( newWithDataAndAllocator)( at :: DataPtr && data , ptrdiff_t size , at :: Allocator * allocator)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( resize)( THStorage * storage , ptrdiff_t size)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( fill)( THStorage * storage , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( set)( THStorage * self , ptrdiff_t idx , scalar_t value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( get)( const THStorage * self , ptrdiff_t idx)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorage.cpp,"THStorage_( swap)( THStorage * storage1 , THStorage * storage2)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorageCopy.cpp,"THStorage_( rawCopy)( THStorage * storage , scalar_t * src)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THStorageCopy.cpp,"THStorage_( copy)( THStorage * storage , THStorage * src)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"sdot_( const int * n , const float * x , const int * incx , const float * y , const int * incy)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( swap)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",33, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( scal)( int64_t n , scalar_t a , scalar_t * x , int64_t incx)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( copy)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( axpy)( int64_t n , scalar_t a , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( dot)( int64_t n , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( gemv)( char trans , int64_t m , int64_t n , scalar_t alpha , scalar_t * a , int64_t lda , scalar_t * x , int64_t incx , scalar_t beta , scalar_t * y , int64_t incy)",58, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( ger)( int64_t m , int64_t n , scalar_t alpha , scalar_t * x , int64_t incx , scalar_t * y , int64_t incy , scalar_t * a , int64_t lda)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THBlas.cpp,"THBlas_( gemm)( char transa , char transb , int64_t m , int64_t n , int64_t k , scalar_t alpha , scalar_t * a , int64_t lda , scalar_t * b , int64_t ldb , scalar_t beta , scalar_t * c , int64_t ldc)",139, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( fill)( THTensor * r_ , scalar_t value)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( zero)( THTensor * r_)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedFill)( THTensor * tensor , THByteTensor * mask , scalar_t value)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedCopy)( THTensor * tensor , THByteTensor * mask , THTensor * src)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maskedSelect)( THTensor * tensor , THTensor * src , THByteTensor * mask)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( nonzero)( THLongTensor * subscript , THTensor * tensor)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexSelect)( THTensor * tensor , THTensor * src , int dim , THLongTensor * index)",75, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexCopy)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",39, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( dataOffset)( THTensor * tensor , ptrdiff_t linearIndex)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( checkLinearIndex)( int64_t linearIndex , int64_t numel)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( wrapLinearIndex)( int64_t linearIndex , int64_t numel)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( take)( THTensor * r_ , THTensor * src , THLongTensor * index)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( put)( THTensor * tensor , THLongTensor * index , THTensor * src , int accumulate)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexAdd)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",40, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( indexFill)( THTensor * tensor , int dim , THLongTensor * index , scalar_t val)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( gather)( THTensor * tensor , THTensor * src , int dim , THLongTensor * index)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatter)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatterAdd)( THTensor * tensor , int dim , THLongTensor * index , THTensor * src)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( scatterFill)( THTensor * tensor , int dim , THLongTensor * index , scalar_t val)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( dot)( THTensor * tensor , THTensor * src)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( minall)( THTensor * tensor)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( maxall)( THTensor * tensor)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sumall)( THTensor * tensor)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( prodall)( THTensor * tensor)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( add)( THTensor * r_ , THTensor * t , scalar_t value)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sub)( THTensor * r_ , THTensor * t , scalar_t value)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( add_scaled)( THTensor * r_ , THTensor * t , scalar_t value , scalar_t alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( sub_scaled)( THTensor * r_ , THTensor * t , scalar_t value , scalar_t alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( mul)( THTensor * r_ , THTensor * t , scalar_t value)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( div)( THTensor * r_ , THTensor * t , scalar_t value)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( lshift)( THTensor * r_ , THTensor * t , scalar_t value)",51, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( rshift)( THTensor * r_ , THTensor * t , scalar_t value)",51, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( fmod)( THTensor * r_ , THTensor * t , scalar_t value)",43, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"modulo_wrap( scalar_t a , scalar_t b)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( remainder)( THTensor * r_ , THTensor * t , scalar_t value)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp,"THTensor_( bitand)( THTensor * r_ , THTensor * t , scalar_t value)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_fill_AVX( double * x , const double c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cdiv_AVX( double * z , const double * x , const double * y , const ptrdiff_t n)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_divs_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cmul_AVX( double * z , const double * x , const double * y , const ptrdiff_t n)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_muls_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_cadd_AVX( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THDoubleVector_adds_AVX( double * y , const double * x , const double c , const ptrdiff_t n)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_fill_AVX( float * x , const float c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cdiv_AVX( float * z , const float * x , const float * y , const ptrdiff_t n)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_divs_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cmul_AVX( float * z , const float * x , const float * y , const ptrdiff_t n)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_muls_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_cadd_AVX( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX.cpp,"THFloatVector_adds_AVX( float * y , const float * x , const float c , const ptrdiff_t n)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_fill_VSX( double * x , const double c , const ptrdiff_t n)",90, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cadd_VSX( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",99, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_adds_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",81, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cmul_VSX( double * z , const double * x , const double * y , const ptrdiff_t n)",96, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_muls_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",81, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_cdiv_VSX( double * z , const double * x , const double * y , const ptrdiff_t n)",96, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THDoubleVector_divs_VSX( double * y , const double * x , const double c , const ptrdiff_t n)",86, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_fill_VSX( float * x , const float c , const ptrdiff_t n)",90, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cadd_VSX( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",99, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_adds_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",79, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cmul_VSX( float * z , const float * y , const float * x , const ptrdiff_t n)",96, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_muls_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",79, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_cdiv_VSX( float * z , const float * x , const float * y , const ptrdiff_t n)",96, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"THFloatVector_divs_VSX( float * y , const float * x , const float c , const ptrdiff_t n)",86, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_fill( double * x , const double c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_fill( float * x , const float c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cadd( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cadd( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_adds( double * y , const double * x , const double c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_adds( float * y , const float * x , const float c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cmul( double * z , const double * x , const double * y , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cmul( float * z , const float * x , const float * y , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_muls( double * y , const double * x , const double c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_muls( float * y , const float * x , const float c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_cdiv( double * z , const double * x , const double * y , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_cdiv( float * z , const float * x , const float * y , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardDouble_divs( double * y , const double * x , const double c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"standardFloat_divs( float * y , const float * x , const float c , const ptrdiff_t n)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"randDouble()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"near( double a , double b)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_fill_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_fill_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cadd_VSX()",70, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cadd_VSX()",70, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_adds_VSX()",65, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_adds_VSX()",66, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cmul_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cmul_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_muls_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_muls_VSX()",68, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_cdiv_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_cdiv_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THDoubleVector_divs_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"test_THFloatVector_divs_VSX()",69, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/VSX.cpp,"main()",56, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THDoubleVector_cadd_AVX2( double * z , const double * x , const double * y , const double c , const ptrdiff_t n)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_cadd_AVX2( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"normal_fill_16_AVX2( float * data , const __m256 * two_pi , const __m256 * one , const __m256 * minus_two , const __m256 * mean , const __m256 * stddev)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_normal_fill_AVX2( float * data , const int64_t size , THGenerator * generator , const float mean , const float stddev)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/AVX2.cpp,"THFloatVector_sigmoid_AVX2( float * y , const float * x , const ptrdiff_t n)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_fill_NEON( float * x , const float c , const ptrdiff_t n)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cmul_NEON( float * z , const float * x , const float * y , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_muls_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cadd_NEON( float * z , const float * x , const float * y , const float c , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_adds_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_cdiv_NEON( float * z , const float * x , const float * y , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/TH/vector/NEON.cpp,"THFloatVector_divs_NEON( float * y , const float * x , const float c , const ptrdiff_t n)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"destroyGenerator( THCState * state , THCGenerator * gen)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"createSeed( std :: random_device & rd)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_init( THCState * state , int devices , int current_device)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_shutdown( THCState * state)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_rawGenerator( THCState * state)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_getGenerator( THCState * state)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_generatorStates( THCState * state)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_seed( THCState * state)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_seedAll( THCState * state)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_manualSeed( THCState * state , uint64_t seed)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_manualSeedAll( THCState * state , uint64_t seed)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensorRandom.cpp,"THCRandom_initialSeed( THCState * state)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimension( THCState * state , const THCTensor * self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimensionLegacyNoScalars( THCState * state , const THCTensor * self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nDimensionLegacyAll( THCState * state , const THCTensor * self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_size( THCState * state , const THCTensor * self , int dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_sizeLegacyNoScalars( THCState * state , const THCTensor * self , int dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_stride( THCState * state , const THCTensor * self , int dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_strideLegacyNoScalars( THCState * state , const THCTensor * self , int dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_new( THCState * state , caffe2 :: TypeMeta type_meta)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resize( THCState * state , THCTensor * self , at :: IntList size , at :: IntList stride)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resizeAs( THCState * state , THCTensor * self , THCTensor * src)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_resizeNd( THCState * state , THCTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_set( THCState * state , THCTensor * self , THCTensor * src)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_setStorage( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_setStorageNd( THCState * state , THCTensor * self , THCStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_squeeze1d( THCState * state , THCTensor * self , THCTensor * src , int dimension)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_unsqueeze1d( THCState * state , THCTensor * self , THCTensor * src , int dimension)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_allContiguous( THCState * state , THCTensor ** inputs , int numInputs)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_nElement( THCState * state , const THCTensor * self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_retain( THCState * state , THCTensor * self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_free( THCState * state , THCTensor * self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_getDevice( THCState * state , const THCTensor * tensor)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_allSameDevice( THCState * state , THCTensor ** inputs , int numInputs)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_canUse32BitIndexMath( THCState * state , const THCTensor * t , ptrdiff_t max_elem)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_all32BitIndexable( THCState * state , THCTensor ** inputs , int numInputs)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_preserveReduceDimSemantics( THCState * state , THCTensor * tensor , int in_dims , int64_t dimension , int keepdim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"compareSizeAndStride( const void * a , const void * b)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCTensor.cpp,"THCTensor_maybeOverlappingIndices( THCState * state , const THCTensor * t)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"BlockSize::BlockSize( size_t size , void * ptr = NULL)",1, 67, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"Block::Block( size_t size , void * ptr , bool allocated)",2, 79, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"BlockComparator( const BlockSize & a , const BlockSize & b)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::HostAllocator()",1, 50, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::malloc( void ** ptr , size_t size)",34, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::free( void * ptr)",36, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::recordEvent( void * ptr , at :: cuda :: CUDAStream stream)",16, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::processEvents()",31, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::emptyCache()",31, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"HostAllocator::insertEvents( Block & block)",27, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator_recordEvent( void * ptr , at :: cuda :: CUDAStream stream)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator_emptyCache()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostDeleter( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator::allocate( size_t size) const",6, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"THCCachingHostAllocator::raw_deleter() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingHostAllocator.cpp,"getTHCCachingHostAllocator()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::DeviceStats()",3, 49, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::increaseAllocated( size_t delta)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::decreaseAllocated( size_t delta)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::increaseCached( size_t delta)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"DeviceStats::decreaseCached( size_t delta)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"Block::Block( int device , cudaStream_t stream , size_t size , char * ptr = NULL)",3, 63, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"BlockComparator( const Block * a , const Block * b)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"format_size( uint64_t size)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::THCCachingAllocator()",3, 39, 6, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::get_stats_for_device( int device)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::malloc( void ** devPtr , size_t size , cudaStream_t stream)",93, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free( void * ptr)",23, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::emptyCache()",6, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::getBaseAllocation( void * ptr , size_t * outSize)",21, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::cacheInfoAux( FreeBlocks & blocks , int dev_id , size_t * total , size_t * largest)",12, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::cacheInfo( int dev_id , size_t * total , size_t * largest)",6, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::recordStream( void * ptr , at :: cuda :: CUDAStream stream)",14, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free_block( Block * block)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::try_merge_blocks( Block * dst , Block * src , FreeBlocks & free_blocks)",21, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::round_size( size_t size)",11, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::cuda_malloc_retry( int device , void ** devPtr , size_t size)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free_cached_blocks( int device)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::free_blocks( FreeBlocks & blocks , FreeBlocks :: iterator it , FreeBlocks :: iterator end)",18, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::find_allocated_block( void * ptr)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::insert_events( Block * block)",20, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator::process_events()",28, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"CudaCachingDeleter( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"CudaCachingAllocator::allocate( size_t size) const",9, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"CudaCachingAllocator::raw_deleter() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_get( void)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_emptyCache( void)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_cacheInfo( int dev_id , size_t * cachedAndFree , size_t * largestBlock)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_getBaseAllocation( void * ptr , size_t * size)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_recordStream( void * ptr , at :: cuda :: CUDAStream stream)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_getCudaFreeMutex()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"assertValidDevice( int device)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_currentMemoryAllocated( int device)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_maxMemoryAllocated( int device)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_currentMemoryCached( int device)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCachingAllocator_maxMemoryCached( int device)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCCachingAllocator.cpp,"THCCaching_CUDAIpcDevptr( std :: string handle)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::~THCIpcDeleter()",1, 35, 0, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"deleteTHCIpcDeleter( void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::makeDataPtr( std :: shared_ptr<void> basePtr , void * data)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCAllocator.cpp,"THCIpcDeleter::THCIpcDeleter( std :: shared_ptr<void> basePtr)",2, 38, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_free( THCState * state)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_alloc( void)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaInit( THCState * state)",64, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaShutdown( THCState * state)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getPeerToPeerAccess( THCState * state , int dev , int devToAccess)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentDeviceProperties( THCState * state)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getDeviceProperties( THCState * state , int device)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getRngState( THCState * state)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCudaHostAllocator( THCState * state)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getNumDevices( THCState * state)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getDeviceResourcePtr( THCState * state , int device)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentStreamOnDevice( THCState * state , int device)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentStream( THCState * state)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentBlasHandle( THCState * state)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentSparseHandle( THCState * state)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCState_getCurrentDeviceScratchSpaceSize( THCState * state)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCudaCheck( cudaError_t err , const char * file , const int line)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCudaCheckWarn( cudaError_t err , const char * file , const int line)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCublasCheck( cublasStatus_t status , const char * file , const int line)",46, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"__THCusparseCheck( cusparseStatus_t status , const char * file , const int line)",48, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaMalloc( THCState * state , size_t size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaFree( THCState * state , void * ptr)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaHostAlloc( THCState * state , size_t size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaHostRecord( THCState * state , void * ptr)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCGeneral.cpp,"THCudaMemGetInfo( THCState * state , size_t * freeBytes , size_t * totalBytes , size_t * largestBlock)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_resize( THCState * state , THCStorage * self , ptrdiff_t size)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_getDevice( THCState * state , const THCStorage * storage)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/THCStorage.cpp,"THCStorage_new( THCState * state , caffe2 :: TypeMeta data_type)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( storage)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( storageOffset)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimension)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimensionLegacyNoScalars)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nDimensionLegacyAll)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( size)( THCState * state , const THCTensor * self , int dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( sizeLegacyNoScalars)( THCState * state , const THCTensor * self , int dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( stride)( THCState * state , const THCTensor * self , int dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( strideLegacyNoScalars)( THCState * state , const THCTensor * self , int dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( data)( THCState * state , const THCTensor * self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( new)( THCState * state)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithTensor)( THCState * state , THCTensor * tensor)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , at :: IntList sizes , at :: IntList strides)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage1d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage2d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage3d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithStorage4d)( THCState * state , THCStorage * storage , ptrdiff_t storageOffset , int64_t size0 , int64_t stride0 , int64_t size1 , int64_t stride1 , int64_t size2 , int64_t stride2 , int64_t size3 , int64_t stride3)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize)( THCState * state , at :: IntList size , at :: IntList stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize1d)( THCState * state , int64_t size0)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize2d)( THCState * state , int64_t size0 , int64_t size1)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize3d)( THCState * state , int64_t size0 , int64_t size1 , int64_t size2)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newWithSize4d)( THCState * state , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newClone)( THCState * state , THCTensor * self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newContiguous)( THCState * state , THCTensor * self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newSelect)( THCState * state , THCTensor * tensor , int dimension_ , int64_t sliceIndex_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newNarrow)( THCState * state , THCTensor * tensor , int dimension_ , int64_t firstIndex_ , int64_t size_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newTranspose)( THCState * state , THCTensor * tensor , int dimension1_ , int dimension2_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newUnfold)( THCState * state , THCTensor * tensor , int dimension_ , int64_t size_ , int64_t step_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newView)( THCState * state , THCTensor * tensor , at :: IntList size)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( newFoldBatchDim)( THCState * state , THCTensor * input)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize)( THCState * state , THCTensor * self , at :: IntList size , at :: IntList stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resizeAs)( THCState * state , THCTensor * self , THCTensor * src)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize0d)( THCState * state , THCTensor * tensor)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize1d)( THCState * state , THCTensor * tensor , int64_t size0)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize2d)( THCState * state , THCTensor * tensor , int64_t size0 , int64_t size1)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize3d)( THCState * state , THCTensor * tensor , int64_t size0 , int64_t size1 , int64_t size2)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize4d)( THCState * state , THCTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resize5d)( THCState * state , THCTensor * self , int64_t size0 , int64_t size1 , int64_t size2 , int64_t size3 , int64_t size4)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set)( THCState * state , THCTensor * self , THCTensor * src)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , at :: IntList size_ , at :: IntList stride_)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage1d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage2d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage3d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorage4d)( THCState * state , THCTensor * self , THCStorage * storage_ , ptrdiff_t storageOffset_ , int64_t size0_ , int64_t stride0_ , int64_t size1_ , int64_t stride1_ , int64_t size2_ , int64_t stride2_ , int64_t size3_ , int64_t stride3_)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( narrow)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t firstIndex , int64_t size)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( select)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t sliceIndex)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( transpose)( THCState * state , THCTensor * self , THCTensor * src , int dimension1 , int dimension2)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( unfold)( THCState * state , THCTensor * self , THCTensor * src , int dimension , int64_t size , int64_t step)",36, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( squeeze)( THCState * state , THCTensor * self , THCTensor * src)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( squeeze1d)( THCState * state , THCTensor * self , THCTensor * src , int dimension)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( unsqueeze1d)( THCState * state , THCTensor * self , THCTensor * src , int dimension)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isContiguous)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isSetTo)( THCState * state , const THCTensor * self , const THCTensor * src)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( isSameSizeAs)( THCState * state , const THCTensor * self , const THCTensor * src)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( nElement)( THCState * state , const THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( retain)( THCState * state , THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( free)( THCState * state , THCTensor * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( freeCopyTo)( THCState * state , THCTensor * self , THCTensor * dst)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( setStorageNd)( THCState * state , THCTensor * self , THCStorage * storage , ptrdiff_t storageOffset , int nDimension , const int64_t * size , const int64_t * stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( resizeNd)( THCState * state , THCTensor * self , int nDimension , const int64_t * size , const int64_t * stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set0d)( THCState * state , THCTensor * tensor , scalar_t value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get0d)( THCState * state , const THCTensor * tensor)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set1d)( THCState * state , THCTensor * tensor , int64_t x0 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get1d)( THCState * state , const THCTensor * tensor , int64_t x0)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set2d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get2d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set3d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get3d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( set4d)( THCState * state , THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3 , scalar_t value)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( get4d)( THCState * state , const THCTensor * tensor , int64_t x0 , int64_t x1 , int64_t x2 , int64_t x3)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( checkGPU)( THCState * state , unsigned int nTensors , ...)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCTensor.cpp,"THCTensor_( sizeDesc)( THCState * state , const THCTensor * tensor)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorageCopy.cpp,"THCStorage_( copyCPU)( THCState * state , THCStorage * self , struct THStorage * src)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorageCopy.cpp,"THStorage_( copyCuda)( THCState * state , THStorage * self , struct THCStorage * src)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( data)( THCState * state , const THCStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( size)( THCState * state , const THCStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( elementSize)( THCState * state)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( set)( THCState * state , THCStorage * self , ptrdiff_t index , scalar_t value)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( get)( THCState * state , const THCStorage * self , ptrdiff_t index)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( new)( THCState * state)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize)( THCState * state , ptrdiff_t size)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithAllocator)( THCState * state , ptrdiff_t size , at :: Allocator * allocator)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize1)( THCState * state , scalar_t data0)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize2)( THCState * state , scalar_t data0 , scalar_t data1)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize3)( THCState * state , scalar_t data0 , scalar_t data1 , scalar_t data2)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithSize4)( THCState * state , scalar_t data0 , scalar_t data1 , scalar_t data2 , scalar_t data3)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithMapping)( THCState * state , const char * fileName , ptrdiff_t size , int isShared)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( newWithDataAndAllocator)( THCState * state , at :: DataPtr && data , ptrdiff_t size , at :: Allocator * allocator)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( retain)( THCState * state , THCStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/THC/generic/THCStorage.cpp,"THCStorage_( free)( THCState * state , THCStorage * self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::sparseTensorIdToDeviceType( TensorTypeId type_id)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::SparseTensorImpl( at :: TensorTypeId type_id , const caffe2 :: TypeMeta & data_type)",6, 122, 4, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::sizes() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::strides() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::is_contiguous() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::stride( int64_t d) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::resize_dim( int64_t ndim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_size( int64_t dim , int64_t new_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_stride( int64_t dim , int64_t new_stride)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_storage_offset( int64_t storage_offset)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::dim() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::maybe_zero_dim( bool condition_when_zero_dim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::storage() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::storage_offset() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/SparseTensorImpl.cpp,"at::SparseTensorImpl::set_indices_and_values_unsafe( const Tensor & indices , const Tensor & values)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGeneral.cpp,"at::set_num_threads( int num_threads_)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGeneral.cpp,"at::get_num_threads()",1, 53, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::errorHandler( const char * msg , void * data)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::argErrorHandler( int arg , const char * msg , void * data)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::Context()",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::globalContext()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::userEnabledCuDNN() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setUserEnabledCuDNN( bool e)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::deterministicCuDNN() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setDeterministicCuDNN( bool b)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::benchmarkCuDNN() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setBenchmarkCuDNN( bool b)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasMKL() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::hasLAPACK() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::Context::setFlushDenormal( bool on)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( TensorOptions options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( const TensorImpl * impl)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getType( const Tensor & t)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getLegacyTHDispatcher( TensorOptions options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getLegacyTHDispatcher( const TensorImpl * impl)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::getCPUAllocator()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::LegacyDeviceTypeInit( LegacyDeviceTypeInitArgs)",1, 52, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initCPU() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initCUDA() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initHIP() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/Context.cpp,"at::LegacyDeviceTypeInit::initComplex() const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/TensorGeometry.cpp,"at::TensorGeometry::is_contiguous() const",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/Utils.cpp,"at::_crash_if_asan( int arg)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::UndefinedType()",2, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::scalarType() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::typeMeta() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::backend() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::allocator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::getDeviceFromPtr( void *) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::unsafeStorageFromTH( void * th_pointer , bool retain) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::storageWithAllocator( int64_t size , Allocator * allocator) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::unsafeTensorFromTH( void * th_pointer , bool retain) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::generator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::ID() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::elementSizeInBytes() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toBackend( Backend b) const",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/UndefinedType.cpp,"at::UndefinedType::toScalarType( ScalarType s) const",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::operator < <( std :: ostream & out , TensorGeometryArg t)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDim( CheckedFrom c , const TensorGeometryArg & t , int64_t dim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDimRange( CheckedFrom c , const TensorGeometryArg & t , int64_t dim_start , int64_t dim_end)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkContiguous( CheckedFrom c , const TensorGeometryArg & t)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllContiguous( CheckedFrom c , at :: ArrayRef<TensorArg> ts)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSize( CheckedFrom c , const TensorGeometryArg & t , IntList sizes)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSize( CheckedFrom c , const TensorGeometryArg & t , int64_t dim , int64_t size)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSame( CheckedFrom c , ArrayRef<TensorArg> tensors , void(*fn)(CheckedFrom,const TensorArg&,const TensorArg&))",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameSize( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameSize( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkNumel( CheckedFrom c , const TensorGeometryArg & t , int64_t numel)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameNumel( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameNumel( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameGPU( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameGPU( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameType( CheckedFrom c , const TensorArg & t1 , const TensorArg & t2)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkScalarType( CheckedFrom c , const TensorArg & t , ScalarType ty)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkScalarTypes( CheckedFrom c , const TensorArg & t , at :: ArrayRef<ScalarType> l)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllSameType( CheckedFrom c , ArrayRef<TensorArg> tensors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkSameDim( CheckedFrom c , const TensorGeometryArg & t1 , const TensorGeometryArg & t2)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkDefined( CheckedFrom c , const TensorArg & t)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkAllDefined( CheckedFrom c , ArrayRef<TensorArg> ts)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkBackend( CheckedFrom c , const Tensor & t , Backend backend)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::checkBackend( CheckedFrom c , ArrayRef<Tensor> tensors , at :: Backend backend)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::maybe_data_ptr( const Tensor & tensor)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::maybe_data_ptr( const TensorArg & tensor)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/TensorUtils.cpp,"at::geometry_is_contiguous( IntList sizes , IntList strides)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::CPUGenerator( Context * context_)",3, 3, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::~CPUGenerator()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::copy( const Generator & from)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::free()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::seed()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::initialSeed()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::manualSeed( uint64_t seed)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::manualSeedAll( uint64_t seed)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUGenerator.cpp,"at::CPUGenerator::unsafeGetTH()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::allocator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::getDeviceFromPtr( void * data) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/CPUTypeDefault.cpp,"at::CPUTypeDefault::generator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/LegacyTHDispatch.cpp,"at::globalLegacyTHDispatch()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/ExpandUtils.cpp,"at::infer_size( IntList a , IntList b)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/ExpandUtils.cpp,"at::inferExpandGeometry( IntList tensor_sizes , IntList tensor_strides , IntList sizes)",54, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getDLDataType( const Type & type)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getDLContext( const Type & type , const int64_t & device_id)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::getATenDeviceType( const DLContext & ctx)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::toScalarType( const DLDataType & dtype)",51, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::deleter( DLManagedTensor * arg)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::toDLPack( const Tensor & src)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/DLConvertor.cpp,"at::fromDLPack( const DLManagedTensor * src)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/LegacyTHDispatcherDerived.cpp,"at::LegacyTHDispatcher( $ { Backend } TensorId())",1, 48, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/RegisterCPU.cpp,"at::register_cpu_types( Context * context)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",1, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::scalarType() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::typeMeta() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::backend() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::toString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::ID() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDerived.cpp,"at::elementSizeInBytes() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::copy_( Tensor & self , const Tensor & src , bool non_blocking) const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::copy( const Tensor & src , bool non_blocking , optional<Device> to_device) const",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::backward( Tensor & self , c10 :: optional<Tensor> gradient , bool keep_graph , bool create_graph) const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::set_data( Tensor & self , Tensor new_data) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::toBackend( Backend b) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::toScalarType( ScalarType s) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::defaultStrides( IntList sizes)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::computeStorageSize( IntList sizes , IntList strides)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorFromBlob( void * data , IntList sizes , const std :: function<void(void*)> & deleter) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorFromBlob( void * data , IntList sizes , IntList strides , const std :: function<void(void*)> & deleter) const",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorWithAllocator( IntList sizes , Allocator * allocator) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::tensorWithAllocator( IntList sizes , IntList strides , Allocator * allocator) const",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::storageFromBlob( void * data , int64_t size , const std :: function<void(void*)> & deleter) const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::storageWithAllocator( int64_t size , Allocator * allocator) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::unsafeTensorFromTH( void * th_pointer , bool retain) const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/TypeDefault.cpp,"at::TypeDefault::unsafeStorageFromTH( void * th_pointer , bool retain) const",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/RegisterCUDA.cpp,"at::register_cuda_types( Context * context)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::TypeDefault( $ { Backend } TensorId() , false , false)",1, 103, 2, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::scalarType() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::typeMeta() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::backend() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::toString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::ID() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/templates/SparseTypeDerived.cpp,"at::elementSizeInBytes() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Types.cpp,"at::native::getCudnnDataType( const at :: Tensor & tensor)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Types.cpp,"at::native::cudnn_version()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::getDataType( const at :: Type & t)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::getDataType( const at :: Tensor & t)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::set( const at :: Tensor & t , size_t pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::set( cudnnDataType_t datatype , IntList t_sizes , IntList t_strides , size_t pad)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::cudnnTypeToString( cudnnDataType_t dtype)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::operator < <( std :: ostream & out , const TensorDescriptor & d)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::TensorDescriptor::print()",1, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Descriptors.cpp,"at::native::FilterDescriptor::set( const at :: Tensor & t , int64_t pad)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::Handle()",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::Handle::~Handle()",14, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/cudnn/Handle.cpp,"at::native::getCudnnHandle()",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/VariableHooksInterface.cpp,"at::detail::getVariableHooks()",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::ivalue::ConstantString::create( std :: string str_)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::printList( std :: ostream & out , const List & v , const std :: string start , const std :: string finish)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::operator < <( std :: ostream & out , const IValue & v)",47, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/ivalue.cpp,"c10::IValue::dump() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"c10::operator < <( std :: ostream & out , Backend b)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::defaultfloat( std :: ios_base & __base)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::FormatGuard::FormatGuard( std :: ostream & out)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::FormatGuard::~FormatGuard()",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::operator < <( std :: ostream & out , const Type & t)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printFormat( std :: ostream & stream , const Tensor & self)",87, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printIndent( std :: ostream & stream , int64_t indent)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::printScale( std :: ostream & stream , double scale)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printMatrix( std :: ostream & stream , const Tensor & self , int64_t linesize , int64_t indent)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::__printTensor( std :: ostream & stream , Tensor & self , int64_t linesize)",39, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Formatting.cpp,"at::print( std :: ostream & stream , const Tensor & tensor_ , int64_t linesize)",48, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::print() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Tensor.cpp,"at::Tensor::toString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::domain_prefix()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::symbol( const std :: string & s)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::string( Symbol sym)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::ns( Symbol sym)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::_symbol( const std :: string & s)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::InternedStrings::customString( Symbol sym)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::globalStrings()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::fromQualString( const std :: string & s)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toUnqualString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toQualString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::toDisplayString() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::ns() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::domainString() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/interned_strings.cpp,"c10::Symbol::fromDomainAndUnqualString( const std :: string & d , const std :: string & s)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::ThreadPool( std :: size_t pool_size , int numa_node_id)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::~ThreadPool()",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::size() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::numAvailable() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::inThreadPool() const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::run( const std :: function<void()> & func)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::waitWorkComplete()",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::ThreadPool::main_loop( std :: size_t index)",51, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/thread_pool.cpp,"c10::global_work_queue()",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::operator < <( std :: ostream & out , const Type & t)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::DynamicType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::UndefinedTensorType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::NumberType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::IntType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::FloatType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::BoolType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::NoneType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::GeneratorType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::StringType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::DeviceObjType::get()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::OptionalType::ofTensor()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofTensors()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofInts()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofFloats()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::ListType::ofBools()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::inferTypeFrom( const IValue & value)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::unifyTypes( const TypePtr & t1 , const TypePtr & t2)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::matchTypeVariables( TypePtr formal , TypePtr actual , TypeEnv & type_env)",104, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::evalTypeVariables( TypePtr type , std :: unordered_map<std::string,TypePtr> & type_env)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::typeKindToString( TypeKind kind)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/type.cpp,"c10::Type::isSubtypeOf( const TypePtr rhs) const",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/register_symbols.cpp,"c10::InternedStrings::InternedStrings()",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyDeviceTypeInit.cpp,"at::getLegacyDeviceTypeInit()",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/LegacyTypeDispatch.cpp,"at::globalLegacyTypeDispatch()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/TensorImpl_test.cpp,"TEST( TensorImplTest , Caffe2Constructor)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/core/Range.cpp,"at::operator < <( std :: ostream & out , const Range & range)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::allocator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::getDeviceFromPtr( void * data) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDATypeDefault.cpp,"at::CUDATypeDefault::generator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::CUDAGenerator( Context * context_)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::~CUDAGenerator()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::copy( const Generator & from)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::free()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::seed()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::initialSeed()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::manualSeed( uint64_t seed)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::manualSeedAll( uint64_t seed)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp,"at::CUDAGenerator::unsafeGetTH()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/PinnedMemoryAllocator.cpp,"at::cuda::getPinnedMemoryAllocator()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::warp_size()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentDeviceProperties()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getDeviceProperties( int64_t device)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCUDADeviceAllocator()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentCUDASparseHandle()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/CUDAContext.cpp,"at::cuda::getCurrentCUDABlasHandle()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::initCUDA() const",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::initCUDAGenerator( Context * context) const",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasCUDA() const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasMAGMA() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::hasCuDNN() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::current_device() const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::getPinnedMemoryAllocator() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::registerCUDATypes( Context * context) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::compiledWithCuDNN() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::compiledWithMIOpen() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::supportsDilatedConvolutionWithCuDNN() const",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::versionCuDNN() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::batchnormMinEpsilonCuDNN() const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTGetPlanCacheMaxSize() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTSetPlanCacheMaxSize( int64_t max_size) const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTGetPlanCacheSize() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::cuFFTClearPlanCache() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp,"at::cuda::detail::CUDAHooks::getNumGPUs() const",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/detail/ComplexHooksInterface.cpp,"at::detail::getComplexHooks()",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/detail/HIPHooksInterface.cpp,"at::detail::getHIPHooks()",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/detail/CUDAHooksInterface.cpp,"at::detail::getCUDAHooks()",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestEmptyTensor( Type & T)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2Basic( Type & T)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2WithScalar( Type & T)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2OldFallback( Type & T)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut2MismatchedSizes( Type & T)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3Basic( Type & T)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3WithScalar( Type & T)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3OldFallback( Type & T)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestOut3MismatchedSizes( Type & T)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2Basic( Type & T)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2WithScalar( Type & T)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn2ExpandError( Type & T)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3Basic( Type & T)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3WithScalar( Type & T)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestIn3ExpandError( Type & T)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimBasic( Type & T)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimWithScalar( Type & T)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TestExplicitDimWithMismatchedSizes( Type & T)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/broadcast_test.cpp,"TEST( BroadcastTest , Broadcast)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"Foo::apply( Tensor a , Tensor b)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"Foo<Half>::apply( Tensor a , Tensor b)",1, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"test_overflow()",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_test.cpp,"TEST( TestScalar , TestScalar)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakPointerGetsInvalidated)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakPointerLock)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/weakref_test.cpp,"TEST( TestWeakPointer , WeakUpdatesRefcountsTest)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"fill_tensor( int64_t scalar , Tensor & t_)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"test( Type & type , IntList shape , int64_t a = 0 , int64_t b = 1)",82, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Contiguous2D)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Small2D)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _2D)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _3D)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , Medium3D)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/apply_utils_test.cpp,"TEST( ApplyUtilsTest , _10D)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestSimpleCase( Type & T)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestExpressionSpecification( Type & T)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestEmptyTensor( Type & T)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TestScalarVs1Dim1Size( Type & T)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/wrapdim_test.cpp,"TEST( TestWrapdim , TestWrapdim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_cudnn_test.cpp,"TEST( CUDNNTest , CUDNNTestCUDA)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/atest.cpp,"trace()",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/atest.cpp,"TEST( atest , atest)",78, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , TestParallel)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , NestedParallel)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/test_parallel.cpp,"TEST( TestParallel , Exceptions)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"requireEqualTensorList( TensorList t1 , TensorList t2)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestSplit( Type & T , Tensor & t)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestChunk( Type & T , Tensor & t)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestStack( Type & T , Tensor & t)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestSize( Type & T , Tensor & t)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestMatmul( Type & T , Tensor & t , Type & AccT)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestStandardGammaGrad( Type & T , Tensor & t)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TestWhere( Type & T , Tensor & t)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"test( Type & T , Type & AccT)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TEST( TestNative , NativeTestCPU)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/native_test.cpp,"TEST( TestNative , NativeTestGPU)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"require_equal_size_dim( const Tensor & lhs , const Tensor & rhs)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"should_expand( const IntList & from_size , const IntList & to_size)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"test( Type & T)",222, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"TEST( TestScalarTensor , TestScalarTensorCPU)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp,"TEST( TestScalarTensor , TestScalarTensorCUDA)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/tbb_init_test.cpp,"test( int given_num_threads)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/tbb_init_test.cpp,"main()",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CopyAndMoveTest)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , GetAndSetTest)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"thread_fun( at :: optional<at::cuda::CUDAStream> & cur_thread_stream)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , MultithreadGetAndSetTest)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CUDAGuardTest)",60, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , StreamPoolTest)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , MultiGPUTest)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CUDAEventSyncTest)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_stream_test.cpp,"TEST( TestStream , CrossDeviceTest)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/verify_api_visibility.cpp,"main()",1, 22, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Arithmetic)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Comparisions)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Cast)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Construction)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"to_string( const Half & h)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , Half2String)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , HalfNumericLimits)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/half_test.cpp,"TEST( TestHalf , CommonMath)",47, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"makeRandomNumber()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"testCudaRNGMultithread()",9, 3, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_rng_test.cpp,"TEST( Cuda_RNGTest , MultithreadRNGTest)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestResize( Type & type)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestOnesAndDot( Type & type)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSort( Type & type)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestRandperm( Type & type)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"SendContext()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAdd( Type & type)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestLoadsOfAdds( Type & type)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestLoadOfAddsWithCopy( Type & type)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIsContiguous( Type & type)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestPermute( Type & type)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestMm( Type & type)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSqueeze( Type & type)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestCopy( Type & type)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestCopyBroadcasting( Type & type)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAbsValue( Type & type)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestAddingAValueWithScalar( Type & type)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestSelect( Type & type)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestZeroDim( Type & type)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestTensorFromTH()",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestToCFloat()",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestToString()",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingByScalar()",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingByZerodimTensor()",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestIndexingMixedDevice( Type & type)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TestDispatch()",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"test( Type & type)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TEST( BasicTest , BasicTestCPU)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/basic.cpp,"TEST( BasicTest , BasicTestCUDA)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/undefined_tensor_test.cpp,"TEST( TestUndefined , UndefinedTest)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/dlconvertor_test.cpp,"TEST( TestDlconvertor , TestDlconvertor)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , Contiguous2D)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , Contiguous3D)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , PartialCollapse3D)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , StridedCollapse2D)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , PartialStridedCollapse4D)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , CollapsesZerosAndOnes)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , CollapseToPointTensor)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , ExcludingInContiguous4D)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , RovingExclusion)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/cuda_apply_test.cpp,"TEST( ApplyTest , InvalidExclusion)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/test/test_install/main.cpp,"main()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Types.cpp,"at::native::getMiopenDataType( const at :: Tensor & tensor)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Types.cpp,"at::native::miopen_version()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::getDataType( const at :: Type & t)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::getDataType( const at :: Tensor & t)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::set( const at :: Tensor & t , size_t pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::set( miopenDataType_t datatype , IntList t_sizes , IntList t_strides , size_t pad)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::miopenTypeToString( miopenDataType_t dtype)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::operator < <( std :: ostream & out , const TensorDescriptor & d)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::TensorDescriptor::print()",1, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Descriptors.cpp,"at::native::FilterDescriptor::set( const at :: Tensor & t , int64_t pad)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::Handle::Handle()",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::Handle::~Handle()",5, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/miopen/Handle.cpp,"at::native::getMiopenHandle()",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/cpu/FlushDenormal.cpp,"at::cpu::set_flush_denormal( bool on)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_out( Tensor & result , const Tensor & self , const Tensor & other , Scalar alpha)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add( const Tensor & self , const Tensor & other , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_( Tensor & self , const Tensor & other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_out( Tensor & result , const Tensor & self , const Tensor & other)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div( const Tensor & self , const Tensor & other)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_out( Tensor & result , const Tensor & self , const Tensor & other)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul( const Tensor & self , const Tensor & other)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_out( Tensor & result , const Tensor & self , const Tensor & other , Scalar alpha)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub( const Tensor & self , const Tensor & other , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_( Tensor & self , const Tensor & other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::rsub( const Tensor & self , const Tensor & other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::wrapped_scalar_tensor( Scalar scalar)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add( const Tensor & self , Scalar other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::add_( Tensor & self , Scalar other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::div_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::mul_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub( const Tensor & self , Scalar other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::sub_( Tensor & self , Scalar other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BinaryOps.cpp,"at::native::rsub( const Tensor & self , Scalar other , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Resize.cpp,"at::native::resize_cpu_( Tensor & self , IntList size)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::get_target_prime( target_t * target , int64_t offset , int64_t stride , int64_t idx , int64_t BLANK)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_cpu_template( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK)",123, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_backward_cpu_template( const Tensor & grad_out , const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , const Tensor & neg_log_likelihood , const Tensor & log_alpha , int64_t BLANK)",136, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_cpu( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss_backward_cpu( const Tensor & grad , const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , const Tensor & neg_log_likelihood , const Tensor & log_alpha , int64_t BLANK)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK , int64_t reduction)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LossCTC.cpp,"at::native::ctc_loss( const Tensor & log_probs , const Tensor & targets , const Tensor & input_lengths , const Tensor & target_lengths , int64_t BLANK , int64_t reduction)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::relu( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::relu_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::selu( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::selu_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::celu( const Tensor & self , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::celu_( Tensor & self , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::rrelu( const Tensor & self , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::rrelu_( Tensor & self , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_out( optional<Tensor> opt_result , const Tensor & self , Scalar threshold , Scalar value , const Tensor & other)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold( const Tensor & self , Scalar threshold , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_( Tensor & self , Scalar threshold , Scalar value)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_out( Tensor & result , const Tensor & self , Scalar threshold , Scalar value)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::threshold_backward( const Tensor & grad , const Tensor & self , Scalar threshold)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_kernel_share_weights( Tensor & result , const Tensor & input , const Tensor & weight)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_kernel_multi_weights( Tensor & result , const Tensor & input , const Tensor & weight , int64_t input_dim0_size , int64_t channel_size , int64_t input_stride0 , int64_t input_stride1)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu( const Tensor & self , const Tensor & weight_)",47, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_backward_kernel_share_weights( const Tensor & input , const Tensor & weight , const Tensor & grad_out , Tensor & input_grad , Tensor & weight_grad)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_cpu_backward_kernel_multi_weights( const Tensor & input , const Tensor & weight , const Tensor & grad_out , Tensor & input_grad , Tensor & weight_grad_collector , int64_t input_dim0_size , int64_t channel_size , int64_t input_stride0 , int64_t input_stride1)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::prelu_backward_cpu( const Tensor & grad_out_ , const Tensor & self , const Tensor & weight_)",62, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::hardshrink_cpu( const Tensor & self , Scalar lambd)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Activation.cpp,"at::native::hardshrink_backward_cpu( const Tensor & grad , const Tensor & self , Scalar lambd)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::check_dims_match_num_input_features( const char * arg_name , int64_t expected , int64_t actual)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::repeat_if_defined( const Tensor & t , int64_t repeat)",6, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::conditional_accessor_1d( const Tensor & t)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu_template( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool train , double momentum , double eps)",76, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_backward_cpu_template( const Tensor & grad_out_ , const Tensor & input , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_invstd , bool train , double eps , std :: array<bool,3> grad_input_mask)",101, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double momentum , double eps , bool cudnn_enabled)",64, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::instance_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool use_input_stats , double momentum , double eps , bool cudnn_enabled)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::layer_norm( const Tensor & input , IntList normalized_shape , const Tensor & weight , const Tensor & bias , double eps , bool cudnn_enabled)",57, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::group_norm( const Tensor & input , int64_t num_groups , const Tensor & weight , const Tensor & bias , double eps , bool cudnn_enabled)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_cpu( const Tensor & self , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool train , double momentum , double eps)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Normalization.cpp,"at::native::batch_norm_backward_cpu( const Tensor & grad_out , const Tensor & self , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_invstd , bool train , double eps , std :: array<bool,3> grad_input_mask)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::data_ptr( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , Storage source)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , Storage source , int64_t storage_offset , IntList size , IntList stride)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self , const Tensor & source)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::set_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::is_set_to( const Tensor & self , const Tensor & tensor)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_fill_( Tensor & self , const Tensor & mask , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_fill_( Tensor & self , const Tensor & mask , const Tensor & value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_scatter_( Tensor & self , const Tensor & mask , const Tensor & source)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::view( const Tensor & self , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::put_( Tensor & self , const Tensor & index , const Tensor & source , bool accumulate)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_add_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_fill_( Tensor & self , int64_t dim , const Tensor & index , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_fill_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & src)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_( Tensor & self , int64_t dim , const Tensor & index , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::scatter_add_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & src)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::tril_( Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::triu_( Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma_( Tensor & self , int64_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm_( Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_( Tensor & self , Scalar exponent)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_( Tensor & self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lerp_( Tensor & self , const Tensor & end , Scalar weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign_( Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm_out( Tensor & result , const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addbmm( const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul_( Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv_( Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , int64_t from , int64_t to , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , int64_t to , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::random_( Tensor & self , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::uniform_( Tensor & self , double from , double to , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_( Tensor & self , double mean , double std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cauchy_( Tensor & self , double median , double sigma , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::log_normal_( Tensor & self , double mean , double std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::exponential_( Tensor & self , double lambd , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geometric_( Tensor & self , double p , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::diag_out( Tensor & result , const Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::diag( const Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cross_out( Tensor & result , const Tensor & self , const Tensor & other , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::cross( const Tensor & self , const Tensor & other , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::triu_out( Tensor & result , const Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::triu( const Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::tril_out( Tensor & result , const Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::tril( const Tensor & self , int64_t diagonal)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trace( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ne( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eq( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ge( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::le( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gt( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lt( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::take_out( Tensor & result , const Tensor & self , const Tensor & index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::take( const Tensor & self , const Tensor & index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_select_out( Tensor & result , const Tensor & self , int64_t dim , const Tensor & index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::index_select( const Tensor & self , int64_t dim , const Tensor & index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_select_out( Tensor & result , const Tensor & self , const Tensor & mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::masked_select( const Tensor & self , const Tensor & mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::nonzero_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::nonzero( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gather_out( Tensor & result , const Tensor & self , int64_t dim , const Tensor & index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gather( const Tensor & self , int64_t dim , const Tensor & index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul_out( Tensor & result , const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcmul( const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv_out( Tensor & result , const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::addcdiv( const Tensor & self , const Tensor & tensor1 , const Tensor & tensor2 , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gels_out( Tensor & X , Tensor & qr , const Tensor & self , const Tensor & A)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::gels( const Tensor & self , const Tensor & A)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trtrs_out( Tensor & X , Tensor & M , const Tensor & self , const Tensor & A , bool upper , bool transpose , bool unitriangular)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::trtrs( const Tensor & self , const Tensor & A , bool upper , bool transpose , bool unitriangular)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::symeig_out( Tensor & e , Tensor & V , const Tensor & self , bool eigenvectors , bool upper)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::symeig( const Tensor & self , bool eigenvectors , bool upper)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eig_out( Tensor & e , Tensor & v , const Tensor & self , bool eigenvectors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::eig( const Tensor & self , bool eigenvectors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::svd_out( Tensor & U , Tensor & S , Tensor & V , const Tensor & self , bool some , bool compute_uv)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::svd( const Tensor & self , bool some , bool compute_uv)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::potri_out( Tensor & result , const Tensor & self , bool upper)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::potri( const Tensor & self , bool upper)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pstrf_out( Tensor & u , Tensor & piv , const Tensor & self , bool upper , Scalar tol)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pstrf( const Tensor & self , bool upper , Scalar tol)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::qr_out( Tensor & Q , Tensor & R , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::qr( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geqrf_out( Tensor & result0 , Tensor & result1 , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::geqrf( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::orgqr_out( Tensor & result , const Tensor & self , const Tensor & input2)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::orgqr( const Tensor & self , const Tensor & input2)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ormqr_out( Tensor & result , const Tensor & self , const Tensor & input2 , const Tensor & input3 , bool left , bool transpose)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::ormqr( const Tensor & self , const Tensor & input2 , const Tensor & input3 , bool left , bool transpose)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact_out( Tensor & A_LU , Tensor & pivots , const Tensor & self , bool pivot)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact( const Tensor & self , bool pivot)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact_with_info_out( Tensor & A_LU , Tensor & pivots , Tensor & info , const Tensor & self , bool pivot)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrifact_with_info( const Tensor & self , bool pivot)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrisolve_out( Tensor & result , const Tensor & self , const Tensor & LU_data , const Tensor & LU_pivots)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::btrisolve( const Tensor & self , const Tensor & LU_data , const Tensor & LU_pivots)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::multinomial_out( Tensor & result , const Tensor & self , int64_t num_samples , bool replacement , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::multinomial( const Tensor & self , int64_t num_samples , bool replacement , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lgamma( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::digamma( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma_out( Tensor & result , int64_t n , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::polygamma( int64_t n , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::erfinv( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::frac( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::dist( const Tensor & self , const Tensor & other , Scalar p)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::reciprocal( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::neg( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::atan2( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lerp_out( Tensor & result , const Tensor & self , const Tensor & end , Scalar weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::lerp( const Tensor & self , const Tensor & end , Scalar weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::histc_out( Tensor & result , const Tensor & self , int64_t bins , Scalar min , Scalar max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::histc( const Tensor & self , int64_t bins , Scalar min , Scalar max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign_out( Tensor & result , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sign( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::fmod( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_out( Tensor & result , const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::remainder( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::min( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max_out( Tensor & result , const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::max( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::median( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sort_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool descending)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::sort( const Tensor & self , int64_t dim , bool descending)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::topk_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t k , int64_t dim , bool largest , bool sorted)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::topk( const Tensor & self , int64_t k , int64_t dim , bool largest , bool sorted)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::all( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::any( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm_out( Tensor & result , const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::renorm( const Tensor & self , Scalar p , int64_t dim , Scalar maxnorm)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::unfold( const Tensor & self , int64_t dimension , int64_t size , int64_t step)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::equal( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_out( Tensor & result , const Tensor & self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow( const Tensor & self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow_out( Tensor & result , Scalar self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::pow( Scalar self , const Tensor & exponent)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , const Tensor & mean , double std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( const Tensor & mean , double std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , double mean , const Tensor & std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( double mean , const Tensor & std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal_out( Tensor & output , const Tensor & mean , const Tensor & std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::normal( const Tensor & mean , const Tensor & std , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::alias( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_dirichlet_grad_out( Tensor & output , const Tensor & x , const Tensor & alpha , const Tensor & total)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::_dirichlet_grad( const Tensor & x , const Tensor & alpha , const Tensor & total)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__and__( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__and__( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__or__( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__or__( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__xor__( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__xor__( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__lshift__( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__lshift__( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__rshift__( const Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__rshift__( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__iand__( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__iand__( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ior__( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ior__( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ixor__( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ixor__( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ilshift__( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__ilshift__( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__irshift__( Tensor & self , Scalar other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp,"at::native::__irshift__( Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::norm_except_dim( const Tensor & v , int64_t pow , int64_t dim)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::_weight_norm( const Tensor & v_in , const Tensor & g_in , int64_t dim)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/WeightNorm.cpp,"at::native::_weight_norm_differentiable_backward( const Tensor & grad_w , const Tensor & saved_v , const Tensor & saved_g , const Tensor & saved_norms , int64_t dim)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::operator < <( std :: ostream & out , const ConvParams & params)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_strided() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_dilated() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_padded() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_output_padding_neg() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_output_padding_big() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_padding_neg() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::view1d_as_2d()",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_cudnn( const at :: Tensor & input) const",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_miopen( const at :: Tensor & input) const",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::use_mkldnn( const at :: Tensor & input) const",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::ConvParams::is_depthwise( const at :: Tensor & input , const at :: Tensor & weight) const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::check_input_shape_forward( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , int64_t groups , bool transposed)",39, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::view4d( const at :: Tensor & tensor)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::view3d( const at :: Tensor & tensor)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::subtensor( at :: Tensor & tensor , int dim , int groups , int g)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv1d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , int64_t groups)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv2d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , int64_t groups)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv3d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , int64_t groups)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose1d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , int64_t groups , IntList dilation)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose2d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , int64_t groups , IntList dilation)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::conv_transpose3d( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , int64_t groups , IntList dilation)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::convolution( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , bool transposed , IntList output_padding , int64_t groups)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::convolution_expand_param_if_needed( IntList list_param , const char * param_name , int64_t expected_dim)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution( const Tensor & input_r , const Tensor & weight_r , const Tensor & bias_r , IntList stride_ , IntList padding_ , IntList dilation_ , bool transposed_ , IntList output_padding_ , int64_t groups_ , bool benchmark , bool deterministic , bool cudnn_enabled)",115, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution_nogroup( const Tensor & input , const Tensor & weight , const Tensor & bias , IntList stride , IntList padding , IntList dilation , bool transposed , IntList output_padding)",58, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::subvariable( const Tensor & var , int dim , int groups , int g)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Convolution.cpp,"at::native::_convolution_double_backward( const Tensor & ggI , const Tensor & ggW_r , const Tensor & ggb , const Tensor & gO_r , const Tensor & weight_r , const Tensor & input , IntList stride_ , IntList padding_ , IntList dilation_ , bool transposed_ , IntList output_padding_ , int64_t groups_ , bool benchmark , bool deterministic , bool cudnn_enabled , std :: array<bool,3> output_mask)",220, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::window_function_checks( const char * function_name , const TensorOptions & options , int64_t window_length)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::getFactoryType( const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar start , Scalar end , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar start , Scalar end)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar start , Scalar end , Scalar step)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange( Scalar end , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::arange_out( Tensor & result , Scalar end)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::_dim_arange( const Tensor & like , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_cpu( IntList size , const TensorOptions & options)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_out( Tensor & result , IntList size)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_strided( IntList size , IntList stride , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_like( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::empty_like( const Tensor & self , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye( int64_t n , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye( int64_t n , int64_t m , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye_out_cpu( Tensor & result , int64_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::eye_out_cpu( Tensor & result , int64_t n , int64_t m)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full( IntList size , Scalar fill_value , const TensorOptions & options)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_out( Tensor & result , IntList size , Scalar fill_value)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_like( const Tensor & self , Scalar fill_value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::full_like( const Tensor & self , Scalar fill_value , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace( Scalar start , Scalar end , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace( Scalar start , Scalar end , int64_t steps , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace_out( Tensor & result , Scalar start , Scalar end)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::linspace_out( Tensor & result , Scalar start , Scalar end , int64_t steps)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace( Scalar start , Scalar end , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace( Scalar start , Scalar end , int64_t steps , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace_out( Tensor & result , Scalar start , Scalar end)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::logspace_out( Tensor & result , Scalar start , Scalar end , int64_t steps)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones( IntList size , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_out( Tensor & result , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_like( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::ones_like( const Tensor & self , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::scalar_tensor( Scalar s , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand( IntList size , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand( IntList size , Generator * generator , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_out( Tensor & result , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_out( Tensor & result , IntList size , Generator * generator)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_like( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::rand_like( const Tensor & self , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t high , IntList size , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t high , IntList size , Generator * generator , const TensorOptions & options)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t low , int64_t high , IntList size , const TensorOptions & options)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint( int64_t low , int64_t high , IntList size , Generator * generator , const TensorOptions & options)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t high , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t high , IntList size , Generator * generator)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t low , int64_t high , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_out( Tensor & result , int64_t low , int64_t high , IntList size , Generator * generator)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t high)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t low , int64_t high)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t high , const TensorOptions & options)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randint_like( const Tensor & self , int64_t low , int64_t high , const TensorOptions & options)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn( IntList size , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn( IntList size , Generator * generator , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_out( Tensor & result , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_out( Tensor & result , IntList size , Generator * generator)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_like( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randn_like( const Tensor & self , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_cpu( Tensor & result , int64_t n , THGenerator * generator)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::get_generator( at :: Generator * gen)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm( int64_t n , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm( int64_t n , Generator * generator , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_out( Tensor & result , int64_t n)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::randperm_out_cpu( Tensor & result , int64_t n , Generator * generator)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range( Scalar start , Scalar end , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range( Scalar start , Scalar end , Scalar step , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range_out( Tensor & result , Scalar start , Scalar end)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::range_out( Tensor & result , Scalar start , Scalar end , Scalar step)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::get_tril_size( int64_t row , int64_t col , int64_t offset)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::check_args( int64_t row , int64_t col , const TensorOptions & options)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tril_indices( int64_t row , int64_t col , int64_t offset , const TensorOptions & options)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::triu_indices( int64_t row , int64_t col , int64_t offset , const TensorOptions & options)",35, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros( IntList size , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_out( Tensor & result , IntList size)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_like( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::zeros_like( const Tensor & self , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::bartlett_window( int64_t window_length , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::bartlett_window( int64_t window_length , bool periodic , const TensorOptions & options)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::blackman_window( int64_t window_length , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::blackman_window( int64_t window_length , bool periodic , const TensorOptions & options)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , const TensorOptions & options)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , double alpha , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hamming_window( int64_t window_length , bool periodic , double alpha , double beta , const TensorOptions & options)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hann_window( int64_t window_length , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::hann_window( int64_t window_length , bool periodic , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tensor_cpu( ArrayRef<T> values , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorFactories.cpp,"at::native::tensor_cuda( ArrayRef<T> values , const TensorOptions & options)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::flip_cpu_kernel( const int64_t total_dims , const std :: vector<int64_t> & stride_contiguous_v , const std :: bitset<dim_bitset_size> & flip_dims_b , const Tensor & in_tensor , Tensor & out_tensor)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::flip_cpu( const Tensor & self , IntList dims)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::roll_cpu( const Tensor & self , IntList shifts , IntList dims)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorTransformations.cpp,"at::native::rot90( const Tensor & self , int64_t k , IntList dims)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/DispatchStub.cpp,"at::native::compute_cpu_capability()",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/DispatchStub.cpp,"at::native::get_cpu_capability()",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::TensorIterator::parallel_reduce( const loop2d_t & loop)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::use_two_pass_reduction( TensorIterator & iter)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::two_pass_reduction( TensorIterator & iter , const loop2d_t & loop)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::find_split_dim( TensorIterator & iter)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::round_columns( TensorIterator & iter , int dim , int multiple , int64_t begin , int64_t end)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::parallel_dim_reduction( TensorIterator & iter , const loop2d_t & loop)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp,"at::TensorIterator::foreach_reduced_elt( const loop_subiter_t & loop)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::host_softmax( Tensor output , const Tensor & input , const int64_t dim)",53, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::host_softmax_backward( Tensor & gI , const Tensor & grad , const Tensor & output , int64_t dim)",51, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax_cpu( const Tensor & input_ , const int64_t dim_ , const bool half_to_float)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax_cpu( const Tensor & input_ , const int64_t dim_ , const bool half_to_float)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax_backward_cpu( const Tensor & grad_ , const Tensor & output_ , int64_t dim_ , const Tensor & input_)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax_backward_cpu( const Tensor & grad_ , const Tensor & output_ , int64_t dim_ , const Tensor & input_)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax( const Tensor & input_ , const int64_t dim_)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::softmax( const Tensor & input_ , const int64_t dim_ , ScalarType dtype)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax( const Tensor & input_ , const int64_t dim_)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SoftMax.cpp,"at::native::log_softmax( const Tensor & input_ , const int64_t dim_ , ScalarType dtype)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv( int n , int nrhs , scalar_t * a , int lda , int * ipiv , scalar_t * b , int ldb , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf( int m , int n , scalar_t * a , int lda , int * ipiv , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri( int n , scalar_t * a , int lda , int * ipiv , scalar_t * work , int lwork , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackPotrs( char uplo , int n , int nrhs , scalar_t * a , int lda , scalar_t * b , int ldb , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky( char uplo , int n , scalar_t * a , int lda , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv<double>( int n , int nrhs , double * a , int lda , int * ipiv , double * b , int ldb , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGesv<float>( int n , int nrhs , float * a , int lda , int * ipiv , float * b , int ldb , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri<double>( int n , double * a , int lda , int * ipiv , double * work , int lwork , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetri<float>( int n , float * a , int lda , int * ipiv , float * work , int lwork , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf<double>( int m , int n , double * a , int lda , int * ipiv , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackGetrf<float>( int m , int n , float * a , int lda , int * ipiv , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackPotrs<double>( char uplo , int n , int nrhs , double * a , int lda , double * b , int ldb , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackPotrs<float>( char uplo , int n , int nrhs , float * a , int lda , float * b , int ldb , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky<double>( char uplo , int n , double * a , int lda , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::lapackCholesky<float>( char uplo , int n , float * a , int lda , int * info)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_gesv( Tensor & b , Tensor & A , std :: vector<int64_t> & infos)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_gesv_helper_cpu( const Tensor & self , const Tensor & A)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::gesv( const Tensor & self , const Tensor & A)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::gesv_out( Tensor & solution , Tensor & lu , const Tensor & self , const Tensor & A)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_inverse( Tensor & self , std :: vector<int64_t> & infos)",40, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_inverse_helper_cpu( const Tensor & self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::inverse( const Tensor & self)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::inverse_out( Tensor & result , const Tensor & self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_potrs( Tensor & b , Tensor & A , bool upper , std :: vector<int64_t> & infos)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_potrs_helper_cpu( const Tensor & self , const Tensor & A , bool upper)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::potrs( const Tensor & self , const Tensor & A , bool upper)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::potrs_out( Tensor & result , const Tensor & self , const Tensor & A , bool upper)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::apply_cholesky( Tensor & self , bool upper , std :: vector<int64_t> & infos)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::_cholesky_helper_cpu( const Tensor & self , bool upper)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky( const Tensor & self , bool upper)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp,"at::native::cholesky_out( Tensor & result , const Tensor & self , bool upper)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::invalid_mask( const Tensor & self , int64_t idx , const Tensor & mask , int64_t maskIdx)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::checkIndexTensorTypes( TensorList indices)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::expandByteTensors( const Tensor & self , TensorList indices)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::hasContiguousSubspace( TensorList tl)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::transposeToFront( Tensor self , TensorList indices)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::computeLinearStride( const Tensor & tensor)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::unsqueezeN( const Tensor & src , int64_t before , int64_t after)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::wrapIndexOnce( const Tensor & index , int64_t dim , int64_t dim_size)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::computeLinearIndex( const Tensor & src , TensorList indices)",53, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::makeLinearIndex( Tensor self , TensorList orig)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::all_strides_match( TensorList tensors)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::shapes_as_str( TensorList tensors)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::restride_src( const Tensor & src , int64_t dims_before , int64_t dims_indexed , IntList replacement_shape)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::reshape_indexer( const Tensor & index , int64_t dims_before , int64_t dims_after)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::AdvancedIndex::AdvancedIndex( const Tensor & src , TensorList indices_list)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_info( Tensor self , TensorList orig)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_index_iterator( const AdvancedIndex & info)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::make_index_put_iterator( const AdvancedIndex & info , const Tensor & value)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index( const Tensor & self , TensorList indices)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_put( const Tensor & self , TensorList indices , const Tensor & value , bool accumulate)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_put_( Tensor & self , TensorList indices , const Tensor & value , bool accumulate)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Indexing.cpp,"at::native::index_copy_( Tensor & self , int64_t dim , const Tensor & index , const Tensor & source)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_lu_det_P_diag_U_info( const Tensor & self)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::det( const Tensor & self)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::logdet( const Tensor & self)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::slogdet( const Tensor & self)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::pinverse( const Tensor & self , double rcond)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_matrix_rank_helper( const Tensor & self , bool symmetric)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_rank( const Tensor & self , double tol , bool symmetric)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_rank( const Tensor & self , bool symmetric)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::check_1d( const Tensor & t , const char * arg , const char * fn)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::ger( const Tensor & self , const Tensor & vec2)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::ger_out( Tensor & result , const Tensor & self , const Tensor & vec2)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mm( const Tensor & self , const Tensor & mat2)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mm_out( Tensor & result , const Tensor & self , const Tensor & mat2)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mv( const Tensor & self , const Tensor & vec)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::mv_out( Tensor & result , const Tensor & self , const Tensor & vec)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv( const Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv_( Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addmv_out( Tensor & result , const Tensor & self , const Tensor & mat , const Tensor & vec , Scalar beta , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr( const Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr_( Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::addr_out( Tensor & result , const Tensor & self , const Tensor & vec1 , const Tensor & vec2 , Scalar beta , Scalar alpha)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_cpu_kernel( const Tensor & result , const Tensor & self , const Tensor & mat2 , Scalar beta_ , Scalar alpha_)",40, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_out_or_baddbmm_( Tensor & self_or_result , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha , bool is_bmm_out)",67, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_cpu( const Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm_out_cpu( Tensor & result , const Tensor & self_ , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::baddbmm__cpu( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_cpu( const Tensor & self , const Tensor & mat2)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::bmm_out_cpu( Tensor & result , const Tensor & batch1 , const Tensor & batch2)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::dot( const Tensor & self , const Tensor & tensor)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::dot_out( Tensor & result , const Tensor & self , const Tensor & tensor)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul( c10 :: optional<Tensor> out_opt , const Tensor & tensor1 , const Tensor & tensor2)",86, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul( const Tensor & tensor1 , const Tensor & tensor2)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matmul_out( Tensor & result , const Tensor & tensor1 , const Tensor & tensor2)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::matrix_power( const Tensor & a , int64_t n)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm( const Tensor & self , IntList dim , bool keepdim)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::frobenius_norm_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::nuclear_norm( const Tensor & self , bool keepdim)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::nuclear_norm_out( Tensor & result , const Tensor & self , bool keepdim)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_chain_matmul_general( TensorList matrices , std :: vector<std::vector<int64_t>> & order , int64_t i , int64_t j)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::_chain_matmul_three_matrices( TensorList matrices)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp,"at::native::chain_matmul( TensorList matrices)",56, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"_copy__cpu( at :: Tensor & self , const at :: Tensor & src)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"_copy__cpu( at :: Tensor & self , const at :: Tensor & src)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"copy_transpose_valid( const at :: Tensor & self , const at :: Tensor & src)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_s_copy__cpu( Tensor & self , const Tensor & src , bool non_blocking)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_copy_same_type_transpose_( Tensor & self , const Tensor & src)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Copy.cpp,"at::native::_copy_same_type__cpu( Tensor & self , const Tensor & src)",40, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::make_feature_noise( const Tensor & input)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::is_fused_kernel_acceptable( const Tensor & input , double p)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::multiply( Tensor & input , const Tensor & noise)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::multiply( const Tensor & input , const Tensor & noise)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::_dropout_impl( T & input , double p , bool train)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::dropout( const Tensor & input , double p , bool train)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::dropout_( Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_dropout( const Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_dropout_( Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::alpha_dropout( const Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::alpha_dropout_( Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_alpha_dropout( const Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Dropout.cpp,"at::native::feature_alpha_dropout_( Tensor & input , double p , bool train)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::start_index( int a , int b , int c)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::end_index( int a , int b , int c)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_frame( scalar_t * input_p , scalar_t * output_p , int64_t sizeD , int64_t isizeH , int64_t isizeW , int64_t osizeH , int64_t osizeW , int64_t istrideD , int64_t istrideH , int64_t istrideW)",52, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_cpu_template( at :: Tensor & output , at :: Tensor const & input , IntList output_size)",88, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_frame( scalar_t * gradInput_p , scalar_t * gradOutput_p , int64_t sizeD , int64_t isizeH , int64_t isizeW , int64_t osizeH , int64_t osizeW)",46, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_cpu_template( Tensor & gradInput , const Tensor & gradOutput_ , const Tensor & input)",72, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_out_cpu( Tensor & output , const Tensor & input , IntList output_size)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_cpu( at :: Tensor const & input , IntList output_size)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_out_cpu( Tensor & gradInput , const Tensor & gradOutput , const Tensor & input)",10, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/AdaptiveAveragePooling.cpp,"at::native::adaptive_avg_pool2d_backward_cpu( const Tensor & gradOutput , const Tensor & input)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RoiPooling.cpp,"at::native::RoiPooling2d_forward_cpu( const Tensor & input , const Tensor & rois , int64_t pooledHeight , int64_t pooledWidth , double spatialScale)",120, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RoiPooling.cpp,"at::native::RoiPooling2d_backward_cpu( const Tensor & input , const Tensor & rois , int64_t pooledHeight , int64_t pooledWidth , double spatialScale , const Tensor & gradOutput , const Tensor & argmaxes)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::integer_upcast( const Tensor & self , optional<ScalarType> dtype)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::make_dim_mask( IntList dims , int ndim)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::allocate_reduction_result( Tensor & result , const Tensor & self , DimMask mask , bool keepdim , ScalarType dtype)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::review_reduce_result( const Tensor & result , int ndim , DimMask mask , bool keepdim)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::make_reduction( const char * name , Tensor & result , const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::n_dim_size( const Tensor & self , IntList dim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim , optional<ScalarType> dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum( const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim , optional<ScalarType> dtype)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumsum_out( Tensor & result , const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim , optional<ScalarType> dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod( const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim , optional<ScalarType> dtype)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::cumprod_out( Tensor & result , const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , optional<ScalarType> dtype)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::get_dtype( Tensor & result , const Tensor & self , optional<ScalarType> dtype , bool promote_integers = false)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> opt_dtype)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> opt_dtype)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean_out( Tensor & result , const Tensor & self , IntList dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum_out( Tensor & result , const Tensor & self , IntList dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim , ScalarType dtype)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod_out( Tensor & result , const Tensor & self , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , bool keepdim , optional<ScalarType> dtype)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::mean( const Tensor & self , IntList dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , bool keepdim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::sum( const Tensor & self , IntList dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , bool keepdim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::prod( const Tensor & self , int64_t dim , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::logsumexp_out( Tensor & result , const Tensor & self , int64_t dim_ , bool keepdim)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::logsumexp( const Tensor & self , int64_t dim_ , bool keepdim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_norm_out_cpu( Tensor & result , const Tensor & self , Scalar p , int64_t dim_ , bool keepdim)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm_out( Tensor & result , const Tensor & self , Scalar p , int64_t dim , bool keepdim)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::_norm( const Tensor & self , Scalar p)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , Scalar p , int64_t dim , bool keepdim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::norm( const Tensor & self , Scalar p)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all( const Tensor & self , int64_t dim , bool keepdim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::all_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any( const Tensor & self , int64_t dim , bool keepdim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::any_out( Tensor & result , const Tensor & self , int64_t dim , bool keepdim)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var( const Tensor & self , bool unbiased)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var( const Tensor & self , int64_t dim , bool unbiased , bool keepdim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::var_out( Tensor & result , const Tensor & self , int64_t dim , bool unbiased , bool keepdim)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std( const Tensor & self , bool unbiased)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std( const Tensor & self , IntList dim , bool unbiased , bool keepdim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ReduceOps.cpp,"at::native::std_out( Tensor & result , const Tensor & self , IntList dim , bool unbiased , bool keepdim)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::make_offset2bag( const Tensor & offsets , const Tensor & indices , Tensor & offset2bag)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::index_select_add( const Tensor & select_indices , const Tensor & add_indices , const Tensor & src , Tensor & output)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::make_bag_size( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & bag_size)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::apply_bag_size( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & output , const Tensor & bag_size)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::apply_bag_size_backward( const Tensor & offsets , const Tensor & indices , const int64_t mode , Tensor & output , const Tensor & offset2bag , const Tensor & bag_size)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::embedding_bag_cpu_max( const Tensor & weight , const Tensor & indices , const Tensor & offset2bag , const Tensor & output , const Tensor & bag_size , const Tensor & offsets)",39, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::embedding_bag( const Tensor & weight , const Tensor & indices , const Tensor & offsets , const bool scale_grad_by_freq , const int64_t mode , bool sparse)",6, 5, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_cpu( const Tensor & weight , const Tensor & indices , const Tensor & offsets , const bool scale_grad_by_freq , const int64_t mode , bool sparse)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_backward( const Tensor & grad , const Tensor & indices , const Tensor & offsets , const Tensor & offset2bag , const Tensor & bag_size_ , const Tensor & max_indices_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode , bool sparse)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_dense_backward_cpu( const Tensor & grad_ , const Tensor & indices_ , const Tensor & offsets_ , const Tensor & offset2bag__ , const Tensor & bag_size_ , const Tensor & max_indices_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode)",98, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/EmbeddingBag.cpp,"at::native::_embedding_bag_sparse_backward( const Tensor & grad_ , const Tensor & indices , const Tensor & offsets , const Tensor & offset2bag , const Tensor & bag_size_ , int64_t num_weights , bool scale_grad_by_freq , int64_t mode)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::binary_cross_entropy_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::mse_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::l1_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss( const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multi_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , Scalar p , Scalar margin , const Tensor & weight , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_forward_out( Tensor & output , Tensor & is_target , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_forward( const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction , const Tensor & is_target)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::multilabel_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction , const Tensor & is_target)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_forward_out( Tensor & output , Tensor & total_weight , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_forward( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_out( Tensor & output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_forward_out( Tensor & output , Tensor & total_weight , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_forward( const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::nll_loss2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , const Tensor & weight , int64_t reduction , int64_t ignore_index , const Tensor & total_weight)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::smooth_l1_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_out( Tensor & output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss( const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::soft_margin_loss_backward( const Tensor & grad_output , const Tensor & self , const Tensor & target , int64_t reduction)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_out( Tensor & output , const Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu( const Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_backward_out( Tensor & grad_input , const Tensor & grad_output , Scalar alpha , Scalar scale , Scalar input_scale , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_backward( const Tensor & grad_output , Scalar alpha , Scalar scale , Scalar input_scale , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::elu_( Tensor & self , Scalar alpha , Scalar scale , Scalar input_scale)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_out( Tensor & output , const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu( const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::glu_backward( const Tensor & grad_output , const Tensor & self , int64_t dim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_out( Tensor & output , const Tensor & self , Scalar min_val , Scalar max_val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh( const Tensor & self , Scalar min_val , Scalar max_val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar min_val , Scalar max_val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_backward( const Tensor & grad_output , const Tensor & self , Scalar min_val , Scalar max_val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::hardtanh_( Tensor & self , Scalar min_val , Scalar max_val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_out( Tensor & output , const Tensor & self , Scalar negative_slope)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu( const Tensor & self , Scalar negative_slope)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar negative_slope)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_backward( const Tensor & grad_output , const Tensor & self , Scalar negative_slope)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::leaky_relu_( Tensor & self , Scalar negative_slope)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_out( Tensor & output , const Tensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_forward_out( Tensor & output , Tensor & buffer , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_forward( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & buffer)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::log_sigmoid_backward( const Tensor & grad_output , const Tensor & self , const Tensor & buffer)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_out( Tensor & output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise( const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_backward( const Tensor & grad_output , const Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::rrelu_with_noise_( Tensor & self , const Tensor & noise , Scalar lower , Scalar upper , bool training , Generator * generator)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_out( Tensor & output , const Tensor & self , Scalar beta , Scalar threshold)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus( const Tensor & self , Scalar beta , Scalar threshold)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar beta , Scalar threshold , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softplus_backward( const Tensor & grad_output , const Tensor & self , Scalar beta , Scalar threshold , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_out( Tensor & output , const Tensor & self , Scalar lambd)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink( const Tensor & self , Scalar lambd)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , Scalar lambd)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::softshrink_backward( const Tensor & grad_output , const Tensor & self , Scalar lambd)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_out( Tensor & output , const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d( const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_avg_pool3d_backward( const Tensor & grad_output , const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_out( Tensor & output , Tensor & indices , const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d( const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_out( Tensor & output , Tensor & indices , const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d( const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::adaptive_max_pool3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_out( Tensor & output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool2d_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_out( Tensor & output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::avg_pool3d_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d_out( Tensor & output , Tensor & indices , const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & random_samples)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d( const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & random_samples)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::fractional_max_pool2d_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList output_size , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_out( Tensor & output , Tensor & indices , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool2d_with_indices_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_out( Tensor & output , Tensor & indices , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_pool3d_with_indices_backward( const Tensor & grad_output , const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode , const Tensor & indices)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_out( Tensor & output , const Tensor & self , const Tensor & indices , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d( const Tensor & self , const Tensor & indices , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_out( Tensor & output , const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d( const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::max_unpool3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & indices , IntList output_size , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d_out( Tensor & output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d( const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad1d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d_out( Tensor & output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d( const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::reflection_pad2d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d_out( Tensor & output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d( const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad1d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d_out( Tensor & output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d( const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad2d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d_out( Tensor & output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d( const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::replication_pad3d_backward( const Tensor & grad_output , const Tensor & self , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_out( Tensor & output , const Tensor & self , IntList output_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d( const Tensor & self , IntList output_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_linear1d_backward( const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_out( Tensor & output , const Tensor & self , IntList output_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d( const Tensor & self , IntList output_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_bilinear2d_backward( const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_out( Tensor & output , const Tensor & self , IntList output_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d( const Tensor & self , IntList output_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_trilinear3d_backward( const Tensor & grad_output , IntList output_size , IntList input_size , bool align_corners)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_out( Tensor & output , const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d( const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest1d_backward( const Tensor & grad_output , IntList output_size , IntList input_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_out( Tensor & output , const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d( const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest2d_backward( const Tensor & grad_output , IntList output_size , IntList input_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_out( Tensor & output , const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d( const Tensor & self , IntList output_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_backward_out( Tensor & grad_input , const Tensor & grad_output , IntList output_size , IntList input_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::upsample_nearest3d_backward( const Tensor & grad_output , IntList output_size , IntList input_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::sigmoid_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::sigmoid_backward( const Tensor & grad_output , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::tanh_backward_out( Tensor & grad_input , const Tensor & grad_output , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::tanh_backward( const Tensor & grad_output , const Tensor & output)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & columns , const Tensor & ones)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList output_padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & finput , const Tensor & fgrad_input)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_transpose3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList output_padding , IntList dilation , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_forward_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_backward_out( Tensor & grad_input , Tensor & grad_weight , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_depthwise2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , std :: array<bool,2> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_forward_out( Tensor & output , Tensor & finput , Tensor & fgrad_input , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , const Tensor & finput , const Tensor & fgrad_input , std :: array<bool,3> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated2d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_out( Tensor & output , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_forward_out( Tensor & output , Tensor & columns , Tensor & ones , const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_forward( const Tensor & self , const Tensor & weight , IntList kernel_size , const Tensor & bias , IntList stride , IntList padding , IntList dilation)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_backward_out( Tensor & grad_input , Tensor & grad_weight , Tensor & grad_bias , const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyNNDefinitions.cpp,"at::native::thnn_conv_dilated3d_backward( const Tensor & grad_output , const Tensor & self , const Tensor & weight , IntList kernel_size , IntList stride , IntList padding , IntList dilation , const Tensor & columns , const Tensor & ones , std :: array<bool,3> output_mask)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding( const Tensor & weight , const Tensor & indices , int64_t padding_idx , bool scale_grad_by_freq , bool sparse)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_backward( const Tensor & grad , const Tensor & indices , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq , bool sparse)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_sparse_backward( const Tensor & grad_ , const Tensor & indices_ , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq)",36, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_dense_backward_cpu( const Tensor & grad_ , const Tensor & indices , int64_t num_weights , int64_t padding_idx , bool scale_grad_by_freq)",67, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Embedding.cpp,"at::native::embedding_renorm_cpu_( Tensor & self , const Tensor & indices , double max_norm , double norm_type)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::linear( const Tensor & input , const Tensor & weight , const Tensor & bias)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::sumproduct_pair( const Tensor & left_ , const Tensor & right_ , IntList sum_dims_ , bool keepdim)",95, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::einsum( std :: string eqn , TensorList tensors)",232, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::_trilinear( const Tensor & i1_ , const Tensor & i2_ , const Tensor & i3_ , IntList expand1_ , IntList expand2_ , IntList expand3_ , IntList sumdim_ , int64_t unroll_dim)",68, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::bilinear( const Tensor & input1 , const Tensor & input2 , const Tensor & weight , const Tensor & bias)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Linear.cpp,"at::native::tensordot( const Tensor & input1 , const Tensor & input2 , IntList dims1 , IntList dims2)",55, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::is_same_size( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::size( const Tensor & self , int64_t dim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::stride( const Tensor & self , int64_t dim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::cudnn_is_acceptable( const Tensor & self)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::detach( const Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::detach_( Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorProperties.cpp,"at::native::contiguous( const Tensor & self)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Scalar.cpp,"at::native::item( const Tensor & self)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Scalar.cpp,"at::native::_local_scalar_dense_cpu( const Tensor & self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::ensure_has_index( Device device)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to_impl( const Tensor & self , const TensorOptions & options , bool non_blocking)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , const TensorOptions & options , bool non_blocking , bool copy)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , Device device , ScalarType dtype , bool non_blocking , bool copy)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , ScalarType dtype , bool non_blocking , bool copy)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorConversions.cpp,"at::native::to( const Tensor & self , const Tensor & other , bool non_blocking , bool copy)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::reorder_dimensions()",53, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::compute_result_type( at :: ArrayRef<OperandInfo> operands , const F & predicate)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_types()",46, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_common_type()",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compatible_stride( int element_size) const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::invert_perm( IntList input) const",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::allocate_outputs()",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::coalesce_dimensions()",52, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::numel() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_dim_strides( int dim) const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_data_ptrs( ArrayRef<char*> base , IntList counter) const",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_base_ptrs() const",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_dim_reduced( int dim) const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::permute_dimensions( IntList perm)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::num_output_elements() const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::num_reduce_dims() const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::loop_wrapper( const loop_t & loop)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::for_each( const loop_t & loop)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::for_each( const loop2d_t & loop)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_strides() const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::serial_for_each( const loop_t & loop , Range range) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::serial_for_each( const loop2d_t & loop , Range range) const",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_trivial_1d() const",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_scalar( int arg) const",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::is_cpu_scalar( int arg) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::data_ptr( int arg) const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::remove_operand( int arg)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::replace_operand( int arg , void * data , IntList stride)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::remove_dimension( int dim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::narrow( int dim , int64_t start , int64_t size)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::select_all_keeping_dim( int start_dim , IntList indices)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::binary_op( Tensor & out , const Tensor & a , const Tensor & b)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::reduce_op( Tensor & out , const Tensor & a)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::mark_outputs()",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_shape()",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::compute_stride( const Tensor & tensor , IntList shape)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::compute_strides()",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::can_use_32bit_indexing() const",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::split( int dim)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::get_dim_to_split() const",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::with_32bit_indexing() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::TensorIterator::Builder::build()",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::iterator( const TensorIterator & iter)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::operator ++()",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::iterator::operator *() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::begin() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::SplitUntil32Bit::end() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::DimCounter( IntList shape , Range range)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::is_done() const",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::increment( const std :: array<int64_t,2> & step)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorIterator.cpp,"at::DimCounter::max_2d_step() const",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConstantPadNd.cpp,"at::native::constant_pad_nd( const Tensor & self , IntList pad , Scalar value)",68, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::pairwise_distance( const Tensor & x1 , const Tensor & x2 , double p , double eps , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::pdist( const Tensor & self , const double p)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::_pdist_forward( const Tensor & self , const double p)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::_pdist_backward( const Tensor & grad , const Tensor & self , const double p , const Tensor & pdist)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distance.cpp,"at::native::cosine_similarity( const Tensor & x1 , const Tensor & x2 , int64_t dim , double eps)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"sample_poisson( double lambda , THGenerator * generator)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli( const Tensor & self , Generator * gen)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli( const Tensor & self , double p , Generator * gen)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_out( Tensor & result , const Tensor & self , Generator * gen)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_tensor_cpu_( Tensor & self , const Tensor & p_ , Generator * gen)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::bernoulli_scalar_cpu_( Tensor & self , double p , Generator * gen)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_standard_gamma_grad_cpu( const Tensor & self , const Tensor & output)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_s_poisson_cpu( const Tensor & lambda , Generator * gen)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Distributions.cpp,"at::native::_s_gamma_cpu( const Tensor & alpha , Generator * gen)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::_type_has_native( const Type & dtype)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::_has_native( const Tensor & self)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::clone( const Tensor & self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::resize_as_( Tensor & self , const Tensor & the_template)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::pow_out( Tensor & result , const Tensor & self , Scalar exponent)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::pow( const Tensor & self , Scalar exponent)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::zero_( Tensor & self)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm_out( Tensor & result , const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm( const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/LegacyBridge.cpp,"at::native::addmm_( Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::linspace_from_neg_one( const Tensor & grid , int64_t num_steps)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::make_base_grid_4D( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::make_base_grid_5D( const Tensor & theta , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_4D( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_5D( const Tensor & theta , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator( const Tensor & theta , IntList size)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_4D_backward( const Tensor & grad_grid , int64_t N , int64_t C , int64_t H , int64_t W)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_5D_backward( const Tensor & grad_grid , int64_t N , int64_t C , int64_t D , int64_t H , int64_t W)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp,"at::native::affine_grid_generator_backward( const Tensor & grad , IntList size)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_cpu_template( const Tensor & self , const bool sorted , const bool return_inverse)",33, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu_impl( ForwardIt first , ForwardIt last , std :: vector<int64_t> & indices , Tensor inverse_indices_vec)",23, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu_template( const Tensor & self , const int64_t dim , const bool return_inverse)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_cpu( const Tensor & self , const bool sorted , const bool return_inverse)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Unique.cpp,"at::native::_unique_dim_cpu( const Tensor & self , const int64_t dim , const bool sorted , const bool return_inverse)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::checkLongTensor( const Tensor & tensor)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pack_padded_sequence( const Tensor & _input , const Tensor & _lengths , bool batch_first)",69, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pack_padded_sequence_backward( const Tensor & grad , at :: IntList input_size , const Tensor & _batch_sizes , bool batch_first)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PackedSequence.cpp,"at::native::_pad_packed_sequence( const Tensor & data , const Tensor & _batch_sizes , bool batch_first , Scalar padding_value , int64_t total_length)",62, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp( const Tensor & self , optional<Scalar> min , optional<Scalar> max)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp_max( const Tensor & self , Scalar max)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::clamp_min( const Tensor & self , Scalar min)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp__cpu( Tensor & self , optional<Scalar> min , optional<Scalar> max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_out_cpu( Tensor & result , const Tensor & self , optional<Scalar> min , optional<Scalar> max)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_max__cpu( Tensor & self , Scalar max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_max_out_cpu( Tensor & result , const Tensor & self , Scalar max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_min__cpu( Tensor & self , Scalar min)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::_clamp_min_out_cpu( Tensor & result , const Tensor & self , Scalar min)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::fill_( Tensor & self , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::fill_( Tensor & self , const Tensor & value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::mvlgamma( const Tensor & self , int64_t p)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/UnaryOps.cpp,"at::native::mvlgamma_( Tensor & self , int64_t p)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"where_cpu( at :: Tensor & ret , const at :: Tensor & condition , const at :: Tensor & self , const at :: Tensor & other)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::allclose( const Tensor & self , const Tensor & other , double rtol , double atol , bool equal_nan)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::isclose( const Tensor & self , const Tensor & other , double rtol , double atol , bool equal_nan)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::is_nonzero( const Tensor & self)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::where( const Tensor & condition , const Tensor & self , const Tensor & other)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_s_where_cpu( const Tensor & condition , const Tensor & self , const Tensor & other)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::kthvalue( const Tensor & self , int64_t k , int64_t dim , bool keepdim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::kthvalue_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t k , int64_t dim , bool keepdim)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::median( const Tensor & self , int64_t dim , bool keepdim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::median_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool keepdim)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::mode( const Tensor & self , int64_t dim , bool keepdim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::mode_out( Tensor & values , Tensor & indices , const Tensor & self , int64_t dim , bool keepdim)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_max_out_cpu( Tensor & max , Tensor & max_indices , const Tensor & self , int64_t dim , bool keepdim)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max( const Tensor & self , int64_t dim , bool keepdim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max_out( Tensor & max , Tensor & max_indices , const Tensor & self , int64_t dim , bool keepdim)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::max_values( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_min_out_cpu( Tensor & min , Tensor & min_indices , const Tensor & self , int64_t dim , bool keepdim)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min( const Tensor & self , int64_t dim , bool keepdim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min_out( Tensor & min , Tensor & min_indices , const Tensor & self , int64_t dim , bool keepdim)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::min_values( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmax( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmax( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmin( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::argmin( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_argmax( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorCompare.cpp,"at::native::_argmin( const Tensor & self , int64_t dim , bool keepdim)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConvolutionTBC.cpp,"at::native::conv_tbc( const Tensor & self , const Tensor & weight , const Tensor & bias , int64_t pad)",51, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/ConvolutionTBC.cpp,"at::native::conv_tbc_backward( const Tensor & dOutput , const Tensor & input , const Tensor & weight , const Tensor & bias , int64_t pad)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Memory.cpp,"at::native::pin_memory( const Tensor & self)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/PixelShuffle.cpp,"at::native::pixel_shuffle( const Tensor & self , int64_t upscale_factor)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::clip_coordinates( scalar_t in , int64_t clip_limit)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::clip_coordinates_set_grad( scalar_t in , int64_t clip_limit , scalar_t * grad_in)",16, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::reflect_coordinates( scalar_t in , int64_t clip_limit)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::reflect_coordinates_set_grad( scalar_t in , int64_t clip_limit , scalar_t * grad_in)",25, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::within_bounds_2d( int64_t h , int64_t w , int64_t H , int64_t W)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::within_bounds_3d( int64_t d , int64_t h , int64_t w , int64_t D , int64_t H , int64_t W)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::safe_add_2d( scalar_t * data , int64_t h , int64_t w , int64_t sH , int64_t sW , int64_t H , int64_t W , scalar_t delta)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::safe_add_3d( scalar_t * data , int64_t d , int64_t h , int64_t w , int64_t sD , int64_t sH , int64_t sW , int64_t D , int64_t H , int64_t W , scalar_t delta)",8, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_cpu_impl( const Tensor & input , const Tensor & grid , GridSamplerInterpolation interpolation_mode , GridSamplerPadding padding_mode)",165, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_backward_cpu_impl( const Tensor & grad_output , const Tensor & input , const Tensor & grid , GridSamplerInterpolation interpolation_mode , GridSamplerPadding padding_mode)",231, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_2d_cpu( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_cpu( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_2d_backward_cpu( const Tensor & grad_output , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler_3d_backward_cpu( const Tensor & grad_output , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/GridSampler.cpp,"at::native::grid_sampler( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",54, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::tanh_f::operator ( )( const Tensor & t) const",1, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::relu_f::operator ( )( const Tensor & t) const",1, 67, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedSequence::PackedSequence( Tensor _data , Tensor _batch_sizes)",2, 70, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::CellParams::CellParams( const Tensor & _w_ih , const Tensor & _w_hh , const Tensor & _b_ih , const Tensor & _b_hh)",2, 61, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::pair_vec( const std :: vector<T> & vals)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::unpair_vec( std :: vector<pair_of<T>> && vals)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gather_params( TensorList params , bool has_biases)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_as_output( const Tensor & t)",1, 55, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_as_output( const tpair_of<Tensor> & t)",1, 78, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::project( at :: ArrayRef<tpair_of<Tensor>> tuples)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_concat( at :: ArrayRef<Tensor> hiddens)",1, 83, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_concat( at :: ArrayRef<tpair_of<Tensor>> hiddens)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_slice( const Tensor & t , int64_t start , int64_t end)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::hidden_slice( const tpair_of<Tensor> & t , int64_t start , int64_t end)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::Cell::~Cell()",1, 104, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::SimpleCell::operator ( )( const Tensor & input , const hidden_type & hidden , const CellParams & params) const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::LSTMCell::operator ( )( const Tensor & input , const hidden_type & hidden , const CellParams & params) const",25, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::GRUCell::operator ( )( const Tensor & input , const hidden_type & hidden , const CellParams & params) const",20, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::Layer::~Layer()",1, 105, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::FullLayer( Cell<hidden_type> & cell)",2, 22, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::operator ( )( std :: vector<Tensor> step_inputs , const hidden_type & input_hidden , const CellParams & params) const",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullLayer::operator ( )( const Tensor & inputs , const hidden_type & input_hidden , const CellParams & params) const",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::FullBidirectionalLayer( Cell<dir_hidden_type> & cell)",2, 23, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::operator ( )( const Tensor & input , const hidden_type & input_hidden , const param_type & params) const",13, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::FullBidirectionalLayer::reverse( std :: vector<Tensor> && x) const",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedLayer::PackedLayer( Cell<hidden_type> & cell)",2, 22, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const CellParams & params) const",35, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::ReversedPackedLayer::ReversedPackedLayer( Cell<hidden_type> & cell)",2, 22, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::ReversedPackedLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const CellParams & params) const",29, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedBidirectionalLayer::PackedBidirectionalLayer( Cell<dir_hidden_type> & cell)",2, 41, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::PackedBidirectionalLayer::operator ( )( const PackedSequence & input , const hidden_type & input_hidden , const param_type & params) const",6, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::dropout( const Tensor & input , double p)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::dropout( const PackedSequence & input , double p)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::apply_layer_stack( const Layer<io_type,hidden_type,weight_type> & layer , const io_type & input , const std :: vector<hidden_type> & hiddens , const std :: vector<weight_type> & weights , int64_t num_layers , double dropout_p , bool train)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_rnn_impl( const io_type & input , const std :: vector<CellParams> & params , const std :: vector<typename CellType::hidden_type> & hiddens , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_rnn_impl_with_concat( const io_type & input , const std :: vector<CellParams> & params , const std :: vector<typename CellType::hidden_type> & hiddens , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::_lstm_impl( const io_type & input , const std :: vector<CellParams> & params , const Tensor & hx , const Tensor & cx , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm( const Tensor & _input , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm( const Tensor & data , const Tensor & batch_sizes , TensorList hx , TensorList _params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::lstm_cell( const Tensor & input , TensorList hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::gru_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::rnn_tanh_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/RNN.cpp,"at::native::rnn_relu_cell( const Tensor & input , const Tensor & hx , const Tensor & w_ih , const Tensor & w_hh , const Tensor & b_ih , const Tensor & b_hh)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"apply_loss_reduction( const at :: Tensor & unreduced , int64_t reduction)",8, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::cosine_embedding_loss( const Tensor & input1 , const Tensor & input2 , const Tensor & target , double margin , int64_t reduction)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::hinge_embedding_loss( const Tensor & self , const Tensor & target , double margin , int64_t reduction)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::triplet_margin_loss( const Tensor & anchor , const Tensor & positive , const Tensor & negative , double margin , double p , double eps , bool swap , int64_t reduction)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::margin_ranking_loss( const Tensor & input1 , const Tensor & input2 , const Tensor & target , double margin , int64_t reduction)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::kl_div( const Tensor & input , const Tensor & target , int64_t reduction)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::kl_div_backward_cpu( const Tensor & grad , const Tensor & input , const Tensor & target , int64_t reduction)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::binary_cross_entropy_with_logits( const Tensor & input , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Loss.cpp,"at::native::binary_cross_entropy_with_logits_backward( const Tensor & grad , const Tensor & input , const Tensor & target , const Tensor & weight , const Tensor & pos_weight , int64_t reduction)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SummaryOps.cpp,"at::native::_bincount_cpu_template( const Tensor & self , const Tensor & weights , int64_t minlength)",40, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SummaryOps.cpp,"at::native::_bincount_cpu( const Tensor & self , const Tensor & weights , int64_t minlength)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_cuda( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_distributed( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_complex( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_floating_point( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_signed( const Tensor & self)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::is_sparse( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TypeProperties.cpp,"at::native::type_as( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::check1d( const char * function_name , const char * argument_name , IntList x)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::adaptive_avg_pool1d( const Tensor & self , IntList output_size)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::adaptive_max_pool1d( const Tensor & self , IntList output_size)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool1d_with_indices( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::avg_pool1d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , bool ceil_mode , bool count_include_pad)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool1d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool2d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/Pooling.cpp,"at::native::max_pool3d( const Tensor & self , IntList kernel_size , IntList stride , IntList padding , IntList dilation , bool ceil_mode)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_reshape_from_tensor( const Tensor & self , const Tensor & shape_tensor)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_shape_as_tensor( const Tensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::broadcast_tensors( TensorList tensors)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_cat_no_zero_dim( TensorList tensors)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat_out( Tensor & result , TensorList tensors , int64_t dim)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sizes_match_except( IntList s1 , IntList s2 , int64_t dim_except)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_cat_sparse_dims( Tensor const & t , int64_t pos , IntList sizes , int64_t wrapped , int64_t sparse_dim , int64_t dense_dim)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat_sparse( TensorList tensors , int64_t dim)",89, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::cat( TensorList tensors , int64_t dim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::chunk( const Tensor & self , int64_t chunks , int64_t dim)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diagflat( const Tensor & self , int64_t offset)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diagonal( const Tensor & self , int64_t offset , int64_t dim1_ , int64_t dim2_)",45, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::diag_embed( const Tensor & self , int64_t offset , int64_t dim1_ , int64_t dim2_)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::expand( const Tensor & self , IntList size , bool implicit)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::expand_as( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided( const Tensor & self , IntList size , IntList stride , int64_t storage_offset)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided_( Tensor & self , IntList size , IntList stride , int64_t storage_offset)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided( const Tensor & self , IntList size , IntList stride)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::as_strided_( Tensor & self , IntList size , IntList stride)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow_copy_sparse( const Tensor & self , int64_t dim , int64_t start , int64_t length)",33, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow_copy_dense( const Tensor & self , int64_t dim , int64_t start , int64_t length)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::narrow( const Tensor & self , int64_t dim , int64_t start , int64_t length)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::permute( const Tensor & self , IntList dims)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::repeat( const Tensor & self , IntList repeats)",29, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::reshape( const Tensor & self , IntList proposed_shape)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::reshape_as( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::select( const Tensor & self , int64_t dim , int64_t index)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::slice( const Tensor & self , int64_t dim , int64_t start , int64_t end , int64_t step)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::split( const Tensor & self , int64_t split_size , int64_t dim)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::split_with_sizes( const Tensor & self , IntList split_sizes , int64_t dim)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::get_stack_inputs( TensorList tensors , int64_t dim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::stack( TensorList tensors , int64_t dim)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::stack_out( Tensor & result , TensorList tensors , int64_t dim)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::sparse_transpose_( Tensor & self , int64_t dim0 , int64_t dim1)",31, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::transpose_( Tensor & self , int64_t dim0 , int64_t dim1)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::transpose( const Tensor & self , int64_t dim0 , int64_t dim1)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::check_t( const Tensor & self , const char * fn)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::t( const Tensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::t_( Tensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferSqueezeGeometry( const Tensor & tensor)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferSqueezeGeometry( const Tensor & tensor , int64_t dim)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::inferUnsqueezeGeometry( const Tensor & tensor , int64_t dim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze( const Tensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze( const Tensor & self , int64_t dim)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze_( Tensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::squeeze_( Tensor & self , int64_t dim)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::_unsafe_view( const Tensor & self , IntList size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze_sparse( Tensor const & self , int64_t dim)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze( const Tensor & self , int64_t dim)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unsqueeze_( Tensor & self , int64_t dim)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::flatten( const Tensor & self , int64_t start_dim , int64_t end_dim)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::view_as( const Tensor & self , const Tensor & other)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::numel( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::unbind( const Tensor & self , int64_t dim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/TensorShape.cpp,"at::native::meshgrid( TensorList tensors)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_fft( const Tensor & self , const int64_t signal_ndim , const bool complex_input , const bool complex_output , const bool inverse , IntList signal_sizes , const bool normalized , const bool onesided)",108, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_get_plan_cache_max_size()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_set_plan_cache_max_size( int64_t max_size)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_get_plan_cache_size()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::_cufft_clear_plan_cache()",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::fft( const Tensor & self , const int64_t signal_ndim , const bool normalized)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::ifft( const Tensor & self , const int64_t signal_ndim , const bool normalized)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::rfft( const Tensor & self , const int64_t signal_ndim , const bool normalized , const bool onesided)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::irfft( const Tensor & self , const int64_t signal_ndim , const bool normalized , const bool onesided , IntList signal_sizes)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/SpectralOps.cpp,"at::native::stft( const Tensor & self , const int64_t n_fft , const int64_t hop_length , const int64_t win_length , const Tensor & window , const bool normalized , const bool onesided)",77, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::_baddbmm_mkl_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::gemm_batched( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const float alpha , const float ** A , const float ** B , const float beta , float ** C)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::gemm_batched( const CBLAS_TRANSPOSE trans_A , const CBLAS_TRANSPOSE trans_B , const int batch_size , const int M , const int N , const int K , const double alpha , const double ** A , const double ** B , const double beta , double ** C)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::baddbmm_mkl_template( const Tensor & res , const Tensor & mat1 , const Tensor & mat2 , Scalar beta_ , Scalar alpha_)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/LinearAlgebra.cpp,"at::native::_baddbmm_mkl_( Tensor & self , const Tensor & batch1 , const Tensor & batch2 , Scalar beta , Scalar alpha)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_mkl( const Tensor & input , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntList checked_signal_sizes , bool normalized , bool onesided , IntList output_sizes)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_fill_with_conjugate_symmetry_slice( Tensor & output , int64_t signal_ndim , int64_t size_last_dim , int64_t start_last_dim_idx , int64_t i , int64_t num)",74, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_fill_with_conjugate_symmetry_( Tensor & input , int64_t signal_ndim , int64_t size_last_dim , int64_t last_dim_start_slice)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp,"at::native::_fft_mkl( const Tensor & self , int64_t signal_ndim , bool complex_input , bool complex_output , bool inverse , IntList checked_signal_sizes , bool normalized , bool onesided , IntList output_sizes)",127, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp,"at::native::_cudnn_ctc_loss( const Tensor & log_probs , const Tensor & targets , IntList input_lengths , IntList target_lengths , int64_t BLANK , bool deterministic)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp,"at::native::_cudnn_ctc_loss( const Tensor & log_probs_t , const Tensor & targets_t , IntList input_lengths_ , IntList target_lengths_ , int64_t BLANK , bool deterministic)",52, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_forward( const Tensor & theta , int64_t N , int64_t C , int64_t H , int64_t W)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_backward( const Tensor & grad_theta , int64_t N , int64_t C , int64_t H , int64_t W)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::setSamplerDescriptor( SpatialTransformerDescriptor & desc , cudnnDataType_t dataType , int N , int C , int H , int W)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_forward( const Tensor & theta_t , int64_t N , int64_t C , int64_t H , int64_t W)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,"at::native::cudnn_affine_grid_generator_backward( const Tensor & grad_grid_t , int64_t N , int64_t C , int64_t H , int64_t W)",22, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_forward( const Tensor & input_t , const Tensor & grid_t)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_backward( const Tensor & input_t , const Tensor & grid_t , const Tensor & grad_output_t)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::setSamplerDescriptor( SpatialTransformerDescriptor & desc , cudnnDataType_t dataType , const at :: Tensor & tensor)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::checkGridSize( CheckedFrom c , TensorArg grid , TensorArg input)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_forward( const Tensor & input_t , const Tensor & grid_t)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp,"at::native::cudnn_grid_sampler_backward( const Tensor & input_t , const Tensor & grid_t , const Tensor & grad_output_t)",42, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_flatten_weight( TensorList weight_arr , int64_t weight_stride0 , int64_t input_size , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , bool fn_bidirectional)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn( const Tensor & input_r , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf_r , const Tensor & hx , const Tensor & cx , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward( const Tensor & input , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output , const Tensor & grad_output_r , const Tensor & grad_hy_r , const Tensor & grad_cy_r , int64_t mode , int64_t hidden_size , int64_t num_layers , bool batch_first , double dropout , bool train , bool bidirectional , IntList batch_sizes , const Tensor & dropout_state , const Tensor & reserve , std :: array<bool,4> output_mask)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_init_dropout_state( double dropout , bool train , int64_t dropout_seed , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::DropoutDescriptorParams()",1, 33, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::set( bool train_ , double dropout_ , Tensor dropout_state_)",5, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutDescriptorParams::descriptor( cudnnHandle_t handle) const",10, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::num_directions() const",3, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_mode( int64_t fn_mode)",22, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_bidirectional( bool fn_bidirectional)",3, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set_algo( cudnnRNNAlgo_t algo)",3, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::set( int64_t mode , int64_t hidden_size , int64_t num_layers , bool bidirectional , cudnnDataType_t datatype)",7, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::descriptor( cudnnHandle_t handle , DropoutDescriptor && dropout_desc) const",5, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptorParams::descriptor( cudnnHandle_t handle) const",5, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::rnn_descriptor_sequence( const Tensor & tensor , IntList batch_sizes)",14, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::rnn_descriptor( const Tensor & tensor , int64_t N)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::is_input_packed() const",3, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::set( IntList input_sizes , IntList batch_sizes_ , bool batch_first)",23, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::TensorDescriptorListParams::descriptors( Tensor x) const",8, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::RNNDescriptors( const RNNParams & fn , cudnnHandle_t handle , Tensor x , Tensor y , Tensor hx , Tensor cx)",11, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_descs( const std :: vector<TensorDescriptor> & descs)",8, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_x_descs()",3, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::RNNDescriptors::get_y_descs()",3, 6, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_num_weights( cudnnHandle_t handle , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , cudnnDataType_t datatype)",8, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_num_linear_layers( cudnnRNNMode_t mode)",14, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_parameters( cudnnHandle_t handle , const RNNDescriptorParams & rnn , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , const FilterDescriptor & w_desc , const Tensor & weight_buf)",85, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_expected_data_ptrs( const Tensor & weight_buf , cudnnHandle_t handle , const RNNDescriptorParams & rnn , const RNNDescriptor & rnn_desc , const TensorDescriptor & x_desc , cudnnDataType_t datatype)",37, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_viewOrCopyParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to , bool copy)",21, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_copyParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_viewParams( MatrixRef<Tensor> params_from , MatrixRef<Tensor> params_to)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_input_size( const TensorDescriptorListParams & tensors)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_hidden_size( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_output_size( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",7, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_algo( const RNNDescriptorParams & rnn , const TensorDescriptorListParams & tensors)",24, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_flatten_weight( TensorList weight_arr , int64_t weight_stride0 , int64_t input_size , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , bool fn_bidirectional)",52, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn( const Tensor & input_r , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf_r , const Tensor & hx , const Tensor & cx , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state)",141, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward_input( const Tensor & input_r , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output_r , const Tensor & grad_output_r , const Tensor & grad_hy , const Tensor & grad_cy , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state , const Tensor & fn_reserve , std :: array<bool,3> output_mask)",119, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward_weight( const Tensor & input_r , TensorList weight_arr , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output_r , int64_t fn_mode , int64_t fn_hidden_size , int64_t fn_num_layers , bool batch_first , double fn_dropout , bool fn_train , bool fn_bidirectional , IntList fn_batch_sizes , const Tensor & fn_dropout_state , const Tensor & fn_reserve)",107, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_rnn_backward( const Tensor & input , TensorList weight , int64_t weight_stride0 , const Tensor & weight_buf , const Tensor & hx , const Tensor & cx , const Tensor & output , const Tensor & grad_output_r , const Tensor & grad_hy_r , const Tensor & grad_cy_r , int64_t mode , int64_t hidden_size , int64_t num_layers , bool batch_first , double dropout , bool train , bool bidirectional , IntList batch_sizes , const Tensor & dropout_state , const Tensor & reserve , std :: array<bool,4> output_mask)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_init_dropout_state( double dropout , bool train , int64_t dropout_seed , const TensorOptions & options)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::unpack_hidden( const Tensor & hidden)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::unpack_hidden( const std :: tuple<Tensor,Tensor> & hidden)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden( const Tensor & hx , const Tensor & cx)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden<Tensor>( const Tensor & hx , const Tensor & cx)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::pack_hidden<std::tuple<Tensor,Tensor>>( const Tensor & hx , const Tensor & cx)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutState::lock()",8, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::DropoutState::unlock()",6, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::get_dropout_state( double dropout_p , bool train , TensorOptions options)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::try_get_weight_buf( const Tensor & input , TensorList parameters , bool has_biases , cudnnRNNMode_t mode , int64_t hidden_size , int64_t num_layers , bool bidirectional)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_impl( const Tensor & input , const Tensor & _batch_sizes , const hidden_type & hidden , TensorList params , bool has_biases , cudnnRNNMode_t mode , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::_cudnn_impl( const Tensor & input , const hidden_type & hidden , TensorList params , bool has_biases , cudnnRNNMode_t mode , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::lstm_cudnn( Tensor & output , Tensor & hy , Tensor & cy , const Tensor & input , TensorList hx , TensorList params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional , bool batch_first)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/RNN.cpp,"at::native::lstm_packed_cudnn( Tensor & output , Tensor & hy , Tensor & cy , const Tensor & data , const Tensor & batch_sizes , TensorList hx , TensorList params , bool has_biases , int64_t num_layers , double dropout_p , bool train , bool bidirectional)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_bias( const at :: Tensor & grad_output)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_input( const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_output_size( IntList input_size , IntList weight_size , IntList padding , IntList stride , IntList dilation , int64_t groups)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_input_size( IntList output_size , IntList weight_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::conv_weight_size( IntList input_size , IntList output_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::narrowGroup( const Tensor & t , int dim , int group_idx , int64_t groups)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::check_args( CheckedFrom c , IntList args , size_t expected_size , const char * arg_name)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::convolution_shape_check( CheckedFrom c , const TensorGeometryArg & input , const TensorGeometryArg & weight , const TensorGeometryArg & output , IntList padding , IntList stride , IntList dilation , int64_t groups)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::setConvolutionParams( ConvolutionParams * params , const at :: Tensor & input , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool deterministic)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::ConvolutionArgs::ConvolutionArgs( const Tensor & input , const Tensor & output , const Tensor & weight)",2, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::BenchmarkCache::find( const ConvolutionParams & params , T * results)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::BenchmarkCache::insert( const ConvolutionParams & params , const T & results)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::Workspace::Workspace( size_t size)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::Workspace::~Workspace()",5, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionFwdAlgo_t algo , size_t * sz)",14, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdDataAlgo_t algo , size_t * sz)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdFilterAlgo_t algo , size_t * sz)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getMaxWorkspaceSize( const ConvolutionArgs & args , const algo_t * algo , int n_algo)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::getBestAlgorithm( perf_t * perfResults , bool deterministic , int n_algo)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::cache()",1, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::findAlgorithm( const ConvolutionArgs & args)",31, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::getAlgorithm( const ConvolutionArgs & args , algo_t * algo)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionFwdAlgo_t>::getWorkspaceSize( const ConvolutionArgs & args , algo_t algo , size_t * workspaceSize)",13, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::cache()",1, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::findAlgorithm( const ConvolutionArgs & args)",29, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::getAlgorithm( const ConvolutionArgs & args , algo_t * algo)",11, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdDataAlgo_t>::getWorkspaceSize( const ConvolutionArgs & args , cudnnConvolutionBwdDataAlgo_t algo , size_t * workspaceSize)",13, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::cache()",1, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::findAlgorithm( const ConvolutionArgs & args)",33, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::getAlgorithm( const ConvolutionArgs & args , algo_t * algo)",12, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::algorithm_search<cudnnConvolutionBwdFilterAlgo_t>::getWorkspaceSize( const ConvolutionArgs & args , algo_t algo , size_t * workspaceSize)",11, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::findAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",39, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::chooseAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_add_bias_( CheckedFrom c , const TensorArg & output , const TensorArg & bias)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",33, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_input( const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( CheckedFrom c , IntList input_size , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_forward( CheckedFrom c , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_input( IntList input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::raw_cudnn_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( CheckedFrom c , IntList weight_size , const TensorArg & grad_output , const TensorArg & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",33, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_transpose_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/Conv.cpp,"at::native::cudnn_convolution_backward_bias( const Tensor & grad_output_t)",26, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double exponential_average_factor , double epsilon)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm_backward( const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var , double epsilon)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::expandScale( const Tensor & t , int64_t dim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , const Tensor & running_mean_t , const Tensor & running_var_t , bool training , double exponential_average_factor , double epsilon)",92, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp,"at::native::cudnn_batch_norm_backward( const Tensor & input_t , const Tensor & grad_output_t , const Tensor & weight_t , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean_t , const Tensor & save_var_t , double epsilon)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp__cuda( Tensor & self , optional<Scalar> min , optional<Scalar> max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_out_cuda( Tensor & result , const Tensor & self , optional<Scalar> min , optional<Scalar> max)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_max__cuda( Tensor & self , Scalar max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_max_out_cuda( Tensor & result , const Tensor & self , Scalar max)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_min__cuda( Tensor & self , Scalar min)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,"at::native::_clamp_min_out_cuda( Tensor & result , const Tensor & self , Scalar min)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm( const Tensor & input , const Tensor & weight , const Tensor & bias , const Tensor & running_mean , const Tensor & running_var , bool training , double exponential_average_factor , double epsilon)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm_backward( const Tensor & input , const Tensor & grad_output , const Tensor & weight , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean , const Tensor & save_var , double epsilon)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::expandScale( const Tensor & t , int64_t dim)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , const Tensor & running_mean_t , const Tensor & running_var_t , bool training , double exponential_average_factor , double epsilon)",85, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,"at::native::miopen_batch_norm_backward( const Tensor & input_t , const Tensor & grad_output_t , const Tensor & weight_t , const Tensor & running_mean , const Tensor & running_var , const Tensor & save_mean_t , const Tensor & save_var_t , double epsilon)",67, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_bias( const at :: Tensor & grad_output)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_input( const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_weight( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_output_size( IntList input_size , IntList weight_size , IntList padding , IntList stride , IntList dilation , int64_t groups)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_input_size( IntList output_size , IntList weight_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::conv_weight_size( IntList input_size , IntList output_size , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::narrowGroup( const Tensor & t , int dim , int group_idx , int64_t groups)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::check_args( CheckedFrom c , IntList args , size_t expected_size , const char * arg_name)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::convolution_shape_check( CheckedFrom c , const TensorGeometryArg & input , const TensorGeometryArg & weight , const TensorGeometryArg & output , IntList padding , IntList stride , IntList dilation , int64_t groups)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::setConvolutionParams( ConvolutionParams * params , const at :: Tensor & input , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool deterministic)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ConvolutionArgs::ConvolutionArgs( const Tensor & input , const Tensor & output , const Tensor & weight)",2, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ParamsHash::operator ( )( const ConvolutionParams & params) const",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::ParamsEqual::operator ( )( const ConvolutionParams & a , const ConvolutionParams & b) const",5, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::BenchmarkCache::find( const ConvolutionParams & params , T * results)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::BenchmarkCache::insert( const ConvolutionParams & params , const T & results)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::Workspace::Workspace( size_t size)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::Workspace::~Workspace()",5, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvFwdAlgorithm_t)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvBwdDataAlgorithm_t)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getWorkspaceSize( const ConvolutionArgs & args , const miopenConvBwdWeightsAlgorithm_t)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::getBestAlgorithm( perf_t * perfResults , bool deterministic , int n_algo)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::cache()",1, 63, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvFwdAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",19, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::cache()",1, 68, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdDataAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",19, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::cache()",1, 70, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::algorithm_search<miopenConvBwdWeightsAlgorithm_t>::findAlgorithm( const ConvolutionArgs & args)",19, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::findAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",25, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::chooseAlgorithm( const ConvolutionArgs & args , bool benchmark , algo_t * algo)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_add_bias_( CheckedFrom c , const TensorArg & output , const TensorArg & bias)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_forward_out( const Tensor & output , const Tensor & input , const Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_forward( CheckedFrom c , const TensorArg & input , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_input( const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_backward_input_out( const at :: Tensor & grad_input , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",30, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( CheckedFrom c , IntList input_size , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_forward( CheckedFrom c , const TensorArg & grad_output , const TensorArg & weight , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_input( IntList input_size , const Tensor & grad_output_t , const Tensor & weight_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",13, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic , std :: array<bool,3> output_mask)",20, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose( const Tensor & input_t , const Tensor & weight_t , const Tensor & bias_t , IntList padding , IntList output_padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::raw_miopen_convolution_backward_weight_out( const Tensor & grad_weight , const Tensor & grad_output , const Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",28, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( CheckedFrom c , IntList weight_size , const TensorArg & grad_output , const TensorArg & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_transpose_backward_weight( IntList weight_size , const Tensor & grad_output_t , const Tensor & input_t , IntList padding , IntList stride , IntList dilation , int64_t groups , bool benchmark , bool deterministic)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp,"at::native::miopen_convolution_backward_bias( const Tensor & grad_output_t)",23, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/Activation.cpp,"at::native::threshold_kernel( TensorIterator & iter , Scalar threshold_scalar , Scalar value_scalar)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::add_kernel( TensorIterator & iter , Scalar alpha_scalar)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::sub_kernel( TensorIterator & iter , Scalar alpha_scalar)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::mul_kernel( TensorIterator & iter)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,"at::native::div_kernel( TensorIterator & iter)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::Indexer::Indexer( int64_t num_indexers , char ** indexers , const int64_t * indexer_strides , IntList original_sizes , IntList original_strides)",10, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::Indexer::get( int64_t idx)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::is_constant_index( int ntensor , const int64_t * strides)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::cpu_index_kernel( TensorIterator & iter , IntList index_size , IntList index_stride , const func_t & f , bool serial_execution = false)",32, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::index_kernel( TensorIterator & iter , IntList index_size , IntList index_stride)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/IndexKernel.cpp,"at::native::index_put_kernel( TensorIterator & iter , IntList index_size , IntList index_stride , bool accumulate)",16, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/CopyKernel.cpp,"at::native::copy_kernel_impl( Tensor & dst , const Tensor & src)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocationBase::ComputeLocationBase( int64_t size)",2, 59, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocationBase::unnormalize( const Vec & in) const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Zeros>::apply( const Vec & in) const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Zeros>::apply_get_grad( const Vec & in) const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::ComputeLocation( int64_t size)",3, 50, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::apply( const Vec & in) const",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Border>::apply_get_grad( const Vec & in) const",13, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::ComputeLocation( int64_t size)",5, 66, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::apply( const Vec & in) const",14, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ComputeLocation<scalar_t,GridSamplerPadding::Reflection>::apply_get_grad( const Vec & in) const",20, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::mask_scatter_add( const scalar_t * src , scalar_t * base_addr , const int_same_size_t<scalar_t> * offsets , const int_same_size_t<scalar_t> * mask , int64_t len)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::ApplyGridSample( const TensorAccessor<scalar_t,4> & input)",9, 34, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::compute_interp_params( const Vec & x , const Vec & y) const",48, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::forward( TensorAccessor<scalar_t,3> & out_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",47, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Bilinear,padding>::backward( TensorAccessor<scalar_t,3> & gInp_slice , TensorAccessor<scalar_t,3> & gGrid_slice , const TensorAccessor<scalar_t,3> & gOut_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",95, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::ApplyGridSample( const TensorAccessor<scalar_t,4> & input)",9, 34, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::forward( TensorAccessor<scalar_t,3> & out_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",33, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::ApplyGridSample<scalar_t,2,GridSamplerInterpolation::Nearest,padding>::backward( TensorAccessor<scalar_t,3> & gInp_slice , TensorAccessor<scalar_t,3> & gGrid_slice , const TensorAccessor<scalar_t,3> & gOut_slice , const TensorAccessor<scalar_t,3> & inp_slice , int64_t offset , const Vec & grid_x , const Vec & grid_y , int64_t len) const",38, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sample_2d_grid_slice_iterator( const TensorAccessor<scalar_t,3> & grid_slice , const ApplyFn & apply_fn)",114, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sampler_2d_cpu_kernel_impl( const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",54, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp,"at::native::grid_sampler_2d_backward_cpu_kernel_impl( const Tensor & grad_output_ , const Tensor & input , const Tensor & grid , int64_t interpolation_mode , int64_t padding_mode)",63, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( scalar_t val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( float val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::_isnan( double val)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::Reduction::apply( Tensor & res , Tensor & res_indices , const Tensor & self , c10 :: optional<int64_t> dim , bool greater)",63, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::max_kernel_impl( Tensor & max , Tensor & max_indices , const Tensor & self , c10 :: optional<int64_t> dim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/TensorCompareKernel.cpp,"at::native::min_kernel_impl( Tensor & min , Tensor & min_indices , const Tensor & self , c10 :: optional<int64_t> dim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::_sigmoid( float * x , float * y , int64_t size)",24, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::_sigmoid( double * x , double * y , int64_t size)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::sigmoid_kernel( Tensor & result , const Tensor & self)",34, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::bernoulli_mkl_kernel( Tensor & output , const double p , Generator * gen)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,"at::native::bernoulli_mkl_kernel( Tensor & self , const double p , Generator * gen)",49, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_log_softmax_lastdim( scalar_t * input_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",62, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_softmax_lastdim( scalar_t * input_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",38, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::_vec_host_softmax_backward_lastdim( scalar_t * grad_input_data_base , scalar_t * grad_data_base , scalar_t * output_data_base , int64_t outer_size , int64_t dim_size)",50, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::vec_host_softmax_lastdim::apply( Tensor & output , const Tensor & input)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::vec_host_softmax_backward_lastdim::apply( Tensor & grad_input , const Tensor & grad , const Tensor & output)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::softmax_lastdim_kernel_impl( Tensor & result , const Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::log_softmax_lastdim_kernel_impl( Tensor & result , const Tensor & self)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::softmax_backward_lastdim_kernel_impl( Tensor & grad_input , const Tensor & grad , const Tensor & output)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp,"at::native::log_softmax_backward_lastdim_kernel_impl( Tensor & grad_input , const Tensor & grad , const Tensor & output)",10, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::sign( Vec val)",4, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::map( const Vec & diff , const Vec & p)",1, 112, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::zdist_calc::finish( const scalar_t agg , const scalar_t p)",1, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::map( const Vec & diff , const Vec & p)",1, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::finish( const scalar_t agg , const scalar_t p)",1, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::odist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 139, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::lttdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 219, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::map( const Vec & diff , const Vec & p)",1, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::finish( const scalar_t agg , const scalar_t p)",1, 99, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::tdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 168, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::map( const Vec & diff , const Vec & p)",1, 81, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::red( const Vec & agg , const Vec & up)",1, 78, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::finish( const scalar_t agg , const scalar_t p)",1, 107, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::pdist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 213, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::map( const Vec & diff , const Vec & p)",1, 74, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::red( const Vec & agg , const Vec & up)",1, 94, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::finish( const scalar_t agg , const scalar_t p)",1, 88, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::idist_calc::backward( const Vec & diff , const scalar_t grad , const scalar_t dist , const Vec & p)",1, 215, 4, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_parallel( Tensor & result , const Tensor & self , const scalar_t p)",39, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply( Tensor & result , const Tensor & self , const scalar_t p)",13, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::backward_down_column( const scalar_t * self_i , scalar_t * res_i , const scalar_t * grad_k , const scalar_t * dist_k , const Vec & pvec , int64_t n , int64_t m , int64_t gs , int64_t count = Vec :: size)",22, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::run_backward_parallel( Tensor & result , const Tensor & grad , const Tensor & self , const scalar_t p , const Tensor & dist)",27, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::PDist::apply_backward( Tensor & result , const Tensor & grad , const Tensor & self , const double p , const Tensor & dist)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::pdist_forward_kernel_impl( Tensor & result , const Tensor & self , const double p)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,"at::native::pdist_backward_kernel_impl( Tensor & result , const Tensor & grad , const Tensor & self , const double p , const Tensor & dist)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::sum_kernel_impl( TensorIterator & iter)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::WelfordData::WelfordData()",1, 43, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::WelfordData::WelfordData( double mean , double m2 , int64_t n)",1, 79, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::std_kernel_impl( TensorIterator & iter , bool unbiased)",37, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::prod_kernel_impl( TensorIterator & iter)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::round_down( int64_t a , int64_t m)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::apply( Tensor & res , const Tensor & self , Scalar p , c10 :: optional<int64_t> dim)",38, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::reduce_all( const scalar_t * data_ , int64_t size , float pval)",15, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::norm_reduce( const scalar_t * data , int64_t n , int64_t stride , float pval)",12, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::norm_reduce_sequential( const scalar_t * data , int64_t n , int64_t stride , float pval)",37, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::NormReduction::norm_reduce128( const scalar_t * data , int64_t n , float pval)",39, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,"at::native::norm_kernel_impl( Tensor & result , const Tensor & self , Scalar p , c10 :: optional<int64_t> dim)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_weights( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , std :: array<bool,3> output_mask)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::conv_output_size( IntList input_size , IntList weight_size , IntList padding , IntList stride , IntList dilation , int64_t groups)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution( const at :: Tensor & input , const at :: Tensor & weight , const at :: Tensor & bias , IntList padding , IntList stride , IntList dilation , int64_t groups)",111, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_input( IntList input_size , const at :: Tensor & grad_output , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",111, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward_weights( IntList weight_size , const at :: Tensor & grad_output , const at :: Tensor & input , IntList padding , IntList stride , IntList dilation , int64_t groups , bool bias_defined)",132, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp,"at::native::mkldnn_convolution_backward( const at :: Tensor & input , const at :: Tensor & grad_output_t , const at :: Tensor & weight , IntList padding , IntList stride , IntList dilation , int64_t groups , std :: array<bool,3> output_mask)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_to_csr( const int64_t * indices , int64_t dim , int64_t nnz)",19, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::zero_sparse_( SparseTensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::wrapped_scalar_tensor( Scalar s)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_zerodim( SparseTensor & r , const SparseTensor & t , const Tensor & value)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_scalar( SparseTensor & r , const SparseTensor & t , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::log1p_out_sparse( SparseTensor & r , const SparseTensor & t)",15, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::log1p_sparse_( SparseTensor & t)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::pow_out_sparse_scalar( SparseTensor & r , const SparseTensor & t_ , Scalar value)",17, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::pow_sparse_scalar( const SparseTensor & t , Scalar value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::div_out_sparse_zerodim( SparseTensor & r , const SparseTensor & t , const Tensor & value)",19, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::div_out_sparse_scalar( SparseTensor & r , const SparseTensor & t , Scalar value)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::norm_sparse( const SparseTensor & self , Scalar value)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_out_sparse_cpu( SparseTensor & r , const SparseTensor & t , const SparseTensor & src , Scalar value)",98, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_dense_sparse_worker_cpu( Tensor & r , Scalar value , const SparseTensor & sparse , const Tensor & indices , const Tensor & values)",18, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::add_out_dense_sparse_cpu( Tensor & r , const Tensor & dense , SparseTensorRef sparse__ , Scalar value)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::mul_out_sparse_cpu( SparseTensor & r , const Tensor & t_ , const Tensor & src_)",96, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_out_sparse_dense_worker( int64_t nnz , int64_t dim_i , int64_t dim_j , int64_t dim_k , Tensor & r , Scalar beta , const Tensor & t , Scalar alpha , const Tensor & csr , const Tensor & indices , const Tensor & values , const Tensor & dense)",45, 3, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_out_sparse_dense_cpu( Tensor & r , const Tensor & t , const SparseTensor & sparse_ , const Tensor & dense , Scalar beta , Scalar alpha)",54, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_sparse_dense_cpu( const Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",11, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::s_addmm_sparse_dense_cpu_( Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_addmm( const Tensor & t , const SparseTensor & sparse , const Tensor & dense , Scalar beta , Scalar alpha)",9, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_mm( const SparseTensor & sparse , const Tensor & dense)",7, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::hspmm_out_sparse_cpu( SparseTensor & r , const SparseTensor & sparse_ , const Tensor & dense)",68, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::hspmm_sparse_cpu( const SparseTensor & sparse , const Tensor & dense)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sspaddmm_out_cpu( SparseTensor & r , const SparseTensor & t , const SparseTensor & sparse_ , const Tensor & dense , Scalar beta , Scalar alpha)",109, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sspaddmm_out_only_sparse( Tensor & result , const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::smm( const Tensor & self , const Tensor & mat2)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::sspaddmm( const Tensor & self , const Tensor & mat1 , const Tensor & mat2 , Scalar beta , Scalar alpha)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , ScalarType dtype)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , IntList dims_to_sum , ScalarType dtype)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum( const SparseTensor & input , IntList dims_to_sum)",71, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp,"at::native::_sparse_sum_backward_cpu( const Tensor & grad_ , const SparseTensor & input_ , IntList dims_to_sum)",105, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_dim_sparse( const SparseTensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_dim_sparse( const SparseTensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::is_coalesced_sparse( const SparseTensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_nnz_sparse( const SparseTensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_indices_sparse( const SparseTensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_values_sparse( const SparseTensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_coalesced_sparse_( SparseTensor & self , bool coalesced)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::indices_sparse( const Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::values_sparse( const Tensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_sparse( const TensorOptions & options)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_with_dims_sparse( int64_t sparse_dim , int64_t dense_dim , ArrayRef<int64_t> size , const TensorOptions & options)",8, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::new_with_dims_and_tensor_sparse( int64_t sparse_dim , int64_t dense_dim , ArrayRef<int64_t> size , const LongTensor & indices , const Tensor & values , const TensorOptions & options)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::empty_sparse( IntList size , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( ArrayRef<int64_t> size , const TensorOptions & options)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::expand_values_if_needed( const Tensor & values)",9, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( const Tensor & indices , const Tensor & values_ , const TensorOptions & options)",47, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_coo_tensor( const Tensor & indices , const Tensor & values_ , ArrayRef<int64_t> size , const TensorOptions & options)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_sparse_coo_tensor_unsafe( const Tensor & indices , const Tensor & values_ , ArrayRef<int64_t> size , const TensorOptions & options)",12, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::clone_sparse( const SparseTensor & self)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_resize_( SparseTensor & self , ArrayRef<int64_t> size , int64_t sparse_dim , int64_t dense_dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_resize_and_clear_( SparseTensor & self , ArrayRef<int64_t> size , int64_t sparse_dim , int64_t dense_dim)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::_is_same_size_as_sparse( const SparseTensor & self , const SparseTensor & src)",3, 4, 2, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::resize_as_sparse_( SparseTensor & self , const SparseTensor & src)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_to_sparse( const Tensor & self)",3, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::dense_to_sparse( const Tensor & self , int64_t sparse_dim)",27, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_to_dense( const SparseTensor & self)",4, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::copy_sparse_( SparseTensor & self , const SparseTensor & src , bool non_blocking)",6, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::coalesce_sparse_cpu( const SparseTensor & self)",72, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_out_cpu_kernel( Tensor & r_values , const Tensor & t , const int64_t r_nnz , const int64_t sparse_dim , const LongTensor & mask_indices)",21, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_out_cpu( SparseTensor & r , const Tensor & t , const SparseTensor & mask)",56, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp,"at::native::sparse_mask_cpu( const Tensor & t , SparseTensorRef mask)",5, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,"at::native::sparse_mask_out_cuda( SparseTensor & r , const Tensor & t , const SparseTensor & mask)",44, 2, 0, 0
repos/cpp/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,"at::native::sparse_mask_cuda( const Tensor & t , SparseTensorRef mask)",5, 2, 0, 0
