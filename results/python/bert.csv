File, Function, Length, Total Width, Leading Space(s), Leading Tab(s)
github/python/bert/create_pretraining_data.py,"__init__( self , tokens , segment_ids , masked_lm_positions , masked_lm_labels , is_random_next )", 2, 81, 2, 0
github/python/bert/create_pretraining_data.py,"__str__( self )", 12, 78, 4, 0
github/python/bert/create_pretraining_data.py,"__repr__( self )", 2, 26, 4, 0
github/python/bert/create_pretraining_data.py,"write_instance_to_example_files( instances , tokenizer , max_seq_length , max_predictions_per_seq , output_files )", 2, 76, 36, 0
github/python/bert/create_pretraining_data.py,"create_int_feature( values )", 3, 80, 2, 0
github/python/bert/create_pretraining_data.py,"create_float_feature( values )", 3, 80, 2, 0
github/python/bert/create_pretraining_data.py,"create_training_instances( input_files , tokenizer , max_seq_length , dupe_factor , short_seq_prob , masked_lm_prob , max_predictions_per_seq , rng )", 3, 75, 30, 0
github/python/bert/create_pretraining_data.py,"create_instances_from_document( all_documents , document_index , max_seq_length , short_seq_prob , masked_lm_prob , max_predictions_per_seq , vocab_words , rng )", 3, 67, 4, 0
github/python/bert/create_pretraining_data.py,"create_masked_lm_predictions( tokens , masked_lm_prob , max_predictions_per_seq , vocab_words , rng )", 2, 77, 33, 0
github/python/bert/create_pretraining_data.py,"truncate_seq_pair( tokens_a , tokens_b , max_num_tokens , rng )", 16, 75, 4, 0
github/python/bert/create_pretraining_data.py,"main( _ )", 27, 81, 6, 0
github/python/bert/extract_features.py,"__init__( self , unique_id , text_a , text_b )", 4, 49, 2, 0
github/python/bert/extract_features.py,"__init__( self , unique_id , tokens , input_ids , input_mask , input_type_ids )", 6, 80, 2, 0
github/python/bert/extract_features.py,"input_fn_builder.input_fn( params )", 30, 79, 12, 0
github/python/bert/extract_features.py,"input_fn_builder( features , seq_length )", 46, 79, 12, 0
github/python/bert/extract_features.py,"model_fn_builder( bert_config , init_checkpoint , layer_indexes , use_tpu , use_one_hot_embeddings )", 2, 75, 0, 0
github/python/bert/extract_features.py,"model_fn.tpu_scaffold( )", 3, 71, 8, 0
github/python/bert/extract_features.py,"model_fn( features , labels , mode , params )", 54, 83, 2, 0
github/python/bert/extract_features.py,"convert_examples_to_features( examples , seq_length , tokenizer )", 90, 81, 6, 0
github/python/bert/extract_features.py,"_truncate_seq_pair( tokens_a , tokens_b , max_length )", 15, 80, 2, 0
github/python/bert/extract_features.py,"read_examples( input_file )", 22, 75, 10, 0
github/python/bert/extract_features.py,"main( _ )", 68, 79, 6, 0
github/python/bert/modeling.py,"__init__( self , vocab_size , hidden_size = 768 , num_hidden_layers = 12 , num_attention_heads = 12 , intermediate_size = 3072 , hidden_act = "gelu" , hidden_dropout_prob = 0 . 1 , attention_probs_dropout_prob = 0 . 1 , max_position_embeddings = 512 , type_vocab_size = 16 , initializer_range = 0 . 02 )", 12, 49, 15, 0
github/python/bert/modeling.py,"from_dict( cls , json_object )", 6, 76, 4, 0
github/python/bert/modeling.py,"from_json_file( cls , json_file )", 5, 68, 4, 0
github/python/bert/modeling.py,"to_dict( self )", 4, 59, 4, 0
github/python/bert/modeling.py,"to_json_string( self )", 3, 71, 4, 0
github/python/bert/modeling.py,"__init__( self , config , is_training , input_ids , input_mask = None , token_type_ids = None , use_one_hot_embeddings = True , scope = None )", 8, 44, 15, 0
github/python/bert/modeling.py,"get_pooled_output( self )", 2, 31, 2, 0
github/python/bert/modeling.py,"get_sequence_output( self )", 8, 80, 6, 0
github/python/bert/modeling.py,"get_all_encoder_layers( self )", 2, 36, 2, 0
github/python/bert/modeling.py,"get_embedding_output( self )", 10, 81, 6, 0
github/python/bert/modeling.py,"get_embedding_table( self )", 2, 33, 2, 0
github/python/bert/modeling.py,"gelu( input_tensor )", 14, 58, 2, 0
github/python/bert/modeling.py,"get_activation( activation_string )", 35, 80, 4, 0
github/python/bert/modeling.py,"get_assignment_map_from_checkpoint( tvars , init_checkpoint )", 25, 77, 2, 0
github/python/bert/modeling.py,"dropout( input_tensor , dropout_prob )", 16, 80, 4, 0
github/python/bert/modeling.py,"layer_norm( input_tensor , name = None )", 4, 81, 6, 0
github/python/bert/modeling.py,"layer_norm_and_dropout( input_tensor , dropout_prob , name = None )", 5, 67, 0, 0
github/python/bert/modeling.py,"create_initializer( initializer_range = 0 . 02 )", 3, 71, 2, 0
github/python/bert/modeling.py,"embedding_lookup( input_ids , vocab_size , embedding_size = 128 , initializer_range = 0 . 02 , word_embedding_name = "word_embeddings" , use_one_hot_embeddings = False )", 6, 60, 21, 0
github/python/bert/modeling.py,"embedding_postprocessor( input_tensor , use_token_type = False , token_type_ids = None , token_type_vocab_size = 16 , token_type_embedding_name = "token_type_embeddings" , use_position_embeddings = True , position_embedding_name = "position_embeddings" , initializer_range = 0 . 02 , max_position_embeddings = 512 , dropout_prob = 0 . 1 )", 10, 79, 28, 0
github/python/bert/modeling.py,"create_attention_mask_from_input_mask( from_tensor , to_mask )", 32, 79, 2, 0
github/python/bert/modeling.py,"attention_layer( from_tensor , to_tensor , attention_mask = None , num_attention_heads = 1 , size_per_head = 512 , query_act = None , key_act = None , value_act = None , attention_probs_dropout_prob = 0 . 0 , initializer_range = 0 . 02 , do_return_2d_tensor = False , batch_size = None , from_seq_length = None , to_seq_length = None )", 14, 54, 20, 0
github/python/bert/modeling.py,"transpose_for_scores( input_tensor , batch_size , num_attention_heads , seq_length , width )", 2, 74, 2, 0
github/python/bert/modeling.py,"transformer_model( input_tensor , attention_mask = None , hidden_size = 768 , num_hidden_layers = 12 , num_attention_heads = 12 , intermediate_size = 3072 , intermediate_act_fn = gelu , hidden_dropout_prob = 0 . 1 , attention_probs_dropout_prob = 0 . 1 , initializer_range = 0 . 02 , do_return_all_layers = False )", 11, 56, 22, 0
github/python/bert/modeling.py,"get_shape_list( tensor , expected_rank = None , name = None )", 35, 77, 4, 0
github/python/bert/modeling.py,"reshape_to_matrix( input_tensor )", 12, 76, 4, 0
github/python/bert/modeling.py,"reshape_from_matrix( output_tensor , orig_shape_list )", 11, 72, 2, 0
github/python/bert/modeling.py,"assert_rank( tensor , expected_rank , name = None )", 28, 80, 8, 0
github/python/bert/modeling_test.py,"__init__( self , parent , batch_size = 13 , seq_length = 7 , is_training = True , use_input_mask = True , use_token_type_ids = True , vocab_size = 99 , hidden_size = 32 , num_hidden_layers = 5 , num_attention_heads = 4 , intermediate_size = 37 , hidden_act = "gelu" , hidden_dropout_prob = 0 . 1 , attention_probs_dropout_prob = 0 . 1 , max_position_embeddings = 512 , type_vocab_size = 16 , initializer_range = 0 . 02 , scope = None )", 19, 51, 17, 0
github/python/bert/modeling_test.py,"create_model( self )", 42, 79, 6, 0
github/python/bert/modeling_test.py,"check_output( self , result )", 11, 70, 33, 0
github/python/bert/modeling_test.py,"test_default( self )", 2, 57, 4, 0
github/python/bert/modeling_test.py,"test_config_to_json_string( self )", 5, 64, 4, 0
github/python/bert/modeling_test.py,"run_tester( self , tester )", 10, 62, 6, 0
github/python/bert/modeling_test.py,"ids_tensor( cls , shape , vocab_size , rng = None , name = None )", 14, 77, 4, 0
github/python/bert/modeling_test.py,"assert_all_tensors_reachable( self , sess , outputs )", 30, 79, 4, 0
github/python/bert/modeling_test.py,"get_unreachable_ops( cls , graph , outputs )", 61, 79, 4, 0
github/python/bert/modeling_test.py,"flatten_recursive( cls , item )", 17, 75, 4, 0
github/python/bert/optimization.py,"create_optimizer( loss , init_lr , num_train_steps , num_warmup_steps , use_tpu )", 57, 81, 0, 0
github/python/bert/optimization.py,"__init__( self , learning_rate , weight_decay_rate = 0 . 0 , beta_1 = 0 . 9 , beta_2 = 0 . 999 , epsilon = 1e - 6 , exclude_from_weight_decay = None , name = "AdamWeightDecayOptimizer" )", 8, 49, 15, 0
github/python/bert/optimization.py,"apply_gradients( self , grads_and_vars , global_step = None , name = None )", 50, 79, 6, 0
github/python/bert/optimization.py,"_do_use_weight_decay( self , param_name )", 9, 59, 4, 0
github/python/bert/optimization.py,"_get_variable_name( self , param_name )", 6, 54, 4, 0
github/python/bert/optimization_test.py,"test_adam( self )", 20, 77, 6, 0
github/python/bert/run_classifier.py,"__init__( self , guid , text_a , text_b = None , label = None )", 16, 78, 6, 0
github/python/bert/run_classifier.py,"__init__( self , input_ids , input_mask , segment_ids , label_id )", 5, 68, 2, 0
github/python/bert/run_classifier.py,"get_train_examples( self , data_dir )", 3, 66, 4, 0
github/python/bert/run_classifier.py,"get_dev_examples( self , data_dir )", 3, 64, 4, 0
github/python/bert/run_classifier.py,"get_test_examples( self , data_dir )", 3, 63, 4, 0
github/python/bert/run_classifier.py,"get_labels( self )", 3, 53, 4, 0
github/python/bert/run_classifier.py,"_read_tsv( cls , input_file , quotechar = None )", 8, 66, 6, 0
github/python/bert/run_classifier.py,"__init__( self )", 2, 25, 4, 0
github/python/bert/run_classifier.py,"get_train_examples( self , data_dir )", 18, 78, 10, 0
github/python/bert/run_classifier.py,"get_dev_examples( self , data_dir )", 17, 78, 10, 0
github/python/bert/run_classifier.py,"get_labels( self )", 3, 54, 4, 0
github/python/bert/run_classifier.py,"get_train_examples( self , data_dir )", 4, 70, 8, 0
github/python/bert/run_classifier.py,"get_dev_examples( self , data_dir )", 5, 67, 8, 0
github/python/bert/run_classifier.py,"get_test_examples( self , data_dir )", 4, 76, 8, 0
github/python/bert/run_classifier.py,"get_labels( self )", 3, 54, 4, 0
github/python/bert/run_classifier.py,"_create_examples( self , lines , set_type )", 16, 78, 10, 0
github/python/bert/run_classifier.py,"get_train_examples( self , data_dir )", 4, 70, 8, 0
github/python/bert/run_classifier.py,"get_dev_examples( self , data_dir )", 4, 66, 8, 0
github/python/bert/run_classifier.py,"get_test_examples( self , data_dir )", 4, 68, 8, 0
github/python/bert/run_classifier.py,"get_labels( self )", 3, 26, 4, 0
github/python/bert/run_classifier.py,"_create_examples( self , lines , set_type )", 16, 78, 10, 0
github/python/bert/run_classifier.py,"get_train_examples( self , data_dir )", 4, 70, 8, 0
github/python/bert/run_classifier.py,"get_dev_examples( self , data_dir )", 4, 66, 8, 0
github/python/bert/run_classifier.py,"get_test_examples( self , data_dir )", 4, 68, 8, 0
github/python/bert/run_classifier.py,"get_labels( self )", 3, 26, 4, 0
github/python/bert/run_classifier.py,"_create_examples( self , lines , set_type )", 17, 76, 10, 0
github/python/bert/run_classifier.py,"convert_single_example( ex_index , example , label_list , max_seq_length , tokenizer )", 2, 74, 0, 0
github/python/bert/run_classifier.py,"file_based_convert_examples_to_features( examples , label_list , max_seq_length , tokenizer , output_file )", 2, 67, 4, 0
github/python/bert/run_classifier.py,"create_int_feature( values )", 3, 78, 6, 0
github/python/bert/run_classifier.py,"file_based_input_fn_builder( input_file , seq_length , is_training , drop_remainder )", 2, 69, 0, 0
github/python/bert/run_classifier.py,"_decode_record( record , name_to_features )", 13, 77, 4, 0
github/python/bert/run_classifier.py,"input_fn( params )", 18, 74, 4, 0
github/python/bert/run_classifier.py,"_truncate_seq_pair( tokens_a , tokens_b , max_length )", 15, 80, 2, 0
github/python/bert/run_classifier.py,"create_model( bert_config , is_training , input_ids , input_mask , segment_ids , labels , num_labels , use_one_hot_embeddings )", 2, 79, 0, 0
github/python/bert/run_classifier.py,"model_fn_builder( bert_config , num_labels , init_checkpoint , learning_rate , num_train_steps , num_warmup_steps , use_tpu , use_one_hot_embeddings )", 3, 78, 0, 0
github/python/bert/run_classifier.py,"model_fn.tpu_scaffold( )", 3, 73, 10, 0
github/python/bert/run_classifier.py,"model_fn.metric_fn( per_example_loss , label_ids , logits )", 8, 71, 8, 0
github/python/bert/run_classifier.py,"model_fn( features , labels , mode , params )", 74, 83, 2, 0
github/python/bert/run_classifier.py,"input_fn_builder.input_fn( params )", 34, 78, 12, 0
github/python/bert/run_classifier.py,"input_fn_builder( features , seq_length , is_training , drop_remainder )", 50, 78, 12, 0
github/python/bert/run_classifier.py,"convert_examples_to_features( examples , label_list , max_seq_length , tokenizer )", 2, 71, 0, 0
github/python/bert/run_classifier.py,"main( _ )", 163, 81, 8, 0
github/python/bert/run_pretraining.py,"model_fn_builder( bert_config , init_checkpoint , learning_rate , num_train_steps , num_warmup_steps , use_tpu , use_one_hot_embeddings )", 3, 66, 0, 0
github/python/bert/run_pretraining.py,"model_fn.tpu_scaffold( )", 3, 73, 10, 0
github/python/bert/run_pretraining.py,"model_fn.metric_fn( masked_lm_example_loss , masked_lm_log_probs , masked_lm_ids , masked_lm_weights , next_sentence_example_loss , next_sentence_log_probs , next_sentence_labels )", 3, 80, 6, 0
github/python/bert/run_pretraining.py,"model_fn( features , labels , mode , params )", 122, 83, 2, 0
github/python/bert/run_pretraining.py,"get_masked_lm_output( bert_config , input_tensor , output_weights , positions , label_ids , label_weights )", 2, 79, 0, 0
github/python/bert/run_pretraining.py,"get_next_sentence_output( bert_config , input_tensor , labels )", 21, 80, 8, 0
github/python/bert/run_pretraining.py,"gather_indexes( sequence_tensor , positions )", 14, 77, 2, 0
github/python/bert/run_pretraining.py,"input_fn_builder( input_files , max_seq_length , max_predictions_per_seq , is_training , num_cpu_threads = 4 )", 5, 46, 21, 0
github/python/bert/run_pretraining.py,"input_fn( params )", 56, 81, 6, 0
github/python/bert/run_pretraining.py,"_decode_record( record , name_to_features )", 13, 75, 2, 0
github/python/bert/run_pretraining.py,"main( _ )", 81, 78, 4, 0
github/python/bert/run_squad.py,"__init__( self , qas_id , question_text , doc_tokens , orig_answer_text = None , start_position = None , end_position = None , is_impossible = False )", 8, 38, 15, 0
github/python/bert/run_squad.py,"__str__( self )", 2, 27, 4, 0
github/python/bert/run_squad.py,"__repr__( self )", 13, 67, 4, 0
github/python/bert/run_squad.py,"__init__( self , unique_id , example_index , doc_span_index , tokens , token_to_orig_map , token_is_max_context , input_ids , input_mask , segment_ids , start_position = None , end_position = None , is_impossible = None )", 13, 37, 15, 0
github/python/bert/run_squad.py,"read_squad_examples.is_whitespace( c )", 4, 76, 4, 0
github/python/bert/run_squad.py,"read_squad_examples( input_file , is_training )", 80, 80, 12, 0
github/python/bert/run_squad.py,"convert_examples_to_features( examples , tokenizer , max_seq_length , doc_stride , max_query_length , is_training , output_fn )", 3, 76, 33, 0
github/python/bert/run_squad.py,"_improve_answer_span( doc_tokens , input_start , input_end , tokenizer , orig_answer_text )", 2, 72, 0, 0
github/python/bert/run_squad.py,"_check_is_max_context( doc_spans , cur_span_index , position )", 35, 80, 2, 0
github/python/bert/run_squad.py,"create_model( bert_config , is_training , input_ids , input_mask , segment_ids , use_one_hot_embeddings )", 2, 79, 0, 0
github/python/bert/run_squad.py,"model_fn_builder( bert_config , init_checkpoint , learning_rate , num_train_steps , num_warmup_steps , use_tpu , use_one_hot_embeddings )", 3, 66, 0, 0
github/python/bert/run_squad.py,"model_fn.tpu_scaffold( )", 3, 73, 10, 0
github/python/bert/run_squad.py,"model_fn.compute_loss( logits , positions )", 7, 67, 12, 0
github/python/bert/run_squad.py,"model_fn( features , labels , mode , params )", 88, 83, 2, 0
github/python/bert/run_squad.py,"input_fn_builder._decode_record( record , name_to_features )", 13, 77, 4, 0
github/python/bert/run_squad.py,"input_fn_builder.input_fn( params )", 18, 74, 4, 0
github/python/bert/run_squad.py,"input_fn_builder( input_file , seq_length , is_training , drop_remainder )", 48, 77, 4, 0
github/python/bert/run_squad.py,"write_predictions( all_examples , all_features , all_results , n_best_size , max_answer_length , do_lower_case , output_prediction_file , output_nbest_file , output_null_log_odds_file )", 3, 80, 22, 0
github/python/bert/run_squad.py,"get_final_text._strip_spaces( text )", 10, 44, 4, 0
github/python/bert/run_squad.py,"get_final_text( pred_text , orig_text , do_lower_case )", 94, 80, 2, 0
github/python/bert/run_squad.py,"_get_best_indexes( logits , n_best_size )", 10, 80, 2, 0
github/python/bert/run_squad.py,"_compute_softmax( scores )", 21, 53, 2, 0
github/python/bert/run_squad.py,"__init__( self , filename , is_training )", 5, 57, 4, 0
github/python/bert/run_squad.py,"process_feature.create_int_feature( values )", 4, 61, 10, 0
github/python/bert/run_squad.py,"process_feature( self , feature )", 25, 81, 6, 0
github/python/bert/run_squad.py,"close( self )", 2, 25, 4, 0
github/python/bert/run_squad.py,"validate_flags_or_throw( bert_config )", 24, 81, 4, 0
github/python/bert/run_squad.py,"main.append_feature( feature )", 3, 43, 6, 0
github/python/bert/run_squad.py,"main( _ )", 151, 81, 4, 0
github/python/bert/tokenization.py,"convert_to_unicode( text )", 18, 80, 2, 0
github/python/bert/tokenization.py,"printable_text( text )", 21, 77, 2, 0
github/python/bert/tokenization.py,"load_vocab( vocab_file )", 13, 52, 6, 0
github/python/bert/tokenization.py,"convert_by_vocab( vocab , items )", 6, 61, 2, 0
github/python/bert/tokenization.py,"convert_tokens_to_ids( vocab , tokens )", 2, 42, 0, 0
github/python/bert/tokenization.py,"convert_ids_to_tokens( inv_vocab , ids )", 2, 43, 0, 0
github/python/bert/tokenization.py,"whitespace_tokenize( text )", 7, 73, 2, 0
github/python/bert/tokenization.py,"__init__( self , vocab_file , do_lower_case = True )", 5, 71, 4, 0
github/python/bert/tokenization.py,"tokenize( self , text )", 7, 65, 6, 0
github/python/bert/tokenization.py,"convert_tokens_to_ids( self , tokens )", 2, 48, 4, 0
github/python/bert/tokenization.py,"convert_ids_to_tokens( self , ids )", 2, 49, 4, 0
github/python/bert/tokenization.py,"__init__( self , do_lower_case = True )", 7, 54, 6, 0
github/python/bert/tokenization.py,"tokenize( self , text )", 23, 77, 4, 0
github/python/bert/tokenization.py,"_run_strip_accents( self , text )", 10, 47, 4, 0
github/python/bert/tokenization.py,"_run_split_on_punc( self , text )", 19, 49, 4, 0
github/python/bert/tokenization.py,"_tokenize_chinese_chars( self , text )", 12, 52, 4, 0
github/python/bert/tokenization.py,"_is_chinese_char( self , cp )", 21, 81, 4, 0
github/python/bert/tokenization.py,"_clean_text( self , text )", 12, 77, 4, 0
github/python/bert/tokenization.py,"__init__( self , vocab , unk_token = "[UNK]" , max_input_chars_per_word = 200 )", 4, 78, 2, 0
github/python/bert/tokenization.py,"tokenize( self , text )", 52, 77, 4, 0
github/python/bert/tokenization.py,"_is_whitespace( char )", 10, 72, 2, 0
github/python/bert/tokenization.py,"_is_control( char )", 10, 77, 2, 0
github/python/bert/tokenization.py,"_is_punctuation( char )", 14, 68, 2, 0
github/python/bert/tokenization_test.py,"test_full_tokenizer( self )", 18, 81, 8, 0
github/python/bert/tokenization_test.py,"test_chinese( self )", 6, 49, 8, 0
github/python/bert/tokenization_test.py,"test_basic_tokenizer_lower( self )", 7, 70, 4, 0
github/python/bert/tokenization_test.py,"test_basic_tokenizer_no_lower( self )", 6, 65, 4, 0
github/python/bert/tokenization_test.py,"test_wordpiece_tokenizer( self )", 19, 81, 8, 0
github/python/bert/tokenization_test.py,"test_convert_tokens_to_ids( self )", 13, 81, 8, 0
github/python/bert/tokenization_test.py,"test_is_whitespace( self )", 9, 60, 4, 0
github/python/bert/tokenization_test.py,"test_is_control( self )", 7, 57, 4, 0
github/python/bert/tokenization_test.py,"test_is_punctuation( self )", 8, 57, 4, 0
